Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Exploring prosody in
interaction control
Jens Edlund & Mattias Heldner
KTH Department of Speech, Music and Hearing

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

1

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Progress in Experimental Phonology:
From Communicative Function to Phonetic
Substance and Vice Versa
Special Issue: Phonetica 2005,
Vol. 62, No. 2-4

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

2

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Abstract
In this seminar we present an investigation of prosodic aspects of
turn-taking in conversation with a view to improving the efficiency
of identifying relevant places at which a machine can legitimately
begin to talk to a human interlocutor.
We examine the relationship between interaction control, the
communicative function of which is to regulate the flow of
information between interlocutors, and its phonetic manifestation.
Specifically, the listener’s perception of such interaction control
phenomena is modelled. Algorithms for automatic online extraction
of prosodic phenomena liable to be relevant for interaction control,
such as silent pauses and intonation patterns, are presented and
evaluated in experiments using Swedish Map Task data.
We show that the automatically extracted prosodic features can be
used to avoid many of the places where current dialogue systems
run the risk of interrupting their users, and also to identify suitable
places to take the turn.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

3

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Outline
•

Motivation
– improving spoken dialogue systems

•

The relationship between
– interaction control
– its manifestation in the conversation

•

/nailon/
– automatic online extraction of prosodic phenomena

•
•

Evaluation using Swedish Map Task data
Automatically extracted prosodic features helps
– avoiding many places where current dialogue systems risk
interrupting their users
– identifying suitable places to take the turn

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

4

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Projects
•
•
•
•

GROG
AdApt
CHIL
Higgins

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

5

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Goal

Investigate conversation with a view to
improving the efficiency of identifying
relevant places at which a machine can
legitimately begin to talk to a human
interlocutor.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

6

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Conversation & interaction
control
Conversation
Interaction
control
•
•
•
•

turntaking
turn keeping
turn yielding
etc.

Common
ground
• feeedback
• backchannels
• etc.

•
•
•
•

grounding
error handling
intitiative
etc.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

7

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Interaction control
•

•

•

Regulate the flow of information between interlocutors
(speakers and listeners) to make it proceed smoothly
and efficiently
Collaborative effort where interlocutors continuously
monitor various aspects of each other’s behaviour in order
to make decisions about turn-taking and feedback
Interaction control includes, for example,
–
–
–
–

what the speaker does to keep the floor, i.e. turn-keeping
or to hand over the floor, i.e. turn-yielding
how the listener finds suitable places to take the floor
or to give feedback to the speaker

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

8

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Example
A: och då är jag väldigt nära den där kust<ehm>remsan och
utanför min kustremsa så ligg finns det sjöhästar
B: mm dom har jag också
A: ja och precis <> ja strax söder om dom där sjöhästarna så når
min bana nästan ända fram till kust<>linjen där
B: okej
A: och sen så fortsätter vi ned och gör en mjuk in<>buktning åt
österut och rundar en stor förskräcklig fågel där
B: mm jag har en förskräcklig fågel också men jag undrar om det är
samma för min fågel är en bit norrut
A: ja
B: min fågel är i nedre högra hörnet av en <> av den nordligaste
bukten på västra sidan av ön
A: ja just det
B: okej
A: mm
Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

9

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Features relevant for
interaction control
•

Auditory
–
–
–
–

•

Silent pauses
Intonation patterns
Creaky voice
Vocal tract configuration (open/closed)

Visual
– Nods
– Glances
– Mimicry Gestures

•

Structural (in)completeness
– Semantic
– Pragmatic
– Syntactic

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

10

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Current dialogue systems

• Use silence
• VAD, SAD, EOU, EOS, EPD...
– ...are all basically silence duration thresholds
– 500ms – 2000ms (sic!)

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

11

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Human speakers

• Frequently pause before they are finished
• These pauses are often longer than 2000 ms
• For example when hesitating

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

12

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Humans talking to dialogue
systems
• Will get long response times, but...
• ...will also run the risk of being interrupted
• Limits to in-speech pause length???
– Can we increase the 2000ms further to eliminate
system interruptions?
– Other methods?

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

13

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Features relevant for
interaction control
•

Auditory
–
–
–
–

•

Silent pauses
Intonation patterns
Creaky voice
Vocal tract configuration (open/closed)

Visual
– Nods
– Glances
– Mimicry Gestures

•

Structural (in)completeness
– Semantic
– Pragmatic
– Syntactic

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

14

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Method: outline
•
•
•
•
•
•

•

Map task dialogues
Segmentation into pause bounded units – IPUs
Classification of IPUs into speaker changes and speaker
holds
Prosodic features extracted from the region immediately
before the IPU boundary using /nailon/
Turn-taking decisions using prosodic features
Evaluation of turn-taking decisions with respect to the
speaker change vs. speaker hold classification
Additional mark-up and analyses
– Perceptual judgments as to whether the IPUs were finished
or not on a 5-point scale by three judges
– Extraction of intercontribution intervals (ICIs)

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

15

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speech material
•

Map task dialogues
–
–
–
–

•

Designed to elicit natural-sounding spontaneous dialogues
Instruction giver & instruction follower
One map each, but maps not identical
Instruction giver describe a route to the follower

Swedish map task dialogues
– 4 dialogues: two pairs of speakers (each speaker acted as
giver once and as follower once)
– 1100 dialogue contributions (incl. feedback/backchannels)
– Near perfect separation of giver and follower channels
– Total duration 1 hour
– Many thanks to Pétur Helgason at Stockholm University!

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

16

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

IPU segmentation
•
•

•

•

Interpausal units (IPUs)
Giver and follower channels automatically segmented
into speech vs. silence using a basic speech activity
detector (SAD) with some smoothing
IPU = Transition from speech to long enough silence
(>300 ms) in giver channel with no overlapping speech
in follower channel
Inter contribution intervals (ICIs) = actual duration of
silent pauses between IPUs extracted automatically

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

17

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speaker change vs. speaker
hold classification
•
•

Each IPU classified as either speaker change or
speaker hold automatically
Speaker change = speech in the giver channel followed
by at least 300 ms silence in the same channel, and nonoverlapping speech in the follower channel
– Minimum inter contribution interval (ICI) in a speaker
change is 10 ms

•

Speaker hold = speech in the giver channel followed by
at least 300 ms silence in the same channel, and then
more speech in the giver channel
– Minimum inter contribution interval (ICI) in a speaker hold
is 300 ms

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

18

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speaker change illustration
Giver
channel:

[...] Speech

Long enough silent
pause [...]
ICI

Follower
channel:

[...] Long enough silent
pause

Speech [...]

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

19

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speaker change vs. speaker
hold used as gold standard
•

Shows the actual turn of events in the dialogue
– Is a direct reflection of the interlocutors’ behaviour
– Ensures that speaker changes and speaker holds were
perceived as such by the interlocutors

•

Does not show how things must be by necessity!
– A speaker hold may be a suitable place to give a
contribution exept one where the other simply refrained
from saying something
– A speaker change may be an unsuitable place to give a
contribution if the speaker was interrupted

•

Makes no distinction between ‘turns’ and ‘backchannels’
– An appropriate place for a backchannel may not be
appropriate for any other contributions than backchannels

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

20

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Slight detour...

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

21

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Is silence only a problem?
• 52.3% of all IPUs were speaker holds
– Some of these may have been potential places for
speaker changes

• 36.7% of all IPUs were judged as unfinished by
the human judges
– These are the cases where a dialogue system using
silence only runs the risk of interrupting its users

• Silence only is a substantial problem!

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

22

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Would increasing the silence
thresholds help?
Silence
threshold

>300 ms

>500 ms

>1000 ms

>1500 ms

>2000 ms

Number of
IPUs

634

441

168

69

22

Speaker
holds

68%

71%

68%

67%

55%

•

Less than 50% of the ICIs were longer than 500 ms
–Waiting for 500 ms (or more) is not the way humans does it

•
•

The percentage of speaker holds virtually unchanged with longer
ICIs
Increasing silence thresholds leads to sluggish behaviour, only!

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

23

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Prosodic analysis
Investigate conversation
with a view to improving
the efficiency of
identifying relevant places
at which a machine can
legitimately begin to talk
to a human interlocutor.

• online
– no right context

• realtime
– responsive
– predictable

• general
– any speaker
– any domain

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

24

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/

•
•
•
•

Software for analysis of prosodic features
Scripting in Tcl/Tk
Based on Kåre Sjölander’s Snack
ESPS F0 extraction

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

25

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - flow

audio
acquisition

voice filtering

voice,
pitch,
intensity
extraction

optional
preprocessing
(filters)

intensity and
pitch
normalisation

silence
detection

quasisyllabification

categorisation

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

26

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - audio acquisition
•
•
•
•

Snack sound object
Fixed size moving window
Frees memory continously
Small footprint

• Optional pre-processing
– e.g. Snack filters

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

27

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - voice, pitch,
intensity extraction
• Basic extraction by Snack/ESPS but
• Incremental:
– repeated over fixed size moving window
– last frame only
– outputs realtime data stream (latency ≈ frame length)
pitch

122

126

135

132

133

130

frames

time
now - 10 frames

window

now

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

28

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - voice, pitch,
intensity
• Pitch semitone transform (optional)
• Intensity dB transform (optional)
• Sanity checks
– Pitch octave errors
– Intensity spikes

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

29

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - voice decision
• Currently only interested in voiced segments
• Robust ”non-flimsy” voice decision
– Cost: a small latency
– Requires a certain number of frames to be judged the
same for a change to take place
– 3 frames, for example, introduces 3/framerate
seconds of latency
Voice
decision

unvoiced

unvoiced

unvoiced

voiced

unvoiced

voiced

voiced

frames
latency

time
now - 10 frames

now

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

30

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - normalisation
• Absolute pitch and intensity numbers
fluctuate too much
• Profiles introduce prerequisites:
– speaker identity
– acoustics
– channel

• Online incremental calculation of mean and
stddev
– stabilises surprisingly quickly (< 30s speech
needed)
– degeneration can be used to ensure flexibility
(not currently implemented)

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

31

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - F0 example

• Cumulative mean ±2 standard deviations based on
semitone transformed F0 data
• Voiced sequences only
• Stabilises after about 20 seconds
• High, mid and low registers

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

32

Exploring prosody in interaction control

/nailon/ -

Jens Edlund & Mattias Heldner

silence detection

• Based on threshold of sequential silence
–Intensity threshold re-calculated continuously
–Intensity threshold currently the valley
following the first peak in an intensity histogram

–Current duration threshold at 300ms of silence

• Reports what frame the silence begun

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

33

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

quasisyllabification

/nailon/ -

• Convex hulls (Mermelstein)
• Based on normalised intensity
– Voiced sequences only
– Incremental search for next complete hull
– Remebers last seen hull only

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

34

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - intonation pattern
classification
• (Currently simplistic) classification of
intonation over one convex hull (quasisyllable) into either
– Low or low and falling
– Mid and level
– Other

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

35

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Turntaking decisions
• Low or low and falling intonation patterns taken to
indicate suitable places for turn-taking turnyielding
• Mid and level intonation patterns indicate unsuitable
places turn-keeping
• Other intonation patterns may indicate turn-keeping
as well as turn-yeilding and were therefore classified
as garbage don’t know here

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

36

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Does prosody help compared
to silence (>300 ms) only?
Turn-keeping

Don’t know

Turn-yielding

Total

23

212

158

393

Hold

105

255

71

431

Total

128

467

229

824

Change

•
•
•
•
•

28% turn-yielding, 16% turn-keeping, 56% don’t know
Speaker changes were in the majority for turn-yielding (69%)
Speaker holds were in the majority for turn-keeping (82%)
Unobtrusive system (pooling turn-keeping & don’t know): identifies 41%
of the suitable places for turn-taking; avoids 84% of the impossible ones
Responsive system (pooling turn-yielding & don’t know): identifies 94% of
the suitable places for turn-taking; avoids 24% of the unsuitable ones

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

37

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Responsivity vs.
unobtrusiveness
•
•
•

All dialogue systems need some kind of interaction control
capabilities
More human-like systems will require more human-like
interaction control
Different dialogue situations put different demands on the
interaction control capabilities

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

38

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

KTH Connector example
S
C
S
C
S
C
S
C
S
S
P
S
S
C

This is the KTH Connector, how may I help you?
Hi, this is Mattias.
Hello Mattias
I was supposed to be at the meeting, but I’m stuck on a train.
Mhm
Could you check with the others if it’s ok to patch me in as a listener?
Ok, please hold for a minute.
Ok.
[The KTH Connector waits for a suitable place to notify the meeting]
Mattias is on the phone and will not be able to make it to the meeting. Is it ok
to patch him in as a listener?
Sure, but give him a speech channel as well.
Ok, I’ll let you know when it’s done.
Ok, Mattias, I’ll patch you in. They suggested I’d give you a speech channel as
well. Do you want one?
No, it’s too noisy on the train. Listening will do.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

39

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Responsivity vs.
Unobtrusiveness (cont.)
•

•

In dialogues with one listener only, responsiveness may
be a key concern for the system, as sluggishness is likely
to annoy the user
Sometimes unobtrusiveness is the most important
concern, for example when establishing a new connection
for notifying the participant in a meeting

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

40

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Future work
Better internal models of interaction control
• Further develoment of /nailon/
– Adding features, e.g. distinguishing open vs closed vocal
tract...
– Machine learning of categorisation

•

Combination with other sources of knowledge
– Semantic completeness

Better models that relate interaction control to the bigger
picture – conversation
• Relation to grounding, error handling
– Utterance concept particularly difficult as regards e.g.
backchannels

•

Initiative

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

41

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Perceptual judgments vs
speaker change/speaker hold
1
2
3
4
definitely
probably
could be probably
unfinished unfinished finished or finished
unfinished

5
definitely
finished

Total

Speaker
change

169

66

40

200

705

1180

Speaker
hold

806

91

25

112

240

1274

Total

975

157

65

312

945

2454

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

42

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Exploring prosody in
interaction control
Jens Edlund & Mattias Heldner
KTH Department of Speech, Music and Hearing

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

1

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Progress in Experimental Phonology:
From Communicative Function to Phonetic
Substance and Vice Versa
Special Issue: Phonetica 2005,
Vol. 62, No. 2-4

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

2

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Abstract
In this seminar we present an investigation of prosodic aspects of
turn-taking in conversation with a view to improving the efficiency
of identifying relevant places at which a machine can legitimately
begin to talk to a human interlocutor.
We examine the relationship between interaction control, the
communicative function of which is to regulate the flow of
information between interlocutors, and its phonetic manifestation.
Specifically, the listener’s perception of such interaction control
phenomena is modelled. Algorithms for automatic online extraction
of prosodic phenomena liable to be relevant for interaction control,
such as silent pauses and intonation patterns, are presented and
evaluated in experiments using Swedish Map Task data.
We show that the automatically extracted prosodic features can be
used to avoid many of the places where current dialogue systems
run the risk of interrupting their users, and also to identify suitable
places to take the turn.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

3

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Outline
•

Motivation
– improving spoken dialogue systems

•

The relationship between
– interaction control
– its manifestation in the conversation

•

/nailon/
– automatic online extraction of prosodic phenomena

•
•

Evaluation using Swedish Map Task data
Automatically extracted prosodic features helps
– avoiding many places where current dialogue systems risk
interrupting their users
– identifying suitable places to take the turn

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

4

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Projects
•
•
•
•

GROG
AdApt
CHIL
Higgins

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

5

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Goal

Investigate conversation with a view to
improving the efficiency of identifying
relevant places at which a machine can
legitimately begin to talk to a human
interlocutor.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

6

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Conversation & interaction
control
Conversation
Interaction
control
•
•
•
•

turntaking
turn keeping
turn yielding
etc.

Common
ground
• feeedback
• backchannels
• etc.

•
•
•
•

grounding
error handling
intitiative
etc.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

7

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Interaction control
•

•

•

Regulate the flow of information between interlocutors
(speakers and listeners) to make it proceed smoothly
and efficiently
Collaborative effort where interlocutors continuously
monitor various aspects of each other’s behaviour in order
to make decisions about turn-taking and feedback
Interaction control includes, for example,
–
–
–
–

what the speaker does to keep the floor, i.e. turn-keeping
or to hand over the floor, i.e. turn-yielding
how the listener finds suitable places to take the floor
or to give feedback to the speaker

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

8

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Example
A: och då är jag väldigt nära den där kust<ehm>remsan och
utanför min kustremsa så ligg finns det sjöhästar
B: mm dom har jag också
A: ja och precis <> ja strax söder om dom där sjöhästarna så når
min bana nästan ända fram till kust<>linjen där
B: okej
A: och sen så fortsätter vi ned och gör en mjuk in<>buktning åt
österut och rundar en stor förskräcklig fågel där
B: mm jag har en förskräcklig fågel också men jag undrar om det är
samma för min fågel är en bit norrut
A: ja
B: min fågel är i nedre högra hörnet av en <> av den nordligaste
bukten på västra sidan av ön
A: ja just det
B: okej
A: mm
Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

9

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Features relevant for
interaction control
•

Auditory
–
–
–
–

•

Silent pauses
Intonation patterns
Creaky voice
Vocal tract configuration (open/closed)

Visual
– Nods
– Glances
– Mimicry Gestures

•

Structural (in)completeness
– Semantic
– Pragmatic
– Syntactic

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

10

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Current dialogue systems

• Use silence
• VAD, SAD, EOU, EOS, EPD...
– ...are all basically silence duration thresholds
– 500ms – 2000ms (sic!)

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

11

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Human speakers

• Frequently pause before they are finished
• These pauses are often longer than 2000 ms
• For example when hesitating

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

12

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Humans talking to dialogue
systems
• Will get long response times, but...
• ...will also run the risk of being interrupted
• Limits to in-speech pause length???
– Can we increase the 2000ms further to eliminate
system interruptions?
– Other methods?

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

13

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Features relevant for
interaction control
•

Auditory
–
–
–
–

•

Silent pauses
Intonation patterns
Creaky voice
Vocal tract configuration (open/closed)

Visual
– Nods
– Glances
– Mimicry Gestures

•

Structural (in)completeness
– Semantic
– Pragmatic
– Syntactic

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

14

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Method: outline
•
•
•
•
•
•

•

Map task dialogues
Segmentation into pause bounded units – IPUs
Classification of IPUs into speaker changes and speaker
holds
Prosodic features extracted from the region immediately
before the IPU boundary using /nailon/
Turn-taking decisions using prosodic features
Evaluation of turn-taking decisions with respect to the
speaker change vs. speaker hold classification
Additional mark-up and analyses
– Perceptual judgments as to whether the IPUs were finished
or not on a 5-point scale by three judges
– Extraction of intercontribution intervals (ICIs)

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

15

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speech material
•

Map task dialogues
–
–
–
–

•

Designed to elicit natural-sounding spontaneous dialogues
Instruction giver & instruction follower
One map each, but maps not identical
Instruction giver describe a route to the follower

Swedish map task dialogues
– 4 dialogues: two pairs of speakers (each speaker acted as
giver once and as follower once)
– 1100 dialogue contributions (incl. feedback/backchannels)
– Near perfect separation of giver and follower channels
– Total duration 1 hour
– Many thanks to Pétur Helgason at Stockholm University!

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

16

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

IPU segmentation
•
•

•

•

Interpausal units (IPUs)
Giver and follower channels automatically segmented
into speech vs. silence using a basic speech activity
detector (SAD) with some smoothing
IPU = Transition from speech to long enough silence
(>300 ms) in giver channel with no overlapping speech
in follower channel
Inter contribution intervals (ICIs) = actual duration of
silent pauses between IPUs extracted automatically

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

17

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speaker change vs. speaker
hold classification
•
•

Each IPU classified as either speaker change or
speaker hold automatically
Speaker change = speech in the giver channel followed
by at least 300 ms silence in the same channel, and nonoverlapping speech in the follower channel
– Minimum inter contribution interval (ICI) in a speaker
change is 10 ms

•

Speaker hold = speech in the giver channel followed by
at least 300 ms silence in the same channel, and then
more speech in the giver channel
– Minimum inter contribution interval (ICI) in a speaker hold
is 300 ms

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

18

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speaker change illustration
Giver
channel:

[...] Speech

Long enough silent
pause [...]
ICI

Follower
channel:

[...] Long enough silent
pause

Speech [...]

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

19

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Speaker change vs. speaker
hold used as gold standard
•

Shows the actual turn of events in the dialogue
– Is a direct reflection of the interlocutors’ behaviour
– Ensures that speaker changes and speaker holds were
perceived as such by the interlocutors

•

Does not show how things must be by necessity!
– A speaker hold may be a suitable place to give a
contribution exept one where the other simply refrained
from saying something
– A speaker change may be an unsuitable place to give a
contribution if the speaker was interrupted

•

Makes no distinction between ‘turns’ and ‘backchannels’
– An appropriate place for a backchannel may not be
appropriate for any other contributions than backchannels

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

20

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Slight detour...

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

21

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Is silence only a problem?
• 52.3% of all IPUs were speaker holds
– Some of these may have been potential places for
speaker changes

• 36.7% of all IPUs were judged as unfinished by
the human judges
– These are the cases where a dialogue system using
silence only runs the risk of interrupting its users

• Silence only is a substantial problem!

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

22

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Would increasing the silence
thresholds help?
Silence
threshold

>300 ms

>500 ms

>1000 ms

>1500 ms

>2000 ms

Number of
IPUs

634

441

168

69

22

Speaker
holds

68%

71%

68%

67%

55%

•

Less than 50% of the ICIs were longer than 500 ms
–Waiting for 500 ms (or more) is not the way humans does it

•
•

The percentage of speaker holds virtually unchanged with longer
ICIs
Increasing silence thresholds leads to sluggish behaviour, only!

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

23

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Prosodic analysis
Investigate conversation
with a view to improving
the efficiency of
identifying relevant places
at which a machine can
legitimately begin to talk
to a human interlocutor.

• online
– no right context

• realtime
– responsive
– predictable

• general
– any speaker
– any domain

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

24

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/

•
•
•
•

Software for analysis of prosodic features
Scripting in Tcl/Tk
Based on Kåre Sjölander’s Snack
ESPS F0 extraction

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

25

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - flow

audio
acquisition

voice filtering

voice,
pitch,
intensity
extraction

optional
preprocessing
(filters)

intensity and
pitch
normalisation

silence
detection

quasisyllabification

categorisation

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

26

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - audio acquisition
•
•
•
•

Snack sound object
Fixed size moving window
Frees memory continously
Small footprint

• Optional pre-processing
– e.g. Snack filters

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

27

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - voice, pitch,
intensity extraction
• Basic extraction by Snack/ESPS but
• Incremental:
– repeated over fixed size moving window
– last frame only
– outputs realtime data stream (latency ≈ frame length)
pitch

122

126

135

132

133

130

frames

time
now - 10 frames

window

now

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

28

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - voice, pitch,
intensity
• Pitch semitone transform (optional)
• Intensity dB transform (optional)
• Sanity checks
– Pitch octave errors
– Intensity spikes

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

29

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - voice decision
• Currently only interested in voiced segments
• Robust ”non-flimsy” voice decision
– Cost: a small latency
– Requires a certain number of frames to be judged the
same for a change to take place
– 3 frames, for example, introduces 3/framerate
seconds of latency
Voice
decision

unvoiced

unvoiced

unvoiced

voiced

unvoiced

voiced

voiced

frames
latency

time
now - 10 frames

now

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

30

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - normalisation
• Absolute pitch and intensity numbers
fluctuate too much
• Profiles introduce prerequisites:
– speaker identity
– acoustics
– channel

• Online incremental calculation of mean and
stddev
– stabilises surprisingly quickly (< 30s speech
needed)
– degeneration can be used to ensure flexibility
(not currently implemented)

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

31

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - F0 example

• Cumulative mean ±2 standard deviations based on
semitone transformed F0 data
• Voiced sequences only
• Stabilises after about 20 seconds
• High, mid and low registers

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

32

Exploring prosody in interaction control

/nailon/ -

Jens Edlund & Mattias Heldner

silence detection

• Based on threshold of sequential silence
–Intensity threshold re-calculated continuously
–Intensity threshold currently the valley
following the first peak in an intensity histogram

–Current duration threshold at 300ms of silence

• Reports what frame the silence begun

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

33

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

quasisyllabification

/nailon/ -

• Convex hulls (Mermelstein)
• Based on normalised intensity
– Voiced sequences only
– Incremental search for next complete hull
– Remebers last seen hull only

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

34

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

/nailon/ - intonation pattern
classification
• (Currently simplistic) classification of
intonation over one convex hull (quasisyllable) into either
– Low or low and falling
– Mid and level
– Other

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

35

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Turntaking decisions
• Low or low and falling intonation patterns taken to
indicate suitable places for turn-taking turnyielding
• Mid and level intonation patterns indicate unsuitable
places turn-keeping
• Other intonation patterns may indicate turn-keeping
as well as turn-yeilding and were therefore classified
as garbage don’t know here

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

36

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Does prosody help compared
to silence (>300 ms) only?
Turn-keeping

Don’t know

Turn-yielding

Total

23

212

158

393

Hold

105

255

71

431

Total

128

467

229

824

Change

•
•
•
•
•

28% turn-yielding, 16% turn-keeping, 56% don’t know
Speaker changes were in the majority for turn-yielding (69%)
Speaker holds were in the majority for turn-keeping (82%)
Unobtrusive system (pooling turn-keeping & don’t know): identifies 41%
of the suitable places for turn-taking; avoids 84% of the impossible ones
Responsive system (pooling turn-yielding & don’t know): identifies 94% of
the suitable places for turn-taking; avoids 24% of the unsuitable ones

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

37

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Responsivity vs.
unobtrusiveness
•
•
•

All dialogue systems need some kind of interaction control
capabilities
More human-like systems will require more human-like
interaction control
Different dialogue situations put different demands on the
interaction control capabilities

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

38

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

KTH Connector example
S
C
S
C
S
C
S
C
S
S
P
S
S
C

This is the KTH Connector, how may I help you?
Hi, this is Mattias.
Hello Mattias
I was supposed to be at the meeting, but I’m stuck on a train.
Mhm
Could you check with the others if it’s ok to patch me in as a listener?
Ok, please hold for a minute.
Ok.
[The KTH Connector waits for a suitable place to notify the meeting]
Mattias is on the phone and will not be able to make it to the meeting. Is it ok
to patch him in as a listener?
Sure, but give him a speech channel as well.
Ok, I’ll let you know when it’s done.
Ok, Mattias, I’ll patch you in. They suggested I’d give you a speech channel as
well. Do you want one?
No, it’s too noisy on the train. Listening will do.

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

39

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Responsivity vs.
Unobtrusiveness (cont.)
•

•

In dialogues with one listener only, responsiveness may
be a key concern for the system, as sluggishness is likely
to annoy the user
Sometimes unobtrusiveness is the most important
concern, for example when establishing a new connection
for notifying the participant in a meeting

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

40

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Future work
Better internal models of interaction control
• Further develoment of /nailon/
– Adding features, e.g. distinguishing open vs closed vocal
tract...
– Machine learning of categorisation

•

Combination with other sources of knowledge
– Semantic completeness

Better models that relate interaction control to the bigger
picture – conversation
• Relation to grounding, error handling
– Utterance concept particularly difficult as regards e.g.
backchannels

•

Initiative

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

41

Exploring prosody in interaction control

Jens Edlund & Mattias Heldner

Perceptual judgments vs
speaker change/speaker hold
1
2
3
4
definitely
probably
could be probably
unfinished unfinished finished or finished
unfinished

5
definitely
finished

Total

Speaker
change

169

66

40

200

705

1180

Speaker
hold

806

91

25

112

240

1274

Total

975

157

65

312

945

2454

Presented 2006-02-22 at KTH, the Department for Speech, Music and Hearing, by Mattias and Jens

42

Boston University Technical Report 2006-006

A customizable camera-based human computer
interaction system allowing people with disabilities
autonomous hands-free navigation of multiple
computing tasks
Wajeeh Akram, Laura Tiberii, and Margrit Betke
Department of Computer Science, Boston University
111 Cummington Street, Boston, MA 02215, USA
{wajeeha, ltiberii, betke}@cs.bu.edu

Abstract. Many people suffer from conditions that lead to deterioration of
motor control making access to the computer using traditional input devices
difficult. In particular, they may loose control of hand movement to the extent
that the standard mouse cannot be used as a pointing device. Most current
alternatives use markers or specialized hardware, for example, wearable
devices, to track and translate a user’s movement to pointer movement. These
approaches may be perceived as intrusive. Camera- based assistive systems that
use visual tracking of features on the user’s body often require cumbersome
manual adjustment. This paper introduces an enhanced computer vision based
strategy where features, for example on a user’s face, viewed through an
inexpensive USB camera, are tracked and translated to pointer movement. The
main contributions of this paper are (1) enhancing a video based interface with
a mechanism for mapping feature movement to pointer movement that allows
users to navigate to all areas of the screen even with very limited physical
movement and (2) providing a customizable, hierarchical navigation framework
for human computer interaction (HCI). This framework provides effective use
of the vision-based interface system for accessing multiple applications in an
autonomous setting. Experiments with several users show the effectiveness of
the mapping strategy and its usage within the application framework as a
practical tool for desktop users with disabilities.

Keywords: Computer-vision, assistive technology, alternative input devices,
video- based human-computer interfaces, autonomous navigation.

1 Introduction
Several conditions may cause computer users to be unable to use the standard mouse.
Paralysis from brain injury, stroke, multiple sclerosis, or Amyotrophic Lateral
Sclerosis (ALS, also called Lou Gehrig's disease) may cause the user to have very
little motor control except for limited head or eye movement. Loss o fine motor
f

control with age and muscle injuries may also make use of the standard mouse
difficult.
According to the National Multiple Sclerosis Society [1], approximately
400,000 Americans and 2 million individuals worldwide suffer from Multiple
Sclerosis, and about 200 people are diagnosed every week in the US. As such
conditions restrict physical mobility and often speaking capability, loss of the ability
to communicate is one of the most limiting problems for these individuals. Being able
to use computers for common tasks such as sending email and browsing the web
opens a huge avenue of possibility to improve quality of life.
A study by Forrester Research for Microsoft Corporation [2] presents
statistics on the need and significance of accessible technology. It is estimated that
about 17% (22.6 million) of computers users who suffer from severe impairments are
very likely to benefit from accessible technology. It is also postulated that the need for
accessibility devices may grow due to the increase in computer users above the age of
65 and the increase in the average age of computer users.
There has been extensive research in the domain of mouse alternatives as
accessibility aids for users who have very limited movement. Broadly, these efforts
can be divided into two main categories : systems that rely on specialized mechanical
or electronic hardware devices and camera-based systems. Mouse-actuated joysticks,
mechanical switches, breath-puffing straws, and electrodes placed on the user’s face
that measure movement of features are some of the strategies in the first category [3].
Many camera based systems track physical markers, for example, infrared markers
placed on the user’s body [4, 5] or markers on glasses. Systems that capture gaze
information often rely on infrared illumination or special headgear-mounted cameras;
a survey of these methods is provided by Magee et al. [6]. Most of these systems are
expensive, require special devices, and may be intrusive. In addition, significant
levels of technical expertise may be required to install and configure these systems.
Betke et al. [7] present ed a vision based solution called the camera mouse which
tracks features on a user’s body in a non-intrusive manner.
There has also been substantial work in developing applications for people
with disabilities [8, 9, 10]. Some existing applications include on-screen keyboards
[11], alternate text entry mechanisms [12, 13], games and learning aids for children
[7], and tools that interact with a web browser to make the internet more accessible
for camera mouse users [14, 15].
In this paper, we present a system that tracks features on the user’s body,
usually the face, and translates feature movement to pointer movement on the screen.
Our work builds on the camera mouse presented by Betke et al. [7], which proposed a
vision based feature tracking approach for pointer movement. Here, we present an
improved mapping strategy that allows translation of minimal feature movement to
pointer movement across the entire range of the screen. A framework for using the
camera mouse to carry out common tasks, with minimal intervention from a
caregiver, is also proposed. Experiments were conducted to determine how well the
users were able to access and perform each of the computing tasks in the HCI
framework. Test results have shown that the system successfully provides access to
common tasks such as opening games, web sites, text entry, and playing music.
The system is cost effective and requires little technical expertise of the user
and caregiver. Use or extension of the proposed system does not incur significant

cost, because the system was developed with open source technologies such as
OpenCV [16] and Java. The only additional hardware required, besides a personal
computer, is a low-cost USB camera. We refer to the interface system as the camera
mouse throughout this paper. However, as an alternative to the camera mouse [7], any
interface system, video-based or even the standard computer mouse that provides a
pointing and selection mechanism can be used with our HCI framework.

2 System Overview
The goal of our work is to provide a customizable camera-based human computer
interaction system allowing people with disabilities autonomous hands free navigation
of multiple computing tasks. We focus on two main aspects of the system; designing a
robust feature tracking strategy and an effective interaction approach that operates
optimally with a camera mouse. The following sections give an overview of the
components of the system.

2.1 Tracking features
This section describes our method to track a feature or set of features on the user’s
body, usually face, and convert the feature movement to pointer movement. The study
by Fagiani et al. [18] gives an experimental comparison of various tracking
mechanisms for use with the camera mouse and recommends either an optical flow or
correlation based tracker. We found the optical flow based algorithm to be both robust
and computationally efficient. Our system operates in real time on a computer with a
1.6 GHz processor, taking up on average less than 5% of processor time. This
demonstrates the use of the camera mouse as a background process that does not
affect the performance of other applications running on the system. Our camera
mouse implementation executes as a standalone application that moves the standard
windows pointer.
A USB Camera is connected to the computer and set up to capture a frontal
view of the user. On starting the application, a window with the video of the user is
displayed. The camera location should be adjusted so that the feature to be tracked is
in clear view. Typically, the user sits within 1 m of the camera. However, if the user is
very close to the camera, even a small physical movement can result in the feature
falling out of the camera’s field of view. Therefore, the distance from the camera
should be carefully adjusted such that the feature remains within the camera’s field of
view throughout the session.
The caregiver selects a feature on the user’s body by clicking at the desired
location of the input video stream. W e designed the system to automatically refine the
feature location by finding an image patch with the highest brightness gradient in the
11-by-11-pixel neighborhood of the manually selected feature [16]. The feature is
then tracked in subsequent frames using the Lucas-Kanade optical flow computation
[17]. We used a pyramid-based implementation of the Lucas -Kanade tracker provided
in Intel's OpenCV library [16].

2.2 Feature movement to pointer movement
Once the feature movement in pixels is known, an effective mapping from pixels of
movement in the video frames to pointer movement on the screen is required.
Pointing devices such as the standard mouse and mouse pad do not have an absolute
mapping of device movement to pointer movement. The pointer is moved in a
differential manner, governed by speed and acceleration parameters set by the user.
Similarly, the camera mouse cannot be used with any degree of flexibility if this
mapping is absolute: an absolute mapping would mean that the feature to be tracked
would have to move the same distance (in pixels, as viewed by the camera) as the
pointer is to move on the screen. Most users do not have such a large range of
movement and even if such movement were possible, it does not complement the
natural movement of a computer user as they view the computer screen. Therefore the
camera mouse operates the pointer in a relative manner.
A relative scheme of pointer movement must consider how to adjust for the
difference in scale of feature movement and pointer movement. The movement of the
detected feature must be scaled in some reasonable manner before being added to the
current pointer position. In previous systems, the scale factor is a user-customizable
setting. However, adjusting the scale factor manually is a cumbersome trial and error
process and requires intervention by a caregiver for manually entering scale factors.
The scale factor is pertinent to the usability of the system, because if the scale factor
is too low, all areas of the screen may not be reachable by the pointer. Alternatively, if
it is too high the pointer may become too sensitive and thus move too quickly.
It can be observed that the scale factor is a function of the user’s distance
from the screen, as well as the range of possible movement of the feature in both
horizontal and vertical directions. The user’s range of movement may be seriously
limited by motor dysfunction. The range of movement is also typically asymmetric in
the vertical and horizont al directions due to the fact that vertical rotation of the head
when viewing a standard computer screen is smaller than horizontal rotation.
From a usability point of view, the scaling factor should not be such that the
system requires the user to move in a way that interferes negatively with the user’s
visual focus on the screen. In other words, during facial feature tracking with the
camera mouse, feature movement and visual focus cannot be decoupled. Feature
movement required for effective use of the system should not be such that it causes a
strain on the visual focusing mechanism of the user.
Designing a mechanism to allow optimal setting of the scale factor by the
user is therefore important towards the end of improving system performance and
usability. A calibration phase was introduced to determine the optimal scale factor for
individual users. Calibration is performed in advance of a usage session. After a
feature is selected to be tracked, the users are lead through a calibration phase, in
which they are directed to rotate their head towards distinct markers shown on the
video stream, while retaining a comfortable view of the computer screen. The users
successively move towards markers on the top, bottom, left and right boundaries of
the screen (Figure 1). It is important to direct users to move within a comfortable
range of motion, which permits clear and non stressful visual focus on the screen.
Pointer movement is calibrated to the range of movement demonstrated by the user,
using a linear mappi g of demonstrated movement range to screen dimensions.
n

Figure 1: System Calibration: The small colored disk shown in the video indicates the tip of
the eyebrow has been selected as the feature to track. The larger disk on the boundary of the
video display window indicates the direction the user should move her head.

After performing the calibration phase once for a particular user and a specific
feature, in situations where the distance from the camera remains approximately the
same across sessions, for example, for a user in a wheelchair, the scale factors found
by the calibration phase may be saved in a user configuration file that can be loaded
for subsequent use.

2.3 Application framework
Applications often have to be tailored to work with the camera mouse, since the
effective movement resolution of the camera mouse is not enough to navigate
windows menus or operate standard windows applications. Several on-screen
keyboards, educational programs, and game applications are available for use with the
camera mouse. However, the user must rely on a caregiver to start the custom
application before they can start using it. If the user wants to start a new application
for another task, there is no means to navigate available program s autonomously
without the caregiver’s help. Our motivation in proposing a hierarchical framework
for application navigation is to provide the camera mouse user with an autonomous
experience with their computer, allowing them to perform common tasks of int erest

such as text entry, internet browsing, and entertainment applications in a manner that
is user friendly, requires little technical expertise, and is configurable to meet the
needs of individual users.
Several considerations must be kept in mind when designing an effective
interface [19].
• The user should be able to clearly identify the target being selected.
• Distinguishing information should be placed at the beginning of headings.
• Clear and simple language should be used.
• The design should be consistent.
• There should be clear navigation.
Our interface opens with a main menu that is a list of common tasks (Figure 2).
The main menu items configured in the test system are: Play this Song launches the
default media player and plays the chosen song, Text Entry launches an on-screen
keyboard, Common Sayings speaks saved text using a speech synthesis program, View
a webpage launches the default browser and displays the chosen website, and Games
launches games, such as Eagle Aliens [6], which have been developed to require only
pointer movement.

Figure 2: Main Menu of Interface.

The list of common tasks desired in the application varies depending on the interests
of each user. The system is designed so that menu items can be added, removed, or
modified. This allows the list to be customized for each individual user.
The user will choose the common task they desire in one of two modes,
select mode or scan mode. In select mode, the user moves the pointer to an item.
When the pointer reaches an item it is highlighted in blue, clearly identifying the
target to be selected. In scan mode, the application scans through the list of items
highlighting each item for a specified time interval. The time interval can be changed
to the length of time that is reasonable for the current user.
To facilitate autonomous use, a dwell feature is available to simulate a
selection command. The dwell feature acts as a timer. When an item is highlighted the
timer is started. If that item stays highlighted for a specified time interval, a selection
command is executed. The gray areas of the interface, shown in Figure 2, represent

rest areas where the pointer can dwell without causing a selection command to occur.
Gray was used to stress the inactive nature of such areas. The dwell feature can be
enabled or disabled, as alternate methods may be available to simulate pointer clicks,
such as blink detection [22], raised eyebrow detection [23], or use of a mechanical
switch.
The font size of the menu items was also a consideration for users who are
unable to sit close to the system due to wheelchairs. The system is designed so that
the font size can be increased or decreased as desired. Items on the main menu are
either links that directly launch programs or links that open a submenu. Every
submenu has the same font type and size. The same color is use to highlight the
d
menu items. This consistency helps maintain usability. A ‘Return to Main Menu’
option is always the last item in the submenu list. This feature supports clear
navigat ion among the various menus. When a submenu item is selected the program
associated wi h that menu item is launched. The ‘Return to Main Menu’ option is
t
displayed on the screen after the program is launched so that the user can return to the
system and navigate to other programs if desired. A strategy for navigation among
opened programs is proposed by our framework, but has not been implemented yet.
An example of navigating through the system and selecting a song to play is
shown in Figure 3.

Figure 3: Navigation from the main menu through the ‘Play this Song’ submenu to launch a
music player that automatically begins playing the selected song.

3 Experiments and Results
The system was tested to determine the performance of the tracking mechanism and
to understand its limitations, as well as to determine the usability of the application
framework proposed. Results from the first test provided input for the design of
interface elements for the application framework.
A test group consisting of 8 subjects did the first set of experiments (Group
1). The subjects were between 14 and 60 years of age with varying levels of computer
skills. The subjects did not have any functional limitations. The same set of users was
asked to perform a control test, where the same sequence of steps was performed with
a standard mouse (Control Group). The second test group (Group 2) consisted of two
patients from The Boston Home [20]. Both subjects suffered from functional
limitations that made it difficult or impossible to use the standard mouse. One of the

subjects was diagnosed with muscular dystrophy more than 15 years ago. His
condition causes muscle weakness and wasting in major joints and muscles, his
shoulders and hips have been affected most. The other subject was diagnosed with
multiple sclerosis more than 15 years ago. His condition causes muscle weakness,
limiting his ability to move his arms, hands, and neck. The limitation in neck
movement has resulted in a very small range of head movement.

3.1 Evaluating tracker performance
The tests were designed to record indicators of tracker performance. Specifically, we
focused on factors pertaining to the tracker’s ability to track features and translate
feature movement to pointer movement on the screen. Specific factors include:
• Effective Dwell Area: the smallest region within which the user can dwell
for 3 seconds. This will allow us to study the tradeoff between tracker
sensitivity and dwelling ability.
• Movement patterns that cause the tracker to lose features while tracking.
• Movement patterns that affect the smoothness of the tracker’s constructed
pointer movement.
A movement evaluation tool was developed to analyze the above factors (Figure
4). During the test, users were asked to move the pointer from box to box. The order
of movement between boxes was chosen so that we could evaluate the user’s ability
to move the pointer vertically, horizontally, and diagonally. The placement of the
boxes on the screen was chosen to allow us to determine if there were areas of the
screen that the users found difficult to reach, or were unable to reach. Different sized
boxes were used to evaluate the smallest area that the user can easily dwell in for a
few seconds. The size and location of the boxes was chosen so as to discern if it was
easier to dwell in smaller boxes in some areas of the screen. The use of color in the
boxes allows the user to recognize the area they are asked to move without having to
read through the labels.
1

3
2
4
6

5

7
8
Figure 4: Movement Evaluation Tool.

The users were asked to move the pointer in the following sequence, dwelling for
three seconds in each box: dark blue box labeled 3, yellow box labeled 7 green box
,
labeled 8, red box labeled 2, light blue box labeled 4, black box labeled 1, purple box
labeled 5, white box labeled 6.
Figure 5 shows a user with multiple sclerosis performing a subset of steps in
the movement evaluation test. It is apparent from the test that despite being restricted
to only slight movements of the head, the user was able to reach all areas of the
screen, including corners, and could dwell even in small regions.
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the green box
labeled 8 to the red box
labeled 2.

8
1

3
2

4
5

6

7

Instruction:
User is asked to move
from the red box
labeled 2 to the light
blue box labeled 4.

8
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the light blue box
labeled 4 to the black box
labeled 1.

8
Figure 5: User with multiple sclerosis while performing movement evaluation test (left),
simultaneous screen shots depicting pointer location (center), and the instruction given (right).
(Note: Pointer is shown enhanced in the figure.)

Figure 6 shows the entire trajectory of pointer movement as a user performs the
movement evaluation test.

Figure 6: Pointer trajectory of the movement evaluation test.
The task in the tracker evaluation test was to move from one colored box to another
(Figure 4) and then focus on the box for several seconds. The test consisted of eight
tasks. The tracker evaluatio n tests showed that all ten users, with and without
disabilities, were able to move the pointer to every location. This indicates that we
were successful in designing a system that tracks features and translates feature
movement to pointer movement on the screen. Table 1 categorizes three levels of
movement error, no overshooting, overshooting once, and overshooting more than
once. Overshooting occurs when the mouse pointer moves beyond the target on the
screen. This did not prevent the user from selecting the desired target. The control
experiment was done using the standard mouse.

Table 1: Results of Movement Evaluation T est

Control

Group 1

Group 2

Average Completion Time
Average % of Tasks Complete d
on the First Trial
Average % of Not Overshooting

1.0 s
8/8 = 100%

1.8 s
8/8 = 100%

3.2 s*
7.5/8 = 94%

8/8 = 100%

4/8 = 50%

2/8 = 25%

Average % of Overshooting Once

0/8 = 0%

2/8 = 25%

2.5/8 = 31%

Average % of Overshooting more
than Once

0/8 = 0%

2/8 = 25%

3.5/8 = 44%

* We discounted the timing result of one of the eight assigned tasks for one user in Group 2 in
computing the average completion time. The reason was that, d
uring the test, the subject was
asking questions and the recorded time of 30 seconds did not reflect the actual time to move the
pointer, which was on average less than 3 seconds for the remaining seven task s performed by
this user.

3.2 Evaluating application d
esign
The tests in this section were designed to capture the usability of the application
framework with respect to the design and layout of the interface elements. The test
consisted of launching five applications in sequence, Text Entry (Keyboard
application), Common Sayings (speech synthesizer), View a webpage (open browser),
Games (open a game), and Play this Song (open a media player).
We were interested in determining how well users were able to navigate
through the menus (average completion time), how many times the users had to try
before they successfully launched the correct application (number of tasks completed
on the first trial), and how often the programs were launched unintentionally (percent
of unintentional launches). Table 2 presents the results.
Table 2: Application Evaluation Results

Control

Group 1

Group 2

Average Completion Time *
Number of Tasks Comp d
lete
on the First Trial **

5.0 s
1

Percent of Unintentional
Launches

0

6.3 s
5/5 = 100%
for 5 users
4/5 = 80% for
2 users
0/5 = 0%
for 5 users
2/7 = 29% for
1 user
3/8 = 38% for
1 user

9.4 s
5/5 = 100%
for user 1
3/5 = 60%
for user 2
0/5 = 0% for
user 1
4/9 = 44%
for use 2
r

*Actual task completion times for Group 1 and 2 were not significantly different. The
computed results for G
roup 2 were affected by the fact that users in G
roup 2 showed much
interest in the system; they stopped to discuss, ask questions, and give ide as. Such instances
skewed the average of the recorded times.
**The users needed more than one trial to complete a task due to unintentional launches. The
unintentional launches were instances where the user diverted from the test to discuss
something and hence caused unintentional launching of the applications. This forced them to
return to the main menu and repeat the task. This also highlights the need for a binary
switching mechanism to turn off the tracker when not in active use.

Another consideration for the application evaluation was the degree of independent
use, i.e., the degree to which the user can effectively use the application without
intervention, once it has been set up. This factor is difficult to measure quantitatively.
From personal observation we saw that the subjects were able to launch all of the
programs independently and interact with the applications. For example, using the
cascading menu selection strategy, they were able to launch and play a game, get back
to the main menu by ho vering above it and then launch and use a text entry
application.
The users were also provided with the opportunity to use the system on their
own, without a guided sequence of steps. This helped determine their opinion on the
overall use of the system. During this period unexpected problems with the system
could be identified. A survey was used to gather the opinions of the sample test group.
Issues were determined by analysis of survey questions and by personal
observation. The tests performed by Group 1 revealed several issues. It was observed
that after a program was launched it was not possible to return to the application
w ithout using the standard mouse. To resolve this issue, the system was configured
such that when the pointer moves over the title area of the partially occluded
application, the application is brought into the foreground. This assumes that the
programs opened will not take the full screen area.
Another issue noticed during preliminary testing was that the testers could
not easily i entify where to rest the pointer without causing a selection command to
d
occur. As a result, programs were opened unintentionally; the Midas touch problem
[21]. To resolve this issue, all areas where the pointer can rest were changed to have a
gray background color distinguishing them from the areas with a white background
that cause a selection command to be executed. The users of Group 2 also found that
the pointer had some jitter, due to the increased sensitivity. We propose a simple
averaging mechanism to solve this problem.
Users showed interest in the prospect of being able to write and save text and
send email autonomously using the camera mouse. Current users rely on a separate
application to enter the text and then the caregiver has to copy and paste the text into
an email application to dispatch the email. Users also expressed interest in a system
that allowed effective web browsing with the camera mouse.

4 Discussion
In summary, we developed a customizable camera-based human computer interaction
system and showed that people with and without disabilities can complete multiple
computing tasks with slight head movements. The improvements made to the camera
mouse have resulted in a robust feature tracker and a calibration of feature movement
to pointer movement that is specific for each individual user. Taking advantage of the
features of the camera mouse, our interaction system was able to provide hands -free
access to many common computing tasks. The test results show that users were able
to successfully open all of the programs available in our system with only a small
percentage of error. This provides evidence that we designed a user-friendly interface
with an effective navigation strategy. Survey results obtained from the test subjects
showed that their holistic experience of the system was positive and they especially
enjoyed playing the games.
Several of the test subjects in the first group used the system more than once.
Their ability to control the pointer movement and dwell in a selection area improved
as quickly as the second use. This indicates that the difference in average completion
time between the control experiment and the camera mouse experiment would be
reduced if all subjects were given more time to become accustomed to moving the
pointer with the camera mouse.
A possibility for extension is to provide automatic feature detection. This
would eliminate the dependence of tracking performance on the manual selection of
an appropriate feature. The type of features best suited for tracking with the camera
mouse was studied by Cloud et al. [24], who suggested that the tip of the nose was a
robust feature. Gorodnichy [25] also discussed the robustness of nose tracking. Our
experiments with the camera mouse showed similar results. Features on the sides of
the face were lost by the tracker frequently as they were occluded upon rotation of the
head. The outer tips of the eyes and features on the outer boundaries of the face were
similarly not suitable for tracking. Features that exhibited good contrast on the central
band of the face, e.g. the inner tip of the eyebrows, the tip of the nose and the outer
boundary of the top or bottom lip, were the best features to track with the camera
positioned so that it has a frontal view of the person’s face. Tracking a feature on the
lips may however be problematic if the user speaks during use. Features on the eye
were often lost during blinking. Also, experiments showed that if the user wore
glasses, especially of a dark color, features on the glasses, such as the bridge of the
glasses, were robust to track.
Directions for future work include:
• Providing an automatic feature detection method.
• Smoothing pointer jitter that resulted from the increased sensitivity.
• Navigation among the opened programs.
• Providing better internet browsing, t ext entry, and email programs.
• Designing interaction strategies that allow the camera mouse to be used with
standard, non-specialized applications. For example, adding features such as
generalized dwell that is decoupled from camera mouse enabled applications
and operates with the desired dwell radius on the entire screen. To overcome

•

the limitation of small interface elements found in many standard
applications, screen magnification could be used to magnify menus as the
pointer hovers above them. A binary switch could then be provided to toggle
to the magnified area and select menu items. A cursor lock could also be
used to aid selection of small interface elements.
Extension to usage scenarios within the ambient intelligence paradigm [26].
The computer vision strategy presented here as a pointer alternative can be
applied to menu selection tasks in common appliances such as telephones,
microwave ovens, web-enabled digital television (DTV) and CD players.

Acknowledgements
The authors thank David Young-Hong from The Boston Home for his help with the
experiments and for sharing his insights regarding technologies needed for people
with disabilities. This work was supported by NSF grants IIS-0093367, IIS-0329009,
and 0202067 .

References
1. National Multiple Sclerosis Society, http://www.nationalmssociety.org, accessed April 2006.
2. Microsoft Accessibility, http://www.microsoft.com/enable/research/agingpop.aspx, accessed
April 2006.
3. J. Gips, P. Olivieri, and J.J. Tecce, "Direct Control of the Computer through
Electrodes Placed Around the Eyes", Human- Computer Interaction: Applications
and Case Studies, M.J. Smith and G. Salvendy (eds.), Elsevier, pages 630-635. 1993.
4. Synapse Adaptive, http://www.synapseadaptive.com/prc/prchead.htm, accessed April 2006.
5. NaturalPoint SmartNAV, http://www.naturalpoint.com/smartnav/, accessed July 2006.
6. J.J. Magee, M.R. Scott, B.N. Waber and M. Betke, "EyeKeys: A Real-time Vision Interface
Based on Gaze Detection from a Low grade Video Camera," In Proceedings of the IEEE
Workshop on Real-Time Vision for Human-Computer Interaction (RTV4HCI), Washington,
D.C., July 2004.
7. M. Betke, J. Gips, and P. Fleming, “The camera mouse: Visual tracking of body features to
provide computer access for people with severe disabilities”, IEEE Transactions on Neural
Systems and Rehabilitation Engineering, 10:1, pages 1-10, March 2002.
8. D.O. Gorodnichy and G. Roth, “Nouse ‘Use your nose as a mouse’ perceptual vision
technology for hands-free games and interfaces ”, Procee dings of the International
Conference on Vision Interface (VI 2002), Calgary, Canada, May 2002.
9. Assistive Technologies, http://www.assistivetechnologies.com , accessed April 2006.
10. Apple Computer Disability Resources, http://www.apple.com/accessibility, accessed April
2006.
11. WiViK on-screen keyboard (virtual keyboard) software, http://www.wivik.com, accessed
April 2006.
12. The Dasher Project, http://www.inference.phy.cam.ac.uk/dasher, accessed April 2006.
13. J. Gips and J. Gips, "A Computer Program Based on Rick Hoyt's Spelling Method for
People with Profound Special Needs," Proceedings International Conference on Computers
Helping People with Special Needs (ICCHP 2000), Karlsruhe, pages 245-250.
14. B.N. Waber, J. J. Magee, and M. Betke, “Web Mediators for Accessible Browsing Boston
,”
University Computer Science Department Technical Report BUCS 2006-007, May 2006.

15. H. Larson and J. Gips, "A Web Browser for People with Quadriplegia." In Universal
Access in HCI: Inclusive Design in the Information Society, Proceedings of the
International Conference on Human-Computer Interaction, Crete, 2003, C. Stephanidis
(ed.), Lawrence Erlbaum Associates, pages 226-230, 2003.
16. OpenCV library. http://sourcforge.net/projects/opencvlibrary, accessed April 2006.
17. B.D. Lucas and T. Kanade . “An iterative image registration technique with an application to
stereo vision.” In Proceedings of the 7th International Joint Conference on Artificial
Intelligence (IJCAI), pages 674-679, Vancouver, Canada, April 1981.
18. C. Fagiani, M. Betke, and J. Gips, “Evaluation of tracking methods for human-computer
interaction.” In Proceedings of the IEEE Workshop on Applications in Computer Vision
(WACV 2002), pages 121-126, Orlando, Florida, December 2002.
19. “Human-centered design processes for interactive systems,” International Organization for
Standardization ISO 13407, 1999.
20. The Boston Home, http://www.thebostonhome.org, accessed April 2006.
21. R.J.K. Jacob, “What you look at is what you get,” Computer, 26:7, pages 65–66, July 1993.
22. M. Chau and M. Betke, “Real Time Eye Tracking and Blink Detection with USB
Cameras,” Boston University Computer Science Technical Report 2005-012, May 2005.
23. J. Lombardi and M. Betke, “A camera-based eyebrow tracker for hands-free computer
control via a binary switch”, In Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (U14All 2002), pages 199-200, Paris, France, October 2002.
24. R. L. Cloud, M. Betke, and J. Gips, “Experiments with a Camera- Based Human-Computer
Interface System.” In Proceedings of the 7th ERCIM Workshop "User Interfaces for All,"
UI4ALL 2002, pages 103-110, Paris, France, October 2002.
25. D.O. Gorodnichy, “On importance of nose for face tracking”, In Proceedings of the IEEE
International Conference on Automatic Face and Gesture Recognition (FG 2002), pages
188-196, Washington, D.C., May 2002.
26. A. Ferscha, “ Contextware: Bridging Physical and Virtual Worlds. ” In Proceedings of the
Ada-Europe Conference on Reliable Software Technologies, 2002.

Boston University Technical Report 2006-006

A customizable camera-based human computer
interaction system allowing people with disabilities
autonomous hands-free navigation of multiple
computing tasks
Wajeeh Akram, Laura Tiberii, and Margrit Betke
Department of Computer Science, Boston University
111 Cummington Street, Boston, MA 02215, USA
{wajeeha, ltiberii, betke}@cs.bu.edu

Abstract. Many people suffer from conditions that lead to deterioration of
motor control making access to the computer using traditional input devices
difficult. In particular, they may loose control of hand movement to the extent
that the standard mouse cannot be used as a pointing device. Most current
alternatives use markers or specialized hardware, for example, wearable
devices, to track and translate a user’s movement to pointer movement. These
approaches may be perceived as intrusive. Camera- based assistive systems that
use visual tracking of features on the user’s body often require cumbersome
manual adjustment. This paper introduces an enhanced computer vision based
strategy where features, for example on a user’s face, viewed through an
inexpensive USB camera, are tracked and translated to pointer movement. The
main contributions of this paper are (1) enhancing a video based interface with
a mechanism for mapping feature movement to pointer movement that allows
users to navigate to all areas of the screen even with very limited physical
movement and (2) providing a customizable, hierarchical navigation framework
for human computer interaction (HCI). This framework provides effective use
of the vision-based interface system for accessing multiple applications in an
autonomous setting. Experiments with several users show the effectiveness of
the mapping strategy and its usage within the application framework as a
practical tool for desktop users with disabilities.

Keywords: Computer-vision, assistive technology, alternative input devices,
video- based human-computer interfaces, autonomous navigation.

1 Introduction
Several conditions may cause computer users to be unable to use the standard mouse.
Paralysis from brain injury, stroke, multiple sclerosis, or Amyotrophic Lateral
Sclerosis (ALS, also called Lou Gehrig's disease) may cause the user to have very
little motor control except for limited head or eye movement. Loss o fine motor
f

control with age and muscle injuries may also make use of the standard mouse
difficult.
According to the National Multiple Sclerosis Society [1], approximately
400,000 Americans and 2 million individuals worldwide suffer from Multiple
Sclerosis, and about 200 people are diagnosed every week in the US. As such
conditions restrict physical mobility and often speaking capability, loss of the ability
to communicate is one of the most limiting problems for these individuals. Being able
to use computers for common tasks such as sending email and browsing the web
opens a huge avenue of possibility to improve quality of life.
A study by Forrester Research for Microsoft Corporation [2] presents
statistics on the need and significance of accessible technology. It is estimated that
about 17% (22.6 million) of computers users who suffer from severe impairments are
very likely to benefit from accessible technology. It is also postulated that the need for
accessibility devices may grow due to the increase in computer users above the age of
65 and the increase in the average age of computer users.
There has been extensive research in the domain of mouse alternatives as
accessibility aids for users who have very limited movement. Broadly, these efforts
can be divided into two main categories : systems that rely on specialized mechanical
or electronic hardware devices and camera-based systems. Mouse-actuated joysticks,
mechanical switches, breath-puffing straws, and electrodes placed on the user’s face
that measure movement of features are some of the strategies in the first category [3].
Many camera based systems track physical markers, for example, infrared markers
placed on the user’s body [4, 5] or markers on glasses. Systems that capture gaze
information often rely on infrared illumination or special headgear-mounted cameras;
a survey of these methods is provided by Magee et al. [6]. Most of these systems are
expensive, require special devices, and may be intrusive. In addition, significant
levels of technical expertise may be required to install and configure these systems.
Betke et al. [7] present ed a vision based solution called the camera mouse which
tracks features on a user’s body in a non-intrusive manner.
There has also been substantial work in developing applications for people
with disabilities [8, 9, 10]. Some existing applications include on-screen keyboards
[11], alternate text entry mechanisms [12, 13], games and learning aids for children
[7], and tools that interact with a web browser to make the internet more accessible
for camera mouse users [14, 15].
In this paper, we present a system that tracks features on the user’s body,
usually the face, and translates feature movement to pointer movement on the screen.
Our work builds on the camera mouse presented by Betke et al. [7], which proposed a
vision based feature tracking approach for pointer movement. Here, we present an
improved mapping strategy that allows translation of minimal feature movement to
pointer movement across the entire range of the screen. A framework for using the
camera mouse to carry out common tasks, with minimal intervention from a
caregiver, is also proposed. Experiments were conducted to determine how well the
users were able to access and perform each of the computing tasks in the HCI
framework. Test results have shown that the system successfully provides access to
common tasks such as opening games, web sites, text entry, and playing music.
The system is cost effective and requires little technical expertise of the user
and caregiver. Use or extension of the proposed system does not incur significant

cost, because the system was developed with open source technologies such as
OpenCV [16] and Java. The only additional hardware required, besides a personal
computer, is a low-cost USB camera. We refer to the interface system as the camera
mouse throughout this paper. However, as an alternative to the camera mouse [7], any
interface system, video-based or even the standard computer mouse that provides a
pointing and selection mechanism can be used with our HCI framework.

2 System Overview
The goal of our work is to provide a customizable camera-based human computer
interaction system allowing people with disabilities autonomous hands free navigation
of multiple computing tasks. We focus on two main aspects of the system; designing a
robust feature tracking strategy and an effective interaction approach that operates
optimally with a camera mouse. The following sections give an overview of the
components of the system.

2.1 Tracking features
This section describes our method to track a feature or set of features on the user’s
body, usually face, and convert the feature movement to pointer movement. The study
by Fagiani et al. [18] gives an experimental comparison of various tracking
mechanisms for use with the camera mouse and recommends either an optical flow or
correlation based tracker. We found the optical flow based algorithm to be both robust
and computationally efficient. Our system operates in real time on a computer with a
1.6 GHz processor, taking up on average less than 5% of processor time. This
demonstrates the use of the camera mouse as a background process that does not
affect the performance of other applications running on the system. Our camera
mouse implementation executes as a standalone application that moves the standard
windows pointer.
A USB Camera is connected to the computer and set up to capture a frontal
view of the user. On starting the application, a window with the video of the user is
displayed. The camera location should be adjusted so that the feature to be tracked is
in clear view. Typically, the user sits within 1 m of the camera. However, if the user is
very close to the camera, even a small physical movement can result in the feature
falling out of the camera’s field of view. Therefore, the distance from the camera
should be carefully adjusted such that the feature remains within the camera’s field of
view throughout the session.
The caregiver selects a feature on the user’s body by clicking at the desired
location of the input video stream. W e designed the system to automatically refine the
feature location by finding an image patch with the highest brightness gradient in the
11-by-11-pixel neighborhood of the manually selected feature [16]. The feature is
then tracked in subsequent frames using the Lucas-Kanade optical flow computation
[17]. We used a pyramid-based implementation of the Lucas -Kanade tracker provided
in Intel's OpenCV library [16].

2.2 Feature movement to pointer movement
Once the feature movement in pixels is known, an effective mapping from pixels of
movement in the video frames to pointer movement on the screen is required.
Pointing devices such as the standard mouse and mouse pad do not have an absolute
mapping of device movement to pointer movement. The pointer is moved in a
differential manner, governed by speed and acceleration parameters set by the user.
Similarly, the camera mouse cannot be used with any degree of flexibility if this
mapping is absolute: an absolute mapping would mean that the feature to be tracked
would have to move the same distance (in pixels, as viewed by the camera) as the
pointer is to move on the screen. Most users do not have such a large range of
movement and even if such movement were possible, it does not complement the
natural movement of a computer user as they view the computer screen. Therefore the
camera mouse operates the pointer in a relative manner.
A relative scheme of pointer movement must consider how to adjust for the
difference in scale of feature movement and pointer movement. The movement of the
detected feature must be scaled in some reasonable manner before being added to the
current pointer position. In previous systems, the scale factor is a user-customizable
setting. However, adjusting the scale factor manually is a cumbersome trial and error
process and requires intervention by a caregiver for manually entering scale factors.
The scale factor is pertinent to the usability of the system, because if the scale factor
is too low, all areas of the screen may not be reachable by the pointer. Alternatively, if
it is too high the pointer may become too sensitive and thus move too quickly.
It can be observed that the scale factor is a function of the user’s distance
from the screen, as well as the range of possible movement of the feature in both
horizontal and vertical directions. The user’s range of movement may be seriously
limited by motor dysfunction. The range of movement is also typically asymmetric in
the vertical and horizont al directions due to the fact that vertical rotation of the head
when viewing a standard computer screen is smaller than horizontal rotation.
From a usability point of view, the scaling factor should not be such that the
system requires the user to move in a way that interferes negatively with the user’s
visual focus on the screen. In other words, during facial feature tracking with the
camera mouse, feature movement and visual focus cannot be decoupled. Feature
movement required for effective use of the system should not be such that it causes a
strain on the visual focusing mechanism of the user.
Designing a mechanism to allow optimal setting of the scale factor by the
user is therefore important towards the end of improving system performance and
usability. A calibration phase was introduced to determine the optimal scale factor for
individual users. Calibration is performed in advance of a usage session. After a
feature is selected to be tracked, the users are lead through a calibration phase, in
which they are directed to rotate their head towards distinct markers shown on the
video stream, while retaining a comfortable view of the computer screen. The users
successively move towards markers on the top, bottom, left and right boundaries of
the screen (Figure 1). It is important to direct users to move within a comfortable
range of motion, which permits clear and non stressful visual focus on the screen.
Pointer movement is calibrated to the range of movement demonstrated by the user,
using a linear mappi g of demonstrated movement range to screen dimensions.
n

Figure 1: System Calibration: The small colored disk shown in the video indicates the tip of
the eyebrow has been selected as the feature to track. The larger disk on the boundary of the
video display window indicates the direction the user should move her head.

After performing the calibration phase once for a particular user and a specific
feature, in situations where the distance from the camera remains approximately the
same across sessions, for example, for a user in a wheelchair, the scale factors found
by the calibration phase may be saved in a user configuration file that can be loaded
for subsequent use.

2.3 Application framework
Applications often have to be tailored to work with the camera mouse, since the
effective movement resolution of the camera mouse is not enough to navigate
windows menus or operate standard windows applications. Several on-screen
keyboards, educational programs, and game applications are available for use with the
camera mouse. However, the user must rely on a caregiver to start the custom
application before they can start using it. If the user wants to start a new application
for another task, there is no means to navigate available program s autonomously
without the caregiver’s help. Our motivation in proposing a hierarchical framework
for application navigation is to provide the camera mouse user with an autonomous
experience with their computer, allowing them to perform common tasks of int erest

such as text entry, internet browsing, and entertainment applications in a manner that
is user friendly, requires little technical expertise, and is configurable to meet the
needs of individual users.
Several considerations must be kept in mind when designing an effective
interface [19].
• The user should be able to clearly identify the target being selected.
• Distinguishing information should be placed at the beginning of headings.
• Clear and simple language should be used.
• The design should be consistent.
• There should be clear navigation.
Our interface opens with a main menu that is a list of common tasks (Figure 2).
The main menu items configured in the test system are: Play this Song launches the
default media player and plays the chosen song, Text Entry launches an on-screen
keyboard, Common Sayings speaks saved text using a speech synthesis program, View
a webpage launches the default browser and displays the chosen website, and Games
launches games, such as Eagle Aliens [6], which have been developed to require only
pointer movement.

Figure 2: Main Menu of Interface.

The list of common tasks desired in the application varies depending on the interests
of each user. The system is designed so that menu items can be added, removed, or
modified. This allows the list to be customized for each individual user.
The user will choose the common task they desire in one of two modes,
select mode or scan mode. In select mode, the user moves the pointer to an item.
When the pointer reaches an item it is highlighted in blue, clearly identifying the
target to be selected. In scan mode, the application scans through the list of items
highlighting each item for a specified time interval. The time interval can be changed
to the length of time that is reasonable for the current user.
To facilitate autonomous use, a dwell feature is available to simulate a
selection command. The dwell feature acts as a timer. When an item is highlighted the
timer is started. If that item stays highlighted for a specified time interval, a selection
command is executed. The gray areas of the interface, shown in Figure 2, represent

rest areas where the pointer can dwell without causing a selection command to occur.
Gray was used to stress the inactive nature of such areas. The dwell feature can be
enabled or disabled, as alternate methods may be available to simulate pointer clicks,
such as blink detection [22], raised eyebrow detection [23], or use of a mechanical
switch.
The font size of the menu items was also a consideration for users who are
unable to sit close to the system due to wheelchairs. The system is designed so that
the font size can be increased or decreased as desired. Items on the main menu are
either links that directly launch programs or links that open a submenu. Every
submenu has the same font type and size. The same color is use to highlight the
d
menu items. This consistency helps maintain usability. A ‘Return to Main Menu’
option is always the last item in the submenu list. This feature supports clear
navigat ion among the various menus. When a submenu item is selected the program
associated wi h that menu item is launched. The ‘Return to Main Menu’ option is
t
displayed on the screen after the program is launched so that the user can return to the
system and navigate to other programs if desired. A strategy for navigation among
opened programs is proposed by our framework, but has not been implemented yet.
An example of navigating through the system and selecting a song to play is
shown in Figure 3.

Figure 3: Navigation from the main menu through the ‘Play this Song’ submenu to launch a
music player that automatically begins playing the selected song.

3 Experiments and Results
The system was tested to determine the performance of the tracking mechanism and
to understand its limitations, as well as to determine the usability of the application
framework proposed. Results from the first test provided input for the design of
interface elements for the application framework.
A test group consisting of 8 subjects did the first set of experiments (Group
1). The subjects were between 14 and 60 years of age with varying levels of computer
skills. The subjects did not have any functional limitations. The same set of users was
asked to perform a control test, where the same sequence of steps was performed with
a standard mouse (Control Group). The second test group (Group 2) consisted of two
patients from The Boston Home [20]. Both subjects suffered from functional
limitations that made it difficult or impossible to use the standard mouse. One of the

subjects was diagnosed with muscular dystrophy more than 15 years ago. His
condition causes muscle weakness and wasting in major joints and muscles, his
shoulders and hips have been affected most. The other subject was diagnosed with
multiple sclerosis more than 15 years ago. His condition causes muscle weakness,
limiting his ability to move his arms, hands, and neck. The limitation in neck
movement has resulted in a very small range of head movement.

3.1 Evaluating tracker performance
The tests were designed to record indicators of tracker performance. Specifically, we
focused on factors pertaining to the tracker’s ability to track features and translate
feature movement to pointer movement on the screen. Specific factors include:
• Effective Dwell Area: the smallest region within which the user can dwell
for 3 seconds. This will allow us to study the tradeoff between tracker
sensitivity and dwelling ability.
• Movement patterns that cause the tracker to lose features while tracking.
• Movement patterns that affect the smoothness of the tracker’s constructed
pointer movement.
A movement evaluation tool was developed to analyze the above factors (Figure
4). During the test, users were asked to move the pointer from box to box. The order
of movement between boxes was chosen so that we could evaluate the user’s ability
to move the pointer vertically, horizontally, and diagonally. The placement of the
boxes on the screen was chosen to allow us to determine if there were areas of the
screen that the users found difficult to reach, or were unable to reach. Different sized
boxes were used to evaluate the smallest area that the user can easily dwell in for a
few seconds. The size and location of the boxes was chosen so as to discern if it was
easier to dwell in smaller boxes in some areas of the screen. The use of color in the
boxes allows the user to recognize the area they are asked to move without having to
read through the labels.
1

3
2
4
6

5

7
8
Figure 4: Movement Evaluation Tool.

The users were asked to move the pointer in the following sequence, dwelling for
three seconds in each box: dark blue box labeled 3, yellow box labeled 7 green box
,
labeled 8, red box labeled 2, light blue box labeled 4, black box labeled 1, purple box
labeled 5, white box labeled 6.
Figure 5 shows a user with multiple sclerosis performing a subset of steps in
the movement evaluation test. It is apparent from the test that despite being restricted
to only slight movements of the head, the user was able to reach all areas of the
screen, including corners, and could dwell even in small regions.
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the green box
labeled 8 to the red box
labeled 2.

8
1

3
2

4
5

6

7

Instruction:
User is asked to move
from the red box
labeled 2 to the light
blue box labeled 4.

8
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the light blue box
labeled 4 to the black box
labeled 1.

8
Figure 5: User with multiple sclerosis while performing movement evaluation test (left),
simultaneous screen shots depicting pointer location (center), and the instruction given (right).
(Note: Pointer is shown enhanced in the figure.)

Figure 6 shows the entire trajectory of pointer movement as a user performs the
movement evaluation test.

Figure 6: Pointer trajectory of the movement evaluation test.
The task in the tracker evaluation test was to move from one colored box to another
(Figure 4) and then focus on the box for several seconds. The test consisted of eight
tasks. The tracker evaluatio n tests showed that all ten users, with and without
disabilities, were able to move the pointer to every location. This indicates that we
were successful in designing a system that tracks features and translates feature
movement to pointer movement on the screen. Table 1 categorizes three levels of
movement error, no overshooting, overshooting once, and overshooting more than
once. Overshooting occurs when the mouse pointer moves beyond the target on the
screen. This did not prevent the user from selecting the desired target. The control
experiment was done using the standard mouse.

Table 1: Results of Movement Evaluation T est

Control

Group 1

Group 2

Average Completion Time
Average % of Tasks Complete d
on the First Trial
Average % of Not Overshooting

1.0 s
8/8 = 100%

1.8 s
8/8 = 100%

3.2 s*
7.5/8 = 94%

8/8 = 100%

4/8 = 50%

2/8 = 25%

Average % of Overshooting Once

0/8 = 0%

2/8 = 25%

2.5/8 = 31%

Average % of Overshooting more
than Once

0/8 = 0%

2/8 = 25%

3.5/8 = 44%

* We discounted the timing result of one of the eight assigned tasks for one user in Group 2 in
computing the average completion time. The reason was that, d
uring the test, the subject was
asking questions and the recorded time of 30 seconds did not reflect the actual time to move the
pointer, which was on average less than 3 seconds for the remaining seven task s performed by
this user.

3.2 Evaluating application d
esign
The tests in this section were designed to capture the usability of the application
framework with respect to the design and layout of the interface elements. The test
consisted of launching five applications in sequence, Text Entry (Keyboard
application), Common Sayings (speech synthesizer), View a webpage (open browser),
Games (open a game), and Play this Song (open a media player).
We were interested in determining how well users were able to navigate
through the menus (average completion time), how many times the users had to try
before they successfully launched the correct application (number of tasks completed
on the first trial), and how often the programs were launched unintentionally (percent
of unintentional launches). Table 2 presents the results.
Table 2: Application Evaluation Results

Control

Group 1

Group 2

Average Completion Time *
Number of Tasks Comp d
lete
on the First Trial **

5.0 s
1

Percent of Unintentional
Launches

0

6.3 s
5/5 = 100%
for 5 users
4/5 = 80% for
2 users
0/5 = 0%
for 5 users
2/7 = 29% for
1 user
3/8 = 38% for
1 user

9.4 s
5/5 = 100%
for user 1
3/5 = 60%
for user 2
0/5 = 0% for
user 1
4/9 = 44%
for use 2
r

*Actual task completion times for Group 1 and 2 were not significantly different. The
computed results for G
roup 2 were affected by the fact that users in G
roup 2 showed much
interest in the system; they stopped to discuss, ask questions, and give ide as. Such instances
skewed the average of the recorded times.
**The users needed more than one trial to complete a task due to unintentional launches. The
unintentional launches were instances where the user diverted from the test to discuss
something and hence caused unintentional launching of the applications. This forced them to
return to the main menu and repeat the task. This also highlights the need for a binary
switching mechanism to turn off the tracker when not in active use.

Another consideration for the application evaluation was the degree of independent
use, i.e., the degree to which the user can effectively use the application without
intervention, once it has been set up. This factor is difficult to measure quantitatively.
From personal observation we saw that the subjects were able to launch all of the
programs independently and interact with the applications. For example, using the
cascading menu selection strategy, they were able to launch and play a game, get back
to the main menu by ho vering above it and then launch and use a text entry
application.
The users were also provided with the opportunity to use the system on their
own, without a guided sequence of steps. This helped determine their opinion on the
overall use of the system. During this period unexpected problems with the system
could be identified. A survey was used to gather the opinions of the sample test group.
Issues were determined by analysis of survey questions and by personal
observation. The tests performed by Group 1 revealed several issues. It was observed
that after a program was launched it was not possible to return to the application
w ithout using the standard mouse. To resolve this issue, the system was configured
such that when the pointer moves over the title area of the partially occluded
application, the application is brought into the foreground. This assumes that the
programs opened will not take the full screen area.
Another issue noticed during preliminary testing was that the testers could
not easily i entify where to rest the pointer without causing a selection command to
d
occur. As a result, programs were opened unintentionally; the Midas touch problem
[21]. To resolve this issue, all areas where the pointer can rest were changed to have a
gray background color distinguishing them from the areas with a white background
that cause a selection command to be executed. The users of Group 2 also found that
the pointer had some jitter, due to the increased sensitivity. We propose a simple
averaging mechanism to solve this problem.
Users showed interest in the prospect of being able to write and save text and
send email autonomously using the camera mouse. Current users rely on a separate
application to enter the text and then the caregiver has to copy and paste the text into
an email application to dispatch the email. Users also expressed interest in a system
that allowed effective web browsing with the camera mouse.

4 Discussion
In summary, we developed a customizable camera-based human computer interaction
system and showed that people with and without disabilities can complete multiple
computing tasks with slight head movements. The improvements made to the camera
mouse have resulted in a robust feature tracker and a calibration of feature movement
to pointer movement that is specific for each individual user. Taking advantage of the
features of the camera mouse, our interaction system was able to provide hands -free
access to many common computing tasks. The test results show that users were able
to successfully open all of the programs available in our system with only a small
percentage of error. This provides evidence that we designed a user-friendly interface
with an effective navigation strategy. Survey results obtained from the test subjects
showed that their holistic experience of the system was positive and they especially
enjoyed playing the games.
Several of the test subjects in the first group used the system more than once.
Their ability to control the pointer movement and dwell in a selection area improved
as quickly as the second use. This indicates that the difference in average completion
time between the control experiment and the camera mouse experiment would be
reduced if all subjects were given more time to become accustomed to moving the
pointer with the camera mouse.
A possibility for extension is to provide automatic feature detection. This
would eliminate the dependence of tracking performance on the manual selection of
an appropriate feature. The type of features best suited for tracking with the camera
mouse was studied by Cloud et al. [24], who suggested that the tip of the nose was a
robust feature. Gorodnichy [25] also discussed the robustness of nose tracking. Our
experiments with the camera mouse showed similar results. Features on the sides of
the face were lost by the tracker frequently as they were occluded upon rotation of the
head. The outer tips of the eyes and features on the outer boundaries of the face were
similarly not suitable for tracking. Features that exhibited good contrast on the central
band of the face, e.g. the inner tip of the eyebrows, the tip of the nose and the outer
boundary of the top or bottom lip, were the best features to track with the camera
positioned so that it has a frontal view of the person’s face. Tracking a feature on the
lips may however be problematic if the user speaks during use. Features on the eye
were often lost during blinking. Also, experiments showed that if the user wore
glasses, especially of a dark color, features on the glasses, such as the bridge of the
glasses, were robust to track.
Directions for future work include:
• Providing an automatic feature detection method.
• Smoothing pointer jitter that resulted from the increased sensitivity.
• Navigation among the opened programs.
• Providing better internet browsing, t ext entry, and email programs.
• Designing interaction strategies that allow the camera mouse to be used with
standard, non-specialized applications. For example, adding features such as
generalized dwell that is decoupled from camera mouse enabled applications
and operates with the desired dwell radius on the entire screen. To overcome

•

the limitation of small interface elements found in many standard
applications, screen magnification could be used to magnify menus as the
pointer hovers above them. A binary switch could then be provided to toggle
to the magnified area and select menu items. A cursor lock could also be
used to aid selection of small interface elements.
Extension to usage scenarios within the ambient intelligence paradigm [26].
The computer vision strategy presented here as a pointer alternative can be
applied to menu selection tasks in common appliances such as telephones,
microwave ovens, web-enabled digital television (DTV) and CD players.

Acknowledgements
The authors thank David Young-Hong from The Boston Home for his help with the
experiments and for sharing his insights regarding technologies needed for people
with disabilities. This work was supported by NSF grants IIS-0093367, IIS-0329009,
and 0202067 .

References
1. National Multiple Sclerosis Society, http://www.nationalmssociety.org, accessed April 2006.
2. Microsoft Accessibility, http://www.microsoft.com/enable/research/agingpop.aspx, accessed
April 2006.
3. J. Gips, P. Olivieri, and J.J. Tecce, "Direct Control of the Computer through
Electrodes Placed Around the Eyes", Human- Computer Interaction: Applications
and Case Studies, M.J. Smith and G. Salvendy (eds.), Elsevier, pages 630-635. 1993.
4. Synapse Adaptive, http://www.synapseadaptive.com/prc/prchead.htm, accessed April 2006.
5. NaturalPoint SmartNAV, http://www.naturalpoint.com/smartnav/, accessed July 2006.
6. J.J. Magee, M.R. Scott, B.N. Waber and M. Betke, "EyeKeys: A Real-time Vision Interface
Based on Gaze Detection from a Low grade Video Camera," In Proceedings of the IEEE
Workshop on Real-Time Vision for Human-Computer Interaction (RTV4HCI), Washington,
D.C., July 2004.
7. M. Betke, J. Gips, and P. Fleming, “The camera mouse: Visual tracking of body features to
provide computer access for people with severe disabilities”, IEEE Transactions on Neural
Systems and Rehabilitation Engineering, 10:1, pages 1-10, March 2002.
8. D.O. Gorodnichy and G. Roth, “Nouse ‘Use your nose as a mouse’ perceptual vision
technology for hands-free games and interfaces ”, Procee dings of the International
Conference on Vision Interface (VI 2002), Calgary, Canada, May 2002.
9. Assistive Technologies, http://www.assistivetechnologies.com , accessed April 2006.
10. Apple Computer Disability Resources, http://www.apple.com/accessibility, accessed April
2006.
11. WiViK on-screen keyboard (virtual keyboard) software, http://www.wivik.com, accessed
April 2006.
12. The Dasher Project, http://www.inference.phy.cam.ac.uk/dasher, accessed April 2006.
13. J. Gips and J. Gips, "A Computer Program Based on Rick Hoyt's Spelling Method for
People with Profound Special Needs," Proceedings International Conference on Computers
Helping People with Special Needs (ICCHP 2000), Karlsruhe, pages 245-250.
14. B.N. Waber, J. J. Magee, and M. Betke, “Web Mediators for Accessible Browsing Boston
,”
University Computer Science Department Technical Report BUCS 2006-007, May 2006.

15. H. Larson and J. Gips, "A Web Browser for People with Quadriplegia." In Universal
Access in HCI: Inclusive Design in the Information Society, Proceedings of the
International Conference on Human-Computer Interaction, Crete, 2003, C. Stephanidis
(ed.), Lawrence Erlbaum Associates, pages 226-230, 2003.
16. OpenCV library. http://sourcforge.net/projects/opencvlibrary, accessed April 2006.
17. B.D. Lucas and T. Kanade . “An iterative image registration technique with an application to
stereo vision.” In Proceedings of the 7th International Joint Conference on Artificial
Intelligence (IJCAI), pages 674-679, Vancouver, Canada, April 1981.
18. C. Fagiani, M. Betke, and J. Gips, “Evaluation of tracking methods for human-computer
interaction.” In Proceedings of the IEEE Workshop on Applications in Computer Vision
(WACV 2002), pages 121-126, Orlando, Florida, December 2002.
19. “Human-centered design processes for interactive systems,” International Organization for
Standardization ISO 13407, 1999.
20. The Boston Home, http://www.thebostonhome.org, accessed April 2006.
21. R.J.K. Jacob, “What you look at is what you get,” Computer, 26:7, pages 65–66, July 1993.
22. M. Chau and M. Betke, “Real Time Eye Tracking and Blink Detection with USB
Cameras,” Boston University Computer Science Technical Report 2005-012, May 2005.
23. J. Lombardi and M. Betke, “A camera-based eyebrow tracker for hands-free computer
control via a binary switch”, In Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (U14All 2002), pages 199-200, Paris, France, October 2002.
24. R. L. Cloud, M. Betke, and J. Gips, “Experiments with a Camera- Based Human-Computer
Interface System.” In Proceedings of the 7th ERCIM Workshop "User Interfaces for All,"
UI4ALL 2002, pages 103-110, Paris, France, October 2002.
25. D.O. Gorodnichy, “On importance of nose for face tracking”, In Proceedings of the IEEE
International Conference on Automatic Face and Gesture Recognition (FG 2002), pages
188-196, Washington, D.C., May 2002.
26. A. Ferscha, “ Contextware: Bridging Physical and Virtual Worlds. ” In Proceedings of the
Ada-Europe Conference on Reliable Software Technologies, 2002.

Boston University Technical Report 2006-006

A customizable camera-based human computer
interaction system allowing people with disabilities
autonomous hands-free navigation of multiple
computing tasks
Wajeeh Akram, Laura Tiberii, and Margrit Betke
Department of Computer Science, Boston University
111 Cummington Street, Boston, MA 02215, USA
{wajeeha, ltiberii, betke}@cs.bu.edu

Abstract. Many people suffer from conditions that lead to deterioration of
motor control making access to the computer using traditional input devices
difficult. In particular, they may loose control of hand movement to the extent
that the standard mouse cannot be used as a pointing device. Most current
alternatives use markers or specialized hardware, for example, wearable
devices, to track and translate a user’s movement to pointer movement. These
approaches may be perceived as intrusive. Camera- based assistive systems that
use visual tracking of features on the user’s body often require cumbersome
manual adjustment. This paper introduces an enhanced computer vision based
strategy where features, for example on a user’s face, viewed through an
inexpensive USB camera, are tracked and translated to pointer movement. The
main contributions of this paper are (1) enhancing a video based interface with
a mechanism for mapping feature movement to pointer movement that allows
users to navigate to all areas of the screen even with very limited physical
movement and (2) providing a customizable, hierarchical navigation framework
for human computer interaction (HCI). This framework provides effective use
of the vision-based interface system for accessing multiple applications in an
autonomous setting. Experiments with several users show the effectiveness of
the mapping strategy and its usage within the application framework as a
practical tool for desktop users with disabilities.

Keywords: Computer-vision, assistive technology, alternative input devices,
video- based human-computer interfaces, autonomous navigation.

1 Introduction
Several conditions may cause computer users to be unable to use the standard mouse.
Paralysis from brain injury, stroke, multiple sclerosis, or Amyotrophic Lateral
Sclerosis (ALS, also called Lou Gehrig's disease) may cause the user to have very
little motor control except for limited head or eye movement. Loss o fine motor
f

control with age and muscle injuries may also make use of the standard mouse
difficult.
According to the National Multiple Sclerosis Society [1], approximately
400,000 Americans and 2 million individuals worldwide suffer from Multiple
Sclerosis, and about 200 people are diagnosed every week in the US. As such
conditions restrict physical mobility and often speaking capability, loss of the ability
to communicate is one of the most limiting problems for these individuals. Being able
to use computers for common tasks such as sending email and browsing the web
opens a huge avenue of possibility to improve quality of life.
A study by Forrester Research for Microsoft Corporation [2] presents
statistics on the need and significance of accessible technology. It is estimated that
about 17% (22.6 million) of computers users who suffer from severe impairments are
very likely to benefit from accessible technology. It is also postulated that the need for
accessibility devices may grow due to the increase in computer users above the age of
65 and the increase in the average age of computer users.
There has been extensive research in the domain of mouse alternatives as
accessibility aids for users who have very limited movement. Broadly, these efforts
can be divided into two main categories : systems that rely on specialized mechanical
or electronic hardware devices and camera-based systems. Mouse-actuated joysticks,
mechanical switches, breath-puffing straws, and electrodes placed on the user’s face
that measure movement of features are some of the strategies in the first category [3].
Many camera based systems track physical markers, for example, infrared markers
placed on the user’s body [4, 5] or markers on glasses. Systems that capture gaze
information often rely on infrared illumination or special headgear-mounted cameras;
a survey of these methods is provided by Magee et al. [6]. Most of these systems are
expensive, require special devices, and may be intrusive. In addition, significant
levels of technical expertise may be required to install and configure these systems.
Betke et al. [7] present ed a vision based solution called the camera mouse which
tracks features on a user’s body in a non-intrusive manner.
There has also been substantial work in developing applications for people
with disabilities [8, 9, 10]. Some existing applications include on-screen keyboards
[11], alternate text entry mechanisms [12, 13], games and learning aids for children
[7], and tools that interact with a web browser to make the internet more accessible
for camera mouse users [14, 15].
In this paper, we present a system that tracks features on the user’s body,
usually the face, and translates feature movement to pointer movement on the screen.
Our work builds on the camera mouse presented by Betke et al. [7], which proposed a
vision based feature tracking approach for pointer movement. Here, we present an
improved mapping strategy that allows translation of minimal feature movement to
pointer movement across the entire range of the screen. A framework for using the
camera mouse to carry out common tasks, with minimal intervention from a
caregiver, is also proposed. Experiments were conducted to determine how well the
users were able to access and perform each of the computing tasks in the HCI
framework. Test results have shown that the system successfully provides access to
common tasks such as opening games, web sites, text entry, and playing music.
The system is cost effective and requires little technical expertise of the user
and caregiver. Use or extension of the proposed system does not incur significant

cost, because the system was developed with open source technologies such as
OpenCV [16] and Java. The only additional hardware required, besides a personal
computer, is a low-cost USB camera. We refer to the interface system as the camera
mouse throughout this paper. However, as an alternative to the camera mouse [7], any
interface system, video-based or even the standard computer mouse that provides a
pointing and selection mechanism can be used with our HCI framework.

2 System Overview
The goal of our work is to provide a customizable camera-based human computer
interaction system allowing people with disabilities autonomous hands free navigation
of multiple computing tasks. We focus on two main aspects of the system; designing a
robust feature tracking strategy and an effective interaction approach that operates
optimally with a camera mouse. The following sections give an overview of the
components of the system.

2.1 Tracking features
This section describes our method to track a feature or set of features on the user’s
body, usually face, and convert the feature movement to pointer movement. The study
by Fagiani et al. [18] gives an experimental comparison of various tracking
mechanisms for use with the camera mouse and recommends either an optical flow or
correlation based tracker. We found the optical flow based algorithm to be both robust
and computationally efficient. Our system operates in real time on a computer with a
1.6 GHz processor, taking up on average less than 5% of processor time. This
demonstrates the use of the camera mouse as a background process that does not
affect the performance of other applications running on the system. Our camera
mouse implementation executes as a standalone application that moves the standard
windows pointer.
A USB Camera is connected to the computer and set up to capture a frontal
view of the user. On starting the application, a window with the video of the user is
displayed. The camera location should be adjusted so that the feature to be tracked is
in clear view. Typically, the user sits within 1 m of the camera. However, if the user is
very close to the camera, even a small physical movement can result in the feature
falling out of the camera’s field of view. Therefore, the distance from the camera
should be carefully adjusted such that the feature remains within the camera’s field of
view throughout the session.
The caregiver selects a feature on the user’s body by clicking at the desired
location of the input video stream. W e designed the system to automatically refine the
feature location by finding an image patch with the highest brightness gradient in the
11-by-11-pixel neighborhood of the manually selected feature [16]. The feature is
then tracked in subsequent frames using the Lucas-Kanade optical flow computation
[17]. We used a pyramid-based implementation of the Lucas -Kanade tracker provided
in Intel's OpenCV library [16].

2.2 Feature movement to pointer movement
Once the feature movement in pixels is known, an effective mapping from pixels of
movement in the video frames to pointer movement on the screen is required.
Pointing devices such as the standard mouse and mouse pad do not have an absolute
mapping of device movement to pointer movement. The pointer is moved in a
differential manner, governed by speed and acceleration parameters set by the user.
Similarly, the camera mouse cannot be used with any degree of flexibility if this
mapping is absolute: an absolute mapping would mean that the feature to be tracked
would have to move the same distance (in pixels, as viewed by the camera) as the
pointer is to move on the screen. Most users do not have such a large range of
movement and even if such movement were possible, it does not complement the
natural movement of a computer user as they view the computer screen. Therefore the
camera mouse operates the pointer in a relative manner.
A relative scheme of pointer movement must consider how to adjust for the
difference in scale of feature movement and pointer movement. The movement of the
detected feature must be scaled in some reasonable manner before being added to the
current pointer position. In previous systems, the scale factor is a user-customizable
setting. However, adjusting the scale factor manually is a cumbersome trial and error
process and requires intervention by a caregiver for manually entering scale factors.
The scale factor is pertinent to the usability of the system, because if the scale factor
is too low, all areas of the screen may not be reachable by the pointer. Alternatively, if
it is too high the pointer may become too sensitive and thus move too quickly.
It can be observed that the scale factor is a function of the user’s distance
from the screen, as well as the range of possible movement of the feature in both
horizontal and vertical directions. The user’s range of movement may be seriously
limited by motor dysfunction. The range of movement is also typically asymmetric in
the vertical and horizont al directions due to the fact that vertical rotation of the head
when viewing a standard computer screen is smaller than horizontal rotation.
From a usability point of view, the scaling factor should not be such that the
system requires the user to move in a way that interferes negatively with the user’s
visual focus on the screen. In other words, during facial feature tracking with the
camera mouse, feature movement and visual focus cannot be decoupled. Feature
movement required for effective use of the system should not be such that it causes a
strain on the visual focusing mechanism of the user.
Designing a mechanism to allow optimal setting of the scale factor by the
user is therefore important towards the end of improving system performance and
usability. A calibration phase was introduced to determine the optimal scale factor for
individual users. Calibration is performed in advance of a usage session. After a
feature is selected to be tracked, the users are lead through a calibration phase, in
which they are directed to rotate their head towards distinct markers shown on the
video stream, while retaining a comfortable view of the computer screen. The users
successively move towards markers on the top, bottom, left and right boundaries of
the screen (Figure 1). It is important to direct users to move within a comfortable
range of motion, which permits clear and non stressful visual focus on the screen.
Pointer movement is calibrated to the range of movement demonstrated by the user,
using a linear mappi g of demonstrated movement range to screen dimensions.
n

Figure 1: System Calibration: The small colored disk shown in the video indicates the tip of
the eyebrow has been selected as the feature to track. The larger disk on the boundary of the
video display window indicates the direction the user should move her head.

After performing the calibration phase once for a particular user and a specific
feature, in situations where the distance from the camera remains approximately the
same across sessions, for example, for a user in a wheelchair, the scale factors found
by the calibration phase may be saved in a user configuration file that can be loaded
for subsequent use.

2.3 Application framework
Applications often have to be tailored to work with the camera mouse, since the
effective movement resolution of the camera mouse is not enough to navigate
windows menus or operate standard windows applications. Several on-screen
keyboards, educational programs, and game applications are available for use with the
camera mouse. However, the user must rely on a caregiver to start the custom
application before they can start using it. If the user wants to start a new application
for another task, there is no means to navigate available program s autonomously
without the caregiver’s help. Our motivation in proposing a hierarchical framework
for application navigation is to provide the camera mouse user with an autonomous
experience with their computer, allowing them to perform common tasks of int erest

such as text entry, internet browsing, and entertainment applications in a manner that
is user friendly, requires little technical expertise, and is configurable to meet the
needs of individual users.
Several considerations must be kept in mind when designing an effective
interface [19].
• The user should be able to clearly identify the target being selected.
• Distinguishing information should be placed at the beginning of headings.
• Clear and simple language should be used.
• The design should be consistent.
• There should be clear navigation.
Our interface opens with a main menu that is a list of common tasks (Figure 2).
The main menu items configured in the test system are: Play this Song launches the
default media player and plays the chosen song, Text Entry launches an on-screen
keyboard, Common Sayings speaks saved text using a speech synthesis program, View
a webpage launches the default browser and displays the chosen website, and Games
launches games, such as Eagle Aliens [6], which have been developed to require only
pointer movement.

Figure 2: Main Menu of Interface.

The list of common tasks desired in the application varies depending on the interests
of each user. The system is designed so that menu items can be added, removed, or
modified. This allows the list to be customized for each individual user.
The user will choose the common task they desire in one of two modes,
select mode or scan mode. In select mode, the user moves the pointer to an item.
When the pointer reaches an item it is highlighted in blue, clearly identifying the
target to be selected. In scan mode, the application scans through the list of items
highlighting each item for a specified time interval. The time interval can be changed
to the length of time that is reasonable for the current user.
To facilitate autonomous use, a dwell feature is available to simulate a
selection command. The dwell feature acts as a timer. When an item is highlighted the
timer is started. If that item stays highlighted for a specified time interval, a selection
command is executed. The gray areas of the interface, shown in Figure 2, represent

rest areas where the pointer can dwell without causing a selection command to occur.
Gray was used to stress the inactive nature of such areas. The dwell feature can be
enabled or disabled, as alternate methods may be available to simulate pointer clicks,
such as blink detection [22], raised eyebrow detection [23], or use of a mechanical
switch.
The font size of the menu items was also a consideration for users who are
unable to sit close to the system due to wheelchairs. The system is designed so that
the font size can be increased or decreased as desired. Items on the main menu are
either links that directly launch programs or links that open a submenu. Every
submenu has the same font type and size. The same color is use to highlight the
d
menu items. This consistency helps maintain usability. A ‘Return to Main Menu’
option is always the last item in the submenu list. This feature supports clear
navigat ion among the various menus. When a submenu item is selected the program
associated wi h that menu item is launched. The ‘Return to Main Menu’ option is
t
displayed on the screen after the program is launched so that the user can return to the
system and navigate to other programs if desired. A strategy for navigation among
opened programs is proposed by our framework, but has not been implemented yet.
An example of navigating through the system and selecting a song to play is
shown in Figure 3.

Figure 3: Navigation from the main menu through the ‘Play this Song’ submenu to launch a
music player that automatically begins playing the selected song.

3 Experiments and Results
The system was tested to determine the performance of the tracking mechanism and
to understand its limitations, as well as to determine the usability of the application
framework proposed. Results from the first test provided input for the design of
interface elements for the application framework.
A test group consisting of 8 subjects did the first set of experiments (Group
1). The subjects were between 14 and 60 years of age with varying levels of computer
skills. The subjects did not have any functional limitations. The same set of users was
asked to perform a control test, where the same sequence of steps was performed with
a standard mouse (Control Group). The second test group (Group 2) consisted of two
patients from The Boston Home [20]. Both subjects suffered from functional
limitations that made it difficult or impossible to use the standard mouse. One of the

subjects was diagnosed with muscular dystrophy more than 15 years ago. His
condition causes muscle weakness and wasting in major joints and muscles, his
shoulders and hips have been affected most. The other subject was diagnosed with
multiple sclerosis more than 15 years ago. His condition causes muscle weakness,
limiting his ability to move his arms, hands, and neck. The limitation in neck
movement has resulted in a very small range of head movement.

3.1 Evaluating tracker performance
The tests were designed to record indicators of tracker performance. Specifically, we
focused on factors pertaining to the tracker’s ability to track features and translate
feature movement to pointer movement on the screen. Specific factors include:
• Effective Dwell Area: the smallest region within which the user can dwell
for 3 seconds. This will allow us to study the tradeoff between tracker
sensitivity and dwelling ability.
• Movement patterns that cause the tracker to lose features while tracking.
• Movement patterns that affect the smoothness of the tracker’s constructed
pointer movement.
A movement evaluation tool was developed to analyze the above factors (Figure
4). During the test, users were asked to move the pointer from box to box. The order
of movement between boxes was chosen so that we could evaluate the user’s ability
to move the pointer vertically, horizontally, and diagonally. The placement of the
boxes on the screen was chosen to allow us to determine if there were areas of the
screen that the users found difficult to reach, or were unable to reach. Different sized
boxes were used to evaluate the smallest area that the user can easily dwell in for a
few seconds. The size and location of the boxes was chosen so as to discern if it was
easier to dwell in smaller boxes in some areas of the screen. The use of color in the
boxes allows the user to recognize the area they are asked to move without having to
read through the labels.
1

3
2
4
6

5

7
8
Figure 4: Movement Evaluation Tool.

The users were asked to move the pointer in the following sequence, dwelling for
three seconds in each box: dark blue box labeled 3, yellow box labeled 7 green box
,
labeled 8, red box labeled 2, light blue box labeled 4, black box labeled 1, purple box
labeled 5, white box labeled 6.
Figure 5 shows a user with multiple sclerosis performing a subset of steps in
the movement evaluation test. It is apparent from the test that despite being restricted
to only slight movements of the head, the user was able to reach all areas of the
screen, including corners, and could dwell even in small regions.
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the green box
labeled 8 to the red box
labeled 2.

8
1

3
2

4
5

6

7

Instruction:
User is asked to move
from the red box
labeled 2 to the light
blue box labeled 4.

8
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the light blue box
labeled 4 to the black box
labeled 1.

8
Figure 5: User with multiple sclerosis while performing movement evaluation test (left),
simultaneous screen shots depicting pointer location (center), and the instruction given (right).
(Note: Pointer is shown enhanced in the figure.)

Figure 6 shows the entire trajectory of pointer movement as a user performs the
movement evaluation test.

Figure 6: Pointer trajectory of the movement evaluation test.
The task in the tracker evaluation test was to move from one colored box to another
(Figure 4) and then focus on the box for several seconds. The test consisted of eight
tasks. The tracker evaluatio n tests showed that all ten users, with and without
disabilities, were able to move the pointer to every location. This indicates that we
were successful in designing a system that tracks features and translates feature
movement to pointer movement on the screen. Table 1 categorizes three levels of
movement error, no overshooting, overshooting once, and overshooting more than
once. Overshooting occurs when the mouse pointer moves beyond the target on the
screen. This did not prevent the user from selecting the desired target. The control
experiment was done using the standard mouse.

Table 1: Results of Movement Evaluation T est

Control

Group 1

Group 2

Average Completion Time
Average % of Tasks Complete d
on the First Trial
Average % of Not Overshooting

1.0 s
8/8 = 100%

1.8 s
8/8 = 100%

3.2 s*
7.5/8 = 94%

8/8 = 100%

4/8 = 50%

2/8 = 25%

Average % of Overshooting Once

0/8 = 0%

2/8 = 25%

2.5/8 = 31%

Average % of Overshooting more
than Once

0/8 = 0%

2/8 = 25%

3.5/8 = 44%

* We discounted the timing result of one of the eight assigned tasks for one user in Group 2 in
computing the average completion time. The reason was that, d
uring the test, the subject was
asking questions and the recorded time of 30 seconds did not reflect the actual time to move the
pointer, which was on average less than 3 seconds for the remaining seven task s performed by
this user.

3.2 Evaluating application d
esign
The tests in this section were designed to capture the usability of the application
framework with respect to the design and layout of the interface elements. The test
consisted of launching five applications in sequence, Text Entry (Keyboard
application), Common Sayings (speech synthesizer), View a webpage (open browser),
Games (open a game), and Play this Song (open a media player).
We were interested in determining how well users were able to navigate
through the menus (average completion time), how many times the users had to try
before they successfully launched the correct application (number of tasks completed
on the first trial), and how often the programs were launched unintentionally (percent
of unintentional launches). Table 2 presents the results.
Table 2: Application Evaluation Results

Control

Group 1

Group 2

Average Completion Time *
Number of Tasks Comp d
lete
on the First Trial **

5.0 s
1

Percent of Unintentional
Launches

0

6.3 s
5/5 = 100%
for 5 users
4/5 = 80% for
2 users
0/5 = 0%
for 5 users
2/7 = 29% for
1 user
3/8 = 38% for
1 user

9.4 s
5/5 = 100%
for user 1
3/5 = 60%
for user 2
0/5 = 0% for
user 1
4/9 = 44%
for use 2
r

*Actual task completion times for Group 1 and 2 were not significantly different. The
computed results for G
roup 2 were affected by the fact that users in G
roup 2 showed much
interest in the system; they stopped to discuss, ask questions, and give ide as. Such instances
skewed the average of the recorded times.
**The users needed more than one trial to complete a task due to unintentional launches. The
unintentional launches were instances where the user diverted from the test to discuss
something and hence caused unintentional launching of the applications. This forced them to
return to the main menu and repeat the task. This also highlights the need for a binary
switching mechanism to turn off the tracker when not in active use.

Another consideration for the application evaluation was the degree of independent
use, i.e., the degree to which the user can effectively use the application without
intervention, once it has been set up. This factor is difficult to measure quantitatively.
From personal observation we saw that the subjects were able to launch all of the
programs independently and interact with the applications. For example, using the
cascading menu selection strategy, they were able to launch and play a game, get back
to the main menu by ho vering above it and then launch and use a text entry
application.
The users were also provided with the opportunity to use the system on their
own, without a guided sequence of steps. This helped determine their opinion on the
overall use of the system. During this period unexpected problems with the system
could be identified. A survey was used to gather the opinions of the sample test group.
Issues were determined by analysis of survey questions and by personal
observation. The tests performed by Group 1 revealed several issues. It was observed
that after a program was launched it was not possible to return to the application
w ithout using the standard mouse. To resolve this issue, the system was configured
such that when the pointer moves over the title area of the partially occluded
application, the application is brought into the foreground. This assumes that the
programs opened will not take the full screen area.
Another issue noticed during preliminary testing was that the testers could
not easily i entify where to rest the pointer without causing a selection command to
d
occur. As a result, programs were opened unintentionally; the Midas touch problem
[21]. To resolve this issue, all areas where the pointer can rest were changed to have a
gray background color distinguishing them from the areas with a white background
that cause a selection command to be executed. The users of Group 2 also found that
the pointer had some jitter, due to the increased sensitivity. We propose a simple
averaging mechanism to solve this problem.
Users showed interest in the prospect of being able to write and save text and
send email autonomously using the camera mouse. Current users rely on a separate
application to enter the text and then the caregiver has to copy and paste the text into
an email application to dispatch the email. Users also expressed interest in a system
that allowed effective web browsing with the camera mouse.

4 Discussion
In summary, we developed a customizable camera-based human computer interaction
system and showed that people with and without disabilities can complete multiple
computing tasks with slight head movements. The improvements made to the camera
mouse have resulted in a robust feature tracker and a calibration of feature movement
to pointer movement that is specific for each individual user. Taking advantage of the
features of the camera mouse, our interaction system was able to provide hands -free
access to many common computing tasks. The test results show that users were able
to successfully open all of the programs available in our system with only a small
percentage of error. This provides evidence that we designed a user-friendly interface
with an effective navigation strategy. Survey results obtained from the test subjects
showed that their holistic experience of the system was positive and they especially
enjoyed playing the games.
Several of the test subjects in the first group used the system more than once.
Their ability to control the pointer movement and dwell in a selection area improved
as quickly as the second use. This indicates that the difference in average completion
time between the control experiment and the camera mouse experiment would be
reduced if all subjects were given more time to become accustomed to moving the
pointer with the camera mouse.
A possibility for extension is to provide automatic feature detection. This
would eliminate the dependence of tracking performance on the manual selection of
an appropriate feature. The type of features best suited for tracking with the camera
mouse was studied by Cloud et al. [24], who suggested that the tip of the nose was a
robust feature. Gorodnichy [25] also discussed the robustness of nose tracking. Our
experiments with the camera mouse showed similar results. Features on the sides of
the face were lost by the tracker frequently as they were occluded upon rotation of the
head. The outer tips of the eyes and features on the outer boundaries of the face were
similarly not suitable for tracking. Features that exhibited good contrast on the central
band of the face, e.g. the inner tip of the eyebrows, the tip of the nose and the outer
boundary of the top or bottom lip, were the best features to track with the camera
positioned so that it has a frontal view of the person’s face. Tracking a feature on the
lips may however be problematic if the user speaks during use. Features on the eye
were often lost during blinking. Also, experiments showed that if the user wore
glasses, especially of a dark color, features on the glasses, such as the bridge of the
glasses, were robust to track.
Directions for future work include:
• Providing an automatic feature detection method.
• Smoothing pointer jitter that resulted from the increased sensitivity.
• Navigation among the opened programs.
• Providing better internet browsing, t ext entry, and email programs.
• Designing interaction strategies that allow the camera mouse to be used with
standard, non-specialized applications. For example, adding features such as
generalized dwell that is decoupled from camera mouse enabled applications
and operates with the desired dwell radius on the entire screen. To overcome

•

the limitation of small interface elements found in many standard
applications, screen magnification could be used to magnify menus as the
pointer hovers above them. A binary switch could then be provided to toggle
to the magnified area and select menu items. A cursor lock could also be
used to aid selection of small interface elements.
Extension to usage scenarios within the ambient intelligence paradigm [26].
The computer vision strategy presented here as a pointer alternative can be
applied to menu selection tasks in common appliances such as telephones,
microwave ovens, web-enabled digital television (DTV) and CD players.

Acknowledgements
The authors thank David Young-Hong from The Boston Home for his help with the
experiments and for sharing his insights regarding technologies needed for people
with disabilities. This work was supported by NSF grants IIS-0093367, IIS-0329009,
and 0202067 .

References
1. National Multiple Sclerosis Society, http://www.nationalmssociety.org, accessed April 2006.
2. Microsoft Accessibility, http://www.microsoft.com/enable/research/agingpop.aspx, accessed
April 2006.
3. J. Gips, P. Olivieri, and J.J. Tecce, "Direct Control of the Computer through
Electrodes Placed Around the Eyes", Human- Computer Interaction: Applications
and Case Studies, M.J. Smith and G. Salvendy (eds.), Elsevier, pages 630-635. 1993.
4. Synapse Adaptive, http://www.synapseadaptive.com/prc/prchead.htm, accessed April 2006.
5. NaturalPoint SmartNAV, http://www.naturalpoint.com/smartnav/, accessed July 2006.
6. J.J. Magee, M.R. Scott, B.N. Waber and M. Betke, "EyeKeys: A Real-time Vision Interface
Based on Gaze Detection from a Low grade Video Camera," In Proceedings of the IEEE
Workshop on Real-Time Vision for Human-Computer Interaction (RTV4HCI), Washington,
D.C., July 2004.
7. M. Betke, J. Gips, and P. Fleming, “The camera mouse: Visual tracking of body features to
provide computer access for people with severe disabilities”, IEEE Transactions on Neural
Systems and Rehabilitation Engineering, 10:1, pages 1-10, March 2002.
8. D.O. Gorodnichy and G. Roth, “Nouse ‘Use your nose as a mouse’ perceptual vision
technology for hands-free games and interfaces ”, Procee dings of the International
Conference on Vision Interface (VI 2002), Calgary, Canada, May 2002.
9. Assistive Technologies, http://www.assistivetechnologies.com , accessed April 2006.
10. Apple Computer Disability Resources, http://www.apple.com/accessibility, accessed April
2006.
11. WiViK on-screen keyboard (virtual keyboard) software, http://www.wivik.com, accessed
April 2006.
12. The Dasher Project, http://www.inference.phy.cam.ac.uk/dasher, accessed April 2006.
13. J. Gips and J. Gips, "A Computer Program Based on Rick Hoyt's Spelling Method for
People with Profound Special Needs," Proceedings International Conference on Computers
Helping People with Special Needs (ICCHP 2000), Karlsruhe, pages 245-250.
14. B.N. Waber, J. J. Magee, and M. Betke, “Web Mediators for Accessible Browsing Boston
,”
University Computer Science Department Technical Report BUCS 2006-007, May 2006.

15. H. Larson and J. Gips, "A Web Browser for People with Quadriplegia." In Universal
Access in HCI: Inclusive Design in the Information Society, Proceedings of the
International Conference on Human-Computer Interaction, Crete, 2003, C. Stephanidis
(ed.), Lawrence Erlbaum Associates, pages 226-230, 2003.
16. OpenCV library. http://sourcforge.net/projects/opencvlibrary, accessed April 2006.
17. B.D. Lucas and T. Kanade . “An iterative image registration technique with an application to
stereo vision.” In Proceedings of the 7th International Joint Conference on Artificial
Intelligence (IJCAI), pages 674-679, Vancouver, Canada, April 1981.
18. C. Fagiani, M. Betke, and J. Gips, “Evaluation of tracking methods for human-computer
interaction.” In Proceedings of the IEEE Workshop on Applications in Computer Vision
(WACV 2002), pages 121-126, Orlando, Florida, December 2002.
19. “Human-centered design processes for interactive systems,” International Organization for
Standardization ISO 13407, 1999.
20. The Boston Home, http://www.thebostonhome.org, accessed April 2006.
21. R.J.K. Jacob, “What you look at is what you get,” Computer, 26:7, pages 65–66, July 1993.
22. M. Chau and M. Betke, “Real Time Eye Tracking and Blink Detection with USB
Cameras,” Boston University Computer Science Technical Report 2005-012, May 2005.
23. J. Lombardi and M. Betke, “A camera-based eyebrow tracker for hands-free computer
control via a binary switch”, In Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (U14All 2002), pages 199-200, Paris, France, October 2002.
24. R. L. Cloud, M. Betke, and J. Gips, “Experiments with a Camera- Based Human-Computer
Interface System.” In Proceedings of the 7th ERCIM Workshop "User Interfaces for All,"
UI4ALL 2002, pages 103-110, Paris, France, October 2002.
25. D.O. Gorodnichy, “On importance of nose for face tracking”, In Proceedings of the IEEE
International Conference on Automatic Face and Gesture Recognition (FG 2002), pages
188-196, Washington, D.C., May 2002.
26. A. Ferscha, “ Contextware: Bridging Physical and Virtual Worlds. ” In Proceedings of the
Ada-Europe Conference on Reliable Software Technologies, 2002.

Boston University Technical Report 2006-006

A customizable camera-based human computer
interaction system allowing people with disabilities
autonomous hands-free navigation of multiple
computing tasks
Wajeeh Akram, Laura Tiberii, and Margrit Betke
Department of Computer Science, Boston University
111 Cummington Street, Boston, MA 02215, USA
{wajeeha, ltiberii, betke}@cs.bu.edu

Abstract. Many people suffer from conditions that lead to deterioration of
motor control making access to the computer using traditional input devices
difficult. In particular, they may loose control of hand movement to the extent
that the standard mouse cannot be used as a pointing device. Most current
alternatives use markers or specialized hardware, for example, wearable
devices, to track and translate a user’s movement to pointer movement. These
approaches may be perceived as intrusive. Camera- based assistive systems that
use visual tracking of features on the user’s body often require cumbersome
manual adjustment. This paper introduces an enhanced computer vision based
strategy where features, for example on a user’s face, viewed through an
inexpensive USB camera, are tracked and translated to pointer movement. The
main contributions of this paper are (1) enhancing a video based interface with
a mechanism for mapping feature movement to pointer movement that allows
users to navigate to all areas of the screen even with very limited physical
movement and (2) providing a customizable, hierarchical navigation framework
for human computer interaction (HCI). This framework provides effective use
of the vision-based interface system for accessing multiple applications in an
autonomous setting. Experiments with several users show the effectiveness of
the mapping strategy and its usage within the application framework as a
practical tool for desktop users with disabilities.

Keywords: Computer-vision, assistive technology, alternative input devices,
video- based human-computer interfaces, autonomous navigation.

1 Introduction
Several conditions may cause computer users to be unable to use the standard mouse.
Paralysis from brain injury, stroke, multiple sclerosis, or Amyotrophic Lateral
Sclerosis (ALS, also called Lou Gehrig's disease) may cause the user to have very
little motor control except for limited head or eye movement. Loss o fine motor
f

control with age and muscle injuries may also make use of the standard mouse
difficult.
According to the National Multiple Sclerosis Society [1], approximately
400,000 Americans and 2 million individuals worldwide suffer from Multiple
Sclerosis, and about 200 people are diagnosed every week in the US. As such
conditions restrict physical mobility and often speaking capability, loss of the ability
to communicate is one of the most limiting problems for these individuals. Being able
to use computers for common tasks such as sending email and browsing the web
opens a huge avenue of possibility to improve quality of life.
A study by Forrester Research for Microsoft Corporation [2] presents
statistics on the need and significance of accessible technology. It is estimated that
about 17% (22.6 million) of computers users who suffer from severe impairments are
very likely to benefit from accessible technology. It is also postulated that the need for
accessibility devices may grow due to the increase in computer users above the age of
65 and the increase in the average age of computer users.
There has been extensive research in the domain of mouse alternatives as
accessibility aids for users who have very limited movement. Broadly, these efforts
can be divided into two main categories : systems that rely on specialized mechanical
or electronic hardware devices and camera-based systems. Mouse-actuated joysticks,
mechanical switches, breath-puffing straws, and electrodes placed on the user’s face
that measure movement of features are some of the strategies in the first category [3].
Many camera based systems track physical markers, for example, infrared markers
placed on the user’s body [4, 5] or markers on glasses. Systems that capture gaze
information often rely on infrared illumination or special headgear-mounted cameras;
a survey of these methods is provided by Magee et al. [6]. Most of these systems are
expensive, require special devices, and may be intrusive. In addition, significant
levels of technical expertise may be required to install and configure these systems.
Betke et al. [7] present ed a vision based solution called the camera mouse which
tracks features on a user’s body in a non-intrusive manner.
There has also been substantial work in developing applications for people
with disabilities [8, 9, 10]. Some existing applications include on-screen keyboards
[11], alternate text entry mechanisms [12, 13], games and learning aids for children
[7], and tools that interact with a web browser to make the internet more accessible
for camera mouse users [14, 15].
In this paper, we present a system that tracks features on the user’s body,
usually the face, and translates feature movement to pointer movement on the screen.
Our work builds on the camera mouse presented by Betke et al. [7], which proposed a
vision based feature tracking approach for pointer movement. Here, we present an
improved mapping strategy that allows translation of minimal feature movement to
pointer movement across the entire range of the screen. A framework for using the
camera mouse to carry out common tasks, with minimal intervention from a
caregiver, is also proposed. Experiments were conducted to determine how well the
users were able to access and perform each of the computing tasks in the HCI
framework. Test results have shown that the system successfully provides access to
common tasks such as opening games, web sites, text entry, and playing music.
The system is cost effective and requires little technical expertise of the user
and caregiver. Use or extension of the proposed system does not incur significant

cost, because the system was developed with open source technologies such as
OpenCV [16] and Java. The only additional hardware required, besides a personal
computer, is a low-cost USB camera. We refer to the interface system as the camera
mouse throughout this paper. However, as an alternative to the camera mouse [7], any
interface system, video-based or even the standard computer mouse that provides a
pointing and selection mechanism can be used with our HCI framework.

2 System Overview
The goal of our work is to provide a customizable camera-based human computer
interaction system allowing people with disabilities autonomous hands free navigation
of multiple computing tasks. We focus on two main aspects of the system; designing a
robust feature tracking strategy and an effective interaction approach that operates
optimally with a camera mouse. The following sections give an overview of the
components of the system.

2.1 Tracking features
This section describes our method to track a feature or set of features on the user’s
body, usually face, and convert the feature movement to pointer movement. The study
by Fagiani et al. [18] gives an experimental comparison of various tracking
mechanisms for use with the camera mouse and recommends either an optical flow or
correlation based tracker. We found the optical flow based algorithm to be both robust
and computationally efficient. Our system operates in real time on a computer with a
1.6 GHz processor, taking up on average less than 5% of processor time. This
demonstrates the use of the camera mouse as a background process that does not
affect the performance of other applications running on the system. Our camera
mouse implementation executes as a standalone application that moves the standard
windows pointer.
A USB Camera is connected to the computer and set up to capture a frontal
view of the user. On starting the application, a window with the video of the user is
displayed. The camera location should be adjusted so that the feature to be tracked is
in clear view. Typically, the user sits within 1 m of the camera. However, if the user is
very close to the camera, even a small physical movement can result in the feature
falling out of the camera’s field of view. Therefore, the distance from the camera
should be carefully adjusted such that the feature remains within the camera’s field of
view throughout the session.
The caregiver selects a feature on the user’s body by clicking at the desired
location of the input video stream. W e designed the system to automatically refine the
feature location by finding an image patch with the highest brightness gradient in the
11-by-11-pixel neighborhood of the manually selected feature [16]. The feature is
then tracked in subsequent frames using the Lucas-Kanade optical flow computation
[17]. We used a pyramid-based implementation of the Lucas -Kanade tracker provided
in Intel's OpenCV library [16].

2.2 Feature movement to pointer movement
Once the feature movement in pixels is known, an effective mapping from pixels of
movement in the video frames to pointer movement on the screen is required.
Pointing devices such as the standard mouse and mouse pad do not have an absolute
mapping of device movement to pointer movement. The pointer is moved in a
differential manner, governed by speed and acceleration parameters set by the user.
Similarly, the camera mouse cannot be used with any degree of flexibility if this
mapping is absolute: an absolute mapping would mean that the feature to be tracked
would have to move the same distance (in pixels, as viewed by the camera) as the
pointer is to move on the screen. Most users do not have such a large range of
movement and even if such movement were possible, it does not complement the
natural movement of a computer user as they view the computer screen. Therefore the
camera mouse operates the pointer in a relative manner.
A relative scheme of pointer movement must consider how to adjust for the
difference in scale of feature movement and pointer movement. The movement of the
detected feature must be scaled in some reasonable manner before being added to the
current pointer position. In previous systems, the scale factor is a user-customizable
setting. However, adjusting the scale factor manually is a cumbersome trial and error
process and requires intervention by a caregiver for manually entering scale factors.
The scale factor is pertinent to the usability of the system, because if the scale factor
is too low, all areas of the screen may not be reachable by the pointer. Alternatively, if
it is too high the pointer may become too sensitive and thus move too quickly.
It can be observed that the scale factor is a function of the user’s distance
from the screen, as well as the range of possible movement of the feature in both
horizontal and vertical directions. The user’s range of movement may be seriously
limited by motor dysfunction. The range of movement is also typically asymmetric in
the vertical and horizont al directions due to the fact that vertical rotation of the head
when viewing a standard computer screen is smaller than horizontal rotation.
From a usability point of view, the scaling factor should not be such that the
system requires the user to move in a way that interferes negatively with the user’s
visual focus on the screen. In other words, during facial feature tracking with the
camera mouse, feature movement and visual focus cannot be decoupled. Feature
movement required for effective use of the system should not be such that it causes a
strain on the visual focusing mechanism of the user.
Designing a mechanism to allow optimal setting of the scale factor by the
user is therefore important towards the end of improving system performance and
usability. A calibration phase was introduced to determine the optimal scale factor for
individual users. Calibration is performed in advance of a usage session. After a
feature is selected to be tracked, the users are lead through a calibration phase, in
which they are directed to rotate their head towards distinct markers shown on the
video stream, while retaining a comfortable view of the computer screen. The users
successively move towards markers on the top, bottom, left and right boundaries of
the screen (Figure 1). It is important to direct users to move within a comfortable
range of motion, which permits clear and non stressful visual focus on the screen.
Pointer movement is calibrated to the range of movement demonstrated by the user,
using a linear mappi g of demonstrated movement range to screen dimensions.
n

Figure 1: System Calibration: The small colored disk shown in the video indicates the tip of
the eyebrow has been selected as the feature to track. The larger disk on the boundary of the
video display window indicates the direction the user should move her head.

After performing the calibration phase once for a particular user and a specific
feature, in situations where the distance from the camera remains approximately the
same across sessions, for example, for a user in a wheelchair, the scale factors found
by the calibration phase may be saved in a user configuration file that can be loaded
for subsequent use.

2.3 Application framework
Applications often have to be tailored to work with the camera mouse, since the
effective movement resolution of the camera mouse is not enough to navigate
windows menus or operate standard windows applications. Several on-screen
keyboards, educational programs, and game applications are available for use with the
camera mouse. However, the user must rely on a caregiver to start the custom
application before they can start using it. If the user wants to start a new application
for another task, there is no means to navigate available program s autonomously
without the caregiver’s help. Our motivation in proposing a hierarchical framework
for application navigation is to provide the camera mouse user with an autonomous
experience with their computer, allowing them to perform common tasks of int erest

such as text entry, internet browsing, and entertainment applications in a manner that
is user friendly, requires little technical expertise, and is configurable to meet the
needs of individual users.
Several considerations must be kept in mind when designing an effective
interface [19].
• The user should be able to clearly identify the target being selected.
• Distinguishing information should be placed at the beginning of headings.
• Clear and simple language should be used.
• The design should be consistent.
• There should be clear navigation.
Our interface opens with a main menu that is a list of common tasks (Figure 2).
The main menu items configured in the test system are: Play this Song launches the
default media player and plays the chosen song, Text Entry launches an on-screen
keyboard, Common Sayings speaks saved text using a speech synthesis program, View
a webpage launches the default browser and displays the chosen website, and Games
launches games, such as Eagle Aliens [6], which have been developed to require only
pointer movement.

Figure 2: Main Menu of Interface.

The list of common tasks desired in the application varies depending on the interests
of each user. The system is designed so that menu items can be added, removed, or
modified. This allows the list to be customized for each individual user.
The user will choose the common task they desire in one of two modes,
select mode or scan mode. In select mode, the user moves the pointer to an item.
When the pointer reaches an item it is highlighted in blue, clearly identifying the
target to be selected. In scan mode, the application scans through the list of items
highlighting each item for a specified time interval. The time interval can be changed
to the length of time that is reasonable for the current user.
To facilitate autonomous use, a dwell feature is available to simulate a
selection command. The dwell feature acts as a timer. When an item is highlighted the
timer is started. If that item stays highlighted for a specified time interval, a selection
command is executed. The gray areas of the interface, shown in Figure 2, represent

rest areas where the pointer can dwell without causing a selection command to occur.
Gray was used to stress the inactive nature of such areas. The dwell feature can be
enabled or disabled, as alternate methods may be available to simulate pointer clicks,
such as blink detection [22], raised eyebrow detection [23], or use of a mechanical
switch.
The font size of the menu items was also a consideration for users who are
unable to sit close to the system due to wheelchairs. The system is designed so that
the font size can be increased or decreased as desired. Items on the main menu are
either links that directly launch programs or links that open a submenu. Every
submenu has the same font type and size. The same color is use to highlight the
d
menu items. This consistency helps maintain usability. A ‘Return to Main Menu’
option is always the last item in the submenu list. This feature supports clear
navigat ion among the various menus. When a submenu item is selected the program
associated wi h that menu item is launched. The ‘Return to Main Menu’ option is
t
displayed on the screen after the program is launched so that the user can return to the
system and navigate to other programs if desired. A strategy for navigation among
opened programs is proposed by our framework, but has not been implemented yet.
An example of navigating through the system and selecting a song to play is
shown in Figure 3.

Figure 3: Navigation from the main menu through the ‘Play this Song’ submenu to launch a
music player that automatically begins playing the selected song.

3 Experiments and Results
The system was tested to determine the performance of the tracking mechanism and
to understand its limitations, as well as to determine the usability of the application
framework proposed. Results from the first test provided input for the design of
interface elements for the application framework.
A test group consisting of 8 subjects did the first set of experiments (Group
1). The subjects were between 14 and 60 years of age with varying levels of computer
skills. The subjects did not have any functional limitations. The same set of users was
asked to perform a control test, where the same sequence of steps was performed with
a standard mouse (Control Group). The second test group (Group 2) consisted of two
patients from The Boston Home [20]. Both subjects suffered from functional
limitations that made it difficult or impossible to use the standard mouse. One of the

subjects was diagnosed with muscular dystrophy more than 15 years ago. His
condition causes muscle weakness and wasting in major joints and muscles, his
shoulders and hips have been affected most. The other subject was diagnosed with
multiple sclerosis more than 15 years ago. His condition causes muscle weakness,
limiting his ability to move his arms, hands, and neck. The limitation in neck
movement has resulted in a very small range of head movement.

3.1 Evaluating tracker performance
The tests were designed to record indicators of tracker performance. Specifically, we
focused on factors pertaining to the tracker’s ability to track features and translate
feature movement to pointer movement on the screen. Specific factors include:
• Effective Dwell Area: the smallest region within which the user can dwell
for 3 seconds. This will allow us to study the tradeoff between tracker
sensitivity and dwelling ability.
• Movement patterns that cause the tracker to lose features while tracking.
• Movement patterns that affect the smoothness of the tracker’s constructed
pointer movement.
A movement evaluation tool was developed to analyze the above factors (Figure
4). During the test, users were asked to move the pointer from box to box. The order
of movement between boxes was chosen so that we could evaluate the user’s ability
to move the pointer vertically, horizontally, and diagonally. The placement of the
boxes on the screen was chosen to allow us to determine if there were areas of the
screen that the users found difficult to reach, or were unable to reach. Different sized
boxes were used to evaluate the smallest area that the user can easily dwell in for a
few seconds. The size and location of the boxes was chosen so as to discern if it was
easier to dwell in smaller boxes in some areas of the screen. The use of color in the
boxes allows the user to recognize the area they are asked to move without having to
read through the labels.
1

3
2
4
6

5

7
8
Figure 4: Movement Evaluation Tool.

The users were asked to move the pointer in the following sequence, dwelling for
three seconds in each box: dark blue box labeled 3, yellow box labeled 7 green box
,
labeled 8, red box labeled 2, light blue box labeled 4, black box labeled 1, purple box
labeled 5, white box labeled 6.
Figure 5 shows a user with multiple sclerosis performing a subset of steps in
the movement evaluation test. It is apparent from the test that despite being restricted
to only slight movements of the head, the user was able to reach all areas of the
screen, including corners, and could dwell even in small regions.
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the green box
labeled 8 to the red box
labeled 2.

8
1

3
2

4
5

6

7

Instruction:
User is asked to move
from the red box
labeled 2 to the light
blue box labeled 4.

8
1

3
2
4
6

5
7

Instruction:
User is asked to move
from the light blue box
labeled 4 to the black box
labeled 1.

8
Figure 5: User with multiple sclerosis while performing movement evaluation test (left),
simultaneous screen shots depicting pointer location (center), and the instruction given (right).
(Note: Pointer is shown enhanced in the figure.)

Figure 6 shows the entire trajectory of pointer movement as a user performs the
movement evaluation test.

Figure 6: Pointer trajectory of the movement evaluation test.
The task in the tracker evaluation test was to move from one colored box to another
(Figure 4) and then focus on the box for several seconds. The test consisted of eight
tasks. The tracker evaluatio n tests showed that all ten users, with and without
disabilities, were able to move the pointer to every location. This indicates that we
were successful in designing a system that tracks features and translates feature
movement to pointer movement on the screen. Table 1 categorizes three levels of
movement error, no overshooting, overshooting once, and overshooting more than
once. Overshooting occurs when the mouse pointer moves beyond the target on the
screen. This did not prevent the user from selecting the desired target. The control
experiment was done using the standard mouse.

Table 1: Results of Movement Evaluation T est

Control

Group 1

Group 2

Average Completion Time
Average % of Tasks Complete d
on the First Trial
Average % of Not Overshooting

1.0 s
8/8 = 100%

1.8 s
8/8 = 100%

3.2 s*
7.5/8 = 94%

8/8 = 100%

4/8 = 50%

2/8 = 25%

Average % of Overshooting Once

0/8 = 0%

2/8 = 25%

2.5/8 = 31%

Average % of Overshooting more
than Once

0/8 = 0%

2/8 = 25%

3.5/8 = 44%

* We discounted the timing result of one of the eight assigned tasks for one user in Group 2 in
computing the average completion time. The reason was that, d
uring the test, the subject was
asking questions and the recorded time of 30 seconds did not reflect the actual time to move the
pointer, which was on average less than 3 seconds for the remaining seven task s performed by
this user.

3.2 Evaluating application d
esign
The tests in this section were designed to capture the usability of the application
framework with respect to the design and layout of the interface elements. The test
consisted of launching five applications in sequence, Text Entry (Keyboard
application), Common Sayings (speech synthesizer), View a webpage (open browser),
Games (open a game), and Play this Song (open a media player).
We were interested in determining how well users were able to navigate
through the menus (average completion time), how many times the users had to try
before they successfully launched the correct application (number of tasks completed
on the first trial), and how often the programs were launched unintentionally (percent
of unintentional launches). Table 2 presents the results.
Table 2: Application Evaluation Results

Control

Group 1

Group 2

Average Completion Time *
Number of Tasks Comp d
lete
on the First Trial **

5.0 s
1

Percent of Unintentional
Launches

0

6.3 s
5/5 = 100%
for 5 users
4/5 = 80% for
2 users
0/5 = 0%
for 5 users
2/7 = 29% for
1 user
3/8 = 38% for
1 user

9.4 s
5/5 = 100%
for user 1
3/5 = 60%
for user 2
0/5 = 0% for
user 1
4/9 = 44%
for use 2
r

*Actual task completion times for Group 1 and 2 were not significantly different. The
computed results for G
roup 2 were affected by the fact that users in G
roup 2 showed much
interest in the system; they stopped to discuss, ask questions, and give ide as. Such instances
skewed the average of the recorded times.
**The users needed more than one trial to complete a task due to unintentional launches. The
unintentional launches were instances where the user diverted from the test to discuss
something and hence caused unintentional launching of the applications. This forced them to
return to the main menu and repeat the task. This also highlights the need for a binary
switching mechanism to turn off the tracker when not in active use.

Another consideration for the application evaluation was the degree of independent
use, i.e., the degree to which the user can effectively use the application without
intervention, once it has been set up. This factor is difficult to measure quantitatively.
From personal observation we saw that the subjects were able to launch all of the
programs independently and interact with the applications. For example, using the
cascading menu selection strategy, they were able to launch and play a game, get back
to the main menu by ho vering above it and then launch and use a text entry
application.
The users were also provided with the opportunity to use the system on their
own, without a guided sequence of steps. This helped determine their opinion on the
overall use of the system. During this period unexpected problems with the system
could be identified. A survey was used to gather the opinions of the sample test group.
Issues were determined by analysis of survey questions and by personal
observation. The tests performed by Group 1 revealed several issues. It was observed
that after a program was launched it was not possible to return to the application
w ithout using the standard mouse. To resolve this issue, the system was configured
such that when the pointer moves over the title area of the partially occluded
application, the application is brought into the foreground. This assumes that the
programs opened will not take the full screen area.
Another issue noticed during preliminary testing was that the testers could
not easily i entify where to rest the pointer without causing a selection command to
d
occur. As a result, programs were opened unintentionally; the Midas touch problem
[21]. To resolve this issue, all areas where the pointer can rest were changed to have a
gray background color distinguishing them from the areas with a white background
that cause a selection command to be executed. The users of Group 2 also found that
the pointer had some jitter, due to the increased sensitivity. We propose a simple
averaging mechanism to solve this problem.
Users showed interest in the prospect of being able to write and save text and
send email autonomously using the camera mouse. Current users rely on a separate
application to enter the text and then the caregiver has to copy and paste the text into
an email application to dispatch the email. Users also expressed interest in a system
that allowed effective web browsing with the camera mouse.

4 Discussion
In summary, we developed a customizable camera-based human computer interaction
system and showed that people with and without disabilities can complete multiple
computing tasks with slight head movements. The improvements made to the camera
mouse have resulted in a robust feature tracker and a calibration of feature movement
to pointer movement that is specific for each individual user. Taking advantage of the
features of the camera mouse, our interaction system was able to provide hands -free
access to many common computing tasks. The test results show that users were able
to successfully open all of the programs available in our system with only a small
percentage of error. This provides evidence that we designed a user-friendly interface
with an effective navigation strategy. Survey results obtained from the test subjects
showed that their holistic experience of the system was positive and they especially
enjoyed playing the games.
Several of the test subjects in the first group used the system more than once.
Their ability to control the pointer movement and dwell in a selection area improved
as quickly as the second use. This indicates that the difference in average completion
time between the control experiment and the camera mouse experiment would be
reduced if all subjects were given more time to become accustomed to moving the
pointer with the camera mouse.
A possibility for extension is to provide automatic feature detection. This
would eliminate the dependence of tracking performance on the manual selection of
an appropriate feature. The type of features best suited for tracking with the camera
mouse was studied by Cloud et al. [24], who suggested that the tip of the nose was a
robust feature. Gorodnichy [25] also discussed the robustness of nose tracking. Our
experiments with the camera mouse showed similar results. Features on the sides of
the face were lost by the tracker frequently as they were occluded upon rotation of the
head. The outer tips of the eyes and features on the outer boundaries of the face were
similarly not suitable for tracking. Features that exhibited good contrast on the central
band of the face, e.g. the inner tip of the eyebrows, the tip of the nose and the outer
boundary of the top or bottom lip, were the best features to track with the camera
positioned so that it has a frontal view of the person’s face. Tracking a feature on the
lips may however be problematic if the user speaks during use. Features on the eye
were often lost during blinking. Also, experiments showed that if the user wore
glasses, especially of a dark color, features on the glasses, such as the bridge of the
glasses, were robust to track.
Directions for future work include:
• Providing an automatic feature detection method.
• Smoothing pointer jitter that resulted from the increased sensitivity.
• Navigation among the opened programs.
• Providing better internet browsing, t ext entry, and email programs.
• Designing interaction strategies that allow the camera mouse to be used with
standard, non-specialized applications. For example, adding features such as
generalized dwell that is decoupled from camera mouse enabled applications
and operates with the desired dwell radius on the entire screen. To overcome

•

the limitation of small interface elements found in many standard
applications, screen magnification could be used to magnify menus as the
pointer hovers above them. A binary switch could then be provided to toggle
to the magnified area and select menu items. A cursor lock could also be
used to aid selection of small interface elements.
Extension to usage scenarios within the ambient intelligence paradigm [26].
The computer vision strategy presented here as a pointer alternative can be
applied to menu selection tasks in common appliances such as telephones,
microwave ovens, web-enabled digital television (DTV) and CD players.

Acknowledgements
The authors thank David Young-Hong from The Boston Home for his help with the
experiments and for sharing his insights regarding technologies needed for people
with disabilities. This work was supported by NSF grants IIS-0093367, IIS-0329009,
and 0202067 .

References
1. National Multiple Sclerosis Society, http://www.nationalmssociety.org, accessed April 2006.
2. Microsoft Accessibility, http://www.microsoft.com/enable/research/agingpop.aspx, accessed
April 2006.
3. J. Gips, P. Olivieri, and J.J. Tecce, "Direct Control of the Computer through
Electrodes Placed Around the Eyes", Human- Computer Interaction: Applications
and Case Studies, M.J. Smith and G. Salvendy (eds.), Elsevier, pages 630-635. 1993.
4. Synapse Adaptive, http://www.synapseadaptive.com/prc/prchead.htm, accessed April 2006.
5. NaturalPoint SmartNAV, http://www.naturalpoint.com/smartnav/, accessed July 2006.
6. J.J. Magee, M.R. Scott, B.N. Waber and M. Betke, "EyeKeys: A Real-time Vision Interface
Based on Gaze Detection from a Low grade Video Camera," In Proceedings of the IEEE
Workshop on Real-Time Vision for Human-Computer Interaction (RTV4HCI), Washington,
D.C., July 2004.
7. M. Betke, J. Gips, and P. Fleming, “The camera mouse: Visual tracking of body features to
provide computer access for people with severe disabilities”, IEEE Transactions on Neural
Systems and Rehabilitation Engineering, 10:1, pages 1-10, March 2002.
8. D.O. Gorodnichy and G. Roth, “Nouse ‘Use your nose as a mouse’ perceptual vision
technology for hands-free games and interfaces ”, Procee dings of the International
Conference on Vision Interface (VI 2002), Calgary, Canada, May 2002.
9. Assistive Technologies, http://www.assistivetechnologies.com , accessed April 2006.
10. Apple Computer Disability Resources, http://www.apple.com/accessibility, accessed April
2006.
11. WiViK on-screen keyboard (virtual keyboard) software, http://www.wivik.com, accessed
April 2006.
12. The Dasher Project, http://www.inference.phy.cam.ac.uk/dasher, accessed April 2006.
13. J. Gips and J. Gips, "A Computer Program Based on Rick Hoyt's Spelling Method for
People with Profound Special Needs," Proceedings International Conference on Computers
Helping People with Special Needs (ICCHP 2000), Karlsruhe, pages 245-250.
14. B.N. Waber, J. J. Magee, and M. Betke, “Web Mediators for Accessible Browsing Boston
,”
University Computer Science Department Technical Report BUCS 2006-007, May 2006.

15. H. Larson and J. Gips, "A Web Browser for People with Quadriplegia." In Universal
Access in HCI: Inclusive Design in the Information Society, Proceedings of the
International Conference on Human-Computer Interaction, Crete, 2003, C. Stephanidis
(ed.), Lawrence Erlbaum Associates, pages 226-230, 2003.
16. OpenCV library. http://sourcforge.net/projects/opencvlibrary, accessed April 2006.
17. B.D. Lucas and T. Kanade . “An iterative image registration technique with an application to
stereo vision.” In Proceedings of the 7th International Joint Conference on Artificial
Intelligence (IJCAI), pages 674-679, Vancouver, Canada, April 1981.
18. C. Fagiani, M. Betke, and J. Gips, “Evaluation of tracking methods for human-computer
interaction.” In Proceedings of the IEEE Workshop on Applications in Computer Vision
(WACV 2002), pages 121-126, Orlando, Florida, December 2002.
19. “Human-centered design processes for interactive systems,” International Organization for
Standardization ISO 13407, 1999.
20. The Boston Home, http://www.thebostonhome.org, accessed April 2006.
21. R.J.K. Jacob, “What you look at is what you get,” Computer, 26:7, pages 65–66, July 1993.
22. M. Chau and M. Betke, “Real Time Eye Tracking and Blink Detection with USB
Cameras,” Boston University Computer Science Technical Report 2005-012, May 2005.
23. J. Lombardi and M. Betke, “A camera-based eyebrow tracker for hands-free computer
control via a binary switch”, In Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (U14All 2002), pages 199-200, Paris, France, October 2002.
24. R. L. Cloud, M. Betke, and J. Gips, “Experiments with a Camera- Based Human-Computer
Interface System.” In Proceedings of the 7th ERCIM Workshop "User Interfaces for All,"
UI4ALL 2002, pages 103-110, Paris, France, October 2002.
25. D.O. Gorodnichy, “On importance of nose for face tracking”, In Proceedings of the IEEE
International Conference on Automatic Face and Gesture Recognition (FG 2002), pages
188-196, Washington, D.C., May 2002.
26. A. Ferscha, “ Contextware: Bridging Physical and Virtual Worlds. ” In Proceedings of the
Ada-Europe Conference on Reliable Software Technologies, 2002.

AN ACOUSTIC-PHONETIC FEATURE-BASED SYSTEM
FOR AUTOMATIC PHONEME RECOGNITION IN
CONTINUOUS SPEECH
Ahmed M. Abdelatty Ali(1), Jan Van der Spiegel(1), Paul Mueller(2), Gavin Haentjens(3) and Jeffrey Berman(1)
(1)

Dept. of Electrical Engineering, University of Pennsylvania, 200 south 33rd St., Philadelphia, PA 191046390, USA, (2)Corticon, Inc., 155 Hughes Rd., King of Prussia, PA 19406, USA, and (3)Dept. of Electrical and
Computer Engineering, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213-3890, USA.

ABSTRACT
An acoustic-phonetic feature- and knowledge-based system for
the automatic segmentation, broad categorization and fine
phoneme recognition of continuous speech is described. The
system uses an auditory-based front-end processing and
incorporates new knowledge-based algorithms to automatically
segments the speech into phoneme-like segments that are
further categorized into 4 main categories: sonorants, stops,
fricatives and silences. The final outputs from the system are 19
class phonemes which contain 7 stops, 6 fricatives, nasals and
semivowels, 4 vowel classes and silences. The system was
tested on continuous speech from 30 speakers having 7
different dialects from the TIMIT database which were not used
in the design process. The results are 92% accuracy for the
segmentation and categorization, 86% for the stop
classification, 90% for the fricative classification, 75% for the
nasal and semivowel extraction and 82% for the vowel
recognition. These results compare favorably with previous
phoneme classification results.

1.

INTRODUCTION

Automatic speech recognition (ASR) has been intensively
researched for more than four decades. In the last decade,
significant improvements and successes were achieved.
However, the understanding of the acoustic-phonetic
characteristics of speech, speech variability and speech
perception is far from complete. All of the state-of-the-art
systems are statistical data-driven systems that rely on training
to model our ignorance about speech. Knowledge-based
approaches and integrating acoustic-phonetic knowledge in
ASR systems has always been an interesting, though elusive,
area of research.
Our work is concerned with the design and implementation of a
new purely knowledge-based system to segment, categorize and
recognize phonemes in continuous speech. The acoustic
features that exist in the literature are evaluated and new
features are proposed. Both hard-decision and soft-decision
algorithms were devised for recognition and to form
articulatory-based features such as voicing, manner of
articulation and place of articulation. We concentrated more on
the obstruent class of phonemes (i.e. fricatives, stops and
affricates) due to their noisy, dynamic, relatively short, weak,
speaker- and context-dependent nature which made them one

of the most challenging phonemes to automatically recognize in
continuous speaker-independent speech.
This system could be used directly for knowledge-based
phoneme recognition, or it could be used as a front-end for a
statistical ASR system (such as Hidden Markov Model (HMM)
or Artificial Neural Network (ANN) systems). The softdecision algorithms of our system generate probability
estimates (certainty factors) for each phoneme class that could
be used as inputs to an HMM or ANN system. The system was
designed with the goal of performing acoustic-phonetic
recognition while minimizing error propagation and loss of
information in order to enable its integration with statistical
ASR systems.

2.

A block diagram of the system is shown in Fig. (1) and an
example of the output is shown in Fig. (2). The front-end
processing is an auditory-based system that is described in
detail in [2,6,13]. The output of the front-end processing is
passed to the segmentation and categorization system [3,6] that
uses the following features:
1.
2.
3.
4.
5.
6.
7.
8.

Total energy.
Spectral Center of Gravity (SCG).
Duration.
Low, medium and high frequency energy.
Formant transitions.
Silence detection.
Voicing detection.
Rate of change of energy in various frequency
bands.
9. Rate of change of SCG.
10. Most prominent peak frequency.
11. Rate of change of the most prominent peak
frequency.
12. Zero-crossing rate.
Using the above features in the rule-based algorithm described
in detail in [3,6], stops, fricatives, sonorants and silences were
extracted successfully with a 92% accuracy (4% substitution,
3% insertion and 1% deletion), when tested on continuous
speech from 30 speakers of 6 different dialects of American
English from the TIMIT database (more than 7000 phonemes
from 300 sentences).

III-118
0-7803-5474-5/99/$10.00(C)1999 IEEE

SYSTEM DESCRIPTION AND
RESULTS

After the extraction of the fricatives and the stops, a
classification system is encountered to classify the different
phonemes. The classification of the fricatives is divided into
voicing detection and place of articulation detection. The
feature used in the voicing detection of fricatives is the duration
of the unvoiced portion (DUP) where voicing is measured by
the presence of low frequency energy in either the mean-rate or
the synchrony outputs. If the DUP is below an empirically
determined threshold, the fricative is detected as voiced,
otherwise it is unvoiced.
The place of articulation detection of fricatives is performed
using the following features:
1.
2.

3.
4.
5.

The Most Dominant Peak (MDP) frequency from
the synchrony detector.
The Maximum Normalized Spectral Slope
(MNSS), defined as the ratio of the largest
spectral slope of the mean-rate spectrum to the
maximum total energy in the utterance. It
demonstrates the flatness and weakness
properties which characterize labial and dental
phonemes.
The Spectral Center of Gravity (SCG).
The Most Dominant Spectral Slope (MDSS)
from the synchrony output.
The Dominance Relative to the Highest Filters
(DRHF), defined as the difference between the
MDP synchrony value and that of the highest 3
filters.

The above features were extracted and manipulated using both
hard-decision (binary output) and soft-decision (certainty
factors or probability estimates) algorithms. Soft decision
algorithms are richer in information and more useful especially
for generating features (probability estimates) that could be
used by a following classification system while minimizing any
information loss due to an erroneous decision.
Stop detection is also divided into voicing detection followed
by place of articulation detection. The features used in the
voicing detection are:
1.
2.
3.

Prevoicing, defined as presence of voicing during
the closure period.
Voicing Onset Time (VOT), defined as the
duration from the release to the start of voicing.
Closure duration.

The place of articulation detection of stop consonants was
performed using the following features:
1.

2.
3.
4.

5.

The Burst Frequency (BF), defined as the
frequency of the most prominent peak during the
release burst of the stop.
The second formant of the following vowel (in
prevocalic stops).
The Maximum Normalized Spectral Slope
(MNSS), similar to that used for the fricatives.
The burst frequency prominence, as described by
two features, namely the DRHF described above,
and the LINP which is the value of the most
prominent peak of the synchrony response after
being laterally inhibited by the higher 10 filters.
Formant transitions before and after the stop.

6.

3.
CONCLUSION AND
COMPARISON WITH PREVIOUS WORK
Despite the recent successes in the Automatic Speech
Recognition (ASR) field, the acoustic-phonetic characteristics
of speech and their variability with context and speaker are not
fully understood yet. More research is still needed to achieve a
good understanding of this topic in order to build improved
front-end processing systems that are able to extract the useful,
information-rich, acoustic features. This knowledge is expected
to have a profound effect on the automatic speech recognition
systems whose performance can significantly improve by
integrating more knowledge into their design.
Our work is concerned with this problem. We studied the
acoustic-phonetic characteristics of continuous speech from
multiple speakers with different dialects from the TIMIT
database. The acoustic features described in our work and the
algorithms developed to extract them are expected to be a
significant contribution to the acoustic characterization and the
automatic recognition of continuous speech. Our work builds
on the previous work in this area and introduces modifications
and additions that resulted in profound improvements in the
overall system performance.
This work could be exploited in Hidden Markov Models
(HMMs) or Artificial Neural Network (ANN) speech
recognition systems. These systems could make use of the
designed system outputs in order to improve the front-end
processing and create additional inputs to the data-driven
(training-based) classifiers. These inputs are rich in information
and incorporate considerable speech knowledge in their design.
This is especially true with the soft-decision algorithms whose
output is in the form of posterior probability estimates which
are compatible with HMMs and could be used as additional
inputs that are rich in information and independent of speaker
or context. They can also be used in a knowledge-based
acoustic-phonetic speech recognition system by making a hard
decision directly from the probability estimates.
The acoustic-phonetic recognition system developed was tested
on the TIMIT database continuous speech of 30 speakers from
6 different dialects of American English. Since no similar
system was developed before, we are going to compare the
results of the system’s subtasks with those of previous research.
For the segmentation and categorization, an accuracy of 92% is
achieved which compares favorably with the best previous
results obtained by Liu [11], who used landmark detection for
this purpose and obtained 90% on a similar task. The accuracy
of extracting the nasals and semivowels and the recognition of
the vowels categories were 75% and 82% respectively which
are comparable to previous results [9]. For fricatives, a
recognition accuracy of 93% is achieved for the place of
articulation, 95% for voicing detection and 90% for the
fricative classification. The best previous result was 77%-80%
for place detection and 83% for voicing detection [10,14]. For
the stops, 90% accuracy is achieved for the place detection,
97% for the voicing detection and 86% for the stop
classification. These results compare favorably too with the

III-119
0-7803-5474-5/99/$10.00(C)1999 IEEE

The voicing decision.

best previous which was 75%-80% classification accuracy
using knowledge-based approaches and 82% using statistical
(training-based) approaches [7,8,12]. A detailed comparison is
given by Ali in [1-6]. Such significant improvement is mainly
due to the use of auditory-based front-end processing, and new
feature extraction techniques and manipulation algorithms,
which integrate several acoustic properties in the decision
making process.

[13] Seneff, S., “A Joint Synchrony/Mean Rate Model of
Auditory Speech Processing”, J. of Phonetics, 16, pp. 5576, 1988.
[14] Stevens, K.N., et al, “Acoustic and perceptual
characteristics of voicing in fricatives and fricative
clusters”, J. Acoust. Soc. Am., 91, pp. 2979-3000, 1992.
Input speech
Front-end Processing

4.

ACKNOWLEDGMENT
Preliminary Segmentation
And Categorization

This work was supported by the National Science Foundation
(NSF REU Grant no. EEC96-19852) and Catalyst Foundation.

5.

REFERENCES

Silences

[1] Ali, A.M.A., et al., “Acoustic-phonetic Features for the
Automatic Recognition of stop consonants”, Journal of
the Acoustical Society of America, 103(5), pp. 2777-2778,
1998.
[2] Ali, A.M.A., et al., “An acoustic-phonetic feature-based
system for the automatic recognition of fricative
consonants”, Proc. IEEE ICASSP’98, pp.961-964, 1998.
[3] Ali, A.M.A., et al., “Automatic detection and
classification of stop consonants using an acousticphonetic feature-based system”, 14th International
Congress of Phonetic Sciences, (accepted), 1999.
[4] Ali, A. M. A., “Acoustic features for the automatic
recognition of fricatives”, Technical Report, TRCST27AUG97, Center for Sensor Technologies,
University of Pennsylvania, 1997.
[5] Ali, A.M.A., “Acoustic-phonetic Features for the
Automatic Recognition of Stop Consonants”, Technical
Report,
TR-CST22DEC97,
Center
for
Sensor
Technologies, University of Pennsylvania, 1997.
[6] Ali, A. M. A., “Segmentation and Categorization of
phonemes in continuous speech”, Technical Report, TRCST25JUL98, Center for Sensor Technologies, University
of Pennsylvania, 1998.
[7] Bush, M. A., et al., “Selecting acoustic features for stop
consonant identification”, Proc. ICASSP, 1983.
[8] De Mori, R. and Flammia, G., “Speaker-independent
consonant classification in continuous speech with
distinctive features and neural networks”, J. Acoust. Soc.
Am., 94 (6), pp. 3091-3103, 1993.
[9] De Mori, R. and Suen, C.Y., “New Systems and
Architectures for Automatic Speech Recognition”,
Springer Verlag, 1985.
[10] Hughes, G. W. and Halle, M., “Spectral Properties of
Fricative Consonants”, J. Acoust. Soc. Am., 28, pp. 303310, 1956.
[11] Liu, S.A., “Landmark detection for distinctive featurebased speech recognition”, J. Acoust. Soc. Am., 100 (5),
pp. 3417-3430, 1996.
[12] Searle, C. J. et al, “Stop consonant discrimination based
on human audition”, J. Acoust. Soc. Am., 65 (3), pp. 799809, 1979.

Sonorants

Obstruent Segmentation

Stops

Fine Sonorant
Identification

Fricatives

Voicing
Detection

Segmentation within
a Sonorant Segment

Voicing
Detection

Place of
Articulation
Detection

Place of
Articulation
Detection

Recognized Phoneme Certainty Factors

Fig. (1) (a) Block diagram of the system.
Input

Hair-Cell Synapse Model

Critical Band Filters
Half-wave Rectification and
Saturating Non-linearity
Low-Pass Filter
AGC
GSD
Synchrony

Short-term Adaptation
AGC
Envelope Detector
Mean-Rate

Fig. (1) (b) Block diagram of the auditory-based frontend processing system.

III-120
0-7803-5474-5/99/$10.00(C)1999 IEEE

Obstruents

Fig. (2) An example of the output from the system for the phrase “dark suit in” spoken by a female speaker. It shows the two
types of pseudo-spectrograms (mean-rate and synchrony), the detected categories (fricatives, stops and sonorants) and the fine
phoneme recognition. The recognized phonemes are listed at the bottom of the figure with the corresponding certainty factors.

III-121
0-7803-5474-5/99/$10.00(C)1999 IEEE

AN ACOUSTIC-PHONETIC FEATURE-BASED SYSTEM
FOR AUTOMATIC PHONEME RECOGNITION IN
CONTINUOUS SPEECH
Ahmed M. Abdelatty Ali(1), Jan Van der Spiegel(1), Paul Mueller(2), Gavin Haentjens(3) and Jeffrey Berman(1)
(1)

Dept. of Electrical Engineering, University of Pennsylvania, 200 south 33rd St., Philadelphia, PA 191046390, USA, (2)Corticon, Inc., 155 Hughes Rd., King of Prussia, PA 19406, USA, and (3)Dept. of Electrical and
Computer Engineering, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213-3890, USA.

ABSTRACT
An acoustic-phonetic feature- and knowledge-based system for
the automatic segmentation, broad categorization and fine
phoneme recognition of continuous speech is described. The
system uses an auditory-based front-end processing and
incorporates new knowledge-based algorithms to automatically
segments the speech into phoneme-like segments that are
further categorized into 4 main categories: sonorants, stops,
fricatives and silences. The final outputs from the system are 19
class phonemes which contain 7 stops, 6 fricatives, nasals and
semivowels, 4 vowel classes and silences. The system was
tested on continuous speech from 30 speakers having 7
different dialects from the TIMIT database which were not used
in the design process. The results are 92% accuracy for the
segmentation and categorization, 86% for the stop
classification, 90% for the fricative classification, 75% for the
nasal and semivowel extraction and 82% for the vowel
recognition. These results compare favorably with previous
phoneme classification results.

1.

INTRODUCTION

Automatic speech recognition (ASR) has been intensively
researched for more than four decades. In the last decade,
significant improvements and successes were achieved.
However, the understanding of the acoustic-phonetic
characteristics of speech, speech variability and speech
perception is far from complete. All of the state-of-the-art
systems are statistical data-driven systems that rely on training
to model our ignorance about speech. Knowledge-based
approaches and integrating acoustic-phonetic knowledge in
ASR systems has always been an interesting, though elusive,
area of research.
Our work is concerned with the design and implementation of a
new purely knowledge-based system to segment, categorize and
recognize phonemes in continuous speech. The acoustic
features that exist in the literature are evaluated and new
features are proposed. Both hard-decision and soft-decision
algorithms were devised for recognition and to form
articulatory-based features such as voicing, manner of
articulation and place of articulation. We concentrated more on
the obstruent class of phonemes (i.e. fricatives, stops and
affricates) due to their noisy, dynamic, relatively short, weak,
speaker- and context-dependent nature which made them one

of the most challenging phonemes to automatically recognize in
continuous speaker-independent speech.
This system could be used directly for knowledge-based
phoneme recognition, or it could be used as a front-end for a
statistical ASR system (such as Hidden Markov Model (HMM)
or Artificial Neural Network (ANN) systems). The softdecision algorithms of our system generate probability
estimates (certainty factors) for each phoneme class that could
be used as inputs to an HMM or ANN system. The system was
designed with the goal of performing acoustic-phonetic
recognition while minimizing error propagation and loss of
information in order to enable its integration with statistical
ASR systems.

2.

A block diagram of the system is shown in Fig. (1) and an
example of the output is shown in Fig. (2). The front-end
processing is an auditory-based system that is described in
detail in [2,6,13]. The output of the front-end processing is
passed to the segmentation and categorization system [3,6] that
uses the following features:
1.
2.
3.
4.
5.
6.
7.
8.

Total energy.
Spectral Center of Gravity (SCG).
Duration.
Low, medium and high frequency energy.
Formant transitions.
Silence detection.
Voicing detection.
Rate of change of energy in various frequency
bands.
9. Rate of change of SCG.
10. Most prominent peak frequency.
11. Rate of change of the most prominent peak
frequency.
12. Zero-crossing rate.
Using the above features in the rule-based algorithm described
in detail in [3,6], stops, fricatives, sonorants and silences were
extracted successfully with a 92% accuracy (4% substitution,
3% insertion and 1% deletion), when tested on continuous
speech from 30 speakers of 6 different dialects of American
English from the TIMIT database (more than 7000 phonemes
from 300 sentences).

III-118
0-7803-5474-5/99/$10.00(C)1999 IEEE

SYSTEM DESCRIPTION AND
RESULTS

After the extraction of the fricatives and the stops, a
classification system is encountered to classify the different
phonemes. The classification of the fricatives is divided into
voicing detection and place of articulation detection. The
feature used in the voicing detection of fricatives is the duration
of the unvoiced portion (DUP) where voicing is measured by
the presence of low frequency energy in either the mean-rate or
the synchrony outputs. If the DUP is below an empirically
determined threshold, the fricative is detected as voiced,
otherwise it is unvoiced.
The place of articulation detection of fricatives is performed
using the following features:
1.
2.

3.
4.
5.

The Most Dominant Peak (MDP) frequency from
the synchrony detector.
The Maximum Normalized Spectral Slope
(MNSS), defined as the ratio of the largest
spectral slope of the mean-rate spectrum to the
maximum total energy in the utterance. It
demonstrates the flatness and weakness
properties which characterize labial and dental
phonemes.
The Spectral Center of Gravity (SCG).
The Most Dominant Spectral Slope (MDSS)
from the synchrony output.
The Dominance Relative to the Highest Filters
(DRHF), defined as the difference between the
MDP synchrony value and that of the highest 3
filters.

The above features were extracted and manipulated using both
hard-decision (binary output) and soft-decision (certainty
factors or probability estimates) algorithms. Soft decision
algorithms are richer in information and more useful especially
for generating features (probability estimates) that could be
used by a following classification system while minimizing any
information loss due to an erroneous decision.
Stop detection is also divided into voicing detection followed
by place of articulation detection. The features used in the
voicing detection are:
1.
2.
3.

Prevoicing, defined as presence of voicing during
the closure period.
Voicing Onset Time (VOT), defined as the
duration from the release to the start of voicing.
Closure duration.

The place of articulation detection of stop consonants was
performed using the following features:
1.

2.
3.
4.

5.

The Burst Frequency (BF), defined as the
frequency of the most prominent peak during the
release burst of the stop.
The second formant of the following vowel (in
prevocalic stops).
The Maximum Normalized Spectral Slope
(MNSS), similar to that used for the fricatives.
The burst frequency prominence, as described by
two features, namely the DRHF described above,
and the LINP which is the value of the most
prominent peak of the synchrony response after
being laterally inhibited by the higher 10 filters.
Formant transitions before and after the stop.

6.

3.
CONCLUSION AND
COMPARISON WITH PREVIOUS WORK
Despite the recent successes in the Automatic Speech
Recognition (ASR) field, the acoustic-phonetic characteristics
of speech and their variability with context and speaker are not
fully understood yet. More research is still needed to achieve a
good understanding of this topic in order to build improved
front-end processing systems that are able to extract the useful,
information-rich, acoustic features. This knowledge is expected
to have a profound effect on the automatic speech recognition
systems whose performance can significantly improve by
integrating more knowledge into their design.
Our work is concerned with this problem. We studied the
acoustic-phonetic characteristics of continuous speech from
multiple speakers with different dialects from the TIMIT
database. The acoustic features described in our work and the
algorithms developed to extract them are expected to be a
significant contribution to the acoustic characterization and the
automatic recognition of continuous speech. Our work builds
on the previous work in this area and introduces modifications
and additions that resulted in profound improvements in the
overall system performance.
This work could be exploited in Hidden Markov Models
(HMMs) or Artificial Neural Network (ANN) speech
recognition systems. These systems could make use of the
designed system outputs in order to improve the front-end
processing and create additional inputs to the data-driven
(training-based) classifiers. These inputs are rich in information
and incorporate considerable speech knowledge in their design.
This is especially true with the soft-decision algorithms whose
output is in the form of posterior probability estimates which
are compatible with HMMs and could be used as additional
inputs that are rich in information and independent of speaker
or context. They can also be used in a knowledge-based
acoustic-phonetic speech recognition system by making a hard
decision directly from the probability estimates.
The acoustic-phonetic recognition system developed was tested
on the TIMIT database continuous speech of 30 speakers from
6 different dialects of American English. Since no similar
system was developed before, we are going to compare the
results of the system’s subtasks with those of previous research.
For the segmentation and categorization, an accuracy of 92% is
achieved which compares favorably with the best previous
results obtained by Liu [11], who used landmark detection for
this purpose and obtained 90% on a similar task. The accuracy
of extracting the nasals and semivowels and the recognition of
the vowels categories were 75% and 82% respectively which
are comparable to previous results [9]. For fricatives, a
recognition accuracy of 93% is achieved for the place of
articulation, 95% for voicing detection and 90% for the
fricative classification. The best previous result was 77%-80%
for place detection and 83% for voicing detection [10,14]. For
the stops, 90% accuracy is achieved for the place detection,
97% for the voicing detection and 86% for the stop
classification. These results compare favorably too with the

III-119
0-7803-5474-5/99/$10.00(C)1999 IEEE

The voicing decision.

best previous which was 75%-80% classification accuracy
using knowledge-based approaches and 82% using statistical
(training-based) approaches [7,8,12]. A detailed comparison is
given by Ali in [1-6]. Such significant improvement is mainly
due to the use of auditory-based front-end processing, and new
feature extraction techniques and manipulation algorithms,
which integrate several acoustic properties in the decision
making process.

[13] Seneff, S., “A Joint Synchrony/Mean Rate Model of
Auditory Speech Processing”, J. of Phonetics, 16, pp. 5576, 1988.
[14] Stevens, K.N., et al, “Acoustic and perceptual
characteristics of voicing in fricatives and fricative
clusters”, J. Acoust. Soc. Am., 91, pp. 2979-3000, 1992.
Input speech
Front-end Processing

4.

ACKNOWLEDGMENT
Preliminary Segmentation
And Categorization

This work was supported by the National Science Foundation
(NSF REU Grant no. EEC96-19852) and Catalyst Foundation.

5.

REFERENCES

Silences

[1] Ali, A.M.A., et al., “Acoustic-phonetic Features for the
Automatic Recognition of stop consonants”, Journal of
the Acoustical Society of America, 103(5), pp. 2777-2778,
1998.
[2] Ali, A.M.A., et al., “An acoustic-phonetic feature-based
system for the automatic recognition of fricative
consonants”, Proc. IEEE ICASSP’98, pp.961-964, 1998.
[3] Ali, A.M.A., et al., “Automatic detection and
classification of stop consonants using an acousticphonetic feature-based system”, 14th International
Congress of Phonetic Sciences, (accepted), 1999.
[4] Ali, A. M. A., “Acoustic features for the automatic
recognition of fricatives”, Technical Report, TRCST27AUG97, Center for Sensor Technologies,
University of Pennsylvania, 1997.
[5] Ali, A.M.A., “Acoustic-phonetic Features for the
Automatic Recognition of Stop Consonants”, Technical
Report,
TR-CST22DEC97,
Center
for
Sensor
Technologies, University of Pennsylvania, 1997.
[6] Ali, A. M. A., “Segmentation and Categorization of
phonemes in continuous speech”, Technical Report, TRCST25JUL98, Center for Sensor Technologies, University
of Pennsylvania, 1998.
[7] Bush, M. A., et al., “Selecting acoustic features for stop
consonant identification”, Proc. ICASSP, 1983.
[8] De Mori, R. and Flammia, G., “Speaker-independent
consonant classification in continuous speech with
distinctive features and neural networks”, J. Acoust. Soc.
Am., 94 (6), pp. 3091-3103, 1993.
[9] De Mori, R. and Suen, C.Y., “New Systems and
Architectures for Automatic Speech Recognition”,
Springer Verlag, 1985.
[10] Hughes, G. W. and Halle, M., “Spectral Properties of
Fricative Consonants”, J. Acoust. Soc. Am., 28, pp. 303310, 1956.
[11] Liu, S.A., “Landmark detection for distinctive featurebased speech recognition”, J. Acoust. Soc. Am., 100 (5),
pp. 3417-3430, 1996.
[12] Searle, C. J. et al, “Stop consonant discrimination based
on human audition”, J. Acoust. Soc. Am., 65 (3), pp. 799809, 1979.

Sonorants

Obstruent Segmentation

Stops

Fine Sonorant
Identification

Fricatives

Voicing
Detection

Segmentation within
a Sonorant Segment

Voicing
Detection

Place of
Articulation
Detection

Place of
Articulation
Detection

Recognized Phoneme Certainty Factors

Fig. (1) (a) Block diagram of the system.
Input

Hair-Cell Synapse Model

Critical Band Filters
Half-wave Rectification and
Saturating Non-linearity
Low-Pass Filter
AGC
GSD
Synchrony

Short-term Adaptation
AGC
Envelope Detector
Mean-Rate

Fig. (1) (b) Block diagram of the auditory-based frontend processing system.

III-120
0-7803-5474-5/99/$10.00(C)1999 IEEE

Obstruents

Fig. (2) An example of the output from the system for the phrase “dark suit in” spoken by a female speaker. It shows the two
types of pseudo-spectrograms (mean-rate and synchrony), the detected categories (fricatives, stops and sonorants) and the fine
phoneme recognition. The recognized phonemes are listed at the bottom of the figure with the corresponding certainty factors.

III-121
0-7803-5474-5/99/$10.00(C)1999 IEEE

Acoustic correlates of information structure 1

Acoustic correlates of information structure
Mara Breen1, Evelina Fedorenko2, Michael Wagner3, Edward Gibson2
1
2

University of Massachusetts Amherst
Massachusetts Institute of Technology
3
McGill University

June 7, 2010
Address correspondence to:
Mara Breen
522 Tobin Hall
University of Massachusetts
Amherst, MA
01003
mbreen@psych.umass.edu

Acoustic correlates of information structure 2
Abstract
This paper reports three studies aimed at addressing three questions about the acoustic
correlates of information structure in English: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? The information structure of subject-verb-object (SVO)
sentences was manipulated via the questions preceding those sentences: elements in the
target sentences were either focused (i.e. the answer to a wh-question) or given (i.e.
mentioned in prior discourse); furthermore, focused elements had either an implicit or an
explicit contrast set in the discourse; finally, either only the object was focused (narrow
object focus) or the entire event was focused (wide focus). The results across all three
experiments demonstrated that people reliably mark (a) focus location (subject, verb, or
object) using greater intensity, longer duration, and higher mean and maximum F0, and
(b) focus breadth, such that narrow object focus is marked with greater intensity, longer
duration, and higher mean and maximum F0 on the object than wide focus. Furthermore,
when participants are made aware of prosodic ambiguity present across different
information structures, they reliably mark focus type, so that contrastively-focused
elements are produced with higher intensity, longer duration, and lower mean and
maximum F0 than non-contrastively focused elements. In addition to having important
theoretical consequences for accounts of semantics and prosody, these experiments
demonstrate that linear residualization successfully removes individual differences in
people’s productions thereby revealing cross-speaker generalizations. Furthermore,
discriminant modeling allows us to objectively determine the acoustic features that
underlie meaning differences.

Acoustic correlates of information structure 3

Introduction
An important component of the meaning of a sentence is its relationship to the context in
which it is produced. Some parts of speakers’ sentences refer to information already
under discussion, while other parts convey information that the speaker is presenting as
new for the listener. Depending on the context, the same sentence can convey different
kinds of information to the listener. For example, consider the three contexts in (1a)-(1c)
for the sentence in (2):

(1) a. Who fried an omelet?
b. What did Damon do to an omelet?
c. What did Damon fry?

(2) Damon fried an omelet.

The event of frying an omelet is already made salient in the context in (1a), and
this part of the answer is therefore given. Consequently, the sentence Damon fried an
omelet conveys Damon as the new or focused information.1 Similarly, the verb fried is
the focused information relative to the context in (1b), and the object noun phrase an
omelet is the focused information relative to the context in (1c). This component of the
meaning of sentences - the differential contributions of different sentence elements to the

1

Numerous terms are used in the literature to refer to the distinction between the information that is old for
the listener and the information that the speaker is adding to the discourse: background and foreground;
given and new; topic and comment; theme and rheme, etc. In this paper, we will use the term given to refer
to the parts of the utterance which are old to the discourse, and focused to refer to the part of the utterance
which is new to the discourse.

Acoustic correlates of information structure 4
overall sentence meaning in its relation to the preceding discourse - is called information
structure.
Three components of information structure have been proposed in the literature:
givenness, focus, and topic (see e.g., Féry and Krifka, 2008, for a recent summary). The
current paper will be concerned with givenness and focus.2 Given material is material
that has been made salient in the discourse, either explicitly, like the event corresponding
to the verb fried and the object corresponding to the noun omelet in (1a), or implicitly, via
inferences based on world knowledge (e.g., mentioning omelet makes the notion of
“eggs” given, Schwarzchild, 1999).
Focused material is what is new to the discourse, or in the foreground. The focus
of a sentence can often be understood as the part that corresponds to the answer to the
wh-part of wh-questions, like Damon in (2) as an answer to (1a) (Paul, 1880; Jackendoff,
1972).
There are two dimensions along which focused elements can differ. The first is
contrastiveness. A contrastively focused element, like Damon in (3b), indicates that the
element in question is one of a set of explicit alternatives or serves to correct a specific
item already present in the discourse, as in the following:

(3) a. Did Harry fry an omelet yesterday?
b. Damon fried an omelet yesterday.

Unlike (1a), where there is no explicit set of individuals from which Damon is being
selected as the “omelet fryer”, in (3a) an explicit alternative “omelet fryer” is being

2

Topic, the third component of information structure, describes which discourse referent focused
information should be associated with, as in the mention of Damon in “As for Damon, he fried an omelet.”
The current studies do not address the prosodic realization of topic.

Acoustic correlates of information structure 5
introduced: Harry. The sentence (3b) in this context thus presents information (i.e.,
Damon) which explicitly contrasts with, or contradicts, some information which has been
introduced into the discourse.
There is no consensus in the literature regarding the relationship between noncontrastive focus and contrastive focus. Some researchers have treated non-contrastive
focus and contrastive focus as separate categories of information structure (Chafe, 1976;
Halliday, 1967; Rochemont, 1986; Molnar, 2002), whereas others have argued that there
is no principled difference between the two (e.g., Bolinger 1961, Rooth, 1985, Rooth,
1992). According to Rooth (1992), for example, each expression evokes two semantic
representations: the expression’s actual meaning, and a set of alternatives. If a
constituent in the expression is focused, then the alternative set contains the expression
itself and all expressions with an alternative substituted for the focus-marked constituent;
if there is no focus within the expression, the alternative set consists only of the
expression itself. Rooth would therefore argue that Damon in (1a) is focused and
introduces alternative propositions that differ only in the agent of the event ({Damon
fried an omelet, Harry fried an omelet, Ada fried an omelet, ...}), even if no alternatives
are explicitly mentioned. In (3a), Damon also evokes alternative omelet fryers, and
therefore has the same focus structure as (1a), but the context makes a specific alternative
(Harry) more salient than other potential alternatives. Importantly, from Rooth’s
standpoint, it does not matter whether the alternatives are explicit in the discourse or not:
the meaning of the expression is the same.
The second dimension along which focused elements can vary is focus breadth
(Selkirk, 1984; 1995; Gussenhoven, 1983; 1999), which refers to the size of the set of
focused elements. Narrow focus refers to cases where only a single aspect of an event
(e.g., the agent, the action, the patient, etc.) is focused, whereas wide focus focuses an

Acoustic correlates of information structure 6
entire event. Take, for example, the difference between (5) as an answer to (4a) versus as
an answer to (4b):

(4) a. What did Damon fry last night?
b. What happened last night?

(5) Damon fried an omelet last night.

(4a) narrowly focuses the patient of frying, omelet in (5), while (4b) widely focuses the
entire event of Damon frying an omelet.
The information status of a sentence element can be conveyed in at least three
ways: (1) using word order (i.e., given information generally precedes focused
information) (e.g., Birner, 1994, Clark & Clark, 1978); (2) using particular lexical items
and syntactic constructions (e.g., using cleft constructions such as “It was Damon who
fried an omelet”) (Lambrecht, 2001); and (3) using prosody. Prosody – which we focus
on in the current paper – refers to the way in which words are grouped in speech, the
relative acoustic prominence of words, and the overall tune of an utterance. Prosody is
comprised of acoustic features like fundamental frequency (F0), duration, and loudness,
the combinations of which give rise to the psychological percepts like phrasing
(grouping), stress (prominence), and tonal movement (intonation).
The goal of the current paper is to investigate the prosodic realization of
information structure in simple English subject-verb-object (SVO) sentences like (2),
with the goal of addressing the following questions:
1) First, do speakers prosodically distinguish focused and unfocused elements?
This question can be broken down into further questions:

Acoustic correlates of information structure 7
(1a) Do speakers distinguish focused elements that have an explicit contrast
set in the discourse from those that do not?
(1b) Do speakers distinguish sentences in which only the object is focused
from those in which the entire event is focused?
(2) What are the acoustic features associated with these different aspects of
information structure?
(3) How well can listeners retrieve this information from the signal?

Although the current experiments are all performed on English, the answers to
these questions will likely be similar for other West Germanic languages. However, the
relationship between prosodic features and information structure across different
languages and language groups remains an open question.
In the remainder of the introduction, we briefly lay out two approaches to the study
of the relationship between prosody and information structure, and summarize empirical
studies which have explored how information structure is realized acoustically and
prosodically. We then discuss methodological issues present in previous studies which
call into question the generalizeability of the reported findings, and outline how the
current methods were designed to better address these questions.
Empirical investigations of prosody and information structure
Two perspectives on the relationship between the acoustics of the speech signal and
the meaning associated with various aspects of information structure have been
articulated in the literature. According to the direct-relationship approach, sets of
acoustic features are directly associated with particular meanings (Fry, 1955; Lieberman,
1960; Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986; Pell, 2001; Xu & Xu,
2005). In contrast, according to the indirect-relationship approach (known as the

Acoustic correlates of information structure 8
intonational phonology framework), the relationship between acoustics and meaning is
mediated by phonological categories (Ladd, 1996; Gussenhoven, 1983; Pierrehumbert,
1980; Dilley, 2005; Hawkins & Warren, 1991). In particular, the phonetic prosodic cues
are hypothesized to be grouped into prosodic categories which are, in turn, associated
with particular meanings. The experiments in the current paper were not designed to
decide between these two approaches. However, In the current paper, we will initially
discuss our experiments in terms of the direct-relationship approach, because it is more
parsimonious. In the general discussion, we will show how the results are also
compatible with the indirect-relationship approach.
Turning now to previous empirical work on the relationship between prosody and
information structure, we start with studies of focused vs. given elements. Several
studies have demonstrated that focused elements are more acoustically prominent than
given elements. However, there has been some debate about which acoustic features
underlie a listener’s perception of acoustic prominence. Some features that have been
proposed to be associated with prominence include pitch (i.e. F0) (Lieberman, 1960;
Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986), duration (Fry, 1954; Beckman,
1986), loudness (i.e. intensity) (Kochanski, Grabe, Coleman, & Rosner, 2005; Beckman,
1986; Turk and Sawusch, 1996), and voice quality (Sluijter & van Heuven, 1996).
In early work on lexical stress, Fry (1954) and Liberman (1960) argued that
intensity and duration of the vowel of the stressed syllable contributed most strongly to
the percept of acoustic prominence, such that stressed vowels were produced with a
greater intensity and a longer duration than non-stressed vowels. In experiments on
phrase-level prominence, Cooper et al. (1985) and Eady and Cooper (1986) also noted
that more prominent syllables are longer than their non-prominent counterparts. Cooper
et al. (see also Liberman, 1960); Rietveld & Gussenhoven, 1985; Gussenhoven et al.,

Acoustic correlates of information structure 9
1997; and Terken, 1991) also argued that F0 was a highly important acoustic feature
underlying prominence. Others have argued that the strongest cue to prominence is
intensity (e.g., Beckman, 1986). More recently, Turk and Sawusch (1996) also found
that intensity (and duration) were better predictors of perceived prominence than pitch, in
a perception task. Finally, in a study of spoken corpora, Kochanski et al. (2005)
demonstrated that loudness (i.e. intensity) was a strong predictor of labelers’ annotations
of prominence, while pitch had very little predictive power.
The question of whether contrastively and non-contrastively focused elements are
prosodically differentiated by speakers, and perceptually differentiated by listeners has
also been extensively debated. Some have argued that there is no difference in the
acoustic features associated with contrastively vs. non-contrastively focused elements
(Cutler, 1977; Bolinger, 1961; t’Hart, Collier, & Cohen, 1990), while others have argued
that some acoustic features differ between contrastively vs. non-contrastively focused
elements (Couper-Kuhlen, 1984; Krahmer & Swerts, 2001; Bartels & Kingston, 1994;
Ito, Speer, & Beckman, 2004). For example, Couper-Kuhlen (1984) reported, on the
basis of corpus work, that speakers produce contrastive focus with a steep drop after a
high F0 target, while high F0 is sustained after non-contrastive focus (see also Krahmer
and Swerts, 2001). However, this finding is in contrast to Bartels and Kingston (1994),
who have argued, based on a series of production studies, that the most salient acoustic
cue to contrastiveness is the height of the peak on a contrastive word, such that a higher
peak is associated with a greater probability of an element being interpreted as
contrastive (see also Ladd and Morton, 1997). Finally, Ito, Speer, & Beckman (2004)
demonstrated that speakers are more likely to use a L+H* accent (i.e. a steep rise from a
low target to a high target), compared to a H* accent (i.e. a gradual rise to a high target),
to indicate an element that has an explicit contrast set in the discourse.

Acoustic correlates of information structure 10
Krahmer and Swerts (2001) observed that listeners were more likely to perceive a
contrastive adjective (e.g., red in red square preceded by blue square) as more prominent
than a new adjective when the adjective was presented with a noun compared to when it
was presented in isolation. They therefore hypothesized that the lack of a consensus in
the literature may be due to the failure of the earlier studies to investigate focused
elements in relation to the prosody of the surrounding elements. Consistent with this
idea, Calhoun (2005) demonstrated that a model’s ability to predict a word’s information
status is significantly improved when information about the acoustics of adjacent words
is included in the model. These results suggest that a more consistent picture of the
acoustic features associated with contrastively and non-contrastively-focused elements
may emerge if acoustic context is taken into account.
Finally, prior work has investigated whether speakers prosodically differentiate
narrow and wide focus. Selkirk (1995), for example, argued that, through a process
called focus projection, an acoustic prominence on the head of a phrase or its internal
argument can project to the entire phrase, thus making the entire phrase focused (see also
Selkirk, 1984; see Gussenhoven, 1983, 1999, for a similar claim). According to Selkirk
(1984) and Gussenhoven (1983) then a clause containing a transitive verb in which the
direct object is acoustically prominent is ambiguous between a reading where the object
alone is focused and a reading where the entire verb phrase is focused. This hypothesis
has been supported in several perception experiments (Welby, 2003; Birch & Clifton,
1995; Gussenhoven, 1983). Welby (2003), for example, demonstrated that listeners rated
a sentence like I read the DISPATCH with a single acoustic prominence on dispatch as a
similarly felicitous response to either a question narrowly focusing the object (i.e. “What
newspaper do you read?”), or a question widely focusing the entire event (i.e. “How do
you keep up with the news?”). However, Gussenhoven (1983) found that at least in some

Acoustic correlates of information structure 11
productions there is actually a perceptible difference between narrow and wide focus
although listeners cannot use this information to reliably tell in which context the
sentence was uttered (see Baumann et al., 2006, for evidence from German showing that
speakers do differentiate between narrow and wide focus, with prosodic cues varying
across speakers). In contrast to Gussenhoven’s perception results, Rump and Collier
(1986) found that listeners can accurately discriminate narrow and wide focus using pitch
cues.
Limitations of previous work
Although the studies summarized above provide evidence for some systematic
differences in the acoustic realization of different aspects of information structure, no
clear picture has yet emerged with regard to any of the three meaning distinctions
discussed above (i.e. focused vs. given elements, non-contrastively focused vs.
contrastively focused elements, and narrow vs. wide focus). Furthermore, previous
studies suffer from several methodological limitations that make the findings
inconclusive. Here, we discuss five limitations of previous studies which the current
studies seek to address in an effort to reveal a clearer picture of the relationship between
acoustic features and information structure.
First, instead of acoustic features, sometimes only ToBI3 annotations are
provided (e.g., Birch & Clifton, 1995; Ito et al., 2004). This includes work of researchers
who adopt the intonational phonology framework and who therefore believe that using
prosodic annotation offers a useful way to extrapolate away from potentially complex
interactions among acoustic features which give rise to the perception of specific
intonational patterns. One particular problem concerns H* and L+H* accents. As
defined in the ToBI system, these accents are meant to be explicit markers of non3

The (ToBI) Tones and Break Indices system was developed in the early 90s as the standard system for
annotation of prosodic features (Silverman et al., 1992).

Acoustic correlates of information structure 12
contrastive focus and contrastive focus, respectively (Beckman & Ayers-Elam, 1997).
However, H* and L+H* are often confused in ToBI annotations (Syrdal & McGory,
2000), and are, in fact, often collapsed in calculating inter-coder agreement (Pitrelli et al.,
1994; Yoon et al., 2004; Breen et al., 2006, submitted). Therefore, it is difficult to
interpret the results of studies which are based on the difference between H* and L+H*
without a discussion of the acoustic differences between these purported categories. In
the current studies, we report acoustic features in order to avoid confusion about what the
ToBI labels might mean and in order to not presuppose the existence of prosodic
categories associated with particular meaning categories of information structure.
A second limitation concerns the method used to generate and select productions
for analysis. A common practice involves eliciting productions from a small number of
speakers (e.g., Baumann et al., 2006; Krahmer & Swerts, 2001), which results in a
potential decrease in experimental power, and could therefore lead to a Type II error. In
addition, several previous experiments have excluded speakers’ data from analysis for not
producing accents consistently (e.g., Eady & Cooper, 1986; Cooper et al., 1985), which
could lead to a Type I error. For the current experiments, we recruited between 13 and
18 speakers. In addition, no speakers’ productions were excluded from the analyses
based on a priori predictions about potential behavior (e.g., placing accents in particular
locations).
A third limitation concerns the tasks used in perception studies. In particular,
some studies asked listeners to make judgments about which of two stimuli was more
prominent (Krahmer & Swerts, 2001), what accent is acceptable in a particular context
(Birch & Clifton, 1995; Welby, 2003), or with which of two questions a particular answer
sounded more natural (Gussenhoven, 1983). The problem with these meta-linguistic
judgments is that they lack a measure of the participants’ interpretation of the sentences.

Acoustic correlates of information structure 13
In the current studies we employ a more natural production-comprehension task, in which
speakers are trying to communicate a particular meaning of a semantically ambiguous
sentence and listeners are trying to understand the intended meaning.
A fourth limitation of previous studies is in how they have dealt with speaker
variability. Presenting data from individual subjects separately, as is commonly done, is
problematic because it fails to capture the shared aspects of individual productions (e.g.,
consistent use by most speakers of some set of acoustic features to mark focused
elements). In the current studies, we combine data across subjects while simultaneously
removing variance due to individual differences using linear regression modeling (e.g.,
Jaeger, 2008).
A fifth limitation is that many have reported differences between conditions based
only on individual acoustic features on single words (Eady & Cooper, 1986; Cooper et
al., 1985; Baumann et al, 2006). If acoustic prominence is perceived in a contextdependent manner, these single-feature/single-word analyses might find spurious
differences, or fail to find real differences. In the current studies, we used discriminant
modeling on the productions in order to simultaneously investigate the contribution of
multiple acoustic features from multiple words in an utterance to the interpretation of
information status of different sentence elements.

Experiments: Overview and general methods
The current paper presents results from three experiments. Experiment 1
investigated whether speakers prosodically disambiguate focus location (subject, verb,
object), focus type (contrastive vs. non-contrastive focus), and focus breadth (narrow vs.
wide) by eliciting semi-naturalistic productions like that in (3b) (e.g., Damon fried an
omelet this morning), whose information status was disambiguated by a preceding

Acoustic correlates of information structure 14
question. Experiment 2 investigated whether speakers disambiguate focus location and
focus type when the task explicitly required them to communicate a particular meaning to
their listeners. Finally, Experiment 3 served as a replication and extension of Experiment
2, in which speakers included an attribution expression (“I heard that”) before the critical
sentence.
The acoustic analysis of the productions elicited in all three experiments
proceeded in three steps. First, we automatically extracted a series of 24 acoustic features
(see Table 2) from the subject, verb, and object of the sentences elicited in Experiments
1, 2, and 3. Second, we subjected all of these features to a stepwise discriminant function
analysis in order to determine which features best discriminated the information status
conditions listed in Table 1 for each of the three experiments. This analysis resulted in a
subset of eight acoustic features. Finally, we used discriminant analyses to evaluate
whether this subset of eight features could effectively discriminate sets of 2 and 3
conditions for each of the three experiments. Specifically, we tested focus location by
comparing the features from productions in which Damon, fried, and omelet were
focused, respectively. We tested focus type by comparing the features from sentences in
which the focused element was contrastively or non-contrastively focused at each of the
three syntactic positions. Last, we tested focus breadth by comparing the features for
sentence with wide-focus to those with narrow object focus. In addition to the analysis of
acoustic features, in Experiments 2 and 3 we investigated whether listeners could
correctly determine the intended information status of the speaker.

Acoustic correlates of information structure 15

Experiment 1
Method
Participants
Nine pairs of participants were recorded. All participants were self-reported native
speakers of American English. All participants were MIT students or members of the
surrounding community. Participants were paid for their participation.
Materials
Each trial consisted of a set-up question and a target sentence, which always had an SVO
structure (e.g., Damon fried an omelet this morning). The target sentence could plausibly
answer any one of the seven set-up questions (see Table 1), which served to focus
different elements of the sentence or the entire event described in the sentence. The first
question focused the entire event (i.e. What happened?). In the remaining conditions,
two factors were manipulated: (1) the element in the target sentence that was focused by
the question (subject, verb, object); and (2) the presence of an explicit contrast set for the
focused element (non-contrastively focused, i.e. explicit contrast set absent, contrastively
focused, i.e. explicit contrast set present).
All subject and object noun phrases (NPs) in the target sentences were bi-syllabic
with first syllable stress, and all verbs were monosyllabic. All subject NPs were proper
names, and object NPs were mostly common inanimate objects, such that the events were
non-reversible. Furthermore, all words were comprised mostly of sonorant phonemes.
These constraints ensured that words could be more easily compared across items, and
facilitated the extraction of acoustic features (which is easier for vowels and sonorant
consonants). An adjunct prepositional phrase (PP) was included at the end of each
sentence so that differences in the production of the object NP due to the experimental
manipulations would be dissociable from prosodic effects on phrase-final, or in this case,

Acoustic correlates of information structure 16
sentence-final, words, which are typically lengthened and produced with lower F0
compared to phrase-medial words (e.g., Wightman et al., 1992).
We constructed 28 sets of materials. Participants saw one condition of each item,
following a Latin Square design. A sample item is presented in Table 1. The complete
set of materials can be found in Appendix A.

Condition

Focus Type

Focused
Argument

Setup Question

1

Non-contrastive

wide

What happened this morning?

2

Non-contrastive

S

Who fried an omelet this morning?

3

Non-contrastive

V

What did Damon do to an omelet this morning?

4

Non-contrastive

O

What did Damon fry this morning?

5

Contrastive

S

Did Harry fry an omelet this morning?

6

Contrastive

V

Did Damon bake an omelet this morning?

7

Contrastive

O

Did Damon fry a chicken this morning?

Table 1: Example item from Experiment 1. The target sentence is “Damon fried an
omelet this morning.”
Procedure
Productions were elicited and pre-screened in a two-part procedure. The first part
was a training session, where participants learned the intended names for pictures of
people, actions, and objects. In the second part, the pairs of participants produced
questions and answers for each other. The method was designed to maximize control
over what speakers were saying, but to also encourage natural-sounding productions.
Pilot testing revealed that having subjects simply read the target sentences resulted in
productions with low prosodic variability. After going through the experiment one time,
the participants switched roles.
Training session
In the training session, participants learned mappings between 96 pictures and
names, so that they could produce the names from memory during the second part of the

Acoustic correlates of information structure 17
experiment. In a PowerPoint presentation, each picture, corresponding to a person, an
action, an object, or a modifier, was presented with its intended name (see Figure 1, left).
The pictures consisted of eight names of people, which were repeated 3-4 items each in
the experimental materials, eight colors (which were used in a concurrently run filler
experiment), 34 verbs, 44 objects, and two temporal modifiers (this morning and last
night). The pictures were presented in alphabetical order, to facilitate memorization and
recall. Participants were instructed to learn the mappings by progressing through the
PowerPoint at their own pace.
When participants felt they had learned the mappings, they were given a picturenaming test, which consisted of 27 items from the full list of 96. The test was identical
for all participants. Participants were told of their mistakes, and, if they made four or
more errors, they were instructed to go back through the PowerPoint to improve their
memory of the picture-name mappings. Once participants could successfully name 23 or
more items on the test, which took between 1 and 3 rounds of testing, they continued with
the second part of the experiment. Early in pilot testing, we discovered that subjects had
poor recall for the names of the people in the pictures. Therefore, in the actual
experiment, subjects could refer to a sheet which had labeled pictures of the people.

Acoustic correlates of information structure 18

Figure 1: Left: Examples from the picture-training task for Experiment 1. Each square
represents a screen shot. Right: Examples of the procedure for the questioner (upper
squares) and answerer (lower squares) for Experiment 1. Two conditions are presented:
Non-contrastive, object (left) and contrastive, verb (right). The top squares represent
screen shots of what the questioner saw on a trial; the bottom squares represent what the
answerer saw on a trial.
Question-Answer Experiment
The experiment was conducted using Linger 2.92 (available at
http://telab.mit.edu/~dr/Linger/), a software platform designed by Doug Rohde for
language processing experiments. Participants were randomly paired and randomly
assigned to the role of questioner or answerer. Participants sat at computers in the same
room such that neither could see the other’s screen. On each trial, as illustrated in Figure
1 (right), the questioner saw a question (e.g., “What did Damon fry this morning?”)
which he/she was instructed to produce aloud for the answerer. The answerer was
instructed to produce an answer aloud using the information contained in the picture on
his/her screen (e.g., “Damon fried an omelet this morning”). The answerer was

Acoustic correlates of information structure 19
instructed to produce complete sentences, including the subject, verb, object, and
temporal abverb,4 and to emphasize the part of the sentence that the questioner had asked
about, or that he/she was correcting. On a random 20% of trials, the answerer was asked
a comprehension question about the answer s/he produced.
Productions were recorded in a quiet room with a head-mounted microphone at a
rate of 44kHz.
Acoustic Feature

Units

Description

duration

ms

Word duration excluding any silence before or after the word.

silence

ms

Duration of silence following the word, not due to stop closure.

duration+silence

ms

The sum of the duration of the word and any following silence.

mean F0

Hz

Mean F0 of the entire word

maximum F0

Hz

Maximum F0 value across the entire word

F0 peak location

0-1

The proportion of the way through the word where the maximum F0 occurs.

minimum F0

Hz

Minimum F0 across the entire word

F0 valley location

0-1

The proportion of the way through the word where the minimum F0 occurs.

initial F0

Hz

early F0

Hz

Mean F0 of the initial 5% of the word
Mean F0 value of 5% of the word centered at the point 25% of the way
through the word

center F0

Hz

late F0

Hz

Mean F0 value of 5% of the word centered on the midpoint of the word
Mean F0 value of 5% of the word centered on a point 75% of the way
through the word

final F0

Hz

Mean F0 of the last 5% of the word

1st quarter F0

The difference between initial F0 and early F0.

Hz

The difference between early F0 and center F0.

3rd quarter F0

Hz

The difference between center F0 and late F0.

4th quarter F0

Hz

The difference between late F0 and final F0.

mean intensity

dB

Mean intensity of the word

maximum intensity

dB

Maximum dB level in the word

minimum intensity
intensity peak
location
intensity valley
location

dB
0-1

Minimum dB level in the word
The proportion of the way through the word where the maximum intensity
occurs
The proportion of the way through the word where the minimum intensity
occurs

maximum amplitude

4

Hz

2nd quarter F0

Pascal

0-1

Maximum amplitude across the word

In the absence of explicit instruction to produce complete sentences, with a lexicalized subject, verb, and
object, speakers would likely resort to pronouns or would omit given elements altogether (e.g., “What did
Damon fry this morning?” “An omelet.”). A complete production account of information structure
meaning distinctions should include not just the prosodic cues used by the speakers, but also syntactic and
lexical production choices, as well as the interaction among these different production strategies. However,
because we focus on prosody in the current investigation, we wanted to be able to compare acoustic
features across identical words. Thus, we required that participants always produce a subject, verb, object
and adverb on every trial.

Acoustic correlates of information structure 20
energy

(Pascal)2 x
Duration

Table 2: Acoustic features extracted from each word in the target sentence for
Experiments 1-3. Stepwise discriminant analyses demonstrated that the measures in bold
provided the best discrimination among conditions and were used in all reported
analyses.
Results
Of the 504 speaker productions from the Question-Answer Experiment, 87 (17%) were
discarded because (a) the answerer failed to use the correct lexical items, (b) the answerer
was disfluent, or (c) the production was poorly recorded. The 417 remaining productions
were subjected to the acoustic analyses described below.
Acoustic Features
Based on previous investigations of prosody and information structure (Fry, 1955;
Lieberman, 1960; Eady et al., 1985; Cooper & Eady, 1986, Bartels & Kingston, 1994;
Krahmer & Swerts, 2001; Baumann et al., 2006), we chose a set of acoustic features to
analyze (see Table 2). These features were obtained automatically using the Praat
program (Boersma & Weenink, 2006). The measures of F0 computed over portions of
the words (e.g., 1st quarter F0) were chosen in order to investigate how F0 changes across
the syllable might contribute to the differentiation of conditions.
Our first goal was to determine which of the 24 candidate acoustic features
mediated differences among conditions. We conducted a series of stepwise linear
discriminant analyses5 on all of the data collected in Experiments 1, 2 and 3 reported in
the current paper. In order to determine the features to be used in the analyses of all three
experiments, we performed a separate stepwise analysis on the data from each
experiment separately. For each analysis we entered all 24 acoustic features across each
5

Linear discriminant analysis (LDA) calculates a function, computed as a linear combination of all
predictors entered, which results in the best separation of two or more groups. For two groups, only one
function is computed. For three groups, the first function provides the best separation of group 1 from
groups 2 & 3; a second, orthogonal, function provides the best separation of groups 2 and 3, after
partialling out variance accounted for by the first function. Stepwise LDA is an iterative procedure which
adds predictors based on which of the candidate predictors provide the best discrimination.

Acoustic correlates of information structure 21
of the three sentence positions (subject, verb, and object) as possible predictors of the
seven experimental conditions, resulting in 72 predictors. Across the three analyses, the
acoustic features which consistently resulted in the best discrimination of conditions were
(1) duration + silence, (2) mean F0, (3) maximum F0, and (4) maximum intensity at the
positions of the (a) Subject, (b) Verb, and (c) Object. The fact that these 12 features (four
acoustic features across three sentence positions) consistently discriminated among
conditions across three independent sets of productions (from different speakers and
across somewhat different sets of materials) serves as evidence that these features are
underlying speaker- and material-independent differentiation of information structure.
Therefore, we use only these 12 features in the linear discriminant analyses reported for
the individual experiments in the paper.
Computing Residual Values
Because of differences among individuals, including age, gender, speech rate and
level of engagement with the task, speakers produce very different versions of the same
sentence even within the same experimental condition, thus adding variance to the
acoustic features of interest. Similarly, there is likely to be variability associated with
different items due to lexical and world knowledge factors. Researchers have previously
dealt with the issue of acoustic variability between speakers by normalizing pitch and/or
duration by speaker (e.g., Shriberg, Stolcke, Hakkani-Tur, & Tur, 2000; Shriberg et al.,
1998; Wightman, Shattuck-Hufnagel, Ostendorf, & Price, 1992). In order to remove
speaker- and item-related variance in the current studies, we computed linear regression
models in which speaker (n = 18) and item (n = 28) predicted each of the 12 acoustic
features identified in the stepwise discriminant analyses described in the previous section.
From each of these models, we calculated the predicted value of each acoustic feature for
a specific item from a specific speaker. We then subtracted this predicted value from

Acoustic correlates of information structure 22
every production. The differences among the resulting residual values should reflect
differences in the acoustic features due only to the experimental manipulations. All
subsequently reported analyses were performed on these residual values.

Focus Location
The extent to which a discriminant function analysis can separate data points into two or
more groups is calculated with a statistical test, Wilks’s lambda6.
To determine how well the acoustic features could differentiate focus location in
speakers’ productions, we computed a model where the 12 acoustic predictors were used
to discriminate among three focus locations: Subject, Verb or Object. In this analysis, we
are averaging across the contrastive and non-contrastive condition for each location.
The overall Wilks’s lambda of the model was significant, Λ = .46, χ2(24) = 271, p
< .001, indicating better-than-chance differentiation of subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .84, χ2(24) = 62.65,
p < .001, indicating that the acoustic predictors could also differentiate verb focus from
object focus (see Figure 2). Leave-one-out classification correctly classified 67% of the
productions. The model correctly classified subject focus 76% of the time, verb focus
58% of the time, and object focus 66% of the time. Table 3 presents the standardized
canonical discriminant function coefficients of the model.7

6

Wilks's lambda is a measure of the distance between groups on means of the independent variables, and is
computed for each function. It ranges in size from 0-1; lower values indicate a larger separation between
groups. The extent to which the model can effectively discriminate a new set of data is simulated by a
leave-one-out classification, in which the acoustic data from each production are iteratively removed from
the dataset, the model is computed, and the left-out case is classified by the resultant functions.
7
The coefficients in Table 3 indicate which acoustic features best discriminate focus location, such that
larger absolute values indicate a greater contribution of that feature to discrimination. For example,
inspection of the plot in Figure 2 and the coefficients in the Focus Location columns of Table 3 shows that
the acoustic features of Damon score around zero, or lower, on the first function (-0.002, 0.001, -0.01, and 0.06) and around zero on the second function (-0.003, 0.021, -0.016, -0.101). Fried shows a different
pattern; specifically, the acoustic features of fried have coefficients around zero for the first function, and
negative coefficients for function 2. Finally, omelet shows a third pattern: its acoustic correlates are
centered around zero on Function 1, but are high on Function 2.

Acoustic correlates of information structure 23
Figure 3 graphically presents the mean values of the four features, demonstrating
that across all three focus locations the intended focus location is produced with the
highest maximum intensity, the longest duration and silence, and the highest relative F0.

Function
1

Function
2

Subj
Focus

Verb
Focus

Obj
Focus

omelet

Focus
Breadth

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.001
-0.006
0.002
-0.037

0.004
0.011
0.001
0.181

0.008
0.011
-0.002
-0.137

0.003
-0.014
0.002
-0.026

0.004
-0.019
0.006
0.189

0.003
0.000
0.003
0.199

fried

Focus Type

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

0.007
0.024
0.002
0.094

-0.001
-0.003
-0.002
-0.010

0.007
0.000
0.004
-0.076

0.002
-0.040
-0.007
0.131

-0.001
-0.013
0.013
-0.043

0.005
-0.025
0.003
0.011

Damon

Focus Location

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.002
0.001
-0.010
-0.060

-0.003
0.021
-0.016
-0.101

0.005
-0.016
-0.012
0.087

-0.002
-0.007
0.020
0.056

0.005
-0.014
-0.011
-0.225

0.003
0.007
-0.005
-0.123

Acoustic correlates of information structure 24
Table 3: Standardized canonical coefficients of the discriminant functions computed for
Experiment 1.

Figure 2: Separation of focus locations on two discriminant functions in Experiment 1.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the upper left quadrant; productions of verb focus are
clustered in the lower half of the plot; productions of object focus are clustered in the
upper right quadrant.

Acoustic correlates of information structure 25

Damon
fried
omelet

Figure 3: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 1.
Focus type
To determine how well the acoustic features could differentiate the type of focus
(i.e. non-contrastive vs. contrastive) in speakers’ productions, we computed three models
in which the 12 acoustic predictors were used to discriminate between two focus type
groups. The three models investigated differences between non-contrastive and
contrastive focus at the three focus locations: subject, verb, and object.
Focus Type – Subject Position
The overall Wilks’s Lambda was not significant, Λ = .898, χ2(12) = 11.95 p = .45,
indicating that the acoustic features could not discriminate between non-contrastive and

Acoustic correlates of information structure 26
contrastive focus. Because the overall model is not significant, we do not present the
scores of the specific acoustic features or the classification statistics here or in the
analyses below.
Focus Type – Verb Position
The overall Wilks’s Lambda was not significant, Λ = .851, χ2(12) = 17.92 p = .12,
indicating that the acoustic features could not discriminate between non-contrastive and
contrastive focus.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .82, χ2(12) = 22.63 p < .05,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus above chance level. Leave-one-out classification correctly classified
59% of the productions. The model correctly classified non-contrastive focus 59% of the
time, and contrastive focus 59% of the time.
The coefficients in the Object Focus column of Table 3 indicate that intensity and
mean F0 contribute most to classification. Figure 4 graphically presents the mean values
of the four features, demonstrating that contrastive focus is produced with a higher
maximum intensity, a longer duration and silence, and higher maximum F0. Noncontrastive focus is produced with a higher mean F0.

Acoustic correlates of information structure 27

Damon
fried
omelet

Figure 4: Values for non-contrastive focus and contrastive focus type on the four
discriminating acoustic features when the direct object “omelet” is focused in
Experiment 1.
Wide Focus vs. Narrow Focus
To determine how well the acoustic features could differentiate focus breadth, we
computed a model in which the 12 critical predictors were used to discriminate between
productions where the entire sentence was focused and productions where the object was
non-contrastively or contrastively focused.
The overall Wilks’s Lambda was significant, Λ = .75, χ2(12) = 47.83, p < .001,
indicating that the acoustic features could successfully discriminate between conditions
where the entire event is focused and conditions where the object is narrowly focused.

Acoustic correlates of information structure 28
Leave-one-out classification correctly classified 72% of the productions. The model
correctly classified wide focus 67% of the time, and narrow focus 74% of the time.
The standardized canonical discriminant function coefficients in the Focus
Breadth column of Table 3 indicate that maximum intensity contributes most to focus
breadth classification. Figure 5 graphically presents the mean values of the four features,
demonstrating that wide focus is produced with a more uniform duration + silence and
maximum F0 across the sentence than object focus. Wide focus is also produced with a
more uniform, though overall greater, intensity than object focus.

Damon
fried
omelet

Figure 5: Values for wide focus vs. narrow object focus on the four discriminating
acoustic features in Experiment 3.
Discussion

Acoustic correlates of information structure 29
Focus Location
The results demonstrate that speakers consistently provide acoustic cues which
disambiguate focus location. Specifically speakers indicated focus with increased
duration, higher intensity, higher mean F0, and higher maximum F0. Furthermore, these
results are consistent with the pattern reported in Eady & Cooper (1986), such that the
word preceding a focused word is less prominent (produced with shorter duration, lower
intensity and lower F0) than the focused word, and the word following the focused word
is less prominent than the word preceding the focused word. Previous studies (Eady et
al., 1986; Rump and Collier, 1986) have reported this reduction in acoustic prominence
following focused elements as being mainly indicated by lower F0 on the post-focal
words, though in our data we also find evidence of this reduction in measures of duration
and intensity.
Focus Type
The results from Experiment 1 indicate that in semi-naturalistic productions
speakers do not systematically differentiate between different focus types (focused
elements which have explicit contrast sets in the discourse and those which do not).
Specifically, at two out of three sentence positions, a discriminant function analysis could
not successfully classify speakers’ productions of contrastively vs. non-contrastively
focused elements. The observation that speakers successfully discriminated contrastive
and non-contrastive focus in object position, but not in subject or verb positions, is
perhaps suggestive, but is likely due to a lack of experimental power, a limitation which
will be addressed in Experiment 2.
Focus Breadth
The results from Experiment 1 demonstrate that speakers do systematically mark
focus breadth prosodically. Narrow object focus is produced with the highest maximum
F0, longest duration, and maximum intensity of the object noun, relative to the other

Acoustic correlates of information structure 30
words in the sentence. For wide focus, the acoustic features are more similar across the
sentence; only intensity and mean F0 are higher on the object than on the other words in
the sentence. These differences are subtle, but sufficient for the model to successfully
discriminate the productions.
The fact that the model failed to systematically classify productions by focus type
(with the exception of the object position), while achieving high accuracy in focus
location and focus breadth indicates that speakers were not marking focus type with
prosody in Experiment 1. However, the method used to elicit productions did not require
that subjects be aware of the information structure ambiguity of the materials. Evidence
from other production studies suggests that speakers may not prosodically disambiguate
ambiguous productions if they are not aware of the ambiguity. Albritton, McKoon, and
Ratcliff (1996), for example, demonstrated that speakers did not disambiguate
syntactically ambiguous constructions like “Dave and Pat or Bob” unless they were
aware of the ambiguity (see also Snedeker and Trueswell, 2003, but cf. Kraljic and
Brennan, 2005, and Schafer, Speer, Warren, and White, 2000, for evidence that speakers
do disambiguate syntactically ambiguous structures even in the absence of ambiguity
awareness). Experiment 2 was designed to be a stronger test of speakers’ ability to
differentiate focus location, focus type, and focus breadth. We used materials similar to
those in Experiment 1, with two important methodological modifications. First, instead
of producing the answers to questions with no feedback, the speaker’s task now involved
trying to enable the answerer to choose the question that s/he was answering from a set of
possible questions. Moreover, we introduced feedback so that the speaker would always
know whether his/her partner had chosen the correct answer. Second, we changed the
design from a between- to a within-subjects manipulation. This ensured that speakers

Acoustic correlates of information structure 31
were aware of the manipulation, as they were producing the same answer seven times
with explicit instructions to differentiate their answers for their partner.
In addition to making the speaker’s task explicit, the new design also allowed us
to analyze the subset of the productions for which the listeners could successfully identify
the question-type and which therefore contain sufficient information for differentiating
utterances along the three relevant dimensions of information structure.

Experiment 2
Method
Participants
Seventeen pairs of participants were recorded for this experiment. Subjects were MIT
students or members of the surrounding community. All reported being native speakers
of American English. None had participated in Experiment 1. Participants were paid for
their participation.
Materials
The materials had the same structure as those from Experiment 1, though the
critical words differed. Specifically, a larger set of names and a wider variety of
temporal adverbs were used, and some verbs and objects differed from Experiment 1.
Unlike Experiment 1, each subject pair was presented with all seven versions of each of
14 items, according to a full within-subjects within-items design. All materials can be
found in Appendix B.
Procedure
Two participants sat at computers in the same room such that neither could see the
other’s screen. One participant was the speaker, and the other was the listener. Speakers
were told that they would be producing answers to questions out loud for their partners

Acoustic correlates of information structure 32
(the listeners), and that the listeners would be required to choose which question the
speaker was answering from a set of seven choices.
At the beginning of each trial, the speaker was presented with a question on the
computer screen to read silently. After pressing a button, the answer to the question
appeared below the question, accompanied by a reminder to the speaker that s/he would
only be producing the answer aloud, and not the question. Following this, the speaker
had one more chance to read the question and answer, and then he/she was instructed to
press a key to begin recording (after being told by the listener that he/she is ready), to
produce the answer, and then to press another key to stop recording.
The listener sat at another computer, and pressed a key to see the seven questions
that s/he would have to choose his/her answer from. When s/he felt familiar with the
questions, s/he told the speaker s/he was ready. After the speaker produced a sentence
out loud for the listener, the listener chose the question s/he thought the speaker was
answering. If the listener answered incorrectly, his/her computer produced a buzzer
sound, like the sound when a contestant makes an incorrect answer on a game show.
This cue was included to ensure that speakers knew when their productions did not
contain enough information for the listener to choose the correct answer.8
Results – Production
Two speaker-listener pairs were excluded as the Listener did not achieve
comprehension accuracy greater than 20%. One further pair was excluded as one
member was not a native speaker of American English. Finally, another pair of subjects
was excluded because they did not take the task seriously, and produced unnaturally
emphatic contrastive accents, often shouting the target word, and laughing while doing

8

In early pilots in which there was no feedback for incorrect responses, we observed that listeners were at
chance in choosing the correct question.

Acoustic correlates of information structure 33
so. These exclusions left a total of 13 pairs of participants whose responses were
analyzed.
Sixty-seven of the 1274 trials (5%) were excluded because (a) the speaker failed
to produce the correct words, (b) the speaker was disfluent, or (c) the production was
poorly recorded. Analyses were performed on all trials, and on the subset of trials for
which the listener correctly identified the question. The results were very similar in the
two analyses. For brevity of presentation, we present results from analyses conducted on
the correct trials (n = 660, 55%). The productions from Experiment 2 were analyzed
using the acoustic features chosen in the feature-selection procedure described in
Experiment 1. All analyses were performed on the residual values of these features, after
removing speaker and item variance with the method described in Experiment 1.
Focus Location
The overall Wilks’s lambda was significant, Λ = .085, χ2(24) = 1335, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .306, χ2(11) = 641, p
< .001, indicating that the acoustic features could also discriminate verb focus from
object focus (see Figure 6).
Leave-one-out classification correctly classified 93% of the productions. For
individual levels of focus location, the discriminant function correctly classified subject
focus 94% of the time, verb focus 90% of the time, and object focus 95% of the time.
The standardized canonical coefficients in the first two columns of Table 4
indicate that the acoustic features contributing most to the discrimination of focus
location are once again mean F0 and maximum intensity, though the other two features
are also contributing. In fact, inspection of the acoustic feature means in Figure 7

Acoustic correlates of information structure 34
demonstrate that the highest value of every acoustic feature is associated with the
intended focused item, with the exception of mean F0 when the subject is focused.

Figure 6: Separation of focus locations on two discriminant functions for Experiment 2.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the lower left quadrant of the plot; productions of verb
focus are clustered in the lower right quadrant; productions of object focus are clustered
in the lower half.
Focus Location

Focus Type

Focus Breadth

Function 2

Subject
Focus

Verb Focus

Object
Focus

Duration+ silence

omelet

Function 1
-0.001

0.004

0.004

0.006

0.003

0.003

Mean F0

-0.006

0.011

-0.003

0.005

-0.023

0.000

Maximum F0

0.002

0.001

0.004

-0.009

-0.003

0.003

Maximum Intensity

-0.025

0.183

-0.052

-0.171

0.012

0.199

Acoustic correlates of information structure 35
-0.002

0.006

0.002

-0.007

0.005

Mean F0

0.024

-0.005

0.001

-0.022

0.006

-0.025

Maximum F0

0.001

-0.002

-0.007

0.001

0.003

0.003

0.093

-0.016

-0.105

0.063

-0.084

0.011

Duration+ silence

Damon

0.007

Maximum Intensity

fried

Duration+ silence

-0.002

-0.002

0.002

0.005

0.009

0.003

Mean F0

0.003

0.021

-0.010

0.004

-0.009

0.007

Maximum F0

-0.011

-0.015

-0.014

-0.012

-0.006

-0.005

Maximum Intensity

-0.067

-0.097

0.094

-0.014

0.010

-0.123

Table 4: Standardized canonical coefficients of all discriminant functions computed for
Experiment 2.

Damon
fried
omelet

Figure 7: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 2.

Acoustic correlates of information structure 36
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .633, χ2(12) = 81.41, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
75% of the productions. The model correctly classified non-contrastive focus 78% of the
time, and contrastive focus 71% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that maximum intensity at all three locations (i.e. large intensity differences between the
subject and verb and the subject and object) contributes most to classification. Figure 8
graphically presents the mean values of the four features, demonstrating that, in addition
to intensity differences, contrastive focus is produced with longer duration and silence, as
well as lower mean and maximum F0.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .654, χ2(12) = 72.27, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
72% of the productions. The model correctly classified non-contrastive focus 70% of the
time, and contrastive focus 75% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that, once again maximum intensity contributes most to classification. Figure 9
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a higher maximum intensity, and a longer duration and silence,
than non-contrastive focus. Once again, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

Acoustic correlates of information structure 37
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .793, χ2(12) = 41.3, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
67% of the productions. The model correctly classified non-contrastive focus 69% of the
time, and contrastive focus 66% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that contrastive focus is most strongly associated with lower mean F0. Figure 10
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a lower mean and maximum F0 than non-contrastive focus.

Damon
fried
omelet

Figure 8. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 2.

Acoustic correlates of information structure 38

Damon
fried
omelet

Figure 9. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 2.

Acoustic correlates of information structure 39

Damon
fried
omelet

Figure 10. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “omelet” is focused in Experiment 2.
Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .59, χ2(12) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 84% of productions; wide
focus was correctly classified 77% of the time, and object focus was correctly classified
88% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 4
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Although intensity is contributing most strongly
to classification, inspection of the acoustic means in Figure 11 indicates that wide focus

Acoustic correlates of information structure 40
is marked by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is marked by greater prominence on the
object, reflected in longer duration, higher F0, and higher intensity.

Damon
fried
omelet

Figure 11: Values for wide vs. narrow object focus on the four discriminating acoustic
features in Experiment 2.

Acoustic correlates of information structure 41
Results – Perception

Figure 12. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 2.
Listeners’ choices of question sorted by the intended question are plotted in
Figure 12. Listeners’ overall accuracy was 55%. To determine whether listeners were
able to determine the speaker’s intended sentence meaning, we compared each subject's
responses to chance performance. Specifically we assessed, for focus location and focus
type, whether each subject's proportion of correct responses exceeded chance; wide
focus productions were excluded from the analysis, so that chance performance for focus

Acoustic correlates of information structure 42
location was .33, and chance performance for focus type was .5. Results demonstrated
that listeners were able to successfully identify focus location: all 13 subjects’
performance significantly exceeded chance performance, p = .05, two-tailed. However,
listeners were unable to successfully identify focus type: only three of 13 subjects
performed at above-chance levels (based on the binomial distribution), p = .05, twotailed. To investigate focus breadth, we assessed, for wide focus and narrow object focus
separately, whether each subject's proportion of correct responses exceeded chance. For
these analyses, we excluded subject and verb focus productions, so that chance
performance was .33 for wide focus, and .67 for narrow object focus. Results
demonstrated that listeners were moderately successful at identifying focus breadth: six
of 13 subjects identified wide focus at rates above chance, and nine out of 13 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.
Discussion
The production results replicated the two main findings from Experiment 1, and provided
evidence for acoustic discrimination of focus type across sentence positions as well.
First, these results demonstrated that focused elements have longer durations than nonfocused elements, incur larger F0 excursions, are more likely to be followed by silence,
and are produced with greater intensity. Second, speakers consistently differentiate
between wide and narrow focus by producing the object in the latter case with higher F0,
longer duration, and greater intensity. Specifically, although object focus was indicated
by increased duration, higher intensity, and higher F0 on the object than on the subject or
the verb, wide focus was indicated by comparatively greater duration, higher intensity,
and higher F0 on the subject and the verb, and shorter duration, lower intensity, and
lower F0 on the object. These results are consistent with those obtained by Baumann et

Acoustic correlates of information structure 43
al. (2006), who demonstrated that narrow focus on an element was indicated with longer
duration and a higher F0 peak than wide focus on an event encompassing that element.
Most importantly, although speakers in Experiment 1 did not differentiate
conditions with and without an explicit contrast set for the focused element (except for
the object position), these conditions were differentiated by speakers in Experiment 2, at
every syntactic position. There are two possible interpretations of this difference. First,
in Experiment 1, speakers produced only four versions of each of the seven conditions,
whereas speakers in Experiment 2 and 3, reported below, produced 14 versions of each of
the seven conditions, resulting in greater power in the latter two experiments. The fact
that, in Experiment 2, speakers successfully discriminated contrastive and noncontrastive focus in all three positions, suggests that the lack of such an effect in
Experiment 1 could be due to a lack of power.
As mentioned above, the difference in the findings between Experiments 1 and 2
is also consistent with results from Allbritton et al. (1996) and Snedeker and Trueswell
(2003) who demonstrated that speakers do not disambiguate syntactically ambiguous
sentences with prosody unless they are aware of the ambiguity. The current results
demonstrate a similar effect for acoustic prominence, such that speakers do not
differentiate two kinds of acoustically prominent elements (contrastively vs. noncontrastively focused elements) unless they are aware of the information structure
ambiguity in the structures they are producing.
The discriminant analyses indicated that contrastively focused words were
produced with longer durations and higher intensity than non-contrastively focused
words, but that non-contrastively focused words were produced with higher F0 than
contrastively focused words. This latter finding is surprising when compared to some
previous studies. For example, Ladd & Morton (1997) found that higher F0 and larger

Acoustic correlates of information structure 44
F0 range is perceived as more ‘emphatic’ or ‘contrastive’ by listeners. Similarly, Ito and
Speer (2008) demonstrated that contrastively focused words were produced with higher
F0 than non-contrastive ones. Given the unexpected results, we inspected individual
pitch tracks to more closely observe the F0 patterns across the entire utterances. The
pitch tracks presented in Figure 13 were generated from the productions of a typical
speaker, and they exemplify the higher F0 observed for non-contrastive focus than
contrastive focus in the subject position (A vs. B) and verb position (C vs. D).
Contrastive focus on the object is realized with the same F0 as non-contrastive focus on
the object (E vs. F).

300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Non-Contrastive

Given

an

omelet

yesterday

Given

0

1.433
Time (s)

Acoustic correlates of information structure 45
A. Non-contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Contrastive

an

Given

omelet

yesterday

Given

0

1.81
Time (s)

B. Contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Non-Contrastive

omelet

yesterday

Given

0

1.514
Time (s)

C. Non-contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Contrastive

an

omelet

<SIL>

yesterday

Given

0

2.377
Time (s)

Acoustic correlates of information structure 46
D. Contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Given

omelet

yesterday

Non-Contrastive
1.582

0
Time (s)

E. Non-contrastive Object Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Given

an

omelet

yesterday

Contrastive
2.188

0
Time (s)

F. Contrastive Object Focus
Figure 13. Pitch tracks for non-contrastive and contrastive subject focus, non-contrastive
and contrastive verb focus, and non-contrastive and contrastive object focus,
respectively, from a typical speaker from Experiment 2.
Note that our finding that non-contrastive focus is realized with higher F0 than
contrastive focus is still consistent with the claim that contrastive focus is more
prominent than non-contrastive focus. As the graphs in Figures 8-10, and the pitch tracks
in Figure 13 indicate, although contrastive elements were consistently produced with
lower pitch, they were also consistently produced with longer durations and greater

Acoustic correlates of information structure 47
intensity than non-contrastive elements. As reviewed in the introduction, there is
9

evidence that intensity and duration can convey prominence more effectively than higher
pitch (Fry, 1954, Lieberman, 1960, Beckman, 1986; Turk & Sawusch,1996; Kochanski et
al., 2005). Our data are therefore consistent with prior claims that contrastive focus is
produced with greater prominence than non-contrastive focus.
As discussed in the introduction, the production elicitation and analysis methods
used in the current experiment are more robust than methods used in many previous
studies, including those whose results are inconsistent with the current findings. In
particular, the current results are based on productions from naïve subjects in a
communication task, and the analyses were performed on data with speaker and item
variability removed. The current results are therefore more likely to reflect the
underlying generalizations about the relationship between acoustics and meaning.
The perception results only partially mirrored the production results. Consistent
with the production results, listeners were highly successful in discriminating among the
three focus locations. In contrast to the production results, however, listeners were only
moderately successful in identifying focus type (non-contrastive vs. contrastive) from the
speakers’ productions. In fact, listeners most often confused non-contrastive focus with
contrastive focus (see Figure 12). These results suggest that, even though speakers may
be consistently signaling focus type with their prosody, listeners are not able to exploit
those cues for comprehension.
With regard to focus breadth, the perception results are incompatible with a strong
version of the focus projection hypothesis (Selkirk, 1995). According to this hypothesis,
an acoustic prominence on the object NP can be interpreted as marking the entire clause
9

Importantly, the F0 results are not artifacts of the residualization procedure employed to remove variance
from the acoustic features due to speaker and item. The same numerical pattern of F0 values is observed
whether residualization is employed or not, though only the residualized acoustic features successfully
discriminate focus type.

Acoustic correlates of information structure 48
as focused. Listeners are therefore predicted to treat a production with an acoustically
prominent object NP as ambiguous between the narrow object focus reading and the wide
focus reading. However, as can be seen in Figure 12, listeners correctly identified narrow
object non-contrastive focus 57% of the time, interpreting it as wide focus only 13% of
the time, and correctly identified narrow object contrastive focus 49% of the time,
interpreting it as wide focus only 6% of the time. These results are not consistent with
Gussenhoven’s (1983) finding that listeners cannot reliably distinguish between narrow
objects focus and wide focus.
Experiments 1 and 2 provide evidence that speakers systematically indicate focus
location and focus breadth using a set of four acoustic features. These experiments
further suggest that speakers can, but don’t always, indicate focus type. In particular, the
results suggest that speakers only prosodically differentiate contrastive from noncontrastive focus when they are aware of the meaning ambiguity and/or when the task
involves conveying a particular meaning to a listener.
To further investigate the speakers’ ability to prosodically differentiate contrastive
from non-contrastive focus, we conducted an additional experiment. Acoustic analyses
in Experiments 1 and 2 were limited to three words (i.e. subject, verb, object) in the
sentence. However, in natural productions, speakers’ utterances are often prefaced by
attribution expressions (e.g., “I think” or “I heard”), or expressions of emotional attitudes
towards the described events (e.g., “Unfortunately”, or “Luckily”). It is therefore
possible that contrastive information might be partially conveyed by prosodically
manipulating these kinds of expressions. We explored this possibility in Experiment 3, in
which we had speakers produce target SVO constructions with a preamble. Experiment 3
was also intended to serve as a replication of the results of Experiment 2; in particular,

Acoustic correlates of information structure 49
the somewhat unexpected finding that non-contrastive focus is produced with higher F0
than contrastive focus.

Experiment 3
Method
Participants
Fourteen pairs of participants (speakers and listeners) were recorded for this
experiment. Subjects were MIT students or members of the surrounding community. All
reported being native speakers of American English. None had participated in
Experiments 1 or 2. Participants were paid for their participation.
Materials
The materials for Experiment 3 were identical to those from Experiment 1
described above with the exception that an attribution expression (“I heard that”) was
appended to the beginning of each target sentence.
Procedure
The procedure for Experiment 3 was identical to that for Experiment 2.
Results – Production
Four speaker-listener pairs were excluded as the listener did not achieve
comprehension accuracy greater than 20%. These exclusions left a total of 10 pairs of
participants whose responses were analyzed. Eighty-one of the 980 recorded trials (8%)
were excluded because (a) the speaker failed to produce the correct words, (b) the
speaker was disfluent, or (c) the production was poorly recorded. Analyses were
performed on all trials, and on the subset of trials for which the listener correctly
identified the question the speaker produced the sentence in response to. As in
Experiment 2, the results were very similar for the two analyses. For brevity of
presentation, we present results from analyses conducted on the correct trials (n = 632,
70%).

Acoustic correlates of information structure 50
Focus Location
In order to investigate the contribution of the prosody of “I heard that” to the
differentiation of the focus type in Experiment 2, we performed a stepwise discriminant
function analysis which included as predictors measures of the four acoustic features we
had selected initially (duration + silence, mean F0, maximum F0, maximum intensity) (1)
for the subject (“Damon”), verb (“fried”), and object (“omelet”), and (2) for each of the
first three words of the sentence (“I”, “heard”, “that”). Of the 24 predictors included in
the stepwise discriminant function analysis, the features which resulted in the best
discrimination of focus type were (1) the duration + silence of “I”, (2) the maximum F0
of “I”, and (3) the maximum intensity of “I”. Based on these results, we conducted an
additional analysis in which we included a subset of 16 predictors: the duration + silence,
mean F0, maximum F0, and maximum intensity of the subject, verb, object, and “I”.
As in Experiments 1 and 2, we conducted a discriminant analysis to determine
whether the measures of (1) duration + silence, (2) maximum F0, (3) mean F0, and (4)
maximum intensity of the four critical words in the sentence could predict focus location.
The overall Wilks’s lambda was significant, Λ = .058, χ2(32) = 1467.09, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .275, χ2(15) =
664.75, p < .001, indicating that the acoustic features could also discriminate verb focus
from object focus (Figure 14). Leave-one-out classification procedure correctly
classified 97% of the productions. At individual focus locations, the model correctly
classified subject focus 96% of the time, verb focus 97% of the time, and object focus
97% of the time.

Acoustic correlates of information structure 51

Figure 14. Separation of focus locations on two discriminant functions for Experiment 3.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the left half of the plot; productions of verb focus are
clustered in the lower right quadrant; productions of object focus are clustered in the
upper right quadrant.
Focus Location

Focus
Breadth

Focus Type

fried

omelet

Function
1

Function
2

Subject
Focus

Verb
Focus

Object
Focus

0.002

0.003

0.000

0.000

0.003

0.002

0.005

0.012

-0.013

-0.009

0.005

-0.010

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

0.003

0.000

0.005

0.006

-0.003

0.003

0.069

0.106

-0.037

-0.011

0.007

0.151

0.001

-0.003

0.000

0.002

0.001

0.005

0.025

-0.021

-0.001

-0.002

-0.001

-0.006

Maximum F0
Maximum
Intensity

-0.005

-0.002

-0.001

-0.005

0.000

-0.003

0.091

-0.077

-0.086

-0.015

0.027

-0.048

Duration+
silence
Mean F0

Acoustic correlates of information structure 52
Duration+
silence
Mean F0

Damon

0.000

0.002

0.000

0.000

0.002

0.011

0.011

-0.011

-0.020

0.019

-0.003

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

-0.014

-0.003

-0.001

0.007

-0.014

-0.008

-0.147

0.011

0.159

-0.006

-0.064

-0.123

0.000

0.000

0.004

0.005

0.005

-0.001

-0.005

0.000

-0.013

-0.008

-0.003

-0.009

Maximum F0
Maximum
Intensity

I

-0.003

0.004

-0.002

0.017

0.010

0.014

0.005

-0.021

-0.017

0.142

0.133

0.126

0.014

Table 5: Standardized canonical coefficients of all discriminant functions computed for
Experiment 3.

I
Damon
fried
omelet

Figure 15. Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 3.

Acoustic correlates of information structure 53
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .39, χ2(16) = 157.44, p<.001,
indicating that the acoustic features could successfully discriminate between noncontrastive and contrastive focus. Leave-one-out classification correctly classified 85%
of the productions. The model correctly classified non-contrastive focus 85% of the time,
and contrastive focus 85% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that maximum intensity overall, and specifically, maximum intensity on “I,” is
contributing most to classification. Figure 16 graphically presents the mean values of the
four features, demonstrating that, in addition to intensity differences, contrastive focus is
produced with longer duration and silence, and with lower mean and maximum F0.

Acoustic correlates of information structure 54

I
Damon
fried
omelet

Figure 16. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 3.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .46, χ2(16) = 139.28, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
80% of the productions. The model correctly classified non-contrastive focus 86% of the
time, and contrastive focus 74% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that, intensity on “I” is contributing the most to classification. Figure 17 graphically

Acoustic correlates of information structure 55
presents the mean values of the four features, demonstrating that contrastive focus is
produced with a higher maximum intensity, and a longer duration and silence, than noncontrastive focus. As in Experiment 2, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

I
Damon
fried
omelet

Figure 17. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 3.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .40, χ2(16) = 133.37, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and

Acoustic correlates of information structure 56
contrastive focus better than chance. Leave-one-out classification correctly classified
83% of the productions. The model correctly classified non-contrastive focus 89% of the
time, and contrastive focus 76% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that intensity and mean F0 on “I” are contributing the most to accurate classification.
Figure 18 graphically presents the mean values of the four features, demonstrating that
contrastive focus is produced with a higher mean and maximum F0 than non-contrastive
focus.

I
Damon
fried
omelet

Acoustic correlates of information structure 57
Figure 18: Values for non-contrastive vs. contrastive focus on the four discriminating
acoustic features when “omelet” is focused in Experiment 3.

Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .48, χ2(16) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 87% of productions; wide
focus was correctly classified 79% of the time, and object focus was correctly classified
92% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 5
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Specifically, greater intensity on the object is a
strong predictor of object focus; less intensity on the subject and the verb are strong
predictors of wide focus. Although intensity is contributing most strongly to
classification, inspection of the acoustic means in Figure 19 indicates that wide focus is
indicated by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is indicated by greater prominence on
the object, reflected in longer duration, higher F0, and higher intensity.

Acoustic correlates of information structure 58

I
Damon
fried
omelet

Figure 19. Values for wide vs narrow object focus on the four discriminating acoustic
features in Experiment 3.
Results – Perception
Listeners’ overall accuracy percentage by condition is plotted in Figure 20.
Listeners’ overall accuracy was 70%. As described in Experiment 2, we compared each
subject's responses to chance performance. Results demonstrated that listeners were able
to successfully identify focus location, as all 10 subjects’ performance significantly
exceeded chance performance, p = .05, two-tailed. Listeners were moderately successful
at discriminating focus type, as six of 10 subjects’ performance exceeded chance levels, p
= .05, two-tailed. Listeners successfully identified focus breadth as eight out of 10

Acoustic correlates of information structure 59
subjects identified wide focus at rates above chance, and eight out of 10 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.

Figure 20. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 3.

Discussion
Experiment 3 was conducted in order to (1) investigate whether speakers could
differentiate focus type with prosody if the sentences contained an attribution expression

Acoustic correlates of information structure 60
that could convey contrastive information, in addition to the elements that describe the
target event, and (2) replicate the results of Experiment 2.
With regard to the second goal, the production results of Experiment 3
successfully replicated the findings from Experiments 1 and 2. As in Experiments 1 and
2, speakers systematically differentiated focus location and focus breadth with a
combination of duration, intensity, and F0 cues. Furthermore, as in Experiment 2, noncontrastive focus was produced with higher F0 than contrastive focus (though only when
the subject or verb was focused), and contrastive focus was always produced with greater
duration and intensity. As discussed above, these F0 results contrast with prior findings
(Bartels & Kingston, 1994; Couper-Kuhlen, 1984; Ladd & Morton, 1997; Ito & Speer,
2008), but can be interpreted in light of more recent evidence that higher intensity is a
stronger cue to greater prominence than higher pitch (Kochanski et al., 2005).
In addition, results from Experiment 3 demonstrated that the strongest cues to
discrimination of focus type were the acoustics of “I” (from the attribution expression “I
heard that”). Specifically, in contrastive focus conditions, the word “I” was produced
with longer duration, higher intensity, and higher mean F0 and maximum F0. Indeed,
discrimination of focus type in Experiment 3 was far better than in Experiment 2. It
therefore appears that speakers can manipulate prosody on sentence elements outside of
the target clause (e.g., in attribution expressions) to convey contrastiveness.
The perception results demonstrated that listeners could accurately determine
focus location, similar to the results of Experiment 2. Furthermore, listeners were more
accurate in determining focus type than listeners in Experiment 2. This increase in
accuracy was likely due to speakers’ tendency to prosodically mark “I” in the contrastive
conditions.

Acoustic correlates of information structure 61

General Discussion
The three experiments reported in the current paper explored the ways in which focus
location, focus type, and focus breadth are conveyed with prosody. In each experiment,
naïve speakers and listeners engaged in tasks in which the information status of sentence
elements in SVO sentences was manipulated via preceding questions. The prosody of the
target sentences was analyzed using a series of classification models to select a subset
from the set of acoustic features that would best be able to discriminate among focus
locations and between focus types. In addition, in Experiments 2 and 3, the production
results were complemented by the perception results that demonstrated listeners’ ability
to use the prosodic cues in the speakers’ utterances to arrive at the intended meaning.
At the beginning of the paper, we posed three questions about the relationship
between acoustics and information structure: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? We are now in a position to answer these questions.
First, we have demonstrated that speakers systematically provide prosodic cues to
the location of focused material. Across all three experiments, speakers provided cues to
focus location whether or not the task explicitly demanded it, across subject, verb and
object positions. In addition, across all three experiments, speakers systematically
provided cues to focus breadth, such that wide focus was prosodically differentiated from
narrow object focus. Finally, we found that speakers can, but don’t always, prosodically
differentiate contrastive and non-contrastive focus. Specifically, speakers did not
prosodically differentiate focus type in Experiment 1, but they did so in Experiment 2
and, even more strongly, in Experiment 3. As discussed above, the fact that speakers did
not differentiate focus type in Experiment 1, where they were plausibly not aware of the

Acoustic correlates of information structure 62
meaning ambiguity, but did differentiate between contrastive and non-contrastive focus
conditions in Experiments 2 and 3, where the task made the meanings more salient, is
consistent with results from the literature on intonational boundary production
demonstrating that speakers only produce disambiguating boundaries when they are
aware of the syntactic ambiguity which could be resolved by the presence of a boundary
(Albritton et al., 1996; Snedeker & Trueswell, 2003; cf. Schafer, et al., 2000 and Kraljic
& Brennan, 2005). Furthermore, the results from Experiment 3, where the critical words
were preceded by the attribution expression “I heard that,” demonstrated even stronger
differentiation of focus type than in Experiment 2, suggesting that speakers are able to
convey contrastiveness using words outside of the clause containing the contrastivelyfocused element.
To answer the question of which acoustic features are associated with different
meaning categories of information structure, we conducted a series of discriminant
function analyses with the goal of objectively identifying which of 24 measures of
duration, intensity, and F0 allowed for the best discrimination of conditions. Across all
experiments, and across different sentence positions, the best differentiation among
conditions was achieved using the following four features: word duration, maximum
word intensity, mean F0, and maximum F0. These results are consistent with many
previous studies in the literature, implicating these features in conveying aspects of
information structure. An important contribution of the current studies is that these
results were obtained using a quantitative analysis across many naïve speakers and items,
and are therefore more likely to be generalizable.
These data also demonstrate how exactly these four features are used in
conveying different aspects of information structure. With regard to focus location,
focused material is produced with longer duration, higher F0, and greater intensity than

Acoustic correlates of information structure 63
non-focused material. With regard to focus type, non-contrastive focus is realized with
higher mean and maximum F0 on the focused word than contrastive focus, whereas
contrastive focus is realized with greater intensity on the focused word than noncontrastive focus. Finally, with regard to focus breadth, narrow focus on the object is
indicated by higher F0 and longer duration on the object, compared to wide focus, and
wide focus is conveyed by higher intensity and F0, and longer duration on pre-focal
words.
To answer the question of how well listeners can retrieve prosodic information
from the signal, we included a perception task in Experiments 2 and 3. When the
relevant acoustic cues were present in the input (as demonstrated by successful
classification by the models), listeners were also able to classify the utterances, although
not quite as successfully as the models. Furthermore, the fact that the model always
achieved high classification accuracy suggests that the utterances contained enough
acoustic information to make these discriminations, and that we did not leave any
particularly informative acoustic features out of the analyses.

Implications for theories of the mapping of acoustics to meaning
While our production and perception results are compatible with a direct
relationship between acoustics and meaning, they are also consistent with the existence of
mediating phonological categories, as in the intonational phonology framework. For
example, a standard assumption within intonational phonology is that there is a
phonological category “accent” mediating acoustics and semantic focus, such that a
focused element is accented, and an unfocused element is unaccented (e.g., Brown,
1983). Our production and perception results are compatible with this assumption. First,
if speakers are signaling focus location by means of placing acoustic features

Acoustic correlates of information structure 64
corresponding to a +accent category on focused elements, then we would expect to see
strong acoustic differences between focused and given elements, as we have observed.
Moreover, if listeners perceive accents categorically, then we would predict successful
discrimination of productions on the basis of focus location, as we have observed.
Second, when the object is focused, it will be accented, resulting in higher acoustic
measures on the object compared to other positions, as we have observed. Furthermore,
in the wide focus condition, the subject, verb, and object – all of which are focused –
would all receive accents, and would therefore be more acoustically similar to one
another than they are in the wide focus condition. This difference in accent placement
would lead to successful discrimination between wide and narrow focus by listeners, as
we have observed. Finally, there has been much debate in the intonational phonology
literature about whether there is a phonological category +/- contrastive. The results of
our experiments are perhaps best explained without such a category. In particular, if
speakers accent focused elements without differentiating between contrastive and noncontrastive focus, then we would expect similar acoustic results between productions
which differ only on focus type, which would lead to poor discrimination by the model.
Moreover, listeners would not be successful in discriminating focus type, as we have
observed. Our experimental results are thus compatible with an intonational
phonological approach which includes an accent category mediating acoustics and
meaning, but no category for contrastiveness. Importantly, although our results do not
support a categorical difference between non-contrastive and contrastive focus, they do
not exclude the possibility that speakers can mark these distinctions with relative
differences in prominence (Calhoun, 2006).

Acoustic correlates of information structure 65
Implications for semantic theories of information structure
The current results are relevant to two open questions in the semantics of
information structure: (1) whether contrastive and non-contrastive focus constitute two
distinct categories; and (2) whether focus on the object of a verb can project to the entire
verb phrase.
As described in the introduction, Rooth (1992) proposed an account of focus which
makes no distinction between non-contrastive focus and contrastive focus. (6) shows the
F-marking (focus-marking) that Rooth’s account would assign to the conditions in
Experiments 1 and 2. Importantly, words and phrases which evoke alternatives, either
explicit or implicit, are considered focused (i.e. F-marked).
(6)
a. Subject, Subject Contrast: DamonF fried an omelet last night.
b. Verb, Verb Contrast:

Damon friedF an omelet last night.

c. Object, Object Contrast: Damon fried an omeletF last night.
d. Wide:

[Damon fried an omelet] F last night.

Our results provide tentative support for Rooth’s proposal that F-marked constituents do
not differ substantively as a function of whether the alternatives they evoke are explicit
(our contrastive condition) or implicit (our non-contrastive condition). Although
speakers differentiated these two conditions acoustically, they only did so when the
contrast between the conditions was made salient (Experiments 2 and 3). Moreover, even
when speakers did mark this distinction, listeners were unable to consistently use this
information to recover the intended meaning (Experiment 2). These results suggest that
there are no consistent semantic differences between foci with explicit alternatives in the
discourse and those with implicit alternatives.

Acoustic correlates of information structure 66
The second semantic issue that these results bear upon is whether narrow focus on
the object can project to the entire verb phrase. According to the theory of focus
projection proposed in Selkirk (1984, 1995), an acoustic prominence on the direct object
(omelet) can project focus to the entire verb phrase (fried an omelet) and then up to the
entire clause/sentence. Gussenhoven (1983, 1999) makes a similar claim. Both Selkirk’s
and Gussenhoven’s accounts therefore predict that a verb phrase with a prominence on
the object would be ambiguous between a narrow object focus interpretation and a wide
focus interpretation. Neither the production nor the perception results were consistent
with this prediction. In production, speakers distinguished between narrow object focus
and wide focus, and in perception, listeners were able to distinguish these two conditions.
One aspect of the production results (the acoustic realization of the subject) for the
narrow object focus and wide focus conditions is, however, predicted by both Selkirk and
Gussenhoven’s accounts. In particular, in the wide focus condition, the subject
constitutes new information while in the narrow object focus condition the subject is
given. Selkirk & Gussenhoven both predict that the subject would be more acoustically
prominent in the wide focus condition than in the narrow object focus condition. This is
exactly what we observed (especially in Experiments 1 and 3). Nevertheless, as
discussed above, speakers also systematically disambiguated wide focus from narrow
object focus across all three experiments with their realization of the object and the verb.
Specifically, wide focus was produced with stable or increasing duration, intensity, and
F0 across the subject, verb, and object; narrow object focus, on the other hand, was
characterized by shorter duration and lower intensity and F0 on the subject and verb,
followed by a steep increase in each of these values on the object.
Similar to our production findings, Gussenhoven (1983) found that, at least in some
productions, wide focus differed from narrow object focus in that the verb was more

Acoustic correlates of information structure 67
prominent under wide focus. Listeners, however, were unable to use this acoustic
information to distinguish wide focus from narrow object focus. Gussenhoven took this
result as evidence that the two conditions are not reliably distinguished (consistent with
his theory). Our results did not replicate this production/perception asymmetry: Listeners
are able to successfully classify productions with a single prominence on omelet as
indicating narrow object focus and did not confuse these productions with those from the
wide focus condition.
Methodological contributions
A further contribution of the current research to investigations of prosody and
information structure is methodological. With regard to the methods used to elicit
productions, we utilized multiple, untrained speakers to ensure that our results are
generalizeable to all speakers and are not due to speakers’ prior beliefs about what pattern
of acoustic prominence signals a particular meaning (see Gibson & Fedorenko, in press,
for similar arguments with respect to linguistic judgments). Furthermore, unlike most
previous work in which productions were selected for analysis based on perceptual
differentiability or on ratings of the appropriateness of prosodic contours, we elicited and
selected for analysis productions using a meaning task. Thus our analyses were based on
the communicative function of language. Finally, we did not exclude speakers based on
our perceptions of their productions; speakers were excluded for failure to provide
information to their listeners.
The analyses used here also constitute an improvement over previous analyses.
First, using discriminant modeling, we were able to simultaneously investigate the
contribution of multiple sentence elements to acoustic differentiation of conditions.
Second, we demonstrated that residualization is a useful method for controlling for
variability among speakers and lexical items. For example, preliminary analyses

Acoustic correlates of information structure 68
performed on the productions from Experiment 2 without first computing residual values
of the acoustic features revealed a 13% average increase in values of Wilks’ lambda
(where lower values indicate better discrimination) and a 7% average decrease in
classification accuracy. Third, the discriminant modeling proved successful in
objectively determining which acoustic features were the biggest contributors to
differences among conditions. The success of the analyses used in the current studies is
encouraging for future investigations of prosodic phenomena previously considered too
variable for study in a laboratory setting with naïve speakers.
One question that arises from the current set of studies is, to what extent the
current results can be generalized to all speakers and all sentences. In production studies,
there is always a trade-off between (1) having enough control over what participants are
producing to ensure sufficient data for analysis, and (2) ensuring that the speech is as
natural as possible. In Experiment 1, we attempted to elicit natural productions, but
failed to find systematic differences between focus types. In making the speakers’ task—
to help their listeners choose the correct question-type—explicit, we may have also
encouraged speakers to produce these sentences with somewhat exaggerated prosody.
Further experiments will be necessary to determine whether speakers normally produce
contrastive meanings in this way.
In conclusion, the current studies used rigorous scientific methods to explore
several important questions about the acoustic correlates of information structure. By
providing some initial answers to these questions, along with some implications for
semantic theory, and by offering a novel, objective way to approach these and other
questions, these studies open the door to future investigations of the relationship between
acoustics and meaning.

Acoustic correlates of information structure 69

References
Albritton, D., McKoon, G., & Ratcliff, R. (1996) Reliability of Prosodic Cues for
Resolving Syntactic Ambiguity. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22, 714-735.
Bartels, C., Kingston, J., (1994). Salient pitch cues in the perception of contrastive focus.
In Boach, P., Van der Sandt, R. (Eds.), Focus & Natural Language Processing, Proc. of J.
Sem. conference on Focus. IBM Working Papers. TR-80, pp. 94–106.
Baumann, S., Grice, M., and Steindamm, S. (2006). Prosodic Marking of Focus Domains
- Categorical or Gradient? In Proceedings of Speech Prosody, Dresden, Germany, pp.
301-304.
Beckman, Mary E. (1986). Stress and Non-Stress Accent. Netherlands Phonetic Archives
Series No. 7. Foris.
Beckman, M., & Ayers Elam, G. (1997). Guidelines for ToBI labeling, version 3: Ohio
State University.
Beckman, M., Hirschberg, J., & Shattuck-Hufnagel, S. (2005). The original ToBI system
and the evolution of the ToBI framework. In S.-A. Jun (Ed.), Prosodic Typology: The
Phonology of Intonation and Phrasing (pp. 9-54): Oxford University Press.
Birch, S. and Clifton, C. (1995) Focus, accent, and argument structure: effects on
language comprehension. Language and Speech, 38 (4), 365-391.
Birner, B. (1994). Information status and Word Order: An Analysis of English Inversion.
Language, 70 (2), 233-259.
Boersma, Paul & Weenink, David (2006). Praat: doing phonetics by computer (Version
4.3.10) [Computer program]. Retrieved June 3, 2005, from http://www.praat.org/
Bolinger, D. (1961). Contrastive accent and contrastive stress. Language, 37, 83-96.
Breen, M., Dilley, L., Gibson, E., Bolivar, M., and Kraemer, J. (2006) Advances in
prosodic annotation: A test of inter-coder reliability for the RaP (Rhythm and Pitch) and
ToBI (Tones and Break Indices) transcription systems. Poster presented at the 19th
CUNY Conference on Human Sentence Processing, New York, NY. March, 2006.
Brown, G. (1983). Prosodic structures and the Given/New distinction. In D. R. Ladd &
A. Cutler (Eds.), Prosody:Models and measurements (pp. 67–77). Berlin: Springer.
Calhoun, S (2004). Phonetic Dimensions of Intonational Categories - the case of L+H*
and H*. In Proceedings of Speech Prosody, Nara, Japan, pp. 103-106.
Calhoun, S. (2005). It's the difference that matters: An argument for contextuallygrounded acoustic intonational phonology. In Linguistics Society of America Annual
Meeting, Oakland, California, January 2005.
Calhoun, S. (2006) Information Structure and the Prosodic Structure of English: a
Probabilistic Relationship. PhD thesis, University of Edinburgh.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics and points of
view. In Charles N. Li, editor, Subject and Topic, pages 27-- 55. Academic Press, 1976.
Clark, E. V., & Clark, H. H. (1978). Universals, relativity, and language processing. In: J.
H. Greenberg (Ed.), Universals of human language, Vol. I. (pp. 225–277). Stanford:
Stanford University Press.

Acoustic correlates of information structure 70
Cooper, W., Eady, S. & Mueller, P. (1985). Acoustical aspects of contrastive stress in
question-answer contexts. Journal of Acoustical Society of America, 77(6), 2142-2156.
Couper-Kuhlen, E. (1984). A new look at contrastive intonation., Modes of
Interpretation: Essays Presented to Ernst Leisi, Watts, R., Weidman, U. (Eds.) Gunter
Narr Verlag, 137–158.
Cutler, A. (1977). The Context-Independence of "Intonational Meaning". Chicago
Linguistic Society (CLS 13), 104-115.
Dilley, L. C. (2005). The phonetics and phonology of tonal systems. Unpublished Ph.D.
Dissertation, MIT.
Dilley, L. C., & Brown, M. (2005). The RaP (Rhythm and Pitch) Labeling System,
Version 1.0: Available at http://tedlab.mit.edu/rap.html.
Eady, S. J., & Cooper, W. E. (1986). Speech intonation and focus location in matched
statements and questions. Journal of the Acoustical Society of America, 80, 402-415.
Féry, C. and Krifka, M. (2008). Information Structure: Notional Distinctions, Ways of
Expression. In Piet van Sterkenburg (ed.), Unity and diversity of languages, Amsterdam:
John Benjamins, 123-136.
Fry, D. B. (1955). Duration and Intensity as Physical Correlates of Linguistic Stress.
Journal of the Acoustical Society of America, 27, 765–768.
Gibson, E. & Fedorenko, E. (In press). Weak quantitative standards in linguistics
research. Trends in Cognitive Sciences.
Gussenhoven, C. (1983). Testing the reality of focus domains. Language and Speech, 26,
61–80.
Gussenhoven, C. (1999). On the limits of focus projection in English. In P. Bosch & R.
van der Sandt (Eds.), Focus: Linguistic, cognitive, and computational perspectives (pp.
43 –55). Cambridge, U.K.: Cambridge University Press.
Gussenhoven, C., Repp, B. H., Rietveld, A., Rump, W. H. & J. Terken, J. (1997). The
perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society
of America, 102, 3009-3022.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hawkins, S. & Warren, P. (1991). Factors affecting the given-new distinction in speech.
In Proceedings of the 12th International Congress of Phonetic Sciences, Aix en
Provence. 66-69.
Ito, K & Speer, S. (2008). Anticipatory effects of intonation: Eye movements
during instructed visual search. Journal of Memory and Language, 58, 541-573.
Ito, K. Speer, S. R. and Beckman, M. E. (2004). Informational status and pitch accent
distribution in spontaneous dialogues in English, In Proceedings of the International
Conference on Spoken Language Processing, Nara: Japan, 279-282.
Jackendoff, R. (1972). Semantic interpretation in generative grammar. Cambridge: MIT
Press.
Jaeger, T. F. (2008). Categorical Data Analysis: Away from ANOVAs (transformation or
not) and towards Logit Mixed Models. Journal of Memory and Language. 59, 434–446.

Acoustic correlates of information structure 71
Kochanski, G., Grabe, E., Coleman, J., & Rosner, B. (2005) Loudness predicts
prominence: fundamental frequency lends little. The Journal of the Acoustical Society of
America, 118 (2), 1038-1054.
Krahmer, E., & Swerts, M. (2001). On the alleged existence of contrastive accents.
Speech Communication, 34, 391-405.
Kraljic, T. & Brennan, S. E. (2005). Prosodic disambiguation of syntactic structure: For
the speaker or for the addressee? Cognitive Psychology 50: 194-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge Studies in Linguistics 79.
Cambridge: Cambridge University Press.
Ladd, D. R. & Morton, R. (1997). The perception of intonational emphasis: continuous or
categorical? Journal of Phonetics, 25, 313–342.
Lambrecht, K. (2001). A framework for the analysis of cleft constructions. Linguistics,
39, 463–516.
Lieberman, P. (1960). Some acoustic correlates of word stress in American English. The
Journal of the Acoustical Society of America, 32(4), 451-454.
Molnar, V. (2002). Information Structure in a Cross-linguistic Perspective. In Hilde
Hasselgård, Stig Johansson, Bergljot Behrens, Cathrine Fabricius-Hansen (Eds.),
Language and Computers, Vol. 39, 147-161(15).
Paul, H. (1880), Prinzipien der Sprachgeschichte, Leipzig.
Pierrehumbert, J.B. (1980). The phonology and phonetics of English intonation.
Unpublished dissertation, MIT.
Pierrehumbert, J. & Hirschberg, J. (1990). The Meaning of Intonational Contours in the
Interpretation of Discourse. In P. R. Cohen & J. Morgan & M. E. Pollack (eds.).
Intentions in Communication. Cambridge/MA: MIT Press, 271-311.
Pierrehumbert, J. & Steele, S. (1989). Categories of tonal alignment in English.
Phonetica, 46, 181-196.
Pitrelli, J., Beckman, M. & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International
Conference on Spoken Language Processing, 123-126.
Rietveld, A. C. M., and Gussenhoven, C. (1985). On the relation between pitch excursion
size and prominence. Journal of Phonetics, 13, 299-308.
Rochemont, M. S. (1986). Focus in Generative Grammar. Amsterdam/Philadelphia: John
Benjamins.
Rooth, M. (1985). Association with Focus. PhD thesis, University of Massachusetts
Amherst.
Rooth, M. (1992). A theory of focus interpretation. Natural Language Semantics, 1, 75 –
116.
Rump, H. H., and Collier, R. (1996). ‘Focus conditions and the prominence of pitchaccented syllables. Language and Speech, 39, 1–17.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational disambiguation
in sentence production and comprehension. Journal of Psycholinguistic Research, 29,
169-182.

Acoustic correlates of information structure 72
Selkirk, E. (1984). Phonology and syntax: The relation between sound and structure.
Cambridge, MA: MIT.
Selkirk, E. (1995). Sentence Prosody: Intonation, Stress, and Phrasing. In: J.Goldsmith
(ed.). The Handbook of Phonological Theory. Oxford: Blackwell, 550-569.
Schwarzchild, R. (1999) GIVENness, AvoidF and other Constraints on the Placement of
Accent. Natural Language Semantics, 7, 141–177.
Shriberg, E., Stolcke, A., Hakkani-Tur, D. & Tur, G. (2000). Prosody-Based Automatic
Segmentation of Speech into Sentences and Topics. Speech Communication, 32, 127-154.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D., Ries, K., Coccaro, N.,
Martin, R., Meteer, M., & Van Ess-Dykema, C. (1998). Can Prosody Aid the Automatic
Classification of Dialog Acts in Conversational Speech? Language and Speech, 41:3-4,
439-487.
Silverman, K. E. A., Beckman, M., Pierrehumbert, J., Ostendorf, M., Wightman, C. W.
S., Price, P., et al. (1992). ToBI: A standard scheme for labeling prosody. In Proceedings
of the 2nd International Conference on Spoken Language Processing (pp. 867-879).
Banff.
Sluijter, A. and van Heuven, V. (1996). Spectral balance as an acoustic correlate of
linguistic stress. Journal of the Acoustical Society of America, 100, 2471–2485.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects of
speaker awareness and referential contest. Journal of Memory and Language, 48, 103–
130.
Stalnaker, R. (2002). Common ground. Linguistics and Philosophy, 25: 701–721.
Syrdal, A. and McGory, J. (2000). Inter-transcriber reliability of ToBI prosodic labeling.
In Proceedings of the International Conference on Spoken Language Processing,
Beijing: China, 235-238.
Terken, J. (1991). Fundamental frequency and perceived prominence accented syllables.
Journal of the Acoustical Society of America, 89, 1768–1776.
't Hart, J. Collier, R. & Cohen, A. (1990). A perceptual study of intonation. Cambridge
University Press, Cambridge.
Turk, A. & Sawusch, J. (1996) The processing of duration and intensity cues to
prominence. Journal of the Acoustical Society of America, 99, 3782-3790.
Welby, P. (2003). Effects of pitch accent position, type, and status on focus projection.
Language and Speech, 46, 53 – 81.
Wightman, C. W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P. J. (1992). Segmental
durations in the vicinity of prosodic phrase boundaries. Journal of the Acoustical Society
of America, 91(3), 1707-1717.
Xu, Y. & Xu, C. X. (2005). Phonetic realization of focus in English declarative
intonation, Journal of Phonetics, 33, 159–197.
Yoon, T., Chavarria, S., Cole, J., & Hasegawa-Johnson, M. (2004). Intertranscriber
reliability of prosodic labeling on telephone conversation using ToBI. In Proceedings of
the International Conference on Spoken Language Processing., Nara: Japan, 2729-2732.

Acoustic correlates of information structure 73

Appendix A
Experiment 1 items
Full items are recoverable as follows: Question A is always “What happened last night?”
Questions B, C, & D are wh-questions about the subject, verb, and object, respectively.
Questions E, F, & G are questions which introduce the explicit alternative subject, verb,
or object, indicated in parentheses.
1.

Question A: What happened last night?
Question B: Who fed a bunny last night?
Question C: What did Damon do to a bunny last night?
Question D: What did Damon feed last night?
Question E: Did Jenny feed a bunny last night?
Question F: Did Damon pet a bunny last night?
Question G: Did Damon feed a baby last night?
Response: Damon fed a bunny last night.

2. Damon (Lauren) caught (pet) a bunny (a squirrel) last night.
3. Damon (Molly) burned (break) a candle (a log) last night.
4. Darren (Lauren) cleaned (eat) a carrot (a chicken) last night.
5. Darren (Molly) peeled (eat) a carrot (a potato) last night.
6. Darren (Nora) found (buy) a diamond (a ring) last night.
7. Darren (Jenny) sold (lose) a diamond (a sapphire) last night.
8. Jenny (Damon) found (lose) a dollar (a quarter) last night.
9. Jenny (Darren) sewed (rip) a dolly (a blanket) last night.
10. Jenny (Logan) read (open) an email (a letter) last night.
11. Jenny (Nolan) smelled (plant) a flower (a skunk) last night.
12. Lauren (Darren) burned (write) a letter (a magazine) last night.
13. Lauren (Logan) mailed (open) a letter (a package) last night.
14. Lauren (Nolan) read (write) a novel (a newspaper) last night.
15. Lauren (Damon) fried (bake) an omelet (a chicken) last night.
16. Logan (Molly) peeled (chop) an onion (an apple) last night.
17. Logan (Nora) fried (chop) an onion (a potato) last night.
18. Logan (Jenny) cleaned (buy) a pillow (a rug) last night.
19. Molly (Logan) dried (wash) a platter (a bowl) last night.
20. Molly (Nolan) sold (find) a platter (a vase) last night.
21. Molly (Damon) poured (drink) a smoothie (a cocktail) last night.
22. Nolan (Nora) pulled (push) a stroller (a sled) last night.
23. Nolan (Jenny) bought (sell) a stroller (a wheelbarrow) last night.
24. Nolan (Lauren) sewed (knit) a sweater (a quilt) last night.
25. Nora (Nolan) killed (trap) a termite (a cockroach) last night.
26. Nora (Damon) changed (wash) a toddler (a baby) last night.
27. Nora (Darren) fed (dress) a toddler (a bunny) last night.
28. Nora (Logan) pulled (push) a wagon (a wheelbarrow) last night.

Acoustic correlates of information structure 74

Appendix B
Items used for Experiments 2-3
Full items are recoverable as follows: Question A always asks “What happened _____?”
where the blank corresponds to the temporal adverb. Questions B, C, & D are whquestions about the subject, verb, and object, respectively. Questions E, F, & G are
questions which introduce the explicit alternative subject, verb, or object, indicated in
parentheses.
1a.
1b.
1c.
1d.
1e.
1f.
1g.

Context: What happened yesterday?
Context: Who fried an omelet yesterday?
Context: What did Damon do to an omelet yesterday?
Context: What did Damon fry yesterday?
Context: Did Harry fry an omelet yesterday?
Context: Did Damon bake an omelet yesterday?
Context: Did Damon fry a chicken yesterday?
Target: No, Damon fried an omelet yesterday.

2. (I heard that) (No,) Megan (Jodi) sold (lose) her diamond (her sapphire) yesterday.
3. (I heard that) (No,) Mother (Daddy) dried (wash) a platter (a bowl) last night.
4. (I heard that) (No,) Norman (Kelly) read (write) an email (a letter) last night.
5. (I heard that) (No,) Lauren (Judy) poured (drink) a smoothie (a cocktail) this morning.
6. (I heard that) (No,) Nora (Jenny) sewed (rip) her dolly (her blanket) this morning.
7. (I heard that) (No,) Molly (Sarah) trimmed (wax) her eyebrows (her hair) on Tuesday.
8. (I heard that) (No,) Nolan (Steven) burned (break) a candle (a log) on Tuesday.
9. (I heard that) (No,) Logan (Billy) killed (trap) a termite (a cockroach) last week.
10. (I heard that) (No,) Radar (Fido) caught (lick) a bunny (a squirrel) last week.
11. (I heard that) (No,) Darren (Maggie) pulled (push) a stroller (a sled) on Sunday.
12. (I heard that) (No,) Brandon (Tommy) peeled (eat) a carrot (a potato) on Sunday.
13. (I heard that) (No,) Maren (Debbie) cleaned (buy) a pillow (a rug) on Friday.
14. (I heard that) (No,) Lindon (Kelly) fooled (fight) a bully (a teacher) on Friday.

Acoustic correlates of information structure 1

Acoustic correlates of information structure
Mara Breen1, Evelina Fedorenko2, Michael Wagner3, Edward Gibson2
1
2

University of Massachusetts Amherst
Massachusetts Institute of Technology
3
McGill University

June 7, 2010
Address correspondence to:
Mara Breen
522 Tobin Hall
University of Massachusetts
Amherst, MA
01003
mbreen@psych.umass.edu

Acoustic correlates of information structure 2
Abstract
This paper reports three studies aimed at addressing three questions about the acoustic
correlates of information structure in English: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? The information structure of subject-verb-object (SVO)
sentences was manipulated via the questions preceding those sentences: elements in the
target sentences were either focused (i.e. the answer to a wh-question) or given (i.e.
mentioned in prior discourse); furthermore, focused elements had either an implicit or an
explicit contrast set in the discourse; finally, either only the object was focused (narrow
object focus) or the entire event was focused (wide focus). The results across all three
experiments demonstrated that people reliably mark (a) focus location (subject, verb, or
object) using greater intensity, longer duration, and higher mean and maximum F0, and
(b) focus breadth, such that narrow object focus is marked with greater intensity, longer
duration, and higher mean and maximum F0 on the object than wide focus. Furthermore,
when participants are made aware of prosodic ambiguity present across different
information structures, they reliably mark focus type, so that contrastively-focused
elements are produced with higher intensity, longer duration, and lower mean and
maximum F0 than non-contrastively focused elements. In addition to having important
theoretical consequences for accounts of semantics and prosody, these experiments
demonstrate that linear residualization successfully removes individual differences in
people’s productions thereby revealing cross-speaker generalizations. Furthermore,
discriminant modeling allows us to objectively determine the acoustic features that
underlie meaning differences.

Acoustic correlates of information structure 3

Introduction
An important component of the meaning of a sentence is its relationship to the context in
which it is produced. Some parts of speakers’ sentences refer to information already
under discussion, while other parts convey information that the speaker is presenting as
new for the listener. Depending on the context, the same sentence can convey different
kinds of information to the listener. For example, consider the three contexts in (1a)-(1c)
for the sentence in (2):

(1) a. Who fried an omelet?
b. What did Damon do to an omelet?
c. What did Damon fry?

(2) Damon fried an omelet.

The event of frying an omelet is already made salient in the context in (1a), and
this part of the answer is therefore given. Consequently, the sentence Damon fried an
omelet conveys Damon as the new or focused information.1 Similarly, the verb fried is
the focused information relative to the context in (1b), and the object noun phrase an
omelet is the focused information relative to the context in (1c). This component of the
meaning of sentences - the differential contributions of different sentence elements to the

1

Numerous terms are used in the literature to refer to the distinction between the information that is old for
the listener and the information that the speaker is adding to the discourse: background and foreground;
given and new; topic and comment; theme and rheme, etc. In this paper, we will use the term given to refer
to the parts of the utterance which are old to the discourse, and focused to refer to the part of the utterance
which is new to the discourse.

Acoustic correlates of information structure 4
overall sentence meaning in its relation to the preceding discourse - is called information
structure.
Three components of information structure have been proposed in the literature:
givenness, focus, and topic (see e.g., Féry and Krifka, 2008, for a recent summary). The
current paper will be concerned with givenness and focus.2 Given material is material
that has been made salient in the discourse, either explicitly, like the event corresponding
to the verb fried and the object corresponding to the noun omelet in (1a), or implicitly, via
inferences based on world knowledge (e.g., mentioning omelet makes the notion of
“eggs” given, Schwarzchild, 1999).
Focused material is what is new to the discourse, or in the foreground. The focus
of a sentence can often be understood as the part that corresponds to the answer to the
wh-part of wh-questions, like Damon in (2) as an answer to (1a) (Paul, 1880; Jackendoff,
1972).
There are two dimensions along which focused elements can differ. The first is
contrastiveness. A contrastively focused element, like Damon in (3b), indicates that the
element in question is one of a set of explicit alternatives or serves to correct a specific
item already present in the discourse, as in the following:

(3) a. Did Harry fry an omelet yesterday?
b. Damon fried an omelet yesterday.

Unlike (1a), where there is no explicit set of individuals from which Damon is being
selected as the “omelet fryer”, in (3a) an explicit alternative “omelet fryer” is being

2

Topic, the third component of information structure, describes which discourse referent focused
information should be associated with, as in the mention of Damon in “As for Damon, he fried an omelet.”
The current studies do not address the prosodic realization of topic.

Acoustic correlates of information structure 5
introduced: Harry. The sentence (3b) in this context thus presents information (i.e.,
Damon) which explicitly contrasts with, or contradicts, some information which has been
introduced into the discourse.
There is no consensus in the literature regarding the relationship between noncontrastive focus and contrastive focus. Some researchers have treated non-contrastive
focus and contrastive focus as separate categories of information structure (Chafe, 1976;
Halliday, 1967; Rochemont, 1986; Molnar, 2002), whereas others have argued that there
is no principled difference between the two (e.g., Bolinger 1961, Rooth, 1985, Rooth,
1992). According to Rooth (1992), for example, each expression evokes two semantic
representations: the expression’s actual meaning, and a set of alternatives. If a
constituent in the expression is focused, then the alternative set contains the expression
itself and all expressions with an alternative substituted for the focus-marked constituent;
if there is no focus within the expression, the alternative set consists only of the
expression itself. Rooth would therefore argue that Damon in (1a) is focused and
introduces alternative propositions that differ only in the agent of the event ({Damon
fried an omelet, Harry fried an omelet, Ada fried an omelet, ...}), even if no alternatives
are explicitly mentioned. In (3a), Damon also evokes alternative omelet fryers, and
therefore has the same focus structure as (1a), but the context makes a specific alternative
(Harry) more salient than other potential alternatives. Importantly, from Rooth’s
standpoint, it does not matter whether the alternatives are explicit in the discourse or not:
the meaning of the expression is the same.
The second dimension along which focused elements can vary is focus breadth
(Selkirk, 1984; 1995; Gussenhoven, 1983; 1999), which refers to the size of the set of
focused elements. Narrow focus refers to cases where only a single aspect of an event
(e.g., the agent, the action, the patient, etc.) is focused, whereas wide focus focuses an

Acoustic correlates of information structure 6
entire event. Take, for example, the difference between (5) as an answer to (4a) versus as
an answer to (4b):

(4) a. What did Damon fry last night?
b. What happened last night?

(5) Damon fried an omelet last night.

(4a) narrowly focuses the patient of frying, omelet in (5), while (4b) widely focuses the
entire event of Damon frying an omelet.
The information status of a sentence element can be conveyed in at least three
ways: (1) using word order (i.e., given information generally precedes focused
information) (e.g., Birner, 1994, Clark & Clark, 1978); (2) using particular lexical items
and syntactic constructions (e.g., using cleft constructions such as “It was Damon who
fried an omelet”) (Lambrecht, 2001); and (3) using prosody. Prosody – which we focus
on in the current paper – refers to the way in which words are grouped in speech, the
relative acoustic prominence of words, and the overall tune of an utterance. Prosody is
comprised of acoustic features like fundamental frequency (F0), duration, and loudness,
the combinations of which give rise to the psychological percepts like phrasing
(grouping), stress (prominence), and tonal movement (intonation).
The goal of the current paper is to investigate the prosodic realization of
information structure in simple English subject-verb-object (SVO) sentences like (2),
with the goal of addressing the following questions:
1) First, do speakers prosodically distinguish focused and unfocused elements?
This question can be broken down into further questions:

Acoustic correlates of information structure 7
(1a) Do speakers distinguish focused elements that have an explicit contrast
set in the discourse from those that do not?
(1b) Do speakers distinguish sentences in which only the object is focused
from those in which the entire event is focused?
(2) What are the acoustic features associated with these different aspects of
information structure?
(3) How well can listeners retrieve this information from the signal?

Although the current experiments are all performed on English, the answers to
these questions will likely be similar for other West Germanic languages. However, the
relationship between prosodic features and information structure across different
languages and language groups remains an open question.
In the remainder of the introduction, we briefly lay out two approaches to the study
of the relationship between prosody and information structure, and summarize empirical
studies which have explored how information structure is realized acoustically and
prosodically. We then discuss methodological issues present in previous studies which
call into question the generalizeability of the reported findings, and outline how the
current methods were designed to better address these questions.
Empirical investigations of prosody and information structure
Two perspectives on the relationship between the acoustics of the speech signal and
the meaning associated with various aspects of information structure have been
articulated in the literature. According to the direct-relationship approach, sets of
acoustic features are directly associated with particular meanings (Fry, 1955; Lieberman,
1960; Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986; Pell, 2001; Xu & Xu,
2005). In contrast, according to the indirect-relationship approach (known as the

Acoustic correlates of information structure 8
intonational phonology framework), the relationship between acoustics and meaning is
mediated by phonological categories (Ladd, 1996; Gussenhoven, 1983; Pierrehumbert,
1980; Dilley, 2005; Hawkins & Warren, 1991). In particular, the phonetic prosodic cues
are hypothesized to be grouped into prosodic categories which are, in turn, associated
with particular meanings. The experiments in the current paper were not designed to
decide between these two approaches. However, In the current paper, we will initially
discuss our experiments in terms of the direct-relationship approach, because it is more
parsimonious. In the general discussion, we will show how the results are also
compatible with the indirect-relationship approach.
Turning now to previous empirical work on the relationship between prosody and
information structure, we start with studies of focused vs. given elements. Several
studies have demonstrated that focused elements are more acoustically prominent than
given elements. However, there has been some debate about which acoustic features
underlie a listener’s perception of acoustic prominence. Some features that have been
proposed to be associated with prominence include pitch (i.e. F0) (Lieberman, 1960;
Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986), duration (Fry, 1954; Beckman,
1986), loudness (i.e. intensity) (Kochanski, Grabe, Coleman, & Rosner, 2005; Beckman,
1986; Turk and Sawusch, 1996), and voice quality (Sluijter & van Heuven, 1996).
In early work on lexical stress, Fry (1954) and Liberman (1960) argued that
intensity and duration of the vowel of the stressed syllable contributed most strongly to
the percept of acoustic prominence, such that stressed vowels were produced with a
greater intensity and a longer duration than non-stressed vowels. In experiments on
phrase-level prominence, Cooper et al. (1985) and Eady and Cooper (1986) also noted
that more prominent syllables are longer than their non-prominent counterparts. Cooper
et al. (see also Liberman, 1960); Rietveld & Gussenhoven, 1985; Gussenhoven et al.,

Acoustic correlates of information structure 9
1997; and Terken, 1991) also argued that F0 was a highly important acoustic feature
underlying prominence. Others have argued that the strongest cue to prominence is
intensity (e.g., Beckman, 1986). More recently, Turk and Sawusch (1996) also found
that intensity (and duration) were better predictors of perceived prominence than pitch, in
a perception task. Finally, in a study of spoken corpora, Kochanski et al. (2005)
demonstrated that loudness (i.e. intensity) was a strong predictor of labelers’ annotations
of prominence, while pitch had very little predictive power.
The question of whether contrastively and non-contrastively focused elements are
prosodically differentiated by speakers, and perceptually differentiated by listeners has
also been extensively debated. Some have argued that there is no difference in the
acoustic features associated with contrastively vs. non-contrastively focused elements
(Cutler, 1977; Bolinger, 1961; t’Hart, Collier, & Cohen, 1990), while others have argued
that some acoustic features differ between contrastively vs. non-contrastively focused
elements (Couper-Kuhlen, 1984; Krahmer & Swerts, 2001; Bartels & Kingston, 1994;
Ito, Speer, & Beckman, 2004). For example, Couper-Kuhlen (1984) reported, on the
basis of corpus work, that speakers produce contrastive focus with a steep drop after a
high F0 target, while high F0 is sustained after non-contrastive focus (see also Krahmer
and Swerts, 2001). However, this finding is in contrast to Bartels and Kingston (1994),
who have argued, based on a series of production studies, that the most salient acoustic
cue to contrastiveness is the height of the peak on a contrastive word, such that a higher
peak is associated with a greater probability of an element being interpreted as
contrastive (see also Ladd and Morton, 1997). Finally, Ito, Speer, & Beckman (2004)
demonstrated that speakers are more likely to use a L+H* accent (i.e. a steep rise from a
low target to a high target), compared to a H* accent (i.e. a gradual rise to a high target),
to indicate an element that has an explicit contrast set in the discourse.

Acoustic correlates of information structure 10
Krahmer and Swerts (2001) observed that listeners were more likely to perceive a
contrastive adjective (e.g., red in red square preceded by blue square) as more prominent
than a new adjective when the adjective was presented with a noun compared to when it
was presented in isolation. They therefore hypothesized that the lack of a consensus in
the literature may be due to the failure of the earlier studies to investigate focused
elements in relation to the prosody of the surrounding elements. Consistent with this
idea, Calhoun (2005) demonstrated that a model’s ability to predict a word’s information
status is significantly improved when information about the acoustics of adjacent words
is included in the model. These results suggest that a more consistent picture of the
acoustic features associated with contrastively and non-contrastively-focused elements
may emerge if acoustic context is taken into account.
Finally, prior work has investigated whether speakers prosodically differentiate
narrow and wide focus. Selkirk (1995), for example, argued that, through a process
called focus projection, an acoustic prominence on the head of a phrase or its internal
argument can project to the entire phrase, thus making the entire phrase focused (see also
Selkirk, 1984; see Gussenhoven, 1983, 1999, for a similar claim). According to Selkirk
(1984) and Gussenhoven (1983) then a clause containing a transitive verb in which the
direct object is acoustically prominent is ambiguous between a reading where the object
alone is focused and a reading where the entire verb phrase is focused. This hypothesis
has been supported in several perception experiments (Welby, 2003; Birch & Clifton,
1995; Gussenhoven, 1983). Welby (2003), for example, demonstrated that listeners rated
a sentence like I read the DISPATCH with a single acoustic prominence on dispatch as a
similarly felicitous response to either a question narrowly focusing the object (i.e. “What
newspaper do you read?”), or a question widely focusing the entire event (i.e. “How do
you keep up with the news?”). However, Gussenhoven (1983) found that at least in some

Acoustic correlates of information structure 11
productions there is actually a perceptible difference between narrow and wide focus
although listeners cannot use this information to reliably tell in which context the
sentence was uttered (see Baumann et al., 2006, for evidence from German showing that
speakers do differentiate between narrow and wide focus, with prosodic cues varying
across speakers). In contrast to Gussenhoven’s perception results, Rump and Collier
(1986) found that listeners can accurately discriminate narrow and wide focus using pitch
cues.
Limitations of previous work
Although the studies summarized above provide evidence for some systematic
differences in the acoustic realization of different aspects of information structure, no
clear picture has yet emerged with regard to any of the three meaning distinctions
discussed above (i.e. focused vs. given elements, non-contrastively focused vs.
contrastively focused elements, and narrow vs. wide focus). Furthermore, previous
studies suffer from several methodological limitations that make the findings
inconclusive. Here, we discuss five limitations of previous studies which the current
studies seek to address in an effort to reveal a clearer picture of the relationship between
acoustic features and information structure.
First, instead of acoustic features, sometimes only ToBI3 annotations are
provided (e.g., Birch & Clifton, 1995; Ito et al., 2004). This includes work of researchers
who adopt the intonational phonology framework and who therefore believe that using
prosodic annotation offers a useful way to extrapolate away from potentially complex
interactions among acoustic features which give rise to the perception of specific
intonational patterns. One particular problem concerns H* and L+H* accents. As
defined in the ToBI system, these accents are meant to be explicit markers of non3

The (ToBI) Tones and Break Indices system was developed in the early 90s as the standard system for
annotation of prosodic features (Silverman et al., 1992).

Acoustic correlates of information structure 12
contrastive focus and contrastive focus, respectively (Beckman & Ayers-Elam, 1997).
However, H* and L+H* are often confused in ToBI annotations (Syrdal & McGory,
2000), and are, in fact, often collapsed in calculating inter-coder agreement (Pitrelli et al.,
1994; Yoon et al., 2004; Breen et al., 2006, submitted). Therefore, it is difficult to
interpret the results of studies which are based on the difference between H* and L+H*
without a discussion of the acoustic differences between these purported categories. In
the current studies, we report acoustic features in order to avoid confusion about what the
ToBI labels might mean and in order to not presuppose the existence of prosodic
categories associated with particular meaning categories of information structure.
A second limitation concerns the method used to generate and select productions
for analysis. A common practice involves eliciting productions from a small number of
speakers (e.g., Baumann et al., 2006; Krahmer & Swerts, 2001), which results in a
potential decrease in experimental power, and could therefore lead to a Type II error. In
addition, several previous experiments have excluded speakers’ data from analysis for not
producing accents consistently (e.g., Eady & Cooper, 1986; Cooper et al., 1985), which
could lead to a Type I error. For the current experiments, we recruited between 13 and
18 speakers. In addition, no speakers’ productions were excluded from the analyses
based on a priori predictions about potential behavior (e.g., placing accents in particular
locations).
A third limitation concerns the tasks used in perception studies. In particular,
some studies asked listeners to make judgments about which of two stimuli was more
prominent (Krahmer & Swerts, 2001), what accent is acceptable in a particular context
(Birch & Clifton, 1995; Welby, 2003), or with which of two questions a particular answer
sounded more natural (Gussenhoven, 1983). The problem with these meta-linguistic
judgments is that they lack a measure of the participants’ interpretation of the sentences.

Acoustic correlates of information structure 13
In the current studies we employ a more natural production-comprehension task, in which
speakers are trying to communicate a particular meaning of a semantically ambiguous
sentence and listeners are trying to understand the intended meaning.
A fourth limitation of previous studies is in how they have dealt with speaker
variability. Presenting data from individual subjects separately, as is commonly done, is
problematic because it fails to capture the shared aspects of individual productions (e.g.,
consistent use by most speakers of some set of acoustic features to mark focused
elements). In the current studies, we combine data across subjects while simultaneously
removing variance due to individual differences using linear regression modeling (e.g.,
Jaeger, 2008).
A fifth limitation is that many have reported differences between conditions based
only on individual acoustic features on single words (Eady & Cooper, 1986; Cooper et
al., 1985; Baumann et al, 2006). If acoustic prominence is perceived in a contextdependent manner, these single-feature/single-word analyses might find spurious
differences, or fail to find real differences. In the current studies, we used discriminant
modeling on the productions in order to simultaneously investigate the contribution of
multiple acoustic features from multiple words in an utterance to the interpretation of
information status of different sentence elements.

Experiments: Overview and general methods
The current paper presents results from three experiments. Experiment 1
investigated whether speakers prosodically disambiguate focus location (subject, verb,
object), focus type (contrastive vs. non-contrastive focus), and focus breadth (narrow vs.
wide) by eliciting semi-naturalistic productions like that in (3b) (e.g., Damon fried an
omelet this morning), whose information status was disambiguated by a preceding

Acoustic correlates of information structure 14
question. Experiment 2 investigated whether speakers disambiguate focus location and
focus type when the task explicitly required them to communicate a particular meaning to
their listeners. Finally, Experiment 3 served as a replication and extension of Experiment
2, in which speakers included an attribution expression (“I heard that”) before the critical
sentence.
The acoustic analysis of the productions elicited in all three experiments
proceeded in three steps. First, we automatically extracted a series of 24 acoustic features
(see Table 2) from the subject, verb, and object of the sentences elicited in Experiments
1, 2, and 3. Second, we subjected all of these features to a stepwise discriminant function
analysis in order to determine which features best discriminated the information status
conditions listed in Table 1 for each of the three experiments. This analysis resulted in a
subset of eight acoustic features. Finally, we used discriminant analyses to evaluate
whether this subset of eight features could effectively discriminate sets of 2 and 3
conditions for each of the three experiments. Specifically, we tested focus location by
comparing the features from productions in which Damon, fried, and omelet were
focused, respectively. We tested focus type by comparing the features from sentences in
which the focused element was contrastively or non-contrastively focused at each of the
three syntactic positions. Last, we tested focus breadth by comparing the features for
sentence with wide-focus to those with narrow object focus. In addition to the analysis of
acoustic features, in Experiments 2 and 3 we investigated whether listeners could
correctly determine the intended information status of the speaker.

Acoustic correlates of information structure 15

Experiment 1
Method
Participants
Nine pairs of participants were recorded. All participants were self-reported native
speakers of American English. All participants were MIT students or members of the
surrounding community. Participants were paid for their participation.
Materials
Each trial consisted of a set-up question and a target sentence, which always had an SVO
structure (e.g., Damon fried an omelet this morning). The target sentence could plausibly
answer any one of the seven set-up questions (see Table 1), which served to focus
different elements of the sentence or the entire event described in the sentence. The first
question focused the entire event (i.e. What happened?). In the remaining conditions,
two factors were manipulated: (1) the element in the target sentence that was focused by
the question (subject, verb, object); and (2) the presence of an explicit contrast set for the
focused element (non-contrastively focused, i.e. explicit contrast set absent, contrastively
focused, i.e. explicit contrast set present).
All subject and object noun phrases (NPs) in the target sentences were bi-syllabic
with first syllable stress, and all verbs were monosyllabic. All subject NPs were proper
names, and object NPs were mostly common inanimate objects, such that the events were
non-reversible. Furthermore, all words were comprised mostly of sonorant phonemes.
These constraints ensured that words could be more easily compared across items, and
facilitated the extraction of acoustic features (which is easier for vowels and sonorant
consonants). An adjunct prepositional phrase (PP) was included at the end of each
sentence so that differences in the production of the object NP due to the experimental
manipulations would be dissociable from prosodic effects on phrase-final, or in this case,

Acoustic correlates of information structure 16
sentence-final, words, which are typically lengthened and produced with lower F0
compared to phrase-medial words (e.g., Wightman et al., 1992).
We constructed 28 sets of materials. Participants saw one condition of each item,
following a Latin Square design. A sample item is presented in Table 1. The complete
set of materials can be found in Appendix A.

Condition

Focus Type

Focused
Argument

Setup Question

1

Non-contrastive

wide

What happened this morning?

2

Non-contrastive

S

Who fried an omelet this morning?

3

Non-contrastive

V

What did Damon do to an omelet this morning?

4

Non-contrastive

O

What did Damon fry this morning?

5

Contrastive

S

Did Harry fry an omelet this morning?

6

Contrastive

V

Did Damon bake an omelet this morning?

7

Contrastive

O

Did Damon fry a chicken this morning?

Table 1: Example item from Experiment 1. The target sentence is “Damon fried an
omelet this morning.”
Procedure
Productions were elicited and pre-screened in a two-part procedure. The first part
was a training session, where participants learned the intended names for pictures of
people, actions, and objects. In the second part, the pairs of participants produced
questions and answers for each other. The method was designed to maximize control
over what speakers were saying, but to also encourage natural-sounding productions.
Pilot testing revealed that having subjects simply read the target sentences resulted in
productions with low prosodic variability. After going through the experiment one time,
the participants switched roles.
Training session
In the training session, participants learned mappings between 96 pictures and
names, so that they could produce the names from memory during the second part of the

Acoustic correlates of information structure 17
experiment. In a PowerPoint presentation, each picture, corresponding to a person, an
action, an object, or a modifier, was presented with its intended name (see Figure 1, left).
The pictures consisted of eight names of people, which were repeated 3-4 items each in
the experimental materials, eight colors (which were used in a concurrently run filler
experiment), 34 verbs, 44 objects, and two temporal modifiers (this morning and last
night). The pictures were presented in alphabetical order, to facilitate memorization and
recall. Participants were instructed to learn the mappings by progressing through the
PowerPoint at their own pace.
When participants felt they had learned the mappings, they were given a picturenaming test, which consisted of 27 items from the full list of 96. The test was identical
for all participants. Participants were told of their mistakes, and, if they made four or
more errors, they were instructed to go back through the PowerPoint to improve their
memory of the picture-name mappings. Once participants could successfully name 23 or
more items on the test, which took between 1 and 3 rounds of testing, they continued with
the second part of the experiment. Early in pilot testing, we discovered that subjects had
poor recall for the names of the people in the pictures. Therefore, in the actual
experiment, subjects could refer to a sheet which had labeled pictures of the people.

Acoustic correlates of information structure 18

Figure 1: Left: Examples from the picture-training task for Experiment 1. Each square
represents a screen shot. Right: Examples of the procedure for the questioner (upper
squares) and answerer (lower squares) for Experiment 1. Two conditions are presented:
Non-contrastive, object (left) and contrastive, verb (right). The top squares represent
screen shots of what the questioner saw on a trial; the bottom squares represent what the
answerer saw on a trial.
Question-Answer Experiment
The experiment was conducted using Linger 2.92 (available at
http://telab.mit.edu/~dr/Linger/), a software platform designed by Doug Rohde for
language processing experiments. Participants were randomly paired and randomly
assigned to the role of questioner or answerer. Participants sat at computers in the same
room such that neither could see the other’s screen. On each trial, as illustrated in Figure
1 (right), the questioner saw a question (e.g., “What did Damon fry this morning?”)
which he/she was instructed to produce aloud for the answerer. The answerer was
instructed to produce an answer aloud using the information contained in the picture on
his/her screen (e.g., “Damon fried an omelet this morning”). The answerer was

Acoustic correlates of information structure 19
instructed to produce complete sentences, including the subject, verb, object, and
temporal abverb,4 and to emphasize the part of the sentence that the questioner had asked
about, or that he/she was correcting. On a random 20% of trials, the answerer was asked
a comprehension question about the answer s/he produced.
Productions were recorded in a quiet room with a head-mounted microphone at a
rate of 44kHz.
Acoustic Feature

Units

Description

duration

ms

Word duration excluding any silence before or after the word.

silence

ms

Duration of silence following the word, not due to stop closure.

duration+silence

ms

The sum of the duration of the word and any following silence.

mean F0

Hz

Mean F0 of the entire word

maximum F0

Hz

Maximum F0 value across the entire word

F0 peak location

0-1

The proportion of the way through the word where the maximum F0 occurs.

minimum F0

Hz

Minimum F0 across the entire word

F0 valley location

0-1

The proportion of the way through the word where the minimum F0 occurs.

initial F0

Hz

early F0

Hz

Mean F0 of the initial 5% of the word
Mean F0 value of 5% of the word centered at the point 25% of the way
through the word

center F0

Hz

late F0

Hz

Mean F0 value of 5% of the word centered on the midpoint of the word
Mean F0 value of 5% of the word centered on a point 75% of the way
through the word

final F0

Hz

Mean F0 of the last 5% of the word

1st quarter F0

The difference between initial F0 and early F0.

Hz

The difference between early F0 and center F0.

3rd quarter F0

Hz

The difference between center F0 and late F0.

4th quarter F0

Hz

The difference between late F0 and final F0.

mean intensity

dB

Mean intensity of the word

maximum intensity

dB

Maximum dB level in the word

minimum intensity
intensity peak
location
intensity valley
location

dB
0-1

Minimum dB level in the word
The proportion of the way through the word where the maximum intensity
occurs
The proportion of the way through the word where the minimum intensity
occurs

maximum amplitude

4

Hz

2nd quarter F0

Pascal

0-1

Maximum amplitude across the word

In the absence of explicit instruction to produce complete sentences, with a lexicalized subject, verb, and
object, speakers would likely resort to pronouns or would omit given elements altogether (e.g., “What did
Damon fry this morning?” “An omelet.”). A complete production account of information structure
meaning distinctions should include not just the prosodic cues used by the speakers, but also syntactic and
lexical production choices, as well as the interaction among these different production strategies. However,
because we focus on prosody in the current investigation, we wanted to be able to compare acoustic
features across identical words. Thus, we required that participants always produce a subject, verb, object
and adverb on every trial.

Acoustic correlates of information structure 20
energy

(Pascal)2 x
Duration

Table 2: Acoustic features extracted from each word in the target sentence for
Experiments 1-3. Stepwise discriminant analyses demonstrated that the measures in bold
provided the best discrimination among conditions and were used in all reported
analyses.
Results
Of the 504 speaker productions from the Question-Answer Experiment, 87 (17%) were
discarded because (a) the answerer failed to use the correct lexical items, (b) the answerer
was disfluent, or (c) the production was poorly recorded. The 417 remaining productions
were subjected to the acoustic analyses described below.
Acoustic Features
Based on previous investigations of prosody and information structure (Fry, 1955;
Lieberman, 1960; Eady et al., 1985; Cooper & Eady, 1986, Bartels & Kingston, 1994;
Krahmer & Swerts, 2001; Baumann et al., 2006), we chose a set of acoustic features to
analyze (see Table 2). These features were obtained automatically using the Praat
program (Boersma & Weenink, 2006). The measures of F0 computed over portions of
the words (e.g., 1st quarter F0) were chosen in order to investigate how F0 changes across
the syllable might contribute to the differentiation of conditions.
Our first goal was to determine which of the 24 candidate acoustic features
mediated differences among conditions. We conducted a series of stepwise linear
discriminant analyses5 on all of the data collected in Experiments 1, 2 and 3 reported in
the current paper. In order to determine the features to be used in the analyses of all three
experiments, we performed a separate stepwise analysis on the data from each
experiment separately. For each analysis we entered all 24 acoustic features across each
5

Linear discriminant analysis (LDA) calculates a function, computed as a linear combination of all
predictors entered, which results in the best separation of two or more groups. For two groups, only one
function is computed. For three groups, the first function provides the best separation of group 1 from
groups 2 & 3; a second, orthogonal, function provides the best separation of groups 2 and 3, after
partialling out variance accounted for by the first function. Stepwise LDA is an iterative procedure which
adds predictors based on which of the candidate predictors provide the best discrimination.

Acoustic correlates of information structure 21
of the three sentence positions (subject, verb, and object) as possible predictors of the
seven experimental conditions, resulting in 72 predictors. Across the three analyses, the
acoustic features which consistently resulted in the best discrimination of conditions were
(1) duration + silence, (2) mean F0, (3) maximum F0, and (4) maximum intensity at the
positions of the (a) Subject, (b) Verb, and (c) Object. The fact that these 12 features (four
acoustic features across three sentence positions) consistently discriminated among
conditions across three independent sets of productions (from different speakers and
across somewhat different sets of materials) serves as evidence that these features are
underlying speaker- and material-independent differentiation of information structure.
Therefore, we use only these 12 features in the linear discriminant analyses reported for
the individual experiments in the paper.
Computing Residual Values
Because of differences among individuals, including age, gender, speech rate and
level of engagement with the task, speakers produce very different versions of the same
sentence even within the same experimental condition, thus adding variance to the
acoustic features of interest. Similarly, there is likely to be variability associated with
different items due to lexical and world knowledge factors. Researchers have previously
dealt with the issue of acoustic variability between speakers by normalizing pitch and/or
duration by speaker (e.g., Shriberg, Stolcke, Hakkani-Tur, & Tur, 2000; Shriberg et al.,
1998; Wightman, Shattuck-Hufnagel, Ostendorf, & Price, 1992). In order to remove
speaker- and item-related variance in the current studies, we computed linear regression
models in which speaker (n = 18) and item (n = 28) predicted each of the 12 acoustic
features identified in the stepwise discriminant analyses described in the previous section.
From each of these models, we calculated the predicted value of each acoustic feature for
a specific item from a specific speaker. We then subtracted this predicted value from

Acoustic correlates of information structure 22
every production. The differences among the resulting residual values should reflect
differences in the acoustic features due only to the experimental manipulations. All
subsequently reported analyses were performed on these residual values.

Focus Location
The extent to which a discriminant function analysis can separate data points into two or
more groups is calculated with a statistical test, Wilks’s lambda6.
To determine how well the acoustic features could differentiate focus location in
speakers’ productions, we computed a model where the 12 acoustic predictors were used
to discriminate among three focus locations: Subject, Verb or Object. In this analysis, we
are averaging across the contrastive and non-contrastive condition for each location.
The overall Wilks’s lambda of the model was significant, Λ = .46, χ2(24) = 271, p
< .001, indicating better-than-chance differentiation of subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .84, χ2(24) = 62.65,
p < .001, indicating that the acoustic predictors could also differentiate verb focus from
object focus (see Figure 2). Leave-one-out classification correctly classified 67% of the
productions. The model correctly classified subject focus 76% of the time, verb focus
58% of the time, and object focus 66% of the time. Table 3 presents the standardized
canonical discriminant function coefficients of the model.7

6

Wilks's lambda is a measure of the distance between groups on means of the independent variables, and is
computed for each function. It ranges in size from 0-1; lower values indicate a larger separation between
groups. The extent to which the model can effectively discriminate a new set of data is simulated by a
leave-one-out classification, in which the acoustic data from each production are iteratively removed from
the dataset, the model is computed, and the left-out case is classified by the resultant functions.
7
The coefficients in Table 3 indicate which acoustic features best discriminate focus location, such that
larger absolute values indicate a greater contribution of that feature to discrimination. For example,
inspection of the plot in Figure 2 and the coefficients in the Focus Location columns of Table 3 shows that
the acoustic features of Damon score around zero, or lower, on the first function (-0.002, 0.001, -0.01, and 0.06) and around zero on the second function (-0.003, 0.021, -0.016, -0.101). Fried shows a different
pattern; specifically, the acoustic features of fried have coefficients around zero for the first function, and
negative coefficients for function 2. Finally, omelet shows a third pattern: its acoustic correlates are
centered around zero on Function 1, but are high on Function 2.

Acoustic correlates of information structure 23
Figure 3 graphically presents the mean values of the four features, demonstrating
that across all three focus locations the intended focus location is produced with the
highest maximum intensity, the longest duration and silence, and the highest relative F0.

Function
1

Function
2

Subj
Focus

Verb
Focus

Obj
Focus

omelet

Focus
Breadth

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.001
-0.006
0.002
-0.037

0.004
0.011
0.001
0.181

0.008
0.011
-0.002
-0.137

0.003
-0.014
0.002
-0.026

0.004
-0.019
0.006
0.189

0.003
0.000
0.003
0.199

fried

Focus Type

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

0.007
0.024
0.002
0.094

-0.001
-0.003
-0.002
-0.010

0.007
0.000
0.004
-0.076

0.002
-0.040
-0.007
0.131

-0.001
-0.013
0.013
-0.043

0.005
-0.025
0.003
0.011

Damon

Focus Location

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.002
0.001
-0.010
-0.060

-0.003
0.021
-0.016
-0.101

0.005
-0.016
-0.012
0.087

-0.002
-0.007
0.020
0.056

0.005
-0.014
-0.011
-0.225

0.003
0.007
-0.005
-0.123

Acoustic correlates of information structure 24
Table 3: Standardized canonical coefficients of the discriminant functions computed for
Experiment 1.

Figure 2: Separation of focus locations on two discriminant functions in Experiment 1.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the upper left quadrant; productions of verb focus are
clustered in the lower half of the plot; productions of object focus are clustered in the
upper right quadrant.

Acoustic correlates of information structure 25

Damon
fried
omelet

Figure 3: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 1.
Focus type
To determine how well the acoustic features could differentiate the type of focus
(i.e. non-contrastive vs. contrastive) in speakers’ productions, we computed three models
in which the 12 acoustic predictors were used to discriminate between two focus type
groups. The three models investigated differences between non-contrastive and
contrastive focus at the three focus locations: subject, verb, and object.
Focus Type – Subject Position
The overall Wilks’s Lambda was not significant, Λ = .898, χ2(12) = 11.95 p = .45,
indicating that the acoustic features could not discriminate between non-contrastive and

Acoustic correlates of information structure 26
contrastive focus. Because the overall model is not significant, we do not present the
scores of the specific acoustic features or the classification statistics here or in the
analyses below.
Focus Type – Verb Position
The overall Wilks’s Lambda was not significant, Λ = .851, χ2(12) = 17.92 p = .12,
indicating that the acoustic features could not discriminate between non-contrastive and
contrastive focus.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .82, χ2(12) = 22.63 p < .05,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus above chance level. Leave-one-out classification correctly classified
59% of the productions. The model correctly classified non-contrastive focus 59% of the
time, and contrastive focus 59% of the time.
The coefficients in the Object Focus column of Table 3 indicate that intensity and
mean F0 contribute most to classification. Figure 4 graphically presents the mean values
of the four features, demonstrating that contrastive focus is produced with a higher
maximum intensity, a longer duration and silence, and higher maximum F0. Noncontrastive focus is produced with a higher mean F0.

Acoustic correlates of information structure 27

Damon
fried
omelet

Figure 4: Values for non-contrastive focus and contrastive focus type on the four
discriminating acoustic features when the direct object “omelet” is focused in
Experiment 1.
Wide Focus vs. Narrow Focus
To determine how well the acoustic features could differentiate focus breadth, we
computed a model in which the 12 critical predictors were used to discriminate between
productions where the entire sentence was focused and productions where the object was
non-contrastively or contrastively focused.
The overall Wilks’s Lambda was significant, Λ = .75, χ2(12) = 47.83, p < .001,
indicating that the acoustic features could successfully discriminate between conditions
where the entire event is focused and conditions where the object is narrowly focused.

Acoustic correlates of information structure 28
Leave-one-out classification correctly classified 72% of the productions. The model
correctly classified wide focus 67% of the time, and narrow focus 74% of the time.
The standardized canonical discriminant function coefficients in the Focus
Breadth column of Table 3 indicate that maximum intensity contributes most to focus
breadth classification. Figure 5 graphically presents the mean values of the four features,
demonstrating that wide focus is produced with a more uniform duration + silence and
maximum F0 across the sentence than object focus. Wide focus is also produced with a
more uniform, though overall greater, intensity than object focus.

Damon
fried
omelet

Figure 5: Values for wide focus vs. narrow object focus on the four discriminating
acoustic features in Experiment 3.
Discussion

Acoustic correlates of information structure 29
Focus Location
The results demonstrate that speakers consistently provide acoustic cues which
disambiguate focus location. Specifically speakers indicated focus with increased
duration, higher intensity, higher mean F0, and higher maximum F0. Furthermore, these
results are consistent with the pattern reported in Eady & Cooper (1986), such that the
word preceding a focused word is less prominent (produced with shorter duration, lower
intensity and lower F0) than the focused word, and the word following the focused word
is less prominent than the word preceding the focused word. Previous studies (Eady et
al., 1986; Rump and Collier, 1986) have reported this reduction in acoustic prominence
following focused elements as being mainly indicated by lower F0 on the post-focal
words, though in our data we also find evidence of this reduction in measures of duration
and intensity.
Focus Type
The results from Experiment 1 indicate that in semi-naturalistic productions
speakers do not systematically differentiate between different focus types (focused
elements which have explicit contrast sets in the discourse and those which do not).
Specifically, at two out of three sentence positions, a discriminant function analysis could
not successfully classify speakers’ productions of contrastively vs. non-contrastively
focused elements. The observation that speakers successfully discriminated contrastive
and non-contrastive focus in object position, but not in subject or verb positions, is
perhaps suggestive, but is likely due to a lack of experimental power, a limitation which
will be addressed in Experiment 2.
Focus Breadth
The results from Experiment 1 demonstrate that speakers do systematically mark
focus breadth prosodically. Narrow object focus is produced with the highest maximum
F0, longest duration, and maximum intensity of the object noun, relative to the other

Acoustic correlates of information structure 30
words in the sentence. For wide focus, the acoustic features are more similar across the
sentence; only intensity and mean F0 are higher on the object than on the other words in
the sentence. These differences are subtle, but sufficient for the model to successfully
discriminate the productions.
The fact that the model failed to systematically classify productions by focus type
(with the exception of the object position), while achieving high accuracy in focus
location and focus breadth indicates that speakers were not marking focus type with
prosody in Experiment 1. However, the method used to elicit productions did not require
that subjects be aware of the information structure ambiguity of the materials. Evidence
from other production studies suggests that speakers may not prosodically disambiguate
ambiguous productions if they are not aware of the ambiguity. Albritton, McKoon, and
Ratcliff (1996), for example, demonstrated that speakers did not disambiguate
syntactically ambiguous constructions like “Dave and Pat or Bob” unless they were
aware of the ambiguity (see also Snedeker and Trueswell, 2003, but cf. Kraljic and
Brennan, 2005, and Schafer, Speer, Warren, and White, 2000, for evidence that speakers
do disambiguate syntactically ambiguous structures even in the absence of ambiguity
awareness). Experiment 2 was designed to be a stronger test of speakers’ ability to
differentiate focus location, focus type, and focus breadth. We used materials similar to
those in Experiment 1, with two important methodological modifications. First, instead
of producing the answers to questions with no feedback, the speaker’s task now involved
trying to enable the answerer to choose the question that s/he was answering from a set of
possible questions. Moreover, we introduced feedback so that the speaker would always
know whether his/her partner had chosen the correct answer. Second, we changed the
design from a between- to a within-subjects manipulation. This ensured that speakers

Acoustic correlates of information structure 31
were aware of the manipulation, as they were producing the same answer seven times
with explicit instructions to differentiate their answers for their partner.
In addition to making the speaker’s task explicit, the new design also allowed us
to analyze the subset of the productions for which the listeners could successfully identify
the question-type and which therefore contain sufficient information for differentiating
utterances along the three relevant dimensions of information structure.

Experiment 2
Method
Participants
Seventeen pairs of participants were recorded for this experiment. Subjects were MIT
students or members of the surrounding community. All reported being native speakers
of American English. None had participated in Experiment 1. Participants were paid for
their participation.
Materials
The materials had the same structure as those from Experiment 1, though the
critical words differed. Specifically, a larger set of names and a wider variety of
temporal adverbs were used, and some verbs and objects differed from Experiment 1.
Unlike Experiment 1, each subject pair was presented with all seven versions of each of
14 items, according to a full within-subjects within-items design. All materials can be
found in Appendix B.
Procedure
Two participants sat at computers in the same room such that neither could see the
other’s screen. One participant was the speaker, and the other was the listener. Speakers
were told that they would be producing answers to questions out loud for their partners

Acoustic correlates of information structure 32
(the listeners), and that the listeners would be required to choose which question the
speaker was answering from a set of seven choices.
At the beginning of each trial, the speaker was presented with a question on the
computer screen to read silently. After pressing a button, the answer to the question
appeared below the question, accompanied by a reminder to the speaker that s/he would
only be producing the answer aloud, and not the question. Following this, the speaker
had one more chance to read the question and answer, and then he/she was instructed to
press a key to begin recording (after being told by the listener that he/she is ready), to
produce the answer, and then to press another key to stop recording.
The listener sat at another computer, and pressed a key to see the seven questions
that s/he would have to choose his/her answer from. When s/he felt familiar with the
questions, s/he told the speaker s/he was ready. After the speaker produced a sentence
out loud for the listener, the listener chose the question s/he thought the speaker was
answering. If the listener answered incorrectly, his/her computer produced a buzzer
sound, like the sound when a contestant makes an incorrect answer on a game show.
This cue was included to ensure that speakers knew when their productions did not
contain enough information for the listener to choose the correct answer.8
Results – Production
Two speaker-listener pairs were excluded as the Listener did not achieve
comprehension accuracy greater than 20%. One further pair was excluded as one
member was not a native speaker of American English. Finally, another pair of subjects
was excluded because they did not take the task seriously, and produced unnaturally
emphatic contrastive accents, often shouting the target word, and laughing while doing

8

In early pilots in which there was no feedback for incorrect responses, we observed that listeners were at
chance in choosing the correct question.

Acoustic correlates of information structure 33
so. These exclusions left a total of 13 pairs of participants whose responses were
analyzed.
Sixty-seven of the 1274 trials (5%) were excluded because (a) the speaker failed
to produce the correct words, (b) the speaker was disfluent, or (c) the production was
poorly recorded. Analyses were performed on all trials, and on the subset of trials for
which the listener correctly identified the question. The results were very similar in the
two analyses. For brevity of presentation, we present results from analyses conducted on
the correct trials (n = 660, 55%). The productions from Experiment 2 were analyzed
using the acoustic features chosen in the feature-selection procedure described in
Experiment 1. All analyses were performed on the residual values of these features, after
removing speaker and item variance with the method described in Experiment 1.
Focus Location
The overall Wilks’s lambda was significant, Λ = .085, χ2(24) = 1335, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .306, χ2(11) = 641, p
< .001, indicating that the acoustic features could also discriminate verb focus from
object focus (see Figure 6).
Leave-one-out classification correctly classified 93% of the productions. For
individual levels of focus location, the discriminant function correctly classified subject
focus 94% of the time, verb focus 90% of the time, and object focus 95% of the time.
The standardized canonical coefficients in the first two columns of Table 4
indicate that the acoustic features contributing most to the discrimination of focus
location are once again mean F0 and maximum intensity, though the other two features
are also contributing. In fact, inspection of the acoustic feature means in Figure 7

Acoustic correlates of information structure 34
demonstrate that the highest value of every acoustic feature is associated with the
intended focused item, with the exception of mean F0 when the subject is focused.

Figure 6: Separation of focus locations on two discriminant functions for Experiment 2.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the lower left quadrant of the plot; productions of verb
focus are clustered in the lower right quadrant; productions of object focus are clustered
in the lower half.
Focus Location

Focus Type

Focus Breadth

Function 2

Subject
Focus

Verb Focus

Object
Focus

Duration+ silence

omelet

Function 1
-0.001

0.004

0.004

0.006

0.003

0.003

Mean F0

-0.006

0.011

-0.003

0.005

-0.023

0.000

Maximum F0

0.002

0.001

0.004

-0.009

-0.003

0.003

Maximum Intensity

-0.025

0.183

-0.052

-0.171

0.012

0.199

Acoustic correlates of information structure 35
-0.002

0.006

0.002

-0.007

0.005

Mean F0

0.024

-0.005

0.001

-0.022

0.006

-0.025

Maximum F0

0.001

-0.002

-0.007

0.001

0.003

0.003

0.093

-0.016

-0.105

0.063

-0.084

0.011

Duration+ silence

Damon

0.007

Maximum Intensity

fried

Duration+ silence

-0.002

-0.002

0.002

0.005

0.009

0.003

Mean F0

0.003

0.021

-0.010

0.004

-0.009

0.007

Maximum F0

-0.011

-0.015

-0.014

-0.012

-0.006

-0.005

Maximum Intensity

-0.067

-0.097

0.094

-0.014

0.010

-0.123

Table 4: Standardized canonical coefficients of all discriminant functions computed for
Experiment 2.

Damon
fried
omelet

Figure 7: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 2.

Acoustic correlates of information structure 36
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .633, χ2(12) = 81.41, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
75% of the productions. The model correctly classified non-contrastive focus 78% of the
time, and contrastive focus 71% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that maximum intensity at all three locations (i.e. large intensity differences between the
subject and verb and the subject and object) contributes most to classification. Figure 8
graphically presents the mean values of the four features, demonstrating that, in addition
to intensity differences, contrastive focus is produced with longer duration and silence, as
well as lower mean and maximum F0.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .654, χ2(12) = 72.27, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
72% of the productions. The model correctly classified non-contrastive focus 70% of the
time, and contrastive focus 75% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that, once again maximum intensity contributes most to classification. Figure 9
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a higher maximum intensity, and a longer duration and silence,
than non-contrastive focus. Once again, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

Acoustic correlates of information structure 37
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .793, χ2(12) = 41.3, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
67% of the productions. The model correctly classified non-contrastive focus 69% of the
time, and contrastive focus 66% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that contrastive focus is most strongly associated with lower mean F0. Figure 10
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a lower mean and maximum F0 than non-contrastive focus.

Damon
fried
omelet

Figure 8. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 2.

Acoustic correlates of information structure 38

Damon
fried
omelet

Figure 9. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 2.

Acoustic correlates of information structure 39

Damon
fried
omelet

Figure 10. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “omelet” is focused in Experiment 2.
Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .59, χ2(12) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 84% of productions; wide
focus was correctly classified 77% of the time, and object focus was correctly classified
88% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 4
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Although intensity is contributing most strongly
to classification, inspection of the acoustic means in Figure 11 indicates that wide focus

Acoustic correlates of information structure 40
is marked by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is marked by greater prominence on the
object, reflected in longer duration, higher F0, and higher intensity.

Damon
fried
omelet

Figure 11: Values for wide vs. narrow object focus on the four discriminating acoustic
features in Experiment 2.

Acoustic correlates of information structure 41
Results – Perception

Figure 12. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 2.
Listeners’ choices of question sorted by the intended question are plotted in
Figure 12. Listeners’ overall accuracy was 55%. To determine whether listeners were
able to determine the speaker’s intended sentence meaning, we compared each subject's
responses to chance performance. Specifically we assessed, for focus location and focus
type, whether each subject's proportion of correct responses exceeded chance; wide
focus productions were excluded from the analysis, so that chance performance for focus

Acoustic correlates of information structure 42
location was .33, and chance performance for focus type was .5. Results demonstrated
that listeners were able to successfully identify focus location: all 13 subjects’
performance significantly exceeded chance performance, p = .05, two-tailed. However,
listeners were unable to successfully identify focus type: only three of 13 subjects
performed at above-chance levels (based on the binomial distribution), p = .05, twotailed. To investigate focus breadth, we assessed, for wide focus and narrow object focus
separately, whether each subject's proportion of correct responses exceeded chance. For
these analyses, we excluded subject and verb focus productions, so that chance
performance was .33 for wide focus, and .67 for narrow object focus. Results
demonstrated that listeners were moderately successful at identifying focus breadth: six
of 13 subjects identified wide focus at rates above chance, and nine out of 13 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.
Discussion
The production results replicated the two main findings from Experiment 1, and provided
evidence for acoustic discrimination of focus type across sentence positions as well.
First, these results demonstrated that focused elements have longer durations than nonfocused elements, incur larger F0 excursions, are more likely to be followed by silence,
and are produced with greater intensity. Second, speakers consistently differentiate
between wide and narrow focus by producing the object in the latter case with higher F0,
longer duration, and greater intensity. Specifically, although object focus was indicated
by increased duration, higher intensity, and higher F0 on the object than on the subject or
the verb, wide focus was indicated by comparatively greater duration, higher intensity,
and higher F0 on the subject and the verb, and shorter duration, lower intensity, and
lower F0 on the object. These results are consistent with those obtained by Baumann et

Acoustic correlates of information structure 43
al. (2006), who demonstrated that narrow focus on an element was indicated with longer
duration and a higher F0 peak than wide focus on an event encompassing that element.
Most importantly, although speakers in Experiment 1 did not differentiate
conditions with and without an explicit contrast set for the focused element (except for
the object position), these conditions were differentiated by speakers in Experiment 2, at
every syntactic position. There are two possible interpretations of this difference. First,
in Experiment 1, speakers produced only four versions of each of the seven conditions,
whereas speakers in Experiment 2 and 3, reported below, produced 14 versions of each of
the seven conditions, resulting in greater power in the latter two experiments. The fact
that, in Experiment 2, speakers successfully discriminated contrastive and noncontrastive focus in all three positions, suggests that the lack of such an effect in
Experiment 1 could be due to a lack of power.
As mentioned above, the difference in the findings between Experiments 1 and 2
is also consistent with results from Allbritton et al. (1996) and Snedeker and Trueswell
(2003) who demonstrated that speakers do not disambiguate syntactically ambiguous
sentences with prosody unless they are aware of the ambiguity. The current results
demonstrate a similar effect for acoustic prominence, such that speakers do not
differentiate two kinds of acoustically prominent elements (contrastively vs. noncontrastively focused elements) unless they are aware of the information structure
ambiguity in the structures they are producing.
The discriminant analyses indicated that contrastively focused words were
produced with longer durations and higher intensity than non-contrastively focused
words, but that non-contrastively focused words were produced with higher F0 than
contrastively focused words. This latter finding is surprising when compared to some
previous studies. For example, Ladd & Morton (1997) found that higher F0 and larger

Acoustic correlates of information structure 44
F0 range is perceived as more ‘emphatic’ or ‘contrastive’ by listeners. Similarly, Ito and
Speer (2008) demonstrated that contrastively focused words were produced with higher
F0 than non-contrastive ones. Given the unexpected results, we inspected individual
pitch tracks to more closely observe the F0 patterns across the entire utterances. The
pitch tracks presented in Figure 13 were generated from the productions of a typical
speaker, and they exemplify the higher F0 observed for non-contrastive focus than
contrastive focus in the subject position (A vs. B) and verb position (C vs. D).
Contrastive focus on the object is realized with the same F0 as non-contrastive focus on
the object (E vs. F).

300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Non-Contrastive

Given

an

omelet

yesterday

Given

0

1.433
Time (s)

Acoustic correlates of information structure 45
A. Non-contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Contrastive

an

Given

omelet

yesterday

Given

0

1.81
Time (s)

B. Contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Non-Contrastive

omelet

yesterday

Given

0

1.514
Time (s)

C. Non-contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Contrastive

an

omelet

<SIL>

yesterday

Given

0

2.377
Time (s)

Acoustic correlates of information structure 46
D. Contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Given

omelet

yesterday

Non-Contrastive
1.582

0
Time (s)

E. Non-contrastive Object Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Given

an

omelet

yesterday

Contrastive
2.188

0
Time (s)

F. Contrastive Object Focus
Figure 13. Pitch tracks for non-contrastive and contrastive subject focus, non-contrastive
and contrastive verb focus, and non-contrastive and contrastive object focus,
respectively, from a typical speaker from Experiment 2.
Note that our finding that non-contrastive focus is realized with higher F0 than
contrastive focus is still consistent with the claim that contrastive focus is more
prominent than non-contrastive focus. As the graphs in Figures 8-10, and the pitch tracks
in Figure 13 indicate, although contrastive elements were consistently produced with
lower pitch, they were also consistently produced with longer durations and greater

Acoustic correlates of information structure 47
intensity than non-contrastive elements. As reviewed in the introduction, there is
9

evidence that intensity and duration can convey prominence more effectively than higher
pitch (Fry, 1954, Lieberman, 1960, Beckman, 1986; Turk & Sawusch,1996; Kochanski et
al., 2005). Our data are therefore consistent with prior claims that contrastive focus is
produced with greater prominence than non-contrastive focus.
As discussed in the introduction, the production elicitation and analysis methods
used in the current experiment are more robust than methods used in many previous
studies, including those whose results are inconsistent with the current findings. In
particular, the current results are based on productions from naïve subjects in a
communication task, and the analyses were performed on data with speaker and item
variability removed. The current results are therefore more likely to reflect the
underlying generalizations about the relationship between acoustics and meaning.
The perception results only partially mirrored the production results. Consistent
with the production results, listeners were highly successful in discriminating among the
three focus locations. In contrast to the production results, however, listeners were only
moderately successful in identifying focus type (non-contrastive vs. contrastive) from the
speakers’ productions. In fact, listeners most often confused non-contrastive focus with
contrastive focus (see Figure 12). These results suggest that, even though speakers may
be consistently signaling focus type with their prosody, listeners are not able to exploit
those cues for comprehension.
With regard to focus breadth, the perception results are incompatible with a strong
version of the focus projection hypothesis (Selkirk, 1995). According to this hypothesis,
an acoustic prominence on the object NP can be interpreted as marking the entire clause
9

Importantly, the F0 results are not artifacts of the residualization procedure employed to remove variance
from the acoustic features due to speaker and item. The same numerical pattern of F0 values is observed
whether residualization is employed or not, though only the residualized acoustic features successfully
discriminate focus type.

Acoustic correlates of information structure 48
as focused. Listeners are therefore predicted to treat a production with an acoustically
prominent object NP as ambiguous between the narrow object focus reading and the wide
focus reading. However, as can be seen in Figure 12, listeners correctly identified narrow
object non-contrastive focus 57% of the time, interpreting it as wide focus only 13% of
the time, and correctly identified narrow object contrastive focus 49% of the time,
interpreting it as wide focus only 6% of the time. These results are not consistent with
Gussenhoven’s (1983) finding that listeners cannot reliably distinguish between narrow
objects focus and wide focus.
Experiments 1 and 2 provide evidence that speakers systematically indicate focus
location and focus breadth using a set of four acoustic features. These experiments
further suggest that speakers can, but don’t always, indicate focus type. In particular, the
results suggest that speakers only prosodically differentiate contrastive from noncontrastive focus when they are aware of the meaning ambiguity and/or when the task
involves conveying a particular meaning to a listener.
To further investigate the speakers’ ability to prosodically differentiate contrastive
from non-contrastive focus, we conducted an additional experiment. Acoustic analyses
in Experiments 1 and 2 were limited to three words (i.e. subject, verb, object) in the
sentence. However, in natural productions, speakers’ utterances are often prefaced by
attribution expressions (e.g., “I think” or “I heard”), or expressions of emotional attitudes
towards the described events (e.g., “Unfortunately”, or “Luckily”). It is therefore
possible that contrastive information might be partially conveyed by prosodically
manipulating these kinds of expressions. We explored this possibility in Experiment 3, in
which we had speakers produce target SVO constructions with a preamble. Experiment 3
was also intended to serve as a replication of the results of Experiment 2; in particular,

Acoustic correlates of information structure 49
the somewhat unexpected finding that non-contrastive focus is produced with higher F0
than contrastive focus.

Experiment 3
Method
Participants
Fourteen pairs of participants (speakers and listeners) were recorded for this
experiment. Subjects were MIT students or members of the surrounding community. All
reported being native speakers of American English. None had participated in
Experiments 1 or 2. Participants were paid for their participation.
Materials
The materials for Experiment 3 were identical to those from Experiment 1
described above with the exception that an attribution expression (“I heard that”) was
appended to the beginning of each target sentence.
Procedure
The procedure for Experiment 3 was identical to that for Experiment 2.
Results – Production
Four speaker-listener pairs were excluded as the listener did not achieve
comprehension accuracy greater than 20%. These exclusions left a total of 10 pairs of
participants whose responses were analyzed. Eighty-one of the 980 recorded trials (8%)
were excluded because (a) the speaker failed to produce the correct words, (b) the
speaker was disfluent, or (c) the production was poorly recorded. Analyses were
performed on all trials, and on the subset of trials for which the listener correctly
identified the question the speaker produced the sentence in response to. As in
Experiment 2, the results were very similar for the two analyses. For brevity of
presentation, we present results from analyses conducted on the correct trials (n = 632,
70%).

Acoustic correlates of information structure 50
Focus Location
In order to investigate the contribution of the prosody of “I heard that” to the
differentiation of the focus type in Experiment 2, we performed a stepwise discriminant
function analysis which included as predictors measures of the four acoustic features we
had selected initially (duration + silence, mean F0, maximum F0, maximum intensity) (1)
for the subject (“Damon”), verb (“fried”), and object (“omelet”), and (2) for each of the
first three words of the sentence (“I”, “heard”, “that”). Of the 24 predictors included in
the stepwise discriminant function analysis, the features which resulted in the best
discrimination of focus type were (1) the duration + silence of “I”, (2) the maximum F0
of “I”, and (3) the maximum intensity of “I”. Based on these results, we conducted an
additional analysis in which we included a subset of 16 predictors: the duration + silence,
mean F0, maximum F0, and maximum intensity of the subject, verb, object, and “I”.
As in Experiments 1 and 2, we conducted a discriminant analysis to determine
whether the measures of (1) duration + silence, (2) maximum F0, (3) mean F0, and (4)
maximum intensity of the four critical words in the sentence could predict focus location.
The overall Wilks’s lambda was significant, Λ = .058, χ2(32) = 1467.09, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .275, χ2(15) =
664.75, p < .001, indicating that the acoustic features could also discriminate verb focus
from object focus (Figure 14). Leave-one-out classification procedure correctly
classified 97% of the productions. At individual focus locations, the model correctly
classified subject focus 96% of the time, verb focus 97% of the time, and object focus
97% of the time.

Acoustic correlates of information structure 51

Figure 14. Separation of focus locations on two discriminant functions for Experiment 3.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the left half of the plot; productions of verb focus are
clustered in the lower right quadrant; productions of object focus are clustered in the
upper right quadrant.
Focus Location

Focus
Breadth

Focus Type

fried

omelet

Function
1

Function
2

Subject
Focus

Verb
Focus

Object
Focus

0.002

0.003

0.000

0.000

0.003

0.002

0.005

0.012

-0.013

-0.009

0.005

-0.010

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

0.003

0.000

0.005

0.006

-0.003

0.003

0.069

0.106

-0.037

-0.011

0.007

0.151

0.001

-0.003

0.000

0.002

0.001

0.005

0.025

-0.021

-0.001

-0.002

-0.001

-0.006

Maximum F0
Maximum
Intensity

-0.005

-0.002

-0.001

-0.005

0.000

-0.003

0.091

-0.077

-0.086

-0.015

0.027

-0.048

Duration+
silence
Mean F0

Acoustic correlates of information structure 52
Duration+
silence
Mean F0

Damon

0.000

0.002

0.000

0.000

0.002

0.011

0.011

-0.011

-0.020

0.019

-0.003

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

-0.014

-0.003

-0.001

0.007

-0.014

-0.008

-0.147

0.011

0.159

-0.006

-0.064

-0.123

0.000

0.000

0.004

0.005

0.005

-0.001

-0.005

0.000

-0.013

-0.008

-0.003

-0.009

Maximum F0
Maximum
Intensity

I

-0.003

0.004

-0.002

0.017

0.010

0.014

0.005

-0.021

-0.017

0.142

0.133

0.126

0.014

Table 5: Standardized canonical coefficients of all discriminant functions computed for
Experiment 3.

I
Damon
fried
omelet

Figure 15. Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 3.

Acoustic correlates of information structure 53
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .39, χ2(16) = 157.44, p<.001,
indicating that the acoustic features could successfully discriminate between noncontrastive and contrastive focus. Leave-one-out classification correctly classified 85%
of the productions. The model correctly classified non-contrastive focus 85% of the time,
and contrastive focus 85% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that maximum intensity overall, and specifically, maximum intensity on “I,” is
contributing most to classification. Figure 16 graphically presents the mean values of the
four features, demonstrating that, in addition to intensity differences, contrastive focus is
produced with longer duration and silence, and with lower mean and maximum F0.

Acoustic correlates of information structure 54

I
Damon
fried
omelet

Figure 16. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 3.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .46, χ2(16) = 139.28, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
80% of the productions. The model correctly classified non-contrastive focus 86% of the
time, and contrastive focus 74% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that, intensity on “I” is contributing the most to classification. Figure 17 graphically

Acoustic correlates of information structure 55
presents the mean values of the four features, demonstrating that contrastive focus is
produced with a higher maximum intensity, and a longer duration and silence, than noncontrastive focus. As in Experiment 2, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

I
Damon
fried
omelet

Figure 17. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 3.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .40, χ2(16) = 133.37, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and

Acoustic correlates of information structure 56
contrastive focus better than chance. Leave-one-out classification correctly classified
83% of the productions. The model correctly classified non-contrastive focus 89% of the
time, and contrastive focus 76% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that intensity and mean F0 on “I” are contributing the most to accurate classification.
Figure 18 graphically presents the mean values of the four features, demonstrating that
contrastive focus is produced with a higher mean and maximum F0 than non-contrastive
focus.

I
Damon
fried
omelet

Acoustic correlates of information structure 57
Figure 18: Values for non-contrastive vs. contrastive focus on the four discriminating
acoustic features when “omelet” is focused in Experiment 3.

Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .48, χ2(16) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 87% of productions; wide
focus was correctly classified 79% of the time, and object focus was correctly classified
92% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 5
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Specifically, greater intensity on the object is a
strong predictor of object focus; less intensity on the subject and the verb are strong
predictors of wide focus. Although intensity is contributing most strongly to
classification, inspection of the acoustic means in Figure 19 indicates that wide focus is
indicated by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is indicated by greater prominence on
the object, reflected in longer duration, higher F0, and higher intensity.

Acoustic correlates of information structure 58

I
Damon
fried
omelet

Figure 19. Values for wide vs narrow object focus on the four discriminating acoustic
features in Experiment 3.
Results – Perception
Listeners’ overall accuracy percentage by condition is plotted in Figure 20.
Listeners’ overall accuracy was 70%. As described in Experiment 2, we compared each
subject's responses to chance performance. Results demonstrated that listeners were able
to successfully identify focus location, as all 10 subjects’ performance significantly
exceeded chance performance, p = .05, two-tailed. Listeners were moderately successful
at discriminating focus type, as six of 10 subjects’ performance exceeded chance levels, p
= .05, two-tailed. Listeners successfully identified focus breadth as eight out of 10

Acoustic correlates of information structure 59
subjects identified wide focus at rates above chance, and eight out of 10 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.

Figure 20. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 3.

Discussion
Experiment 3 was conducted in order to (1) investigate whether speakers could
differentiate focus type with prosody if the sentences contained an attribution expression

Acoustic correlates of information structure 60
that could convey contrastive information, in addition to the elements that describe the
target event, and (2) replicate the results of Experiment 2.
With regard to the second goal, the production results of Experiment 3
successfully replicated the findings from Experiments 1 and 2. As in Experiments 1 and
2, speakers systematically differentiated focus location and focus breadth with a
combination of duration, intensity, and F0 cues. Furthermore, as in Experiment 2, noncontrastive focus was produced with higher F0 than contrastive focus (though only when
the subject or verb was focused), and contrastive focus was always produced with greater
duration and intensity. As discussed above, these F0 results contrast with prior findings
(Bartels & Kingston, 1994; Couper-Kuhlen, 1984; Ladd & Morton, 1997; Ito & Speer,
2008), but can be interpreted in light of more recent evidence that higher intensity is a
stronger cue to greater prominence than higher pitch (Kochanski et al., 2005).
In addition, results from Experiment 3 demonstrated that the strongest cues to
discrimination of focus type were the acoustics of “I” (from the attribution expression “I
heard that”). Specifically, in contrastive focus conditions, the word “I” was produced
with longer duration, higher intensity, and higher mean F0 and maximum F0. Indeed,
discrimination of focus type in Experiment 3 was far better than in Experiment 2. It
therefore appears that speakers can manipulate prosody on sentence elements outside of
the target clause (e.g., in attribution expressions) to convey contrastiveness.
The perception results demonstrated that listeners could accurately determine
focus location, similar to the results of Experiment 2. Furthermore, listeners were more
accurate in determining focus type than listeners in Experiment 2. This increase in
accuracy was likely due to speakers’ tendency to prosodically mark “I” in the contrastive
conditions.

Acoustic correlates of information structure 61

General Discussion
The three experiments reported in the current paper explored the ways in which focus
location, focus type, and focus breadth are conveyed with prosody. In each experiment,
naïve speakers and listeners engaged in tasks in which the information status of sentence
elements in SVO sentences was manipulated via preceding questions. The prosody of the
target sentences was analyzed using a series of classification models to select a subset
from the set of acoustic features that would best be able to discriminate among focus
locations and between focus types. In addition, in Experiments 2 and 3, the production
results were complemented by the perception results that demonstrated listeners’ ability
to use the prosodic cues in the speakers’ utterances to arrive at the intended meaning.
At the beginning of the paper, we posed three questions about the relationship
between acoustics and information structure: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? We are now in a position to answer these questions.
First, we have demonstrated that speakers systematically provide prosodic cues to
the location of focused material. Across all three experiments, speakers provided cues to
focus location whether or not the task explicitly demanded it, across subject, verb and
object positions. In addition, across all three experiments, speakers systematically
provided cues to focus breadth, such that wide focus was prosodically differentiated from
narrow object focus. Finally, we found that speakers can, but don’t always, prosodically
differentiate contrastive and non-contrastive focus. Specifically, speakers did not
prosodically differentiate focus type in Experiment 1, but they did so in Experiment 2
and, even more strongly, in Experiment 3. As discussed above, the fact that speakers did
not differentiate focus type in Experiment 1, where they were plausibly not aware of the

Acoustic correlates of information structure 62
meaning ambiguity, but did differentiate between contrastive and non-contrastive focus
conditions in Experiments 2 and 3, where the task made the meanings more salient, is
consistent with results from the literature on intonational boundary production
demonstrating that speakers only produce disambiguating boundaries when they are
aware of the syntactic ambiguity which could be resolved by the presence of a boundary
(Albritton et al., 1996; Snedeker & Trueswell, 2003; cf. Schafer, et al., 2000 and Kraljic
& Brennan, 2005). Furthermore, the results from Experiment 3, where the critical words
were preceded by the attribution expression “I heard that,” demonstrated even stronger
differentiation of focus type than in Experiment 2, suggesting that speakers are able to
convey contrastiveness using words outside of the clause containing the contrastivelyfocused element.
To answer the question of which acoustic features are associated with different
meaning categories of information structure, we conducted a series of discriminant
function analyses with the goal of objectively identifying which of 24 measures of
duration, intensity, and F0 allowed for the best discrimination of conditions. Across all
experiments, and across different sentence positions, the best differentiation among
conditions was achieved using the following four features: word duration, maximum
word intensity, mean F0, and maximum F0. These results are consistent with many
previous studies in the literature, implicating these features in conveying aspects of
information structure. An important contribution of the current studies is that these
results were obtained using a quantitative analysis across many naïve speakers and items,
and are therefore more likely to be generalizable.
These data also demonstrate how exactly these four features are used in
conveying different aspects of information structure. With regard to focus location,
focused material is produced with longer duration, higher F0, and greater intensity than

Acoustic correlates of information structure 63
non-focused material. With regard to focus type, non-contrastive focus is realized with
higher mean and maximum F0 on the focused word than contrastive focus, whereas
contrastive focus is realized with greater intensity on the focused word than noncontrastive focus. Finally, with regard to focus breadth, narrow focus on the object is
indicated by higher F0 and longer duration on the object, compared to wide focus, and
wide focus is conveyed by higher intensity and F0, and longer duration on pre-focal
words.
To answer the question of how well listeners can retrieve prosodic information
from the signal, we included a perception task in Experiments 2 and 3. When the
relevant acoustic cues were present in the input (as demonstrated by successful
classification by the models), listeners were also able to classify the utterances, although
not quite as successfully as the models. Furthermore, the fact that the model always
achieved high classification accuracy suggests that the utterances contained enough
acoustic information to make these discriminations, and that we did not leave any
particularly informative acoustic features out of the analyses.

Implications for theories of the mapping of acoustics to meaning
While our production and perception results are compatible with a direct
relationship between acoustics and meaning, they are also consistent with the existence of
mediating phonological categories, as in the intonational phonology framework. For
example, a standard assumption within intonational phonology is that there is a
phonological category “accent” mediating acoustics and semantic focus, such that a
focused element is accented, and an unfocused element is unaccented (e.g., Brown,
1983). Our production and perception results are compatible with this assumption. First,
if speakers are signaling focus location by means of placing acoustic features

Acoustic correlates of information structure 64
corresponding to a +accent category on focused elements, then we would expect to see
strong acoustic differences between focused and given elements, as we have observed.
Moreover, if listeners perceive accents categorically, then we would predict successful
discrimination of productions on the basis of focus location, as we have observed.
Second, when the object is focused, it will be accented, resulting in higher acoustic
measures on the object compared to other positions, as we have observed. Furthermore,
in the wide focus condition, the subject, verb, and object – all of which are focused –
would all receive accents, and would therefore be more acoustically similar to one
another than they are in the wide focus condition. This difference in accent placement
would lead to successful discrimination between wide and narrow focus by listeners, as
we have observed. Finally, there has been much debate in the intonational phonology
literature about whether there is a phonological category +/- contrastive. The results of
our experiments are perhaps best explained without such a category. In particular, if
speakers accent focused elements without differentiating between contrastive and noncontrastive focus, then we would expect similar acoustic results between productions
which differ only on focus type, which would lead to poor discrimination by the model.
Moreover, listeners would not be successful in discriminating focus type, as we have
observed. Our experimental results are thus compatible with an intonational
phonological approach which includes an accent category mediating acoustics and
meaning, but no category for contrastiveness. Importantly, although our results do not
support a categorical difference between non-contrastive and contrastive focus, they do
not exclude the possibility that speakers can mark these distinctions with relative
differences in prominence (Calhoun, 2006).

Acoustic correlates of information structure 65
Implications for semantic theories of information structure
The current results are relevant to two open questions in the semantics of
information structure: (1) whether contrastive and non-contrastive focus constitute two
distinct categories; and (2) whether focus on the object of a verb can project to the entire
verb phrase.
As described in the introduction, Rooth (1992) proposed an account of focus which
makes no distinction between non-contrastive focus and contrastive focus. (6) shows the
F-marking (focus-marking) that Rooth’s account would assign to the conditions in
Experiments 1 and 2. Importantly, words and phrases which evoke alternatives, either
explicit or implicit, are considered focused (i.e. F-marked).
(6)
a. Subject, Subject Contrast: DamonF fried an omelet last night.
b. Verb, Verb Contrast:

Damon friedF an omelet last night.

c. Object, Object Contrast: Damon fried an omeletF last night.
d. Wide:

[Damon fried an omelet] F last night.

Our results provide tentative support for Rooth’s proposal that F-marked constituents do
not differ substantively as a function of whether the alternatives they evoke are explicit
(our contrastive condition) or implicit (our non-contrastive condition). Although
speakers differentiated these two conditions acoustically, they only did so when the
contrast between the conditions was made salient (Experiments 2 and 3). Moreover, even
when speakers did mark this distinction, listeners were unable to consistently use this
information to recover the intended meaning (Experiment 2). These results suggest that
there are no consistent semantic differences between foci with explicit alternatives in the
discourse and those with implicit alternatives.

Acoustic correlates of information structure 66
The second semantic issue that these results bear upon is whether narrow focus on
the object can project to the entire verb phrase. According to the theory of focus
projection proposed in Selkirk (1984, 1995), an acoustic prominence on the direct object
(omelet) can project focus to the entire verb phrase (fried an omelet) and then up to the
entire clause/sentence. Gussenhoven (1983, 1999) makes a similar claim. Both Selkirk’s
and Gussenhoven’s accounts therefore predict that a verb phrase with a prominence on
the object would be ambiguous between a narrow object focus interpretation and a wide
focus interpretation. Neither the production nor the perception results were consistent
with this prediction. In production, speakers distinguished between narrow object focus
and wide focus, and in perception, listeners were able to distinguish these two conditions.
One aspect of the production results (the acoustic realization of the subject) for the
narrow object focus and wide focus conditions is, however, predicted by both Selkirk and
Gussenhoven’s accounts. In particular, in the wide focus condition, the subject
constitutes new information while in the narrow object focus condition the subject is
given. Selkirk & Gussenhoven both predict that the subject would be more acoustically
prominent in the wide focus condition than in the narrow object focus condition. This is
exactly what we observed (especially in Experiments 1 and 3). Nevertheless, as
discussed above, speakers also systematically disambiguated wide focus from narrow
object focus across all three experiments with their realization of the object and the verb.
Specifically, wide focus was produced with stable or increasing duration, intensity, and
F0 across the subject, verb, and object; narrow object focus, on the other hand, was
characterized by shorter duration and lower intensity and F0 on the subject and verb,
followed by a steep increase in each of these values on the object.
Similar to our production findings, Gussenhoven (1983) found that, at least in some
productions, wide focus differed from narrow object focus in that the verb was more

Acoustic correlates of information structure 67
prominent under wide focus. Listeners, however, were unable to use this acoustic
information to distinguish wide focus from narrow object focus. Gussenhoven took this
result as evidence that the two conditions are not reliably distinguished (consistent with
his theory). Our results did not replicate this production/perception asymmetry: Listeners
are able to successfully classify productions with a single prominence on omelet as
indicating narrow object focus and did not confuse these productions with those from the
wide focus condition.
Methodological contributions
A further contribution of the current research to investigations of prosody and
information structure is methodological. With regard to the methods used to elicit
productions, we utilized multiple, untrained speakers to ensure that our results are
generalizeable to all speakers and are not due to speakers’ prior beliefs about what pattern
of acoustic prominence signals a particular meaning (see Gibson & Fedorenko, in press,
for similar arguments with respect to linguistic judgments). Furthermore, unlike most
previous work in which productions were selected for analysis based on perceptual
differentiability or on ratings of the appropriateness of prosodic contours, we elicited and
selected for analysis productions using a meaning task. Thus our analyses were based on
the communicative function of language. Finally, we did not exclude speakers based on
our perceptions of their productions; speakers were excluded for failure to provide
information to their listeners.
The analyses used here also constitute an improvement over previous analyses.
First, using discriminant modeling, we were able to simultaneously investigate the
contribution of multiple sentence elements to acoustic differentiation of conditions.
Second, we demonstrated that residualization is a useful method for controlling for
variability among speakers and lexical items. For example, preliminary analyses

Acoustic correlates of information structure 68
performed on the productions from Experiment 2 without first computing residual values
of the acoustic features revealed a 13% average increase in values of Wilks’ lambda
(where lower values indicate better discrimination) and a 7% average decrease in
classification accuracy. Third, the discriminant modeling proved successful in
objectively determining which acoustic features were the biggest contributors to
differences among conditions. The success of the analyses used in the current studies is
encouraging for future investigations of prosodic phenomena previously considered too
variable for study in a laboratory setting with naïve speakers.
One question that arises from the current set of studies is, to what extent the
current results can be generalized to all speakers and all sentences. In production studies,
there is always a trade-off between (1) having enough control over what participants are
producing to ensure sufficient data for analysis, and (2) ensuring that the speech is as
natural as possible. In Experiment 1, we attempted to elicit natural productions, but
failed to find systematic differences between focus types. In making the speakers’ task—
to help their listeners choose the correct question-type—explicit, we may have also
encouraged speakers to produce these sentences with somewhat exaggerated prosody.
Further experiments will be necessary to determine whether speakers normally produce
contrastive meanings in this way.
In conclusion, the current studies used rigorous scientific methods to explore
several important questions about the acoustic correlates of information structure. By
providing some initial answers to these questions, along with some implications for
semantic theory, and by offering a novel, objective way to approach these and other
questions, these studies open the door to future investigations of the relationship between
acoustics and meaning.

Acoustic correlates of information structure 69

References
Albritton, D., McKoon, G., & Ratcliff, R. (1996) Reliability of Prosodic Cues for
Resolving Syntactic Ambiguity. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22, 714-735.
Bartels, C., Kingston, J., (1994). Salient pitch cues in the perception of contrastive focus.
In Boach, P., Van der Sandt, R. (Eds.), Focus & Natural Language Processing, Proc. of J.
Sem. conference on Focus. IBM Working Papers. TR-80, pp. 94–106.
Baumann, S., Grice, M., and Steindamm, S. (2006). Prosodic Marking of Focus Domains
- Categorical or Gradient? In Proceedings of Speech Prosody, Dresden, Germany, pp.
301-304.
Beckman, Mary E. (1986). Stress and Non-Stress Accent. Netherlands Phonetic Archives
Series No. 7. Foris.
Beckman, M., & Ayers Elam, G. (1997). Guidelines for ToBI labeling, version 3: Ohio
State University.
Beckman, M., Hirschberg, J., & Shattuck-Hufnagel, S. (2005). The original ToBI system
and the evolution of the ToBI framework. In S.-A. Jun (Ed.), Prosodic Typology: The
Phonology of Intonation and Phrasing (pp. 9-54): Oxford University Press.
Birch, S. and Clifton, C. (1995) Focus, accent, and argument structure: effects on
language comprehension. Language and Speech, 38 (4), 365-391.
Birner, B. (1994). Information status and Word Order: An Analysis of English Inversion.
Language, 70 (2), 233-259.
Boersma, Paul & Weenink, David (2006). Praat: doing phonetics by computer (Version
4.3.10) [Computer program]. Retrieved June 3, 2005, from http://www.praat.org/
Bolinger, D. (1961). Contrastive accent and contrastive stress. Language, 37, 83-96.
Breen, M., Dilley, L., Gibson, E., Bolivar, M., and Kraemer, J. (2006) Advances in
prosodic annotation: A test of inter-coder reliability for the RaP (Rhythm and Pitch) and
ToBI (Tones and Break Indices) transcription systems. Poster presented at the 19th
CUNY Conference on Human Sentence Processing, New York, NY. March, 2006.
Brown, G. (1983). Prosodic structures and the Given/New distinction. In D. R. Ladd &
A. Cutler (Eds.), Prosody:Models and measurements (pp. 67–77). Berlin: Springer.
Calhoun, S (2004). Phonetic Dimensions of Intonational Categories - the case of L+H*
and H*. In Proceedings of Speech Prosody, Nara, Japan, pp. 103-106.
Calhoun, S. (2005). It's the difference that matters: An argument for contextuallygrounded acoustic intonational phonology. In Linguistics Society of America Annual
Meeting, Oakland, California, January 2005.
Calhoun, S. (2006) Information Structure and the Prosodic Structure of English: a
Probabilistic Relationship. PhD thesis, University of Edinburgh.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics and points of
view. In Charles N. Li, editor, Subject and Topic, pages 27-- 55. Academic Press, 1976.
Clark, E. V., & Clark, H. H. (1978). Universals, relativity, and language processing. In: J.
H. Greenberg (Ed.), Universals of human language, Vol. I. (pp. 225–277). Stanford:
Stanford University Press.

Acoustic correlates of information structure 70
Cooper, W., Eady, S. & Mueller, P. (1985). Acoustical aspects of contrastive stress in
question-answer contexts. Journal of Acoustical Society of America, 77(6), 2142-2156.
Couper-Kuhlen, E. (1984). A new look at contrastive intonation., Modes of
Interpretation: Essays Presented to Ernst Leisi, Watts, R., Weidman, U. (Eds.) Gunter
Narr Verlag, 137–158.
Cutler, A. (1977). The Context-Independence of "Intonational Meaning". Chicago
Linguistic Society (CLS 13), 104-115.
Dilley, L. C. (2005). The phonetics and phonology of tonal systems. Unpublished Ph.D.
Dissertation, MIT.
Dilley, L. C., & Brown, M. (2005). The RaP (Rhythm and Pitch) Labeling System,
Version 1.0: Available at http://tedlab.mit.edu/rap.html.
Eady, S. J., & Cooper, W. E. (1986). Speech intonation and focus location in matched
statements and questions. Journal of the Acoustical Society of America, 80, 402-415.
Féry, C. and Krifka, M. (2008). Information Structure: Notional Distinctions, Ways of
Expression. In Piet van Sterkenburg (ed.), Unity and diversity of languages, Amsterdam:
John Benjamins, 123-136.
Fry, D. B. (1955). Duration and Intensity as Physical Correlates of Linguistic Stress.
Journal of the Acoustical Society of America, 27, 765–768.
Gibson, E. & Fedorenko, E. (In press). Weak quantitative standards in linguistics
research. Trends in Cognitive Sciences.
Gussenhoven, C. (1983). Testing the reality of focus domains. Language and Speech, 26,
61–80.
Gussenhoven, C. (1999). On the limits of focus projection in English. In P. Bosch & R.
van der Sandt (Eds.), Focus: Linguistic, cognitive, and computational perspectives (pp.
43 –55). Cambridge, U.K.: Cambridge University Press.
Gussenhoven, C., Repp, B. H., Rietveld, A., Rump, W. H. & J. Terken, J. (1997). The
perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society
of America, 102, 3009-3022.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hawkins, S. & Warren, P. (1991). Factors affecting the given-new distinction in speech.
In Proceedings of the 12th International Congress of Phonetic Sciences, Aix en
Provence. 66-69.
Ito, K & Speer, S. (2008). Anticipatory effects of intonation: Eye movements
during instructed visual search. Journal of Memory and Language, 58, 541-573.
Ito, K. Speer, S. R. and Beckman, M. E. (2004). Informational status and pitch accent
distribution in spontaneous dialogues in English, In Proceedings of the International
Conference on Spoken Language Processing, Nara: Japan, 279-282.
Jackendoff, R. (1972). Semantic interpretation in generative grammar. Cambridge: MIT
Press.
Jaeger, T. F. (2008). Categorical Data Analysis: Away from ANOVAs (transformation or
not) and towards Logit Mixed Models. Journal of Memory and Language. 59, 434–446.

Acoustic correlates of information structure 71
Kochanski, G., Grabe, E., Coleman, J., & Rosner, B. (2005) Loudness predicts
prominence: fundamental frequency lends little. The Journal of the Acoustical Society of
America, 118 (2), 1038-1054.
Krahmer, E., & Swerts, M. (2001). On the alleged existence of contrastive accents.
Speech Communication, 34, 391-405.
Kraljic, T. & Brennan, S. E. (2005). Prosodic disambiguation of syntactic structure: For
the speaker or for the addressee? Cognitive Psychology 50: 194-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge Studies in Linguistics 79.
Cambridge: Cambridge University Press.
Ladd, D. R. & Morton, R. (1997). The perception of intonational emphasis: continuous or
categorical? Journal of Phonetics, 25, 313–342.
Lambrecht, K. (2001). A framework for the analysis of cleft constructions. Linguistics,
39, 463–516.
Lieberman, P. (1960). Some acoustic correlates of word stress in American English. The
Journal of the Acoustical Society of America, 32(4), 451-454.
Molnar, V. (2002). Information Structure in a Cross-linguistic Perspective. In Hilde
Hasselgård, Stig Johansson, Bergljot Behrens, Cathrine Fabricius-Hansen (Eds.),
Language and Computers, Vol. 39, 147-161(15).
Paul, H. (1880), Prinzipien der Sprachgeschichte, Leipzig.
Pierrehumbert, J.B. (1980). The phonology and phonetics of English intonation.
Unpublished dissertation, MIT.
Pierrehumbert, J. & Hirschberg, J. (1990). The Meaning of Intonational Contours in the
Interpretation of Discourse. In P. R. Cohen & J. Morgan & M. E. Pollack (eds.).
Intentions in Communication. Cambridge/MA: MIT Press, 271-311.
Pierrehumbert, J. & Steele, S. (1989). Categories of tonal alignment in English.
Phonetica, 46, 181-196.
Pitrelli, J., Beckman, M. & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International
Conference on Spoken Language Processing, 123-126.
Rietveld, A. C. M., and Gussenhoven, C. (1985). On the relation between pitch excursion
size and prominence. Journal of Phonetics, 13, 299-308.
Rochemont, M. S. (1986). Focus in Generative Grammar. Amsterdam/Philadelphia: John
Benjamins.
Rooth, M. (1985). Association with Focus. PhD thesis, University of Massachusetts
Amherst.
Rooth, M. (1992). A theory of focus interpretation. Natural Language Semantics, 1, 75 –
116.
Rump, H. H., and Collier, R. (1996). ‘Focus conditions and the prominence of pitchaccented syllables. Language and Speech, 39, 1–17.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational disambiguation
in sentence production and comprehension. Journal of Psycholinguistic Research, 29,
169-182.

Acoustic correlates of information structure 72
Selkirk, E. (1984). Phonology and syntax: The relation between sound and structure.
Cambridge, MA: MIT.
Selkirk, E. (1995). Sentence Prosody: Intonation, Stress, and Phrasing. In: J.Goldsmith
(ed.). The Handbook of Phonological Theory. Oxford: Blackwell, 550-569.
Schwarzchild, R. (1999) GIVENness, AvoidF and other Constraints on the Placement of
Accent. Natural Language Semantics, 7, 141–177.
Shriberg, E., Stolcke, A., Hakkani-Tur, D. & Tur, G. (2000). Prosody-Based Automatic
Segmentation of Speech into Sentences and Topics. Speech Communication, 32, 127-154.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D., Ries, K., Coccaro, N.,
Martin, R., Meteer, M., & Van Ess-Dykema, C. (1998). Can Prosody Aid the Automatic
Classification of Dialog Acts in Conversational Speech? Language and Speech, 41:3-4,
439-487.
Silverman, K. E. A., Beckman, M., Pierrehumbert, J., Ostendorf, M., Wightman, C. W.
S., Price, P., et al. (1992). ToBI: A standard scheme for labeling prosody. In Proceedings
of the 2nd International Conference on Spoken Language Processing (pp. 867-879).
Banff.
Sluijter, A. and van Heuven, V. (1996). Spectral balance as an acoustic correlate of
linguistic stress. Journal of the Acoustical Society of America, 100, 2471–2485.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects of
speaker awareness and referential contest. Journal of Memory and Language, 48, 103–
130.
Stalnaker, R. (2002). Common ground. Linguistics and Philosophy, 25: 701–721.
Syrdal, A. and McGory, J. (2000). Inter-transcriber reliability of ToBI prosodic labeling.
In Proceedings of the International Conference on Spoken Language Processing,
Beijing: China, 235-238.
Terken, J. (1991). Fundamental frequency and perceived prominence accented syllables.
Journal of the Acoustical Society of America, 89, 1768–1776.
't Hart, J. Collier, R. & Cohen, A. (1990). A perceptual study of intonation. Cambridge
University Press, Cambridge.
Turk, A. & Sawusch, J. (1996) The processing of duration and intensity cues to
prominence. Journal of the Acoustical Society of America, 99, 3782-3790.
Welby, P. (2003). Effects of pitch accent position, type, and status on focus projection.
Language and Speech, 46, 53 – 81.
Wightman, C. W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P. J. (1992). Segmental
durations in the vicinity of prosodic phrase boundaries. Journal of the Acoustical Society
of America, 91(3), 1707-1717.
Xu, Y. & Xu, C. X. (2005). Phonetic realization of focus in English declarative
intonation, Journal of Phonetics, 33, 159–197.
Yoon, T., Chavarria, S., Cole, J., & Hasegawa-Johnson, M. (2004). Intertranscriber
reliability of prosodic labeling on telephone conversation using ToBI. In Proceedings of
the International Conference on Spoken Language Processing., Nara: Japan, 2729-2732.

Acoustic correlates of information structure 73

Appendix A
Experiment 1 items
Full items are recoverable as follows: Question A is always “What happened last night?”
Questions B, C, & D are wh-questions about the subject, verb, and object, respectively.
Questions E, F, & G are questions which introduce the explicit alternative subject, verb,
or object, indicated in parentheses.
1.

Question A: What happened last night?
Question B: Who fed a bunny last night?
Question C: What did Damon do to a bunny last night?
Question D: What did Damon feed last night?
Question E: Did Jenny feed a bunny last night?
Question F: Did Damon pet a bunny last night?
Question G: Did Damon feed a baby last night?
Response: Damon fed a bunny last night.

2. Damon (Lauren) caught (pet) a bunny (a squirrel) last night.
3. Damon (Molly) burned (break) a candle (a log) last night.
4. Darren (Lauren) cleaned (eat) a carrot (a chicken) last night.
5. Darren (Molly) peeled (eat) a carrot (a potato) last night.
6. Darren (Nora) found (buy) a diamond (a ring) last night.
7. Darren (Jenny) sold (lose) a diamond (a sapphire) last night.
8. Jenny (Damon) found (lose) a dollar (a quarter) last night.
9. Jenny (Darren) sewed (rip) a dolly (a blanket) last night.
10. Jenny (Logan) read (open) an email (a letter) last night.
11. Jenny (Nolan) smelled (plant) a flower (a skunk) last night.
12. Lauren (Darren) burned (write) a letter (a magazine) last night.
13. Lauren (Logan) mailed (open) a letter (a package) last night.
14. Lauren (Nolan) read (write) a novel (a newspaper) last night.
15. Lauren (Damon) fried (bake) an omelet (a chicken) last night.
16. Logan (Molly) peeled (chop) an onion (an apple) last night.
17. Logan (Nora) fried (chop) an onion (a potato) last night.
18. Logan (Jenny) cleaned (buy) a pillow (a rug) last night.
19. Molly (Logan) dried (wash) a platter (a bowl) last night.
20. Molly (Nolan) sold (find) a platter (a vase) last night.
21. Molly (Damon) poured (drink) a smoothie (a cocktail) last night.
22. Nolan (Nora) pulled (push) a stroller (a sled) last night.
23. Nolan (Jenny) bought (sell) a stroller (a wheelbarrow) last night.
24. Nolan (Lauren) sewed (knit) a sweater (a quilt) last night.
25. Nora (Nolan) killed (trap) a termite (a cockroach) last night.
26. Nora (Damon) changed (wash) a toddler (a baby) last night.
27. Nora (Darren) fed (dress) a toddler (a bunny) last night.
28. Nora (Logan) pulled (push) a wagon (a wheelbarrow) last night.

Acoustic correlates of information structure 74

Appendix B
Items used for Experiments 2-3
Full items are recoverable as follows: Question A always asks “What happened _____?”
where the blank corresponds to the temporal adverb. Questions B, C, & D are whquestions about the subject, verb, and object, respectively. Questions E, F, & G are
questions which introduce the explicit alternative subject, verb, or object, indicated in
parentheses.
1a.
1b.
1c.
1d.
1e.
1f.
1g.

Context: What happened yesterday?
Context: Who fried an omelet yesterday?
Context: What did Damon do to an omelet yesterday?
Context: What did Damon fry yesterday?
Context: Did Harry fry an omelet yesterday?
Context: Did Damon bake an omelet yesterday?
Context: Did Damon fry a chicken yesterday?
Target: No, Damon fried an omelet yesterday.

2. (I heard that) (No,) Megan (Jodi) sold (lose) her diamond (her sapphire) yesterday.
3. (I heard that) (No,) Mother (Daddy) dried (wash) a platter (a bowl) last night.
4. (I heard that) (No,) Norman (Kelly) read (write) an email (a letter) last night.
5. (I heard that) (No,) Lauren (Judy) poured (drink) a smoothie (a cocktail) this morning.
6. (I heard that) (No,) Nora (Jenny) sewed (rip) her dolly (her blanket) this morning.
7. (I heard that) (No,) Molly (Sarah) trimmed (wax) her eyebrows (her hair) on Tuesday.
8. (I heard that) (No,) Nolan (Steven) burned (break) a candle (a log) on Tuesday.
9. (I heard that) (No,) Logan (Billy) killed (trap) a termite (a cockroach) last week.
10. (I heard that) (No,) Radar (Fido) caught (lick) a bunny (a squirrel) last week.
11. (I heard that) (No,) Darren (Maggie) pulled (push) a stroller (a sled) on Sunday.
12. (I heard that) (No,) Brandon (Tommy) peeled (eat) a carrot (a potato) on Sunday.
13. (I heard that) (No,) Maren (Debbie) cleaned (buy) a pillow (a rug) on Friday.
14. (I heard that) (No,) Lindon (Kelly) fooled (fight) a bully (a teacher) on Friday.

Acoustic correlates of information structure 1

Acoustic correlates of information structure
Mara Breen1, Evelina Fedorenko2, Michael Wagner3, Edward Gibson2
1
2

University of Massachusetts Amherst
Massachusetts Institute of Technology
3
McGill University

June 7, 2010
Address correspondence to:
Mara Breen
522 Tobin Hall
University of Massachusetts
Amherst, MA
01003
mbreen@psych.umass.edu

Acoustic correlates of information structure 2
Abstract
This paper reports three studies aimed at addressing three questions about the acoustic
correlates of information structure in English: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? The information structure of subject-verb-object (SVO)
sentences was manipulated via the questions preceding those sentences: elements in the
target sentences were either focused (i.e. the answer to a wh-question) or given (i.e.
mentioned in prior discourse); furthermore, focused elements had either an implicit or an
explicit contrast set in the discourse; finally, either only the object was focused (narrow
object focus) or the entire event was focused (wide focus). The results across all three
experiments demonstrated that people reliably mark (a) focus location (subject, verb, or
object) using greater intensity, longer duration, and higher mean and maximum F0, and
(b) focus breadth, such that narrow object focus is marked with greater intensity, longer
duration, and higher mean and maximum F0 on the object than wide focus. Furthermore,
when participants are made aware of prosodic ambiguity present across different
information structures, they reliably mark focus type, so that contrastively-focused
elements are produced with higher intensity, longer duration, and lower mean and
maximum F0 than non-contrastively focused elements. In addition to having important
theoretical consequences for accounts of semantics and prosody, these experiments
demonstrate that linear residualization successfully removes individual differences in
people’s productions thereby revealing cross-speaker generalizations. Furthermore,
discriminant modeling allows us to objectively determine the acoustic features that
underlie meaning differences.

Acoustic correlates of information structure 3

Introduction
An important component of the meaning of a sentence is its relationship to the context in
which it is produced. Some parts of speakers’ sentences refer to information already
under discussion, while other parts convey information that the speaker is presenting as
new for the listener. Depending on the context, the same sentence can convey different
kinds of information to the listener. For example, consider the three contexts in (1a)-(1c)
for the sentence in (2):

(1) a. Who fried an omelet?
b. What did Damon do to an omelet?
c. What did Damon fry?

(2) Damon fried an omelet.

The event of frying an omelet is already made salient in the context in (1a), and
this part of the answer is therefore given. Consequently, the sentence Damon fried an
omelet conveys Damon as the new or focused information.1 Similarly, the verb fried is
the focused information relative to the context in (1b), and the object noun phrase an
omelet is the focused information relative to the context in (1c). This component of the
meaning of sentences - the differential contributions of different sentence elements to the

1

Numerous terms are used in the literature to refer to the distinction between the information that is old for
the listener and the information that the speaker is adding to the discourse: background and foreground;
given and new; topic and comment; theme and rheme, etc. In this paper, we will use the term given to refer
to the parts of the utterance which are old to the discourse, and focused to refer to the part of the utterance
which is new to the discourse.

Acoustic correlates of information structure 4
overall sentence meaning in its relation to the preceding discourse - is called information
structure.
Three components of information structure have been proposed in the literature:
givenness, focus, and topic (see e.g., Féry and Krifka, 2008, for a recent summary). The
current paper will be concerned with givenness and focus.2 Given material is material
that has been made salient in the discourse, either explicitly, like the event corresponding
to the verb fried and the object corresponding to the noun omelet in (1a), or implicitly, via
inferences based on world knowledge (e.g., mentioning omelet makes the notion of
“eggs” given, Schwarzchild, 1999).
Focused material is what is new to the discourse, or in the foreground. The focus
of a sentence can often be understood as the part that corresponds to the answer to the
wh-part of wh-questions, like Damon in (2) as an answer to (1a) (Paul, 1880; Jackendoff,
1972).
There are two dimensions along which focused elements can differ. The first is
contrastiveness. A contrastively focused element, like Damon in (3b), indicates that the
element in question is one of a set of explicit alternatives or serves to correct a specific
item already present in the discourse, as in the following:

(3) a. Did Harry fry an omelet yesterday?
b. Damon fried an omelet yesterday.

Unlike (1a), where there is no explicit set of individuals from which Damon is being
selected as the “omelet fryer”, in (3a) an explicit alternative “omelet fryer” is being

2

Topic, the third component of information structure, describes which discourse referent focused
information should be associated with, as in the mention of Damon in “As for Damon, he fried an omelet.”
The current studies do not address the prosodic realization of topic.

Acoustic correlates of information structure 5
introduced: Harry. The sentence (3b) in this context thus presents information (i.e.,
Damon) which explicitly contrasts with, or contradicts, some information which has been
introduced into the discourse.
There is no consensus in the literature regarding the relationship between noncontrastive focus and contrastive focus. Some researchers have treated non-contrastive
focus and contrastive focus as separate categories of information structure (Chafe, 1976;
Halliday, 1967; Rochemont, 1986; Molnar, 2002), whereas others have argued that there
is no principled difference between the two (e.g., Bolinger 1961, Rooth, 1985, Rooth,
1992). According to Rooth (1992), for example, each expression evokes two semantic
representations: the expression’s actual meaning, and a set of alternatives. If a
constituent in the expression is focused, then the alternative set contains the expression
itself and all expressions with an alternative substituted for the focus-marked constituent;
if there is no focus within the expression, the alternative set consists only of the
expression itself. Rooth would therefore argue that Damon in (1a) is focused and
introduces alternative propositions that differ only in the agent of the event ({Damon
fried an omelet, Harry fried an omelet, Ada fried an omelet, ...}), even if no alternatives
are explicitly mentioned. In (3a), Damon also evokes alternative omelet fryers, and
therefore has the same focus structure as (1a), but the context makes a specific alternative
(Harry) more salient than other potential alternatives. Importantly, from Rooth’s
standpoint, it does not matter whether the alternatives are explicit in the discourse or not:
the meaning of the expression is the same.
The second dimension along which focused elements can vary is focus breadth
(Selkirk, 1984; 1995; Gussenhoven, 1983; 1999), which refers to the size of the set of
focused elements. Narrow focus refers to cases where only a single aspect of an event
(e.g., the agent, the action, the patient, etc.) is focused, whereas wide focus focuses an

Acoustic correlates of information structure 6
entire event. Take, for example, the difference between (5) as an answer to (4a) versus as
an answer to (4b):

(4) a. What did Damon fry last night?
b. What happened last night?

(5) Damon fried an omelet last night.

(4a) narrowly focuses the patient of frying, omelet in (5), while (4b) widely focuses the
entire event of Damon frying an omelet.
The information status of a sentence element can be conveyed in at least three
ways: (1) using word order (i.e., given information generally precedes focused
information) (e.g., Birner, 1994, Clark & Clark, 1978); (2) using particular lexical items
and syntactic constructions (e.g., using cleft constructions such as “It was Damon who
fried an omelet”) (Lambrecht, 2001); and (3) using prosody. Prosody – which we focus
on in the current paper – refers to the way in which words are grouped in speech, the
relative acoustic prominence of words, and the overall tune of an utterance. Prosody is
comprised of acoustic features like fundamental frequency (F0), duration, and loudness,
the combinations of which give rise to the psychological percepts like phrasing
(grouping), stress (prominence), and tonal movement (intonation).
The goal of the current paper is to investigate the prosodic realization of
information structure in simple English subject-verb-object (SVO) sentences like (2),
with the goal of addressing the following questions:
1) First, do speakers prosodically distinguish focused and unfocused elements?
This question can be broken down into further questions:

Acoustic correlates of information structure 7
(1a) Do speakers distinguish focused elements that have an explicit contrast
set in the discourse from those that do not?
(1b) Do speakers distinguish sentences in which only the object is focused
from those in which the entire event is focused?
(2) What are the acoustic features associated with these different aspects of
information structure?
(3) How well can listeners retrieve this information from the signal?

Although the current experiments are all performed on English, the answers to
these questions will likely be similar for other West Germanic languages. However, the
relationship between prosodic features and information structure across different
languages and language groups remains an open question.
In the remainder of the introduction, we briefly lay out two approaches to the study
of the relationship between prosody and information structure, and summarize empirical
studies which have explored how information structure is realized acoustically and
prosodically. We then discuss methodological issues present in previous studies which
call into question the generalizeability of the reported findings, and outline how the
current methods were designed to better address these questions.
Empirical investigations of prosody and information structure
Two perspectives on the relationship between the acoustics of the speech signal and
the meaning associated with various aspects of information structure have been
articulated in the literature. According to the direct-relationship approach, sets of
acoustic features are directly associated with particular meanings (Fry, 1955; Lieberman,
1960; Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986; Pell, 2001; Xu & Xu,
2005). In contrast, according to the indirect-relationship approach (known as the

Acoustic correlates of information structure 8
intonational phonology framework), the relationship between acoustics and meaning is
mediated by phonological categories (Ladd, 1996; Gussenhoven, 1983; Pierrehumbert,
1980; Dilley, 2005; Hawkins & Warren, 1991). In particular, the phonetic prosodic cues
are hypothesized to be grouped into prosodic categories which are, in turn, associated
with particular meanings. The experiments in the current paper were not designed to
decide between these two approaches. However, In the current paper, we will initially
discuss our experiments in terms of the direct-relationship approach, because it is more
parsimonious. In the general discussion, we will show how the results are also
compatible with the indirect-relationship approach.
Turning now to previous empirical work on the relationship between prosody and
information structure, we start with studies of focused vs. given elements. Several
studies have demonstrated that focused elements are more acoustically prominent than
given elements. However, there has been some debate about which acoustic features
underlie a listener’s perception of acoustic prominence. Some features that have been
proposed to be associated with prominence include pitch (i.e. F0) (Lieberman, 1960;
Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986), duration (Fry, 1954; Beckman,
1986), loudness (i.e. intensity) (Kochanski, Grabe, Coleman, & Rosner, 2005; Beckman,
1986; Turk and Sawusch, 1996), and voice quality (Sluijter & van Heuven, 1996).
In early work on lexical stress, Fry (1954) and Liberman (1960) argued that
intensity and duration of the vowel of the stressed syllable contributed most strongly to
the percept of acoustic prominence, such that stressed vowels were produced with a
greater intensity and a longer duration than non-stressed vowels. In experiments on
phrase-level prominence, Cooper et al. (1985) and Eady and Cooper (1986) also noted
that more prominent syllables are longer than their non-prominent counterparts. Cooper
et al. (see also Liberman, 1960); Rietveld & Gussenhoven, 1985; Gussenhoven et al.,

Acoustic correlates of information structure 9
1997; and Terken, 1991) also argued that F0 was a highly important acoustic feature
underlying prominence. Others have argued that the strongest cue to prominence is
intensity (e.g., Beckman, 1986). More recently, Turk and Sawusch (1996) also found
that intensity (and duration) were better predictors of perceived prominence than pitch, in
a perception task. Finally, in a study of spoken corpora, Kochanski et al. (2005)
demonstrated that loudness (i.e. intensity) was a strong predictor of labelers’ annotations
of prominence, while pitch had very little predictive power.
The question of whether contrastively and non-contrastively focused elements are
prosodically differentiated by speakers, and perceptually differentiated by listeners has
also been extensively debated. Some have argued that there is no difference in the
acoustic features associated with contrastively vs. non-contrastively focused elements
(Cutler, 1977; Bolinger, 1961; t’Hart, Collier, & Cohen, 1990), while others have argued
that some acoustic features differ between contrastively vs. non-contrastively focused
elements (Couper-Kuhlen, 1984; Krahmer & Swerts, 2001; Bartels & Kingston, 1994;
Ito, Speer, & Beckman, 2004). For example, Couper-Kuhlen (1984) reported, on the
basis of corpus work, that speakers produce contrastive focus with a steep drop after a
high F0 target, while high F0 is sustained after non-contrastive focus (see also Krahmer
and Swerts, 2001). However, this finding is in contrast to Bartels and Kingston (1994),
who have argued, based on a series of production studies, that the most salient acoustic
cue to contrastiveness is the height of the peak on a contrastive word, such that a higher
peak is associated with a greater probability of an element being interpreted as
contrastive (see also Ladd and Morton, 1997). Finally, Ito, Speer, & Beckman (2004)
demonstrated that speakers are more likely to use a L+H* accent (i.e. a steep rise from a
low target to a high target), compared to a H* accent (i.e. a gradual rise to a high target),
to indicate an element that has an explicit contrast set in the discourse.

Acoustic correlates of information structure 10
Krahmer and Swerts (2001) observed that listeners were more likely to perceive a
contrastive adjective (e.g., red in red square preceded by blue square) as more prominent
than a new adjective when the adjective was presented with a noun compared to when it
was presented in isolation. They therefore hypothesized that the lack of a consensus in
the literature may be due to the failure of the earlier studies to investigate focused
elements in relation to the prosody of the surrounding elements. Consistent with this
idea, Calhoun (2005) demonstrated that a model’s ability to predict a word’s information
status is significantly improved when information about the acoustics of adjacent words
is included in the model. These results suggest that a more consistent picture of the
acoustic features associated with contrastively and non-contrastively-focused elements
may emerge if acoustic context is taken into account.
Finally, prior work has investigated whether speakers prosodically differentiate
narrow and wide focus. Selkirk (1995), for example, argued that, through a process
called focus projection, an acoustic prominence on the head of a phrase or its internal
argument can project to the entire phrase, thus making the entire phrase focused (see also
Selkirk, 1984; see Gussenhoven, 1983, 1999, for a similar claim). According to Selkirk
(1984) and Gussenhoven (1983) then a clause containing a transitive verb in which the
direct object is acoustically prominent is ambiguous between a reading where the object
alone is focused and a reading where the entire verb phrase is focused. This hypothesis
has been supported in several perception experiments (Welby, 2003; Birch & Clifton,
1995; Gussenhoven, 1983). Welby (2003), for example, demonstrated that listeners rated
a sentence like I read the DISPATCH with a single acoustic prominence on dispatch as a
similarly felicitous response to either a question narrowly focusing the object (i.e. “What
newspaper do you read?”), or a question widely focusing the entire event (i.e. “How do
you keep up with the news?”). However, Gussenhoven (1983) found that at least in some

Acoustic correlates of information structure 11
productions there is actually a perceptible difference between narrow and wide focus
although listeners cannot use this information to reliably tell in which context the
sentence was uttered (see Baumann et al., 2006, for evidence from German showing that
speakers do differentiate between narrow and wide focus, with prosodic cues varying
across speakers). In contrast to Gussenhoven’s perception results, Rump and Collier
(1986) found that listeners can accurately discriminate narrow and wide focus using pitch
cues.
Limitations of previous work
Although the studies summarized above provide evidence for some systematic
differences in the acoustic realization of different aspects of information structure, no
clear picture has yet emerged with regard to any of the three meaning distinctions
discussed above (i.e. focused vs. given elements, non-contrastively focused vs.
contrastively focused elements, and narrow vs. wide focus). Furthermore, previous
studies suffer from several methodological limitations that make the findings
inconclusive. Here, we discuss five limitations of previous studies which the current
studies seek to address in an effort to reveal a clearer picture of the relationship between
acoustic features and information structure.
First, instead of acoustic features, sometimes only ToBI3 annotations are
provided (e.g., Birch & Clifton, 1995; Ito et al., 2004). This includes work of researchers
who adopt the intonational phonology framework and who therefore believe that using
prosodic annotation offers a useful way to extrapolate away from potentially complex
interactions among acoustic features which give rise to the perception of specific
intonational patterns. One particular problem concerns H* and L+H* accents. As
defined in the ToBI system, these accents are meant to be explicit markers of non3

The (ToBI) Tones and Break Indices system was developed in the early 90s as the standard system for
annotation of prosodic features (Silverman et al., 1992).

Acoustic correlates of information structure 12
contrastive focus and contrastive focus, respectively (Beckman & Ayers-Elam, 1997).
However, H* and L+H* are often confused in ToBI annotations (Syrdal & McGory,
2000), and are, in fact, often collapsed in calculating inter-coder agreement (Pitrelli et al.,
1994; Yoon et al., 2004; Breen et al., 2006, submitted). Therefore, it is difficult to
interpret the results of studies which are based on the difference between H* and L+H*
without a discussion of the acoustic differences between these purported categories. In
the current studies, we report acoustic features in order to avoid confusion about what the
ToBI labels might mean and in order to not presuppose the existence of prosodic
categories associated with particular meaning categories of information structure.
A second limitation concerns the method used to generate and select productions
for analysis. A common practice involves eliciting productions from a small number of
speakers (e.g., Baumann et al., 2006; Krahmer & Swerts, 2001), which results in a
potential decrease in experimental power, and could therefore lead to a Type II error. In
addition, several previous experiments have excluded speakers’ data from analysis for not
producing accents consistently (e.g., Eady & Cooper, 1986; Cooper et al., 1985), which
could lead to a Type I error. For the current experiments, we recruited between 13 and
18 speakers. In addition, no speakers’ productions were excluded from the analyses
based on a priori predictions about potential behavior (e.g., placing accents in particular
locations).
A third limitation concerns the tasks used in perception studies. In particular,
some studies asked listeners to make judgments about which of two stimuli was more
prominent (Krahmer & Swerts, 2001), what accent is acceptable in a particular context
(Birch & Clifton, 1995; Welby, 2003), or with which of two questions a particular answer
sounded more natural (Gussenhoven, 1983). The problem with these meta-linguistic
judgments is that they lack a measure of the participants’ interpretation of the sentences.

Acoustic correlates of information structure 13
In the current studies we employ a more natural production-comprehension task, in which
speakers are trying to communicate a particular meaning of a semantically ambiguous
sentence and listeners are trying to understand the intended meaning.
A fourth limitation of previous studies is in how they have dealt with speaker
variability. Presenting data from individual subjects separately, as is commonly done, is
problematic because it fails to capture the shared aspects of individual productions (e.g.,
consistent use by most speakers of some set of acoustic features to mark focused
elements). In the current studies, we combine data across subjects while simultaneously
removing variance due to individual differences using linear regression modeling (e.g.,
Jaeger, 2008).
A fifth limitation is that many have reported differences between conditions based
only on individual acoustic features on single words (Eady & Cooper, 1986; Cooper et
al., 1985; Baumann et al, 2006). If acoustic prominence is perceived in a contextdependent manner, these single-feature/single-word analyses might find spurious
differences, or fail to find real differences. In the current studies, we used discriminant
modeling on the productions in order to simultaneously investigate the contribution of
multiple acoustic features from multiple words in an utterance to the interpretation of
information status of different sentence elements.

Experiments: Overview and general methods
The current paper presents results from three experiments. Experiment 1
investigated whether speakers prosodically disambiguate focus location (subject, verb,
object), focus type (contrastive vs. non-contrastive focus), and focus breadth (narrow vs.
wide) by eliciting semi-naturalistic productions like that in (3b) (e.g., Damon fried an
omelet this morning), whose information status was disambiguated by a preceding

Acoustic correlates of information structure 14
question. Experiment 2 investigated whether speakers disambiguate focus location and
focus type when the task explicitly required them to communicate a particular meaning to
their listeners. Finally, Experiment 3 served as a replication and extension of Experiment
2, in which speakers included an attribution expression (“I heard that”) before the critical
sentence.
The acoustic analysis of the productions elicited in all three experiments
proceeded in three steps. First, we automatically extracted a series of 24 acoustic features
(see Table 2) from the subject, verb, and object of the sentences elicited in Experiments
1, 2, and 3. Second, we subjected all of these features to a stepwise discriminant function
analysis in order to determine which features best discriminated the information status
conditions listed in Table 1 for each of the three experiments. This analysis resulted in a
subset of eight acoustic features. Finally, we used discriminant analyses to evaluate
whether this subset of eight features could effectively discriminate sets of 2 and 3
conditions for each of the three experiments. Specifically, we tested focus location by
comparing the features from productions in which Damon, fried, and omelet were
focused, respectively. We tested focus type by comparing the features from sentences in
which the focused element was contrastively or non-contrastively focused at each of the
three syntactic positions. Last, we tested focus breadth by comparing the features for
sentence with wide-focus to those with narrow object focus. In addition to the analysis of
acoustic features, in Experiments 2 and 3 we investigated whether listeners could
correctly determine the intended information status of the speaker.

Acoustic correlates of information structure 15

Experiment 1
Method
Participants
Nine pairs of participants were recorded. All participants were self-reported native
speakers of American English. All participants were MIT students or members of the
surrounding community. Participants were paid for their participation.
Materials
Each trial consisted of a set-up question and a target sentence, which always had an SVO
structure (e.g., Damon fried an omelet this morning). The target sentence could plausibly
answer any one of the seven set-up questions (see Table 1), which served to focus
different elements of the sentence or the entire event described in the sentence. The first
question focused the entire event (i.e. What happened?). In the remaining conditions,
two factors were manipulated: (1) the element in the target sentence that was focused by
the question (subject, verb, object); and (2) the presence of an explicit contrast set for the
focused element (non-contrastively focused, i.e. explicit contrast set absent, contrastively
focused, i.e. explicit contrast set present).
All subject and object noun phrases (NPs) in the target sentences were bi-syllabic
with first syllable stress, and all verbs were monosyllabic. All subject NPs were proper
names, and object NPs were mostly common inanimate objects, such that the events were
non-reversible. Furthermore, all words were comprised mostly of sonorant phonemes.
These constraints ensured that words could be more easily compared across items, and
facilitated the extraction of acoustic features (which is easier for vowels and sonorant
consonants). An adjunct prepositional phrase (PP) was included at the end of each
sentence so that differences in the production of the object NP due to the experimental
manipulations would be dissociable from prosodic effects on phrase-final, or in this case,

Acoustic correlates of information structure 16
sentence-final, words, which are typically lengthened and produced with lower F0
compared to phrase-medial words (e.g., Wightman et al., 1992).
We constructed 28 sets of materials. Participants saw one condition of each item,
following a Latin Square design. A sample item is presented in Table 1. The complete
set of materials can be found in Appendix A.

Condition

Focus Type

Focused
Argument

Setup Question

1

Non-contrastive

wide

What happened this morning?

2

Non-contrastive

S

Who fried an omelet this morning?

3

Non-contrastive

V

What did Damon do to an omelet this morning?

4

Non-contrastive

O

What did Damon fry this morning?

5

Contrastive

S

Did Harry fry an omelet this morning?

6

Contrastive

V

Did Damon bake an omelet this morning?

7

Contrastive

O

Did Damon fry a chicken this morning?

Table 1: Example item from Experiment 1. The target sentence is “Damon fried an
omelet this morning.”
Procedure
Productions were elicited and pre-screened in a two-part procedure. The first part
was a training session, where participants learned the intended names for pictures of
people, actions, and objects. In the second part, the pairs of participants produced
questions and answers for each other. The method was designed to maximize control
over what speakers were saying, but to also encourage natural-sounding productions.
Pilot testing revealed that having subjects simply read the target sentences resulted in
productions with low prosodic variability. After going through the experiment one time,
the participants switched roles.
Training session
In the training session, participants learned mappings between 96 pictures and
names, so that they could produce the names from memory during the second part of the

Acoustic correlates of information structure 17
experiment. In a PowerPoint presentation, each picture, corresponding to a person, an
action, an object, or a modifier, was presented with its intended name (see Figure 1, left).
The pictures consisted of eight names of people, which were repeated 3-4 items each in
the experimental materials, eight colors (which were used in a concurrently run filler
experiment), 34 verbs, 44 objects, and two temporal modifiers (this morning and last
night). The pictures were presented in alphabetical order, to facilitate memorization and
recall. Participants were instructed to learn the mappings by progressing through the
PowerPoint at their own pace.
When participants felt they had learned the mappings, they were given a picturenaming test, which consisted of 27 items from the full list of 96. The test was identical
for all participants. Participants were told of their mistakes, and, if they made four or
more errors, they were instructed to go back through the PowerPoint to improve their
memory of the picture-name mappings. Once participants could successfully name 23 or
more items on the test, which took between 1 and 3 rounds of testing, they continued with
the second part of the experiment. Early in pilot testing, we discovered that subjects had
poor recall for the names of the people in the pictures. Therefore, in the actual
experiment, subjects could refer to a sheet which had labeled pictures of the people.

Acoustic correlates of information structure 18

Figure 1: Left: Examples from the picture-training task for Experiment 1. Each square
represents a screen shot. Right: Examples of the procedure for the questioner (upper
squares) and answerer (lower squares) for Experiment 1. Two conditions are presented:
Non-contrastive, object (left) and contrastive, verb (right). The top squares represent
screen shots of what the questioner saw on a trial; the bottom squares represent what the
answerer saw on a trial.
Question-Answer Experiment
The experiment was conducted using Linger 2.92 (available at
http://telab.mit.edu/~dr/Linger/), a software platform designed by Doug Rohde for
language processing experiments. Participants were randomly paired and randomly
assigned to the role of questioner or answerer. Participants sat at computers in the same
room such that neither could see the other’s screen. On each trial, as illustrated in Figure
1 (right), the questioner saw a question (e.g., “What did Damon fry this morning?”)
which he/she was instructed to produce aloud for the answerer. The answerer was
instructed to produce an answer aloud using the information contained in the picture on
his/her screen (e.g., “Damon fried an omelet this morning”). The answerer was

Acoustic correlates of information structure 19
instructed to produce complete sentences, including the subject, verb, object, and
temporal abverb,4 and to emphasize the part of the sentence that the questioner had asked
about, or that he/she was correcting. On a random 20% of trials, the answerer was asked
a comprehension question about the answer s/he produced.
Productions were recorded in a quiet room with a head-mounted microphone at a
rate of 44kHz.
Acoustic Feature

Units

Description

duration

ms

Word duration excluding any silence before or after the word.

silence

ms

Duration of silence following the word, not due to stop closure.

duration+silence

ms

The sum of the duration of the word and any following silence.

mean F0

Hz

Mean F0 of the entire word

maximum F0

Hz

Maximum F0 value across the entire word

F0 peak location

0-1

The proportion of the way through the word where the maximum F0 occurs.

minimum F0

Hz

Minimum F0 across the entire word

F0 valley location

0-1

The proportion of the way through the word where the minimum F0 occurs.

initial F0

Hz

early F0

Hz

Mean F0 of the initial 5% of the word
Mean F0 value of 5% of the word centered at the point 25% of the way
through the word

center F0

Hz

late F0

Hz

Mean F0 value of 5% of the word centered on the midpoint of the word
Mean F0 value of 5% of the word centered on a point 75% of the way
through the word

final F0

Hz

Mean F0 of the last 5% of the word

1st quarter F0

The difference between initial F0 and early F0.

Hz

The difference between early F0 and center F0.

3rd quarter F0

Hz

The difference between center F0 and late F0.

4th quarter F0

Hz

The difference between late F0 and final F0.

mean intensity

dB

Mean intensity of the word

maximum intensity

dB

Maximum dB level in the word

minimum intensity
intensity peak
location
intensity valley
location

dB
0-1

Minimum dB level in the word
The proportion of the way through the word where the maximum intensity
occurs
The proportion of the way through the word where the minimum intensity
occurs

maximum amplitude

4

Hz

2nd quarter F0

Pascal

0-1

Maximum amplitude across the word

In the absence of explicit instruction to produce complete sentences, with a lexicalized subject, verb, and
object, speakers would likely resort to pronouns or would omit given elements altogether (e.g., “What did
Damon fry this morning?” “An omelet.”). A complete production account of information structure
meaning distinctions should include not just the prosodic cues used by the speakers, but also syntactic and
lexical production choices, as well as the interaction among these different production strategies. However,
because we focus on prosody in the current investigation, we wanted to be able to compare acoustic
features across identical words. Thus, we required that participants always produce a subject, verb, object
and adverb on every trial.

Acoustic correlates of information structure 20
energy

(Pascal)2 x
Duration

Table 2: Acoustic features extracted from each word in the target sentence for
Experiments 1-3. Stepwise discriminant analyses demonstrated that the measures in bold
provided the best discrimination among conditions and were used in all reported
analyses.
Results
Of the 504 speaker productions from the Question-Answer Experiment, 87 (17%) were
discarded because (a) the answerer failed to use the correct lexical items, (b) the answerer
was disfluent, or (c) the production was poorly recorded. The 417 remaining productions
were subjected to the acoustic analyses described below.
Acoustic Features
Based on previous investigations of prosody and information structure (Fry, 1955;
Lieberman, 1960; Eady et al., 1985; Cooper & Eady, 1986, Bartels & Kingston, 1994;
Krahmer & Swerts, 2001; Baumann et al., 2006), we chose a set of acoustic features to
analyze (see Table 2). These features were obtained automatically using the Praat
program (Boersma & Weenink, 2006). The measures of F0 computed over portions of
the words (e.g., 1st quarter F0) were chosen in order to investigate how F0 changes across
the syllable might contribute to the differentiation of conditions.
Our first goal was to determine which of the 24 candidate acoustic features
mediated differences among conditions. We conducted a series of stepwise linear
discriminant analyses5 on all of the data collected in Experiments 1, 2 and 3 reported in
the current paper. In order to determine the features to be used in the analyses of all three
experiments, we performed a separate stepwise analysis on the data from each
experiment separately. For each analysis we entered all 24 acoustic features across each
5

Linear discriminant analysis (LDA) calculates a function, computed as a linear combination of all
predictors entered, which results in the best separation of two or more groups. For two groups, only one
function is computed. For three groups, the first function provides the best separation of group 1 from
groups 2 & 3; a second, orthogonal, function provides the best separation of groups 2 and 3, after
partialling out variance accounted for by the first function. Stepwise LDA is an iterative procedure which
adds predictors based on which of the candidate predictors provide the best discrimination.

Acoustic correlates of information structure 21
of the three sentence positions (subject, verb, and object) as possible predictors of the
seven experimental conditions, resulting in 72 predictors. Across the three analyses, the
acoustic features which consistently resulted in the best discrimination of conditions were
(1) duration + silence, (2) mean F0, (3) maximum F0, and (4) maximum intensity at the
positions of the (a) Subject, (b) Verb, and (c) Object. The fact that these 12 features (four
acoustic features across three sentence positions) consistently discriminated among
conditions across three independent sets of productions (from different speakers and
across somewhat different sets of materials) serves as evidence that these features are
underlying speaker- and material-independent differentiation of information structure.
Therefore, we use only these 12 features in the linear discriminant analyses reported for
the individual experiments in the paper.
Computing Residual Values
Because of differences among individuals, including age, gender, speech rate and
level of engagement with the task, speakers produce very different versions of the same
sentence even within the same experimental condition, thus adding variance to the
acoustic features of interest. Similarly, there is likely to be variability associated with
different items due to lexical and world knowledge factors. Researchers have previously
dealt with the issue of acoustic variability between speakers by normalizing pitch and/or
duration by speaker (e.g., Shriberg, Stolcke, Hakkani-Tur, & Tur, 2000; Shriberg et al.,
1998; Wightman, Shattuck-Hufnagel, Ostendorf, & Price, 1992). In order to remove
speaker- and item-related variance in the current studies, we computed linear regression
models in which speaker (n = 18) and item (n = 28) predicted each of the 12 acoustic
features identified in the stepwise discriminant analyses described in the previous section.
From each of these models, we calculated the predicted value of each acoustic feature for
a specific item from a specific speaker. We then subtracted this predicted value from

Acoustic correlates of information structure 22
every production. The differences among the resulting residual values should reflect
differences in the acoustic features due only to the experimental manipulations. All
subsequently reported analyses were performed on these residual values.

Focus Location
The extent to which a discriminant function analysis can separate data points into two or
more groups is calculated with a statistical test, Wilks’s lambda6.
To determine how well the acoustic features could differentiate focus location in
speakers’ productions, we computed a model where the 12 acoustic predictors were used
to discriminate among three focus locations: Subject, Verb or Object. In this analysis, we
are averaging across the contrastive and non-contrastive condition for each location.
The overall Wilks’s lambda of the model was significant, Λ = .46, χ2(24) = 271, p
< .001, indicating better-than-chance differentiation of subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .84, χ2(24) = 62.65,
p < .001, indicating that the acoustic predictors could also differentiate verb focus from
object focus (see Figure 2). Leave-one-out classification correctly classified 67% of the
productions. The model correctly classified subject focus 76% of the time, verb focus
58% of the time, and object focus 66% of the time. Table 3 presents the standardized
canonical discriminant function coefficients of the model.7

6

Wilks's lambda is a measure of the distance between groups on means of the independent variables, and is
computed for each function. It ranges in size from 0-1; lower values indicate a larger separation between
groups. The extent to which the model can effectively discriminate a new set of data is simulated by a
leave-one-out classification, in which the acoustic data from each production are iteratively removed from
the dataset, the model is computed, and the left-out case is classified by the resultant functions.
7
The coefficients in Table 3 indicate which acoustic features best discriminate focus location, such that
larger absolute values indicate a greater contribution of that feature to discrimination. For example,
inspection of the plot in Figure 2 and the coefficients in the Focus Location columns of Table 3 shows that
the acoustic features of Damon score around zero, or lower, on the first function (-0.002, 0.001, -0.01, and 0.06) and around zero on the second function (-0.003, 0.021, -0.016, -0.101). Fried shows a different
pattern; specifically, the acoustic features of fried have coefficients around zero for the first function, and
negative coefficients for function 2. Finally, omelet shows a third pattern: its acoustic correlates are
centered around zero on Function 1, but are high on Function 2.

Acoustic correlates of information structure 23
Figure 3 graphically presents the mean values of the four features, demonstrating
that across all three focus locations the intended focus location is produced with the
highest maximum intensity, the longest duration and silence, and the highest relative F0.

Function
1

Function
2

Subj
Focus

Verb
Focus

Obj
Focus

omelet

Focus
Breadth

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.001
-0.006
0.002
-0.037

0.004
0.011
0.001
0.181

0.008
0.011
-0.002
-0.137

0.003
-0.014
0.002
-0.026

0.004
-0.019
0.006
0.189

0.003
0.000
0.003
0.199

fried

Focus Type

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

0.007
0.024
0.002
0.094

-0.001
-0.003
-0.002
-0.010

0.007
0.000
0.004
-0.076

0.002
-0.040
-0.007
0.131

-0.001
-0.013
0.013
-0.043

0.005
-0.025
0.003
0.011

Damon

Focus Location

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.002
0.001
-0.010
-0.060

-0.003
0.021
-0.016
-0.101

0.005
-0.016
-0.012
0.087

-0.002
-0.007
0.020
0.056

0.005
-0.014
-0.011
-0.225

0.003
0.007
-0.005
-0.123

Acoustic correlates of information structure 24
Table 3: Standardized canonical coefficients of the discriminant functions computed for
Experiment 1.

Figure 2: Separation of focus locations on two discriminant functions in Experiment 1.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the upper left quadrant; productions of verb focus are
clustered in the lower half of the plot; productions of object focus are clustered in the
upper right quadrant.

Acoustic correlates of information structure 25

Damon
fried
omelet

Figure 3: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 1.
Focus type
To determine how well the acoustic features could differentiate the type of focus
(i.e. non-contrastive vs. contrastive) in speakers’ productions, we computed three models
in which the 12 acoustic predictors were used to discriminate between two focus type
groups. The three models investigated differences between non-contrastive and
contrastive focus at the three focus locations: subject, verb, and object.
Focus Type – Subject Position
The overall Wilks’s Lambda was not significant, Λ = .898, χ2(12) = 11.95 p = .45,
indicating that the acoustic features could not discriminate between non-contrastive and

Acoustic correlates of information structure 26
contrastive focus. Because the overall model is not significant, we do not present the
scores of the specific acoustic features or the classification statistics here or in the
analyses below.
Focus Type – Verb Position
The overall Wilks’s Lambda was not significant, Λ = .851, χ2(12) = 17.92 p = .12,
indicating that the acoustic features could not discriminate between non-contrastive and
contrastive focus.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .82, χ2(12) = 22.63 p < .05,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus above chance level. Leave-one-out classification correctly classified
59% of the productions. The model correctly classified non-contrastive focus 59% of the
time, and contrastive focus 59% of the time.
The coefficients in the Object Focus column of Table 3 indicate that intensity and
mean F0 contribute most to classification. Figure 4 graphically presents the mean values
of the four features, demonstrating that contrastive focus is produced with a higher
maximum intensity, a longer duration and silence, and higher maximum F0. Noncontrastive focus is produced with a higher mean F0.

Acoustic correlates of information structure 27

Damon
fried
omelet

Figure 4: Values for non-contrastive focus and contrastive focus type on the four
discriminating acoustic features when the direct object “omelet” is focused in
Experiment 1.
Wide Focus vs. Narrow Focus
To determine how well the acoustic features could differentiate focus breadth, we
computed a model in which the 12 critical predictors were used to discriminate between
productions where the entire sentence was focused and productions where the object was
non-contrastively or contrastively focused.
The overall Wilks’s Lambda was significant, Λ = .75, χ2(12) = 47.83, p < .001,
indicating that the acoustic features could successfully discriminate between conditions
where the entire event is focused and conditions where the object is narrowly focused.

Acoustic correlates of information structure 28
Leave-one-out classification correctly classified 72% of the productions. The model
correctly classified wide focus 67% of the time, and narrow focus 74% of the time.
The standardized canonical discriminant function coefficients in the Focus
Breadth column of Table 3 indicate that maximum intensity contributes most to focus
breadth classification. Figure 5 graphically presents the mean values of the four features,
demonstrating that wide focus is produced with a more uniform duration + silence and
maximum F0 across the sentence than object focus. Wide focus is also produced with a
more uniform, though overall greater, intensity than object focus.

Damon
fried
omelet

Figure 5: Values for wide focus vs. narrow object focus on the four discriminating
acoustic features in Experiment 3.
Discussion

Acoustic correlates of information structure 29
Focus Location
The results demonstrate that speakers consistently provide acoustic cues which
disambiguate focus location. Specifically speakers indicated focus with increased
duration, higher intensity, higher mean F0, and higher maximum F0. Furthermore, these
results are consistent with the pattern reported in Eady & Cooper (1986), such that the
word preceding a focused word is less prominent (produced with shorter duration, lower
intensity and lower F0) than the focused word, and the word following the focused word
is less prominent than the word preceding the focused word. Previous studies (Eady et
al., 1986; Rump and Collier, 1986) have reported this reduction in acoustic prominence
following focused elements as being mainly indicated by lower F0 on the post-focal
words, though in our data we also find evidence of this reduction in measures of duration
and intensity.
Focus Type
The results from Experiment 1 indicate that in semi-naturalistic productions
speakers do not systematically differentiate between different focus types (focused
elements which have explicit contrast sets in the discourse and those which do not).
Specifically, at two out of three sentence positions, a discriminant function analysis could
not successfully classify speakers’ productions of contrastively vs. non-contrastively
focused elements. The observation that speakers successfully discriminated contrastive
and non-contrastive focus in object position, but not in subject or verb positions, is
perhaps suggestive, but is likely due to a lack of experimental power, a limitation which
will be addressed in Experiment 2.
Focus Breadth
The results from Experiment 1 demonstrate that speakers do systematically mark
focus breadth prosodically. Narrow object focus is produced with the highest maximum
F0, longest duration, and maximum intensity of the object noun, relative to the other

Acoustic correlates of information structure 30
words in the sentence. For wide focus, the acoustic features are more similar across the
sentence; only intensity and mean F0 are higher on the object than on the other words in
the sentence. These differences are subtle, but sufficient for the model to successfully
discriminate the productions.
The fact that the model failed to systematically classify productions by focus type
(with the exception of the object position), while achieving high accuracy in focus
location and focus breadth indicates that speakers were not marking focus type with
prosody in Experiment 1. However, the method used to elicit productions did not require
that subjects be aware of the information structure ambiguity of the materials. Evidence
from other production studies suggests that speakers may not prosodically disambiguate
ambiguous productions if they are not aware of the ambiguity. Albritton, McKoon, and
Ratcliff (1996), for example, demonstrated that speakers did not disambiguate
syntactically ambiguous constructions like “Dave and Pat or Bob” unless they were
aware of the ambiguity (see also Snedeker and Trueswell, 2003, but cf. Kraljic and
Brennan, 2005, and Schafer, Speer, Warren, and White, 2000, for evidence that speakers
do disambiguate syntactically ambiguous structures even in the absence of ambiguity
awareness). Experiment 2 was designed to be a stronger test of speakers’ ability to
differentiate focus location, focus type, and focus breadth. We used materials similar to
those in Experiment 1, with two important methodological modifications. First, instead
of producing the answers to questions with no feedback, the speaker’s task now involved
trying to enable the answerer to choose the question that s/he was answering from a set of
possible questions. Moreover, we introduced feedback so that the speaker would always
know whether his/her partner had chosen the correct answer. Second, we changed the
design from a between- to a within-subjects manipulation. This ensured that speakers

Acoustic correlates of information structure 31
were aware of the manipulation, as they were producing the same answer seven times
with explicit instructions to differentiate their answers for their partner.
In addition to making the speaker’s task explicit, the new design also allowed us
to analyze the subset of the productions for which the listeners could successfully identify
the question-type and which therefore contain sufficient information for differentiating
utterances along the three relevant dimensions of information structure.

Experiment 2
Method
Participants
Seventeen pairs of participants were recorded for this experiment. Subjects were MIT
students or members of the surrounding community. All reported being native speakers
of American English. None had participated in Experiment 1. Participants were paid for
their participation.
Materials
The materials had the same structure as those from Experiment 1, though the
critical words differed. Specifically, a larger set of names and a wider variety of
temporal adverbs were used, and some verbs and objects differed from Experiment 1.
Unlike Experiment 1, each subject pair was presented with all seven versions of each of
14 items, according to a full within-subjects within-items design. All materials can be
found in Appendix B.
Procedure
Two participants sat at computers in the same room such that neither could see the
other’s screen. One participant was the speaker, and the other was the listener. Speakers
were told that they would be producing answers to questions out loud for their partners

Acoustic correlates of information structure 32
(the listeners), and that the listeners would be required to choose which question the
speaker was answering from a set of seven choices.
At the beginning of each trial, the speaker was presented with a question on the
computer screen to read silently. After pressing a button, the answer to the question
appeared below the question, accompanied by a reminder to the speaker that s/he would
only be producing the answer aloud, and not the question. Following this, the speaker
had one more chance to read the question and answer, and then he/she was instructed to
press a key to begin recording (after being told by the listener that he/she is ready), to
produce the answer, and then to press another key to stop recording.
The listener sat at another computer, and pressed a key to see the seven questions
that s/he would have to choose his/her answer from. When s/he felt familiar with the
questions, s/he told the speaker s/he was ready. After the speaker produced a sentence
out loud for the listener, the listener chose the question s/he thought the speaker was
answering. If the listener answered incorrectly, his/her computer produced a buzzer
sound, like the sound when a contestant makes an incorrect answer on a game show.
This cue was included to ensure that speakers knew when their productions did not
contain enough information for the listener to choose the correct answer.8
Results – Production
Two speaker-listener pairs were excluded as the Listener did not achieve
comprehension accuracy greater than 20%. One further pair was excluded as one
member was not a native speaker of American English. Finally, another pair of subjects
was excluded because they did not take the task seriously, and produced unnaturally
emphatic contrastive accents, often shouting the target word, and laughing while doing

8

In early pilots in which there was no feedback for incorrect responses, we observed that listeners were at
chance in choosing the correct question.

Acoustic correlates of information structure 33
so. These exclusions left a total of 13 pairs of participants whose responses were
analyzed.
Sixty-seven of the 1274 trials (5%) were excluded because (a) the speaker failed
to produce the correct words, (b) the speaker was disfluent, or (c) the production was
poorly recorded. Analyses were performed on all trials, and on the subset of trials for
which the listener correctly identified the question. The results were very similar in the
two analyses. For brevity of presentation, we present results from analyses conducted on
the correct trials (n = 660, 55%). The productions from Experiment 2 were analyzed
using the acoustic features chosen in the feature-selection procedure described in
Experiment 1. All analyses were performed on the residual values of these features, after
removing speaker and item variance with the method described in Experiment 1.
Focus Location
The overall Wilks’s lambda was significant, Λ = .085, χ2(24) = 1335, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .306, χ2(11) = 641, p
< .001, indicating that the acoustic features could also discriminate verb focus from
object focus (see Figure 6).
Leave-one-out classification correctly classified 93% of the productions. For
individual levels of focus location, the discriminant function correctly classified subject
focus 94% of the time, verb focus 90% of the time, and object focus 95% of the time.
The standardized canonical coefficients in the first two columns of Table 4
indicate that the acoustic features contributing most to the discrimination of focus
location are once again mean F0 and maximum intensity, though the other two features
are also contributing. In fact, inspection of the acoustic feature means in Figure 7

Acoustic correlates of information structure 34
demonstrate that the highest value of every acoustic feature is associated with the
intended focused item, with the exception of mean F0 when the subject is focused.

Figure 6: Separation of focus locations on two discriminant functions for Experiment 2.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the lower left quadrant of the plot; productions of verb
focus are clustered in the lower right quadrant; productions of object focus are clustered
in the lower half.
Focus Location

Focus Type

Focus Breadth

Function 2

Subject
Focus

Verb Focus

Object
Focus

Duration+ silence

omelet

Function 1
-0.001

0.004

0.004

0.006

0.003

0.003

Mean F0

-0.006

0.011

-0.003

0.005

-0.023

0.000

Maximum F0

0.002

0.001

0.004

-0.009

-0.003

0.003

Maximum Intensity

-0.025

0.183

-0.052

-0.171

0.012

0.199

Acoustic correlates of information structure 35
-0.002

0.006

0.002

-0.007

0.005

Mean F0

0.024

-0.005

0.001

-0.022

0.006

-0.025

Maximum F0

0.001

-0.002

-0.007

0.001

0.003

0.003

0.093

-0.016

-0.105

0.063

-0.084

0.011

Duration+ silence

Damon

0.007

Maximum Intensity

fried

Duration+ silence

-0.002

-0.002

0.002

0.005

0.009

0.003

Mean F0

0.003

0.021

-0.010

0.004

-0.009

0.007

Maximum F0

-0.011

-0.015

-0.014

-0.012

-0.006

-0.005

Maximum Intensity

-0.067

-0.097

0.094

-0.014

0.010

-0.123

Table 4: Standardized canonical coefficients of all discriminant functions computed for
Experiment 2.

Damon
fried
omelet

Figure 7: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 2.

Acoustic correlates of information structure 36
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .633, χ2(12) = 81.41, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
75% of the productions. The model correctly classified non-contrastive focus 78% of the
time, and contrastive focus 71% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that maximum intensity at all three locations (i.e. large intensity differences between the
subject and verb and the subject and object) contributes most to classification. Figure 8
graphically presents the mean values of the four features, demonstrating that, in addition
to intensity differences, contrastive focus is produced with longer duration and silence, as
well as lower mean and maximum F0.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .654, χ2(12) = 72.27, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
72% of the productions. The model correctly classified non-contrastive focus 70% of the
time, and contrastive focus 75% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that, once again maximum intensity contributes most to classification. Figure 9
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a higher maximum intensity, and a longer duration and silence,
than non-contrastive focus. Once again, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

Acoustic correlates of information structure 37
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .793, χ2(12) = 41.3, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
67% of the productions. The model correctly classified non-contrastive focus 69% of the
time, and contrastive focus 66% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that contrastive focus is most strongly associated with lower mean F0. Figure 10
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a lower mean and maximum F0 than non-contrastive focus.

Damon
fried
omelet

Figure 8. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 2.

Acoustic correlates of information structure 38

Damon
fried
omelet

Figure 9. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 2.

Acoustic correlates of information structure 39

Damon
fried
omelet

Figure 10. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “omelet” is focused in Experiment 2.
Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .59, χ2(12) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 84% of productions; wide
focus was correctly classified 77% of the time, and object focus was correctly classified
88% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 4
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Although intensity is contributing most strongly
to classification, inspection of the acoustic means in Figure 11 indicates that wide focus

Acoustic correlates of information structure 40
is marked by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is marked by greater prominence on the
object, reflected in longer duration, higher F0, and higher intensity.

Damon
fried
omelet

Figure 11: Values for wide vs. narrow object focus on the four discriminating acoustic
features in Experiment 2.

Acoustic correlates of information structure 41
Results – Perception

Figure 12. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 2.
Listeners’ choices of question sorted by the intended question are plotted in
Figure 12. Listeners’ overall accuracy was 55%. To determine whether listeners were
able to determine the speaker’s intended sentence meaning, we compared each subject's
responses to chance performance. Specifically we assessed, for focus location and focus
type, whether each subject's proportion of correct responses exceeded chance; wide
focus productions were excluded from the analysis, so that chance performance for focus

Acoustic correlates of information structure 42
location was .33, and chance performance for focus type was .5. Results demonstrated
that listeners were able to successfully identify focus location: all 13 subjects’
performance significantly exceeded chance performance, p = .05, two-tailed. However,
listeners were unable to successfully identify focus type: only three of 13 subjects
performed at above-chance levels (based on the binomial distribution), p = .05, twotailed. To investigate focus breadth, we assessed, for wide focus and narrow object focus
separately, whether each subject's proportion of correct responses exceeded chance. For
these analyses, we excluded subject and verb focus productions, so that chance
performance was .33 for wide focus, and .67 for narrow object focus. Results
demonstrated that listeners were moderately successful at identifying focus breadth: six
of 13 subjects identified wide focus at rates above chance, and nine out of 13 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.
Discussion
The production results replicated the two main findings from Experiment 1, and provided
evidence for acoustic discrimination of focus type across sentence positions as well.
First, these results demonstrated that focused elements have longer durations than nonfocused elements, incur larger F0 excursions, are more likely to be followed by silence,
and are produced with greater intensity. Second, speakers consistently differentiate
between wide and narrow focus by producing the object in the latter case with higher F0,
longer duration, and greater intensity. Specifically, although object focus was indicated
by increased duration, higher intensity, and higher F0 on the object than on the subject or
the verb, wide focus was indicated by comparatively greater duration, higher intensity,
and higher F0 on the subject and the verb, and shorter duration, lower intensity, and
lower F0 on the object. These results are consistent with those obtained by Baumann et

Acoustic correlates of information structure 43
al. (2006), who demonstrated that narrow focus on an element was indicated with longer
duration and a higher F0 peak than wide focus on an event encompassing that element.
Most importantly, although speakers in Experiment 1 did not differentiate
conditions with and without an explicit contrast set for the focused element (except for
the object position), these conditions were differentiated by speakers in Experiment 2, at
every syntactic position. There are two possible interpretations of this difference. First,
in Experiment 1, speakers produced only four versions of each of the seven conditions,
whereas speakers in Experiment 2 and 3, reported below, produced 14 versions of each of
the seven conditions, resulting in greater power in the latter two experiments. The fact
that, in Experiment 2, speakers successfully discriminated contrastive and noncontrastive focus in all three positions, suggests that the lack of such an effect in
Experiment 1 could be due to a lack of power.
As mentioned above, the difference in the findings between Experiments 1 and 2
is also consistent with results from Allbritton et al. (1996) and Snedeker and Trueswell
(2003) who demonstrated that speakers do not disambiguate syntactically ambiguous
sentences with prosody unless they are aware of the ambiguity. The current results
demonstrate a similar effect for acoustic prominence, such that speakers do not
differentiate two kinds of acoustically prominent elements (contrastively vs. noncontrastively focused elements) unless they are aware of the information structure
ambiguity in the structures they are producing.
The discriminant analyses indicated that contrastively focused words were
produced with longer durations and higher intensity than non-contrastively focused
words, but that non-contrastively focused words were produced with higher F0 than
contrastively focused words. This latter finding is surprising when compared to some
previous studies. For example, Ladd & Morton (1997) found that higher F0 and larger

Acoustic correlates of information structure 44
F0 range is perceived as more ‘emphatic’ or ‘contrastive’ by listeners. Similarly, Ito and
Speer (2008) demonstrated that contrastively focused words were produced with higher
F0 than non-contrastive ones. Given the unexpected results, we inspected individual
pitch tracks to more closely observe the F0 patterns across the entire utterances. The
pitch tracks presented in Figure 13 were generated from the productions of a typical
speaker, and they exemplify the higher F0 observed for non-contrastive focus than
contrastive focus in the subject position (A vs. B) and verb position (C vs. D).
Contrastive focus on the object is realized with the same F0 as non-contrastive focus on
the object (E vs. F).

300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Non-Contrastive

Given

an

omelet

yesterday

Given

0

1.433
Time (s)

Acoustic correlates of information structure 45
A. Non-contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Contrastive

an

Given

omelet

yesterday

Given

0

1.81
Time (s)

B. Contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Non-Contrastive

omelet

yesterday

Given

0

1.514
Time (s)

C. Non-contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Contrastive

an

omelet

<SIL>

yesterday

Given

0

2.377
Time (s)

Acoustic correlates of information structure 46
D. Contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Given

omelet

yesterday

Non-Contrastive
1.582

0
Time (s)

E. Non-contrastive Object Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Given

an

omelet

yesterday

Contrastive
2.188

0
Time (s)

F. Contrastive Object Focus
Figure 13. Pitch tracks for non-contrastive and contrastive subject focus, non-contrastive
and contrastive verb focus, and non-contrastive and contrastive object focus,
respectively, from a typical speaker from Experiment 2.
Note that our finding that non-contrastive focus is realized with higher F0 than
contrastive focus is still consistent with the claim that contrastive focus is more
prominent than non-contrastive focus. As the graphs in Figures 8-10, and the pitch tracks
in Figure 13 indicate, although contrastive elements were consistently produced with
lower pitch, they were also consistently produced with longer durations and greater

Acoustic correlates of information structure 47
intensity than non-contrastive elements. As reviewed in the introduction, there is
9

evidence that intensity and duration can convey prominence more effectively than higher
pitch (Fry, 1954, Lieberman, 1960, Beckman, 1986; Turk & Sawusch,1996; Kochanski et
al., 2005). Our data are therefore consistent with prior claims that contrastive focus is
produced with greater prominence than non-contrastive focus.
As discussed in the introduction, the production elicitation and analysis methods
used in the current experiment are more robust than methods used in many previous
studies, including those whose results are inconsistent with the current findings. In
particular, the current results are based on productions from naïve subjects in a
communication task, and the analyses were performed on data with speaker and item
variability removed. The current results are therefore more likely to reflect the
underlying generalizations about the relationship between acoustics and meaning.
The perception results only partially mirrored the production results. Consistent
with the production results, listeners were highly successful in discriminating among the
three focus locations. In contrast to the production results, however, listeners were only
moderately successful in identifying focus type (non-contrastive vs. contrastive) from the
speakers’ productions. In fact, listeners most often confused non-contrastive focus with
contrastive focus (see Figure 12). These results suggest that, even though speakers may
be consistently signaling focus type with their prosody, listeners are not able to exploit
those cues for comprehension.
With regard to focus breadth, the perception results are incompatible with a strong
version of the focus projection hypothesis (Selkirk, 1995). According to this hypothesis,
an acoustic prominence on the object NP can be interpreted as marking the entire clause
9

Importantly, the F0 results are not artifacts of the residualization procedure employed to remove variance
from the acoustic features due to speaker and item. The same numerical pattern of F0 values is observed
whether residualization is employed or not, though only the residualized acoustic features successfully
discriminate focus type.

Acoustic correlates of information structure 48
as focused. Listeners are therefore predicted to treat a production with an acoustically
prominent object NP as ambiguous between the narrow object focus reading and the wide
focus reading. However, as can be seen in Figure 12, listeners correctly identified narrow
object non-contrastive focus 57% of the time, interpreting it as wide focus only 13% of
the time, and correctly identified narrow object contrastive focus 49% of the time,
interpreting it as wide focus only 6% of the time. These results are not consistent with
Gussenhoven’s (1983) finding that listeners cannot reliably distinguish between narrow
objects focus and wide focus.
Experiments 1 and 2 provide evidence that speakers systematically indicate focus
location and focus breadth using a set of four acoustic features. These experiments
further suggest that speakers can, but don’t always, indicate focus type. In particular, the
results suggest that speakers only prosodically differentiate contrastive from noncontrastive focus when they are aware of the meaning ambiguity and/or when the task
involves conveying a particular meaning to a listener.
To further investigate the speakers’ ability to prosodically differentiate contrastive
from non-contrastive focus, we conducted an additional experiment. Acoustic analyses
in Experiments 1 and 2 were limited to three words (i.e. subject, verb, object) in the
sentence. However, in natural productions, speakers’ utterances are often prefaced by
attribution expressions (e.g., “I think” or “I heard”), or expressions of emotional attitudes
towards the described events (e.g., “Unfortunately”, or “Luckily”). It is therefore
possible that contrastive information might be partially conveyed by prosodically
manipulating these kinds of expressions. We explored this possibility in Experiment 3, in
which we had speakers produce target SVO constructions with a preamble. Experiment 3
was also intended to serve as a replication of the results of Experiment 2; in particular,

Acoustic correlates of information structure 49
the somewhat unexpected finding that non-contrastive focus is produced with higher F0
than contrastive focus.

Experiment 3
Method
Participants
Fourteen pairs of participants (speakers and listeners) were recorded for this
experiment. Subjects were MIT students or members of the surrounding community. All
reported being native speakers of American English. None had participated in
Experiments 1 or 2. Participants were paid for their participation.
Materials
The materials for Experiment 3 were identical to those from Experiment 1
described above with the exception that an attribution expression (“I heard that”) was
appended to the beginning of each target sentence.
Procedure
The procedure for Experiment 3 was identical to that for Experiment 2.
Results – Production
Four speaker-listener pairs were excluded as the listener did not achieve
comprehension accuracy greater than 20%. These exclusions left a total of 10 pairs of
participants whose responses were analyzed. Eighty-one of the 980 recorded trials (8%)
were excluded because (a) the speaker failed to produce the correct words, (b) the
speaker was disfluent, or (c) the production was poorly recorded. Analyses were
performed on all trials, and on the subset of trials for which the listener correctly
identified the question the speaker produced the sentence in response to. As in
Experiment 2, the results were very similar for the two analyses. For brevity of
presentation, we present results from analyses conducted on the correct trials (n = 632,
70%).

Acoustic correlates of information structure 50
Focus Location
In order to investigate the contribution of the prosody of “I heard that” to the
differentiation of the focus type in Experiment 2, we performed a stepwise discriminant
function analysis which included as predictors measures of the four acoustic features we
had selected initially (duration + silence, mean F0, maximum F0, maximum intensity) (1)
for the subject (“Damon”), verb (“fried”), and object (“omelet”), and (2) for each of the
first three words of the sentence (“I”, “heard”, “that”). Of the 24 predictors included in
the stepwise discriminant function analysis, the features which resulted in the best
discrimination of focus type were (1) the duration + silence of “I”, (2) the maximum F0
of “I”, and (3) the maximum intensity of “I”. Based on these results, we conducted an
additional analysis in which we included a subset of 16 predictors: the duration + silence,
mean F0, maximum F0, and maximum intensity of the subject, verb, object, and “I”.
As in Experiments 1 and 2, we conducted a discriminant analysis to determine
whether the measures of (1) duration + silence, (2) maximum F0, (3) mean F0, and (4)
maximum intensity of the four critical words in the sentence could predict focus location.
The overall Wilks’s lambda was significant, Λ = .058, χ2(32) = 1467.09, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .275, χ2(15) =
664.75, p < .001, indicating that the acoustic features could also discriminate verb focus
from object focus (Figure 14). Leave-one-out classification procedure correctly
classified 97% of the productions. At individual focus locations, the model correctly
classified subject focus 96% of the time, verb focus 97% of the time, and object focus
97% of the time.

Acoustic correlates of information structure 51

Figure 14. Separation of focus locations on two discriminant functions for Experiment 3.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the left half of the plot; productions of verb focus are
clustered in the lower right quadrant; productions of object focus are clustered in the
upper right quadrant.
Focus Location

Focus
Breadth

Focus Type

fried

omelet

Function
1

Function
2

Subject
Focus

Verb
Focus

Object
Focus

0.002

0.003

0.000

0.000

0.003

0.002

0.005

0.012

-0.013

-0.009

0.005

-0.010

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

0.003

0.000

0.005

0.006

-0.003

0.003

0.069

0.106

-0.037

-0.011

0.007

0.151

0.001

-0.003

0.000

0.002

0.001

0.005

0.025

-0.021

-0.001

-0.002

-0.001

-0.006

Maximum F0
Maximum
Intensity

-0.005

-0.002

-0.001

-0.005

0.000

-0.003

0.091

-0.077

-0.086

-0.015

0.027

-0.048

Duration+
silence
Mean F0

Acoustic correlates of information structure 52
Duration+
silence
Mean F0

Damon

0.000

0.002

0.000

0.000

0.002

0.011

0.011

-0.011

-0.020

0.019

-0.003

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

-0.014

-0.003

-0.001

0.007

-0.014

-0.008

-0.147

0.011

0.159

-0.006

-0.064

-0.123

0.000

0.000

0.004

0.005

0.005

-0.001

-0.005

0.000

-0.013

-0.008

-0.003

-0.009

Maximum F0
Maximum
Intensity

I

-0.003

0.004

-0.002

0.017

0.010

0.014

0.005

-0.021

-0.017

0.142

0.133

0.126

0.014

Table 5: Standardized canonical coefficients of all discriminant functions computed for
Experiment 3.

I
Damon
fried
omelet

Figure 15. Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 3.

Acoustic correlates of information structure 53
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .39, χ2(16) = 157.44, p<.001,
indicating that the acoustic features could successfully discriminate between noncontrastive and contrastive focus. Leave-one-out classification correctly classified 85%
of the productions. The model correctly classified non-contrastive focus 85% of the time,
and contrastive focus 85% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that maximum intensity overall, and specifically, maximum intensity on “I,” is
contributing most to classification. Figure 16 graphically presents the mean values of the
four features, demonstrating that, in addition to intensity differences, contrastive focus is
produced with longer duration and silence, and with lower mean and maximum F0.

Acoustic correlates of information structure 54

I
Damon
fried
omelet

Figure 16. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 3.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .46, χ2(16) = 139.28, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
80% of the productions. The model correctly classified non-contrastive focus 86% of the
time, and contrastive focus 74% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that, intensity on “I” is contributing the most to classification. Figure 17 graphically

Acoustic correlates of information structure 55
presents the mean values of the four features, demonstrating that contrastive focus is
produced with a higher maximum intensity, and a longer duration and silence, than noncontrastive focus. As in Experiment 2, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

I
Damon
fried
omelet

Figure 17. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 3.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .40, χ2(16) = 133.37, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and

Acoustic correlates of information structure 56
contrastive focus better than chance. Leave-one-out classification correctly classified
83% of the productions. The model correctly classified non-contrastive focus 89% of the
time, and contrastive focus 76% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that intensity and mean F0 on “I” are contributing the most to accurate classification.
Figure 18 graphically presents the mean values of the four features, demonstrating that
contrastive focus is produced with a higher mean and maximum F0 than non-contrastive
focus.

I
Damon
fried
omelet

Acoustic correlates of information structure 57
Figure 18: Values for non-contrastive vs. contrastive focus on the four discriminating
acoustic features when “omelet” is focused in Experiment 3.

Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .48, χ2(16) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 87% of productions; wide
focus was correctly classified 79% of the time, and object focus was correctly classified
92% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 5
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Specifically, greater intensity on the object is a
strong predictor of object focus; less intensity on the subject and the verb are strong
predictors of wide focus. Although intensity is contributing most strongly to
classification, inspection of the acoustic means in Figure 19 indicates that wide focus is
indicated by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is indicated by greater prominence on
the object, reflected in longer duration, higher F0, and higher intensity.

Acoustic correlates of information structure 58

I
Damon
fried
omelet

Figure 19. Values for wide vs narrow object focus on the four discriminating acoustic
features in Experiment 3.
Results – Perception
Listeners’ overall accuracy percentage by condition is plotted in Figure 20.
Listeners’ overall accuracy was 70%. As described in Experiment 2, we compared each
subject's responses to chance performance. Results demonstrated that listeners were able
to successfully identify focus location, as all 10 subjects’ performance significantly
exceeded chance performance, p = .05, two-tailed. Listeners were moderately successful
at discriminating focus type, as six of 10 subjects’ performance exceeded chance levels, p
= .05, two-tailed. Listeners successfully identified focus breadth as eight out of 10

Acoustic correlates of information structure 59
subjects identified wide focus at rates above chance, and eight out of 10 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.

Figure 20. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 3.

Discussion
Experiment 3 was conducted in order to (1) investigate whether speakers could
differentiate focus type with prosody if the sentences contained an attribution expression

Acoustic correlates of information structure 60
that could convey contrastive information, in addition to the elements that describe the
target event, and (2) replicate the results of Experiment 2.
With regard to the second goal, the production results of Experiment 3
successfully replicated the findings from Experiments 1 and 2. As in Experiments 1 and
2, speakers systematically differentiated focus location and focus breadth with a
combination of duration, intensity, and F0 cues. Furthermore, as in Experiment 2, noncontrastive focus was produced with higher F0 than contrastive focus (though only when
the subject or verb was focused), and contrastive focus was always produced with greater
duration and intensity. As discussed above, these F0 results contrast with prior findings
(Bartels & Kingston, 1994; Couper-Kuhlen, 1984; Ladd & Morton, 1997; Ito & Speer,
2008), but can be interpreted in light of more recent evidence that higher intensity is a
stronger cue to greater prominence than higher pitch (Kochanski et al., 2005).
In addition, results from Experiment 3 demonstrated that the strongest cues to
discrimination of focus type were the acoustics of “I” (from the attribution expression “I
heard that”). Specifically, in contrastive focus conditions, the word “I” was produced
with longer duration, higher intensity, and higher mean F0 and maximum F0. Indeed,
discrimination of focus type in Experiment 3 was far better than in Experiment 2. It
therefore appears that speakers can manipulate prosody on sentence elements outside of
the target clause (e.g., in attribution expressions) to convey contrastiveness.
The perception results demonstrated that listeners could accurately determine
focus location, similar to the results of Experiment 2. Furthermore, listeners were more
accurate in determining focus type than listeners in Experiment 2. This increase in
accuracy was likely due to speakers’ tendency to prosodically mark “I” in the contrastive
conditions.

Acoustic correlates of information structure 61

General Discussion
The three experiments reported in the current paper explored the ways in which focus
location, focus type, and focus breadth are conveyed with prosody. In each experiment,
naïve speakers and listeners engaged in tasks in which the information status of sentence
elements in SVO sentences was manipulated via preceding questions. The prosody of the
target sentences was analyzed using a series of classification models to select a subset
from the set of acoustic features that would best be able to discriminate among focus
locations and between focus types. In addition, in Experiments 2 and 3, the production
results were complemented by the perception results that demonstrated listeners’ ability
to use the prosodic cues in the speakers’ utterances to arrive at the intended meaning.
At the beginning of the paper, we posed three questions about the relationship
between acoustics and information structure: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? We are now in a position to answer these questions.
First, we have demonstrated that speakers systematically provide prosodic cues to
the location of focused material. Across all three experiments, speakers provided cues to
focus location whether or not the task explicitly demanded it, across subject, verb and
object positions. In addition, across all three experiments, speakers systematically
provided cues to focus breadth, such that wide focus was prosodically differentiated from
narrow object focus. Finally, we found that speakers can, but don’t always, prosodically
differentiate contrastive and non-contrastive focus. Specifically, speakers did not
prosodically differentiate focus type in Experiment 1, but they did so in Experiment 2
and, even more strongly, in Experiment 3. As discussed above, the fact that speakers did
not differentiate focus type in Experiment 1, where they were plausibly not aware of the

Acoustic correlates of information structure 62
meaning ambiguity, but did differentiate between contrastive and non-contrastive focus
conditions in Experiments 2 and 3, where the task made the meanings more salient, is
consistent with results from the literature on intonational boundary production
demonstrating that speakers only produce disambiguating boundaries when they are
aware of the syntactic ambiguity which could be resolved by the presence of a boundary
(Albritton et al., 1996; Snedeker & Trueswell, 2003; cf. Schafer, et al., 2000 and Kraljic
& Brennan, 2005). Furthermore, the results from Experiment 3, where the critical words
were preceded by the attribution expression “I heard that,” demonstrated even stronger
differentiation of focus type than in Experiment 2, suggesting that speakers are able to
convey contrastiveness using words outside of the clause containing the contrastivelyfocused element.
To answer the question of which acoustic features are associated with different
meaning categories of information structure, we conducted a series of discriminant
function analyses with the goal of objectively identifying which of 24 measures of
duration, intensity, and F0 allowed for the best discrimination of conditions. Across all
experiments, and across different sentence positions, the best differentiation among
conditions was achieved using the following four features: word duration, maximum
word intensity, mean F0, and maximum F0. These results are consistent with many
previous studies in the literature, implicating these features in conveying aspects of
information structure. An important contribution of the current studies is that these
results were obtained using a quantitative analysis across many naïve speakers and items,
and are therefore more likely to be generalizable.
These data also demonstrate how exactly these four features are used in
conveying different aspects of information structure. With regard to focus location,
focused material is produced with longer duration, higher F0, and greater intensity than

Acoustic correlates of information structure 63
non-focused material. With regard to focus type, non-contrastive focus is realized with
higher mean and maximum F0 on the focused word than contrastive focus, whereas
contrastive focus is realized with greater intensity on the focused word than noncontrastive focus. Finally, with regard to focus breadth, narrow focus on the object is
indicated by higher F0 and longer duration on the object, compared to wide focus, and
wide focus is conveyed by higher intensity and F0, and longer duration on pre-focal
words.
To answer the question of how well listeners can retrieve prosodic information
from the signal, we included a perception task in Experiments 2 and 3. When the
relevant acoustic cues were present in the input (as demonstrated by successful
classification by the models), listeners were also able to classify the utterances, although
not quite as successfully as the models. Furthermore, the fact that the model always
achieved high classification accuracy suggests that the utterances contained enough
acoustic information to make these discriminations, and that we did not leave any
particularly informative acoustic features out of the analyses.

Implications for theories of the mapping of acoustics to meaning
While our production and perception results are compatible with a direct
relationship between acoustics and meaning, they are also consistent with the existence of
mediating phonological categories, as in the intonational phonology framework. For
example, a standard assumption within intonational phonology is that there is a
phonological category “accent” mediating acoustics and semantic focus, such that a
focused element is accented, and an unfocused element is unaccented (e.g., Brown,
1983). Our production and perception results are compatible with this assumption. First,
if speakers are signaling focus location by means of placing acoustic features

Acoustic correlates of information structure 64
corresponding to a +accent category on focused elements, then we would expect to see
strong acoustic differences between focused and given elements, as we have observed.
Moreover, if listeners perceive accents categorically, then we would predict successful
discrimination of productions on the basis of focus location, as we have observed.
Second, when the object is focused, it will be accented, resulting in higher acoustic
measures on the object compared to other positions, as we have observed. Furthermore,
in the wide focus condition, the subject, verb, and object – all of which are focused –
would all receive accents, and would therefore be more acoustically similar to one
another than they are in the wide focus condition. This difference in accent placement
would lead to successful discrimination between wide and narrow focus by listeners, as
we have observed. Finally, there has been much debate in the intonational phonology
literature about whether there is a phonological category +/- contrastive. The results of
our experiments are perhaps best explained without such a category. In particular, if
speakers accent focused elements without differentiating between contrastive and noncontrastive focus, then we would expect similar acoustic results between productions
which differ only on focus type, which would lead to poor discrimination by the model.
Moreover, listeners would not be successful in discriminating focus type, as we have
observed. Our experimental results are thus compatible with an intonational
phonological approach which includes an accent category mediating acoustics and
meaning, but no category for contrastiveness. Importantly, although our results do not
support a categorical difference between non-contrastive and contrastive focus, they do
not exclude the possibility that speakers can mark these distinctions with relative
differences in prominence (Calhoun, 2006).

Acoustic correlates of information structure 65
Implications for semantic theories of information structure
The current results are relevant to two open questions in the semantics of
information structure: (1) whether contrastive and non-contrastive focus constitute two
distinct categories; and (2) whether focus on the object of a verb can project to the entire
verb phrase.
As described in the introduction, Rooth (1992) proposed an account of focus which
makes no distinction between non-contrastive focus and contrastive focus. (6) shows the
F-marking (focus-marking) that Rooth’s account would assign to the conditions in
Experiments 1 and 2. Importantly, words and phrases which evoke alternatives, either
explicit or implicit, are considered focused (i.e. F-marked).
(6)
a. Subject, Subject Contrast: DamonF fried an omelet last night.
b. Verb, Verb Contrast:

Damon friedF an omelet last night.

c. Object, Object Contrast: Damon fried an omeletF last night.
d. Wide:

[Damon fried an omelet] F last night.

Our results provide tentative support for Rooth’s proposal that F-marked constituents do
not differ substantively as a function of whether the alternatives they evoke are explicit
(our contrastive condition) or implicit (our non-contrastive condition). Although
speakers differentiated these two conditions acoustically, they only did so when the
contrast between the conditions was made salient (Experiments 2 and 3). Moreover, even
when speakers did mark this distinction, listeners were unable to consistently use this
information to recover the intended meaning (Experiment 2). These results suggest that
there are no consistent semantic differences between foci with explicit alternatives in the
discourse and those with implicit alternatives.

Acoustic correlates of information structure 66
The second semantic issue that these results bear upon is whether narrow focus on
the object can project to the entire verb phrase. According to the theory of focus
projection proposed in Selkirk (1984, 1995), an acoustic prominence on the direct object
(omelet) can project focus to the entire verb phrase (fried an omelet) and then up to the
entire clause/sentence. Gussenhoven (1983, 1999) makes a similar claim. Both Selkirk’s
and Gussenhoven’s accounts therefore predict that a verb phrase with a prominence on
the object would be ambiguous between a narrow object focus interpretation and a wide
focus interpretation. Neither the production nor the perception results were consistent
with this prediction. In production, speakers distinguished between narrow object focus
and wide focus, and in perception, listeners were able to distinguish these two conditions.
One aspect of the production results (the acoustic realization of the subject) for the
narrow object focus and wide focus conditions is, however, predicted by both Selkirk and
Gussenhoven’s accounts. In particular, in the wide focus condition, the subject
constitutes new information while in the narrow object focus condition the subject is
given. Selkirk & Gussenhoven both predict that the subject would be more acoustically
prominent in the wide focus condition than in the narrow object focus condition. This is
exactly what we observed (especially in Experiments 1 and 3). Nevertheless, as
discussed above, speakers also systematically disambiguated wide focus from narrow
object focus across all three experiments with their realization of the object and the verb.
Specifically, wide focus was produced with stable or increasing duration, intensity, and
F0 across the subject, verb, and object; narrow object focus, on the other hand, was
characterized by shorter duration and lower intensity and F0 on the subject and verb,
followed by a steep increase in each of these values on the object.
Similar to our production findings, Gussenhoven (1983) found that, at least in some
productions, wide focus differed from narrow object focus in that the verb was more

Acoustic correlates of information structure 67
prominent under wide focus. Listeners, however, were unable to use this acoustic
information to distinguish wide focus from narrow object focus. Gussenhoven took this
result as evidence that the two conditions are not reliably distinguished (consistent with
his theory). Our results did not replicate this production/perception asymmetry: Listeners
are able to successfully classify productions with a single prominence on omelet as
indicating narrow object focus and did not confuse these productions with those from the
wide focus condition.
Methodological contributions
A further contribution of the current research to investigations of prosody and
information structure is methodological. With regard to the methods used to elicit
productions, we utilized multiple, untrained speakers to ensure that our results are
generalizeable to all speakers and are not due to speakers’ prior beliefs about what pattern
of acoustic prominence signals a particular meaning (see Gibson & Fedorenko, in press,
for similar arguments with respect to linguistic judgments). Furthermore, unlike most
previous work in which productions were selected for analysis based on perceptual
differentiability or on ratings of the appropriateness of prosodic contours, we elicited and
selected for analysis productions using a meaning task. Thus our analyses were based on
the communicative function of language. Finally, we did not exclude speakers based on
our perceptions of their productions; speakers were excluded for failure to provide
information to their listeners.
The analyses used here also constitute an improvement over previous analyses.
First, using discriminant modeling, we were able to simultaneously investigate the
contribution of multiple sentence elements to acoustic differentiation of conditions.
Second, we demonstrated that residualization is a useful method for controlling for
variability among speakers and lexical items. For example, preliminary analyses

Acoustic correlates of information structure 68
performed on the productions from Experiment 2 without first computing residual values
of the acoustic features revealed a 13% average increase in values of Wilks’ lambda
(where lower values indicate better discrimination) and a 7% average decrease in
classification accuracy. Third, the discriminant modeling proved successful in
objectively determining which acoustic features were the biggest contributors to
differences among conditions. The success of the analyses used in the current studies is
encouraging for future investigations of prosodic phenomena previously considered too
variable for study in a laboratory setting with naïve speakers.
One question that arises from the current set of studies is, to what extent the
current results can be generalized to all speakers and all sentences. In production studies,
there is always a trade-off between (1) having enough control over what participants are
producing to ensure sufficient data for analysis, and (2) ensuring that the speech is as
natural as possible. In Experiment 1, we attempted to elicit natural productions, but
failed to find systematic differences between focus types. In making the speakers’ task—
to help their listeners choose the correct question-type—explicit, we may have also
encouraged speakers to produce these sentences with somewhat exaggerated prosody.
Further experiments will be necessary to determine whether speakers normally produce
contrastive meanings in this way.
In conclusion, the current studies used rigorous scientific methods to explore
several important questions about the acoustic correlates of information structure. By
providing some initial answers to these questions, along with some implications for
semantic theory, and by offering a novel, objective way to approach these and other
questions, these studies open the door to future investigations of the relationship between
acoustics and meaning.

Acoustic correlates of information structure 69

References
Albritton, D., McKoon, G., & Ratcliff, R. (1996) Reliability of Prosodic Cues for
Resolving Syntactic Ambiguity. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22, 714-735.
Bartels, C., Kingston, J., (1994). Salient pitch cues in the perception of contrastive focus.
In Boach, P., Van der Sandt, R. (Eds.), Focus & Natural Language Processing, Proc. of J.
Sem. conference on Focus. IBM Working Papers. TR-80, pp. 94–106.
Baumann, S., Grice, M., and Steindamm, S. (2006). Prosodic Marking of Focus Domains
- Categorical or Gradient? In Proceedings of Speech Prosody, Dresden, Germany, pp.
301-304.
Beckman, Mary E. (1986). Stress and Non-Stress Accent. Netherlands Phonetic Archives
Series No. 7. Foris.
Beckman, M., & Ayers Elam, G. (1997). Guidelines for ToBI labeling, version 3: Ohio
State University.
Beckman, M., Hirschberg, J., & Shattuck-Hufnagel, S. (2005). The original ToBI system
and the evolution of the ToBI framework. In S.-A. Jun (Ed.), Prosodic Typology: The
Phonology of Intonation and Phrasing (pp. 9-54): Oxford University Press.
Birch, S. and Clifton, C. (1995) Focus, accent, and argument structure: effects on
language comprehension. Language and Speech, 38 (4), 365-391.
Birner, B. (1994). Information status and Word Order: An Analysis of English Inversion.
Language, 70 (2), 233-259.
Boersma, Paul & Weenink, David (2006). Praat: doing phonetics by computer (Version
4.3.10) [Computer program]. Retrieved June 3, 2005, from http://www.praat.org/
Bolinger, D. (1961). Contrastive accent and contrastive stress. Language, 37, 83-96.
Breen, M., Dilley, L., Gibson, E., Bolivar, M., and Kraemer, J. (2006) Advances in
prosodic annotation: A test of inter-coder reliability for the RaP (Rhythm and Pitch) and
ToBI (Tones and Break Indices) transcription systems. Poster presented at the 19th
CUNY Conference on Human Sentence Processing, New York, NY. March, 2006.
Brown, G. (1983). Prosodic structures and the Given/New distinction. In D. R. Ladd &
A. Cutler (Eds.), Prosody:Models and measurements (pp. 67–77). Berlin: Springer.
Calhoun, S (2004). Phonetic Dimensions of Intonational Categories - the case of L+H*
and H*. In Proceedings of Speech Prosody, Nara, Japan, pp. 103-106.
Calhoun, S. (2005). It's the difference that matters: An argument for contextuallygrounded acoustic intonational phonology. In Linguistics Society of America Annual
Meeting, Oakland, California, January 2005.
Calhoun, S. (2006) Information Structure and the Prosodic Structure of English: a
Probabilistic Relationship. PhD thesis, University of Edinburgh.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics and points of
view. In Charles N. Li, editor, Subject and Topic, pages 27-- 55. Academic Press, 1976.
Clark, E. V., & Clark, H. H. (1978). Universals, relativity, and language processing. In: J.
H. Greenberg (Ed.), Universals of human language, Vol. I. (pp. 225–277). Stanford:
Stanford University Press.

Acoustic correlates of information structure 70
Cooper, W., Eady, S. & Mueller, P. (1985). Acoustical aspects of contrastive stress in
question-answer contexts. Journal of Acoustical Society of America, 77(6), 2142-2156.
Couper-Kuhlen, E. (1984). A new look at contrastive intonation., Modes of
Interpretation: Essays Presented to Ernst Leisi, Watts, R., Weidman, U. (Eds.) Gunter
Narr Verlag, 137–158.
Cutler, A. (1977). The Context-Independence of "Intonational Meaning". Chicago
Linguistic Society (CLS 13), 104-115.
Dilley, L. C. (2005). The phonetics and phonology of tonal systems. Unpublished Ph.D.
Dissertation, MIT.
Dilley, L. C., & Brown, M. (2005). The RaP (Rhythm and Pitch) Labeling System,
Version 1.0: Available at http://tedlab.mit.edu/rap.html.
Eady, S. J., & Cooper, W. E. (1986). Speech intonation and focus location in matched
statements and questions. Journal of the Acoustical Society of America, 80, 402-415.
Féry, C. and Krifka, M. (2008). Information Structure: Notional Distinctions, Ways of
Expression. In Piet van Sterkenburg (ed.), Unity and diversity of languages, Amsterdam:
John Benjamins, 123-136.
Fry, D. B. (1955). Duration and Intensity as Physical Correlates of Linguistic Stress.
Journal of the Acoustical Society of America, 27, 765–768.
Gibson, E. & Fedorenko, E. (In press). Weak quantitative standards in linguistics
research. Trends in Cognitive Sciences.
Gussenhoven, C. (1983). Testing the reality of focus domains. Language and Speech, 26,
61–80.
Gussenhoven, C. (1999). On the limits of focus projection in English. In P. Bosch & R.
van der Sandt (Eds.), Focus: Linguistic, cognitive, and computational perspectives (pp.
43 –55). Cambridge, U.K.: Cambridge University Press.
Gussenhoven, C., Repp, B. H., Rietveld, A., Rump, W. H. & J. Terken, J. (1997). The
perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society
of America, 102, 3009-3022.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hawkins, S. & Warren, P. (1991). Factors affecting the given-new distinction in speech.
In Proceedings of the 12th International Congress of Phonetic Sciences, Aix en
Provence. 66-69.
Ito, K & Speer, S. (2008). Anticipatory effects of intonation: Eye movements
during instructed visual search. Journal of Memory and Language, 58, 541-573.
Ito, K. Speer, S. R. and Beckman, M. E. (2004). Informational status and pitch accent
distribution in spontaneous dialogues in English, In Proceedings of the International
Conference on Spoken Language Processing, Nara: Japan, 279-282.
Jackendoff, R. (1972). Semantic interpretation in generative grammar. Cambridge: MIT
Press.
Jaeger, T. F. (2008). Categorical Data Analysis: Away from ANOVAs (transformation or
not) and towards Logit Mixed Models. Journal of Memory and Language. 59, 434–446.

Acoustic correlates of information structure 71
Kochanski, G., Grabe, E., Coleman, J., & Rosner, B. (2005) Loudness predicts
prominence: fundamental frequency lends little. The Journal of the Acoustical Society of
America, 118 (2), 1038-1054.
Krahmer, E., & Swerts, M. (2001). On the alleged existence of contrastive accents.
Speech Communication, 34, 391-405.
Kraljic, T. & Brennan, S. E. (2005). Prosodic disambiguation of syntactic structure: For
the speaker or for the addressee? Cognitive Psychology 50: 194-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge Studies in Linguistics 79.
Cambridge: Cambridge University Press.
Ladd, D. R. & Morton, R. (1997). The perception of intonational emphasis: continuous or
categorical? Journal of Phonetics, 25, 313–342.
Lambrecht, K. (2001). A framework for the analysis of cleft constructions. Linguistics,
39, 463–516.
Lieberman, P. (1960). Some acoustic correlates of word stress in American English. The
Journal of the Acoustical Society of America, 32(4), 451-454.
Molnar, V. (2002). Information Structure in a Cross-linguistic Perspective. In Hilde
Hasselgård, Stig Johansson, Bergljot Behrens, Cathrine Fabricius-Hansen (Eds.),
Language and Computers, Vol. 39, 147-161(15).
Paul, H. (1880), Prinzipien der Sprachgeschichte, Leipzig.
Pierrehumbert, J.B. (1980). The phonology and phonetics of English intonation.
Unpublished dissertation, MIT.
Pierrehumbert, J. & Hirschberg, J. (1990). The Meaning of Intonational Contours in the
Interpretation of Discourse. In P. R. Cohen & J. Morgan & M. E. Pollack (eds.).
Intentions in Communication. Cambridge/MA: MIT Press, 271-311.
Pierrehumbert, J. & Steele, S. (1989). Categories of tonal alignment in English.
Phonetica, 46, 181-196.
Pitrelli, J., Beckman, M. & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International
Conference on Spoken Language Processing, 123-126.
Rietveld, A. C. M., and Gussenhoven, C. (1985). On the relation between pitch excursion
size and prominence. Journal of Phonetics, 13, 299-308.
Rochemont, M. S. (1986). Focus in Generative Grammar. Amsterdam/Philadelphia: John
Benjamins.
Rooth, M. (1985). Association with Focus. PhD thesis, University of Massachusetts
Amherst.
Rooth, M. (1992). A theory of focus interpretation. Natural Language Semantics, 1, 75 –
116.
Rump, H. H., and Collier, R. (1996). ‘Focus conditions and the prominence of pitchaccented syllables. Language and Speech, 39, 1–17.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational disambiguation
in sentence production and comprehension. Journal of Psycholinguistic Research, 29,
169-182.

Acoustic correlates of information structure 72
Selkirk, E. (1984). Phonology and syntax: The relation between sound and structure.
Cambridge, MA: MIT.
Selkirk, E. (1995). Sentence Prosody: Intonation, Stress, and Phrasing. In: J.Goldsmith
(ed.). The Handbook of Phonological Theory. Oxford: Blackwell, 550-569.
Schwarzchild, R. (1999) GIVENness, AvoidF and other Constraints on the Placement of
Accent. Natural Language Semantics, 7, 141–177.
Shriberg, E., Stolcke, A., Hakkani-Tur, D. & Tur, G. (2000). Prosody-Based Automatic
Segmentation of Speech into Sentences and Topics. Speech Communication, 32, 127-154.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D., Ries, K., Coccaro, N.,
Martin, R., Meteer, M., & Van Ess-Dykema, C. (1998). Can Prosody Aid the Automatic
Classification of Dialog Acts in Conversational Speech? Language and Speech, 41:3-4,
439-487.
Silverman, K. E. A., Beckman, M., Pierrehumbert, J., Ostendorf, M., Wightman, C. W.
S., Price, P., et al. (1992). ToBI: A standard scheme for labeling prosody. In Proceedings
of the 2nd International Conference on Spoken Language Processing (pp. 867-879).
Banff.
Sluijter, A. and van Heuven, V. (1996). Spectral balance as an acoustic correlate of
linguistic stress. Journal of the Acoustical Society of America, 100, 2471–2485.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects of
speaker awareness and referential contest. Journal of Memory and Language, 48, 103–
130.
Stalnaker, R. (2002). Common ground. Linguistics and Philosophy, 25: 701–721.
Syrdal, A. and McGory, J. (2000). Inter-transcriber reliability of ToBI prosodic labeling.
In Proceedings of the International Conference on Spoken Language Processing,
Beijing: China, 235-238.
Terken, J. (1991). Fundamental frequency and perceived prominence accented syllables.
Journal of the Acoustical Society of America, 89, 1768–1776.
't Hart, J. Collier, R. & Cohen, A. (1990). A perceptual study of intonation. Cambridge
University Press, Cambridge.
Turk, A. & Sawusch, J. (1996) The processing of duration and intensity cues to
prominence. Journal of the Acoustical Society of America, 99, 3782-3790.
Welby, P. (2003). Effects of pitch accent position, type, and status on focus projection.
Language and Speech, 46, 53 – 81.
Wightman, C. W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P. J. (1992). Segmental
durations in the vicinity of prosodic phrase boundaries. Journal of the Acoustical Society
of America, 91(3), 1707-1717.
Xu, Y. & Xu, C. X. (2005). Phonetic realization of focus in English declarative
intonation, Journal of Phonetics, 33, 159–197.
Yoon, T., Chavarria, S., Cole, J., & Hasegawa-Johnson, M. (2004). Intertranscriber
reliability of prosodic labeling on telephone conversation using ToBI. In Proceedings of
the International Conference on Spoken Language Processing., Nara: Japan, 2729-2732.

Acoustic correlates of information structure 73

Appendix A
Experiment 1 items
Full items are recoverable as follows: Question A is always “What happened last night?”
Questions B, C, & D are wh-questions about the subject, verb, and object, respectively.
Questions E, F, & G are questions which introduce the explicit alternative subject, verb,
or object, indicated in parentheses.
1.

Question A: What happened last night?
Question B: Who fed a bunny last night?
Question C: What did Damon do to a bunny last night?
Question D: What did Damon feed last night?
Question E: Did Jenny feed a bunny last night?
Question F: Did Damon pet a bunny last night?
Question G: Did Damon feed a baby last night?
Response: Damon fed a bunny last night.

2. Damon (Lauren) caught (pet) a bunny (a squirrel) last night.
3. Damon (Molly) burned (break) a candle (a log) last night.
4. Darren (Lauren) cleaned (eat) a carrot (a chicken) last night.
5. Darren (Molly) peeled (eat) a carrot (a potato) last night.
6. Darren (Nora) found (buy) a diamond (a ring) last night.
7. Darren (Jenny) sold (lose) a diamond (a sapphire) last night.
8. Jenny (Damon) found (lose) a dollar (a quarter) last night.
9. Jenny (Darren) sewed (rip) a dolly (a blanket) last night.
10. Jenny (Logan) read (open) an email (a letter) last night.
11. Jenny (Nolan) smelled (plant) a flower (a skunk) last night.
12. Lauren (Darren) burned (write) a letter (a magazine) last night.
13. Lauren (Logan) mailed (open) a letter (a package) last night.
14. Lauren (Nolan) read (write) a novel (a newspaper) last night.
15. Lauren (Damon) fried (bake) an omelet (a chicken) last night.
16. Logan (Molly) peeled (chop) an onion (an apple) last night.
17. Logan (Nora) fried (chop) an onion (a potato) last night.
18. Logan (Jenny) cleaned (buy) a pillow (a rug) last night.
19. Molly (Logan) dried (wash) a platter (a bowl) last night.
20. Molly (Nolan) sold (find) a platter (a vase) last night.
21. Molly (Damon) poured (drink) a smoothie (a cocktail) last night.
22. Nolan (Nora) pulled (push) a stroller (a sled) last night.
23. Nolan (Jenny) bought (sell) a stroller (a wheelbarrow) last night.
24. Nolan (Lauren) sewed (knit) a sweater (a quilt) last night.
25. Nora (Nolan) killed (trap) a termite (a cockroach) last night.
26. Nora (Damon) changed (wash) a toddler (a baby) last night.
27. Nora (Darren) fed (dress) a toddler (a bunny) last night.
28. Nora (Logan) pulled (push) a wagon (a wheelbarrow) last night.

Acoustic correlates of information structure 74

Appendix B
Items used for Experiments 2-3
Full items are recoverable as follows: Question A always asks “What happened _____?”
where the blank corresponds to the temporal adverb. Questions B, C, & D are whquestions about the subject, verb, and object, respectively. Questions E, F, & G are
questions which introduce the explicit alternative subject, verb, or object, indicated in
parentheses.
1a.
1b.
1c.
1d.
1e.
1f.
1g.

Context: What happened yesterday?
Context: Who fried an omelet yesterday?
Context: What did Damon do to an omelet yesterday?
Context: What did Damon fry yesterday?
Context: Did Harry fry an omelet yesterday?
Context: Did Damon bake an omelet yesterday?
Context: Did Damon fry a chicken yesterday?
Target: No, Damon fried an omelet yesterday.

2. (I heard that) (No,) Megan (Jodi) sold (lose) her diamond (her sapphire) yesterday.
3. (I heard that) (No,) Mother (Daddy) dried (wash) a platter (a bowl) last night.
4. (I heard that) (No,) Norman (Kelly) read (write) an email (a letter) last night.
5. (I heard that) (No,) Lauren (Judy) poured (drink) a smoothie (a cocktail) this morning.
6. (I heard that) (No,) Nora (Jenny) sewed (rip) her dolly (her blanket) this morning.
7. (I heard that) (No,) Molly (Sarah) trimmed (wax) her eyebrows (her hair) on Tuesday.
8. (I heard that) (No,) Nolan (Steven) burned (break) a candle (a log) on Tuesday.
9. (I heard that) (No,) Logan (Billy) killed (trap) a termite (a cockroach) last week.
10. (I heard that) (No,) Radar (Fido) caught (lick) a bunny (a squirrel) last week.
11. (I heard that) (No,) Darren (Maggie) pulled (push) a stroller (a sled) on Sunday.
12. (I heard that) (No,) Brandon (Tommy) peeled (eat) a carrot (a potato) on Sunday.
13. (I heard that) (No,) Maren (Debbie) cleaned (buy) a pillow (a rug) on Friday.
14. (I heard that) (No,) Lindon (Kelly) fooled (fight) a bully (a teacher) on Friday.

Acoustic correlates of information structure 1

Acoustic correlates of information structure
Mara Breen1, Evelina Fedorenko2, Michael Wagner3, Edward Gibson2
1
2

University of Massachusetts Amherst
Massachusetts Institute of Technology
3
McGill University

June 7, 2010
Address correspondence to:
Mara Breen
522 Tobin Hall
University of Massachusetts
Amherst, MA
01003
mbreen@psych.umass.edu

Acoustic correlates of information structure 2
Abstract
This paper reports three studies aimed at addressing three questions about the acoustic
correlates of information structure in English: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? The information structure of subject-verb-object (SVO)
sentences was manipulated via the questions preceding those sentences: elements in the
target sentences were either focused (i.e. the answer to a wh-question) or given (i.e.
mentioned in prior discourse); furthermore, focused elements had either an implicit or an
explicit contrast set in the discourse; finally, either only the object was focused (narrow
object focus) or the entire event was focused (wide focus). The results across all three
experiments demonstrated that people reliably mark (a) focus location (subject, verb, or
object) using greater intensity, longer duration, and higher mean and maximum F0, and
(b) focus breadth, such that narrow object focus is marked with greater intensity, longer
duration, and higher mean and maximum F0 on the object than wide focus. Furthermore,
when participants are made aware of prosodic ambiguity present across different
information structures, they reliably mark focus type, so that contrastively-focused
elements are produced with higher intensity, longer duration, and lower mean and
maximum F0 than non-contrastively focused elements. In addition to having important
theoretical consequences for accounts of semantics and prosody, these experiments
demonstrate that linear residualization successfully removes individual differences in
people’s productions thereby revealing cross-speaker generalizations. Furthermore,
discriminant modeling allows us to objectively determine the acoustic features that
underlie meaning differences.

Acoustic correlates of information structure 3

Introduction
An important component of the meaning of a sentence is its relationship to the context in
which it is produced. Some parts of speakers’ sentences refer to information already
under discussion, while other parts convey information that the speaker is presenting as
new for the listener. Depending on the context, the same sentence can convey different
kinds of information to the listener. For example, consider the three contexts in (1a)-(1c)
for the sentence in (2):

(1) a. Who fried an omelet?
b. What did Damon do to an omelet?
c. What did Damon fry?

(2) Damon fried an omelet.

The event of frying an omelet is already made salient in the context in (1a), and
this part of the answer is therefore given. Consequently, the sentence Damon fried an
omelet conveys Damon as the new or focused information.1 Similarly, the verb fried is
the focused information relative to the context in (1b), and the object noun phrase an
omelet is the focused information relative to the context in (1c). This component of the
meaning of sentences - the differential contributions of different sentence elements to the

1

Numerous terms are used in the literature to refer to the distinction between the information that is old for
the listener and the information that the speaker is adding to the discourse: background and foreground;
given and new; topic and comment; theme and rheme, etc. In this paper, we will use the term given to refer
to the parts of the utterance which are old to the discourse, and focused to refer to the part of the utterance
which is new to the discourse.

Acoustic correlates of information structure 4
overall sentence meaning in its relation to the preceding discourse - is called information
structure.
Three components of information structure have been proposed in the literature:
givenness, focus, and topic (see e.g., Féry and Krifka, 2008, for a recent summary). The
current paper will be concerned with givenness and focus.2 Given material is material
that has been made salient in the discourse, either explicitly, like the event corresponding
to the verb fried and the object corresponding to the noun omelet in (1a), or implicitly, via
inferences based on world knowledge (e.g., mentioning omelet makes the notion of
“eggs” given, Schwarzchild, 1999).
Focused material is what is new to the discourse, or in the foreground. The focus
of a sentence can often be understood as the part that corresponds to the answer to the
wh-part of wh-questions, like Damon in (2) as an answer to (1a) (Paul, 1880; Jackendoff,
1972).
There are two dimensions along which focused elements can differ. The first is
contrastiveness. A contrastively focused element, like Damon in (3b), indicates that the
element in question is one of a set of explicit alternatives or serves to correct a specific
item already present in the discourse, as in the following:

(3) a. Did Harry fry an omelet yesterday?
b. Damon fried an omelet yesterday.

Unlike (1a), where there is no explicit set of individuals from which Damon is being
selected as the “omelet fryer”, in (3a) an explicit alternative “omelet fryer” is being

2

Topic, the third component of information structure, describes which discourse referent focused
information should be associated with, as in the mention of Damon in “As for Damon, he fried an omelet.”
The current studies do not address the prosodic realization of topic.

Acoustic correlates of information structure 5
introduced: Harry. The sentence (3b) in this context thus presents information (i.e.,
Damon) which explicitly contrasts with, or contradicts, some information which has been
introduced into the discourse.
There is no consensus in the literature regarding the relationship between noncontrastive focus and contrastive focus. Some researchers have treated non-contrastive
focus and contrastive focus as separate categories of information structure (Chafe, 1976;
Halliday, 1967; Rochemont, 1986; Molnar, 2002), whereas others have argued that there
is no principled difference between the two (e.g., Bolinger 1961, Rooth, 1985, Rooth,
1992). According to Rooth (1992), for example, each expression evokes two semantic
representations: the expression’s actual meaning, and a set of alternatives. If a
constituent in the expression is focused, then the alternative set contains the expression
itself and all expressions with an alternative substituted for the focus-marked constituent;
if there is no focus within the expression, the alternative set consists only of the
expression itself. Rooth would therefore argue that Damon in (1a) is focused and
introduces alternative propositions that differ only in the agent of the event ({Damon
fried an omelet, Harry fried an omelet, Ada fried an omelet, ...}), even if no alternatives
are explicitly mentioned. In (3a), Damon also evokes alternative omelet fryers, and
therefore has the same focus structure as (1a), but the context makes a specific alternative
(Harry) more salient than other potential alternatives. Importantly, from Rooth’s
standpoint, it does not matter whether the alternatives are explicit in the discourse or not:
the meaning of the expression is the same.
The second dimension along which focused elements can vary is focus breadth
(Selkirk, 1984; 1995; Gussenhoven, 1983; 1999), which refers to the size of the set of
focused elements. Narrow focus refers to cases where only a single aspect of an event
(e.g., the agent, the action, the patient, etc.) is focused, whereas wide focus focuses an

Acoustic correlates of information structure 6
entire event. Take, for example, the difference between (5) as an answer to (4a) versus as
an answer to (4b):

(4) a. What did Damon fry last night?
b. What happened last night?

(5) Damon fried an omelet last night.

(4a) narrowly focuses the patient of frying, omelet in (5), while (4b) widely focuses the
entire event of Damon frying an omelet.
The information status of a sentence element can be conveyed in at least three
ways: (1) using word order (i.e., given information generally precedes focused
information) (e.g., Birner, 1994, Clark & Clark, 1978); (2) using particular lexical items
and syntactic constructions (e.g., using cleft constructions such as “It was Damon who
fried an omelet”) (Lambrecht, 2001); and (3) using prosody. Prosody – which we focus
on in the current paper – refers to the way in which words are grouped in speech, the
relative acoustic prominence of words, and the overall tune of an utterance. Prosody is
comprised of acoustic features like fundamental frequency (F0), duration, and loudness,
the combinations of which give rise to the psychological percepts like phrasing
(grouping), stress (prominence), and tonal movement (intonation).
The goal of the current paper is to investigate the prosodic realization of
information structure in simple English subject-verb-object (SVO) sentences like (2),
with the goal of addressing the following questions:
1) First, do speakers prosodically distinguish focused and unfocused elements?
This question can be broken down into further questions:

Acoustic correlates of information structure 7
(1a) Do speakers distinguish focused elements that have an explicit contrast
set in the discourse from those that do not?
(1b) Do speakers distinguish sentences in which only the object is focused
from those in which the entire event is focused?
(2) What are the acoustic features associated with these different aspects of
information structure?
(3) How well can listeners retrieve this information from the signal?

Although the current experiments are all performed on English, the answers to
these questions will likely be similar for other West Germanic languages. However, the
relationship between prosodic features and information structure across different
languages and language groups remains an open question.
In the remainder of the introduction, we briefly lay out two approaches to the study
of the relationship between prosody and information structure, and summarize empirical
studies which have explored how information structure is realized acoustically and
prosodically. We then discuss methodological issues present in previous studies which
call into question the generalizeability of the reported findings, and outline how the
current methods were designed to better address these questions.
Empirical investigations of prosody and information structure
Two perspectives on the relationship between the acoustics of the speech signal and
the meaning associated with various aspects of information structure have been
articulated in the literature. According to the direct-relationship approach, sets of
acoustic features are directly associated with particular meanings (Fry, 1955; Lieberman,
1960; Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986; Pell, 2001; Xu & Xu,
2005). In contrast, according to the indirect-relationship approach (known as the

Acoustic correlates of information structure 8
intonational phonology framework), the relationship between acoustics and meaning is
mediated by phonological categories (Ladd, 1996; Gussenhoven, 1983; Pierrehumbert,
1980; Dilley, 2005; Hawkins & Warren, 1991). In particular, the phonetic prosodic cues
are hypothesized to be grouped into prosodic categories which are, in turn, associated
with particular meanings. The experiments in the current paper were not designed to
decide between these two approaches. However, In the current paper, we will initially
discuss our experiments in terms of the direct-relationship approach, because it is more
parsimonious. In the general discussion, we will show how the results are also
compatible with the indirect-relationship approach.
Turning now to previous empirical work on the relationship between prosody and
information structure, we start with studies of focused vs. given elements. Several
studies have demonstrated that focused elements are more acoustically prominent than
given elements. However, there has been some debate about which acoustic features
underlie a listener’s perception of acoustic prominence. Some features that have been
proposed to be associated with prominence include pitch (i.e. F0) (Lieberman, 1960;
Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986), duration (Fry, 1954; Beckman,
1986), loudness (i.e. intensity) (Kochanski, Grabe, Coleman, & Rosner, 2005; Beckman,
1986; Turk and Sawusch, 1996), and voice quality (Sluijter & van Heuven, 1996).
In early work on lexical stress, Fry (1954) and Liberman (1960) argued that
intensity and duration of the vowel of the stressed syllable contributed most strongly to
the percept of acoustic prominence, such that stressed vowels were produced with a
greater intensity and a longer duration than non-stressed vowels. In experiments on
phrase-level prominence, Cooper et al. (1985) and Eady and Cooper (1986) also noted
that more prominent syllables are longer than their non-prominent counterparts. Cooper
et al. (see also Liberman, 1960); Rietveld & Gussenhoven, 1985; Gussenhoven et al.,

Acoustic correlates of information structure 9
1997; and Terken, 1991) also argued that F0 was a highly important acoustic feature
underlying prominence. Others have argued that the strongest cue to prominence is
intensity (e.g., Beckman, 1986). More recently, Turk and Sawusch (1996) also found
that intensity (and duration) were better predictors of perceived prominence than pitch, in
a perception task. Finally, in a study of spoken corpora, Kochanski et al. (2005)
demonstrated that loudness (i.e. intensity) was a strong predictor of labelers’ annotations
of prominence, while pitch had very little predictive power.
The question of whether contrastively and non-contrastively focused elements are
prosodically differentiated by speakers, and perceptually differentiated by listeners has
also been extensively debated. Some have argued that there is no difference in the
acoustic features associated with contrastively vs. non-contrastively focused elements
(Cutler, 1977; Bolinger, 1961; t’Hart, Collier, & Cohen, 1990), while others have argued
that some acoustic features differ between contrastively vs. non-contrastively focused
elements (Couper-Kuhlen, 1984; Krahmer & Swerts, 2001; Bartels & Kingston, 1994;
Ito, Speer, & Beckman, 2004). For example, Couper-Kuhlen (1984) reported, on the
basis of corpus work, that speakers produce contrastive focus with a steep drop after a
high F0 target, while high F0 is sustained after non-contrastive focus (see also Krahmer
and Swerts, 2001). However, this finding is in contrast to Bartels and Kingston (1994),
who have argued, based on a series of production studies, that the most salient acoustic
cue to contrastiveness is the height of the peak on a contrastive word, such that a higher
peak is associated with a greater probability of an element being interpreted as
contrastive (see also Ladd and Morton, 1997). Finally, Ito, Speer, & Beckman (2004)
demonstrated that speakers are more likely to use a L+H* accent (i.e. a steep rise from a
low target to a high target), compared to a H* accent (i.e. a gradual rise to a high target),
to indicate an element that has an explicit contrast set in the discourse.

Acoustic correlates of information structure 10
Krahmer and Swerts (2001) observed that listeners were more likely to perceive a
contrastive adjective (e.g., red in red square preceded by blue square) as more prominent
than a new adjective when the adjective was presented with a noun compared to when it
was presented in isolation. They therefore hypothesized that the lack of a consensus in
the literature may be due to the failure of the earlier studies to investigate focused
elements in relation to the prosody of the surrounding elements. Consistent with this
idea, Calhoun (2005) demonstrated that a model’s ability to predict a word’s information
status is significantly improved when information about the acoustics of adjacent words
is included in the model. These results suggest that a more consistent picture of the
acoustic features associated with contrastively and non-contrastively-focused elements
may emerge if acoustic context is taken into account.
Finally, prior work has investigated whether speakers prosodically differentiate
narrow and wide focus. Selkirk (1995), for example, argued that, through a process
called focus projection, an acoustic prominence on the head of a phrase or its internal
argument can project to the entire phrase, thus making the entire phrase focused (see also
Selkirk, 1984; see Gussenhoven, 1983, 1999, for a similar claim). According to Selkirk
(1984) and Gussenhoven (1983) then a clause containing a transitive verb in which the
direct object is acoustically prominent is ambiguous between a reading where the object
alone is focused and a reading where the entire verb phrase is focused. This hypothesis
has been supported in several perception experiments (Welby, 2003; Birch & Clifton,
1995; Gussenhoven, 1983). Welby (2003), for example, demonstrated that listeners rated
a sentence like I read the DISPATCH with a single acoustic prominence on dispatch as a
similarly felicitous response to either a question narrowly focusing the object (i.e. “What
newspaper do you read?”), or a question widely focusing the entire event (i.e. “How do
you keep up with the news?”). However, Gussenhoven (1983) found that at least in some

Acoustic correlates of information structure 11
productions there is actually a perceptible difference between narrow and wide focus
although listeners cannot use this information to reliably tell in which context the
sentence was uttered (see Baumann et al., 2006, for evidence from German showing that
speakers do differentiate between narrow and wide focus, with prosodic cues varying
across speakers). In contrast to Gussenhoven’s perception results, Rump and Collier
(1986) found that listeners can accurately discriminate narrow and wide focus using pitch
cues.
Limitations of previous work
Although the studies summarized above provide evidence for some systematic
differences in the acoustic realization of different aspects of information structure, no
clear picture has yet emerged with regard to any of the three meaning distinctions
discussed above (i.e. focused vs. given elements, non-contrastively focused vs.
contrastively focused elements, and narrow vs. wide focus). Furthermore, previous
studies suffer from several methodological limitations that make the findings
inconclusive. Here, we discuss five limitations of previous studies which the current
studies seek to address in an effort to reveal a clearer picture of the relationship between
acoustic features and information structure.
First, instead of acoustic features, sometimes only ToBI3 annotations are
provided (e.g., Birch & Clifton, 1995; Ito et al., 2004). This includes work of researchers
who adopt the intonational phonology framework and who therefore believe that using
prosodic annotation offers a useful way to extrapolate away from potentially complex
interactions among acoustic features which give rise to the perception of specific
intonational patterns. One particular problem concerns H* and L+H* accents. As
defined in the ToBI system, these accents are meant to be explicit markers of non3

The (ToBI) Tones and Break Indices system was developed in the early 90s as the standard system for
annotation of prosodic features (Silverman et al., 1992).

Acoustic correlates of information structure 12
contrastive focus and contrastive focus, respectively (Beckman & Ayers-Elam, 1997).
However, H* and L+H* are often confused in ToBI annotations (Syrdal & McGory,
2000), and are, in fact, often collapsed in calculating inter-coder agreement (Pitrelli et al.,
1994; Yoon et al., 2004; Breen et al., 2006, submitted). Therefore, it is difficult to
interpret the results of studies which are based on the difference between H* and L+H*
without a discussion of the acoustic differences between these purported categories. In
the current studies, we report acoustic features in order to avoid confusion about what the
ToBI labels might mean and in order to not presuppose the existence of prosodic
categories associated with particular meaning categories of information structure.
A second limitation concerns the method used to generate and select productions
for analysis. A common practice involves eliciting productions from a small number of
speakers (e.g., Baumann et al., 2006; Krahmer & Swerts, 2001), which results in a
potential decrease in experimental power, and could therefore lead to a Type II error. In
addition, several previous experiments have excluded speakers’ data from analysis for not
producing accents consistently (e.g., Eady & Cooper, 1986; Cooper et al., 1985), which
could lead to a Type I error. For the current experiments, we recruited between 13 and
18 speakers. In addition, no speakers’ productions were excluded from the analyses
based on a priori predictions about potential behavior (e.g., placing accents in particular
locations).
A third limitation concerns the tasks used in perception studies. In particular,
some studies asked listeners to make judgments about which of two stimuli was more
prominent (Krahmer & Swerts, 2001), what accent is acceptable in a particular context
(Birch & Clifton, 1995; Welby, 2003), or with which of two questions a particular answer
sounded more natural (Gussenhoven, 1983). The problem with these meta-linguistic
judgments is that they lack a measure of the participants’ interpretation of the sentences.

Acoustic correlates of information structure 13
In the current studies we employ a more natural production-comprehension task, in which
speakers are trying to communicate a particular meaning of a semantically ambiguous
sentence and listeners are trying to understand the intended meaning.
A fourth limitation of previous studies is in how they have dealt with speaker
variability. Presenting data from individual subjects separately, as is commonly done, is
problematic because it fails to capture the shared aspects of individual productions (e.g.,
consistent use by most speakers of some set of acoustic features to mark focused
elements). In the current studies, we combine data across subjects while simultaneously
removing variance due to individual differences using linear regression modeling (e.g.,
Jaeger, 2008).
A fifth limitation is that many have reported differences between conditions based
only on individual acoustic features on single words (Eady & Cooper, 1986; Cooper et
al., 1985; Baumann et al, 2006). If acoustic prominence is perceived in a contextdependent manner, these single-feature/single-word analyses might find spurious
differences, or fail to find real differences. In the current studies, we used discriminant
modeling on the productions in order to simultaneously investigate the contribution of
multiple acoustic features from multiple words in an utterance to the interpretation of
information status of different sentence elements.

Experiments: Overview and general methods
The current paper presents results from three experiments. Experiment 1
investigated whether speakers prosodically disambiguate focus location (subject, verb,
object), focus type (contrastive vs. non-contrastive focus), and focus breadth (narrow vs.
wide) by eliciting semi-naturalistic productions like that in (3b) (e.g., Damon fried an
omelet this morning), whose information status was disambiguated by a preceding

Acoustic correlates of information structure 14
question. Experiment 2 investigated whether speakers disambiguate focus location and
focus type when the task explicitly required them to communicate a particular meaning to
their listeners. Finally, Experiment 3 served as a replication and extension of Experiment
2, in which speakers included an attribution expression (“I heard that”) before the critical
sentence.
The acoustic analysis of the productions elicited in all three experiments
proceeded in three steps. First, we automatically extracted a series of 24 acoustic features
(see Table 2) from the subject, verb, and object of the sentences elicited in Experiments
1, 2, and 3. Second, we subjected all of these features to a stepwise discriminant function
analysis in order to determine which features best discriminated the information status
conditions listed in Table 1 for each of the three experiments. This analysis resulted in a
subset of eight acoustic features. Finally, we used discriminant analyses to evaluate
whether this subset of eight features could effectively discriminate sets of 2 and 3
conditions for each of the three experiments. Specifically, we tested focus location by
comparing the features from productions in which Damon, fried, and omelet were
focused, respectively. We tested focus type by comparing the features from sentences in
which the focused element was contrastively or non-contrastively focused at each of the
three syntactic positions. Last, we tested focus breadth by comparing the features for
sentence with wide-focus to those with narrow object focus. In addition to the analysis of
acoustic features, in Experiments 2 and 3 we investigated whether listeners could
correctly determine the intended information status of the speaker.

Acoustic correlates of information structure 15

Experiment 1
Method
Participants
Nine pairs of participants were recorded. All participants were self-reported native
speakers of American English. All participants were MIT students or members of the
surrounding community. Participants were paid for their participation.
Materials
Each trial consisted of a set-up question and a target sentence, which always had an SVO
structure (e.g., Damon fried an omelet this morning). The target sentence could plausibly
answer any one of the seven set-up questions (see Table 1), which served to focus
different elements of the sentence or the entire event described in the sentence. The first
question focused the entire event (i.e. What happened?). In the remaining conditions,
two factors were manipulated: (1) the element in the target sentence that was focused by
the question (subject, verb, object); and (2) the presence of an explicit contrast set for the
focused element (non-contrastively focused, i.e. explicit contrast set absent, contrastively
focused, i.e. explicit contrast set present).
All subject and object noun phrases (NPs) in the target sentences were bi-syllabic
with first syllable stress, and all verbs were monosyllabic. All subject NPs were proper
names, and object NPs were mostly common inanimate objects, such that the events were
non-reversible. Furthermore, all words were comprised mostly of sonorant phonemes.
These constraints ensured that words could be more easily compared across items, and
facilitated the extraction of acoustic features (which is easier for vowels and sonorant
consonants). An adjunct prepositional phrase (PP) was included at the end of each
sentence so that differences in the production of the object NP due to the experimental
manipulations would be dissociable from prosodic effects on phrase-final, or in this case,

Acoustic correlates of information structure 16
sentence-final, words, which are typically lengthened and produced with lower F0
compared to phrase-medial words (e.g., Wightman et al., 1992).
We constructed 28 sets of materials. Participants saw one condition of each item,
following a Latin Square design. A sample item is presented in Table 1. The complete
set of materials can be found in Appendix A.

Condition

Focus Type

Focused
Argument

Setup Question

1

Non-contrastive

wide

What happened this morning?

2

Non-contrastive

S

Who fried an omelet this morning?

3

Non-contrastive

V

What did Damon do to an omelet this morning?

4

Non-contrastive

O

What did Damon fry this morning?

5

Contrastive

S

Did Harry fry an omelet this morning?

6

Contrastive

V

Did Damon bake an omelet this morning?

7

Contrastive

O

Did Damon fry a chicken this morning?

Table 1: Example item from Experiment 1. The target sentence is “Damon fried an
omelet this morning.”
Procedure
Productions were elicited and pre-screened in a two-part procedure. The first part
was a training session, where participants learned the intended names for pictures of
people, actions, and objects. In the second part, the pairs of participants produced
questions and answers for each other. The method was designed to maximize control
over what speakers were saying, but to also encourage natural-sounding productions.
Pilot testing revealed that having subjects simply read the target sentences resulted in
productions with low prosodic variability. After going through the experiment one time,
the participants switched roles.
Training session
In the training session, participants learned mappings between 96 pictures and
names, so that they could produce the names from memory during the second part of the

Acoustic correlates of information structure 17
experiment. In a PowerPoint presentation, each picture, corresponding to a person, an
action, an object, or a modifier, was presented with its intended name (see Figure 1, left).
The pictures consisted of eight names of people, which were repeated 3-4 items each in
the experimental materials, eight colors (which were used in a concurrently run filler
experiment), 34 verbs, 44 objects, and two temporal modifiers (this morning and last
night). The pictures were presented in alphabetical order, to facilitate memorization and
recall. Participants were instructed to learn the mappings by progressing through the
PowerPoint at their own pace.
When participants felt they had learned the mappings, they were given a picturenaming test, which consisted of 27 items from the full list of 96. The test was identical
for all participants. Participants were told of their mistakes, and, if they made four or
more errors, they were instructed to go back through the PowerPoint to improve their
memory of the picture-name mappings. Once participants could successfully name 23 or
more items on the test, which took between 1 and 3 rounds of testing, they continued with
the second part of the experiment. Early in pilot testing, we discovered that subjects had
poor recall for the names of the people in the pictures. Therefore, in the actual
experiment, subjects could refer to a sheet which had labeled pictures of the people.

Acoustic correlates of information structure 18

Figure 1: Left: Examples from the picture-training task for Experiment 1. Each square
represents a screen shot. Right: Examples of the procedure for the questioner (upper
squares) and answerer (lower squares) for Experiment 1. Two conditions are presented:
Non-contrastive, object (left) and contrastive, verb (right). The top squares represent
screen shots of what the questioner saw on a trial; the bottom squares represent what the
answerer saw on a trial.
Question-Answer Experiment
The experiment was conducted using Linger 2.92 (available at
http://telab.mit.edu/~dr/Linger/), a software platform designed by Doug Rohde for
language processing experiments. Participants were randomly paired and randomly
assigned to the role of questioner or answerer. Participants sat at computers in the same
room such that neither could see the other’s screen. On each trial, as illustrated in Figure
1 (right), the questioner saw a question (e.g., “What did Damon fry this morning?”)
which he/she was instructed to produce aloud for the answerer. The answerer was
instructed to produce an answer aloud using the information contained in the picture on
his/her screen (e.g., “Damon fried an omelet this morning”). The answerer was

Acoustic correlates of information structure 19
instructed to produce complete sentences, including the subject, verb, object, and
temporal abverb,4 and to emphasize the part of the sentence that the questioner had asked
about, or that he/she was correcting. On a random 20% of trials, the answerer was asked
a comprehension question about the answer s/he produced.
Productions were recorded in a quiet room with a head-mounted microphone at a
rate of 44kHz.
Acoustic Feature

Units

Description

duration

ms

Word duration excluding any silence before or after the word.

silence

ms

Duration of silence following the word, not due to stop closure.

duration+silence

ms

The sum of the duration of the word and any following silence.

mean F0

Hz

Mean F0 of the entire word

maximum F0

Hz

Maximum F0 value across the entire word

F0 peak location

0-1

The proportion of the way through the word where the maximum F0 occurs.

minimum F0

Hz

Minimum F0 across the entire word

F0 valley location

0-1

The proportion of the way through the word where the minimum F0 occurs.

initial F0

Hz

early F0

Hz

Mean F0 of the initial 5% of the word
Mean F0 value of 5% of the word centered at the point 25% of the way
through the word

center F0

Hz

late F0

Hz

Mean F0 value of 5% of the word centered on the midpoint of the word
Mean F0 value of 5% of the word centered on a point 75% of the way
through the word

final F0

Hz

Mean F0 of the last 5% of the word

1st quarter F0

The difference between initial F0 and early F0.

Hz

The difference between early F0 and center F0.

3rd quarter F0

Hz

The difference between center F0 and late F0.

4th quarter F0

Hz

The difference between late F0 and final F0.

mean intensity

dB

Mean intensity of the word

maximum intensity

dB

Maximum dB level in the word

minimum intensity
intensity peak
location
intensity valley
location

dB
0-1

Minimum dB level in the word
The proportion of the way through the word where the maximum intensity
occurs
The proportion of the way through the word where the minimum intensity
occurs

maximum amplitude

4

Hz

2nd quarter F0

Pascal

0-1

Maximum amplitude across the word

In the absence of explicit instruction to produce complete sentences, with a lexicalized subject, verb, and
object, speakers would likely resort to pronouns or would omit given elements altogether (e.g., “What did
Damon fry this morning?” “An omelet.”). A complete production account of information structure
meaning distinctions should include not just the prosodic cues used by the speakers, but also syntactic and
lexical production choices, as well as the interaction among these different production strategies. However,
because we focus on prosody in the current investigation, we wanted to be able to compare acoustic
features across identical words. Thus, we required that participants always produce a subject, verb, object
and adverb on every trial.

Acoustic correlates of information structure 20
energy

(Pascal)2 x
Duration

Table 2: Acoustic features extracted from each word in the target sentence for
Experiments 1-3. Stepwise discriminant analyses demonstrated that the measures in bold
provided the best discrimination among conditions and were used in all reported
analyses.
Results
Of the 504 speaker productions from the Question-Answer Experiment, 87 (17%) were
discarded because (a) the answerer failed to use the correct lexical items, (b) the answerer
was disfluent, or (c) the production was poorly recorded. The 417 remaining productions
were subjected to the acoustic analyses described below.
Acoustic Features
Based on previous investigations of prosody and information structure (Fry, 1955;
Lieberman, 1960; Eady et al., 1985; Cooper & Eady, 1986, Bartels & Kingston, 1994;
Krahmer & Swerts, 2001; Baumann et al., 2006), we chose a set of acoustic features to
analyze (see Table 2). These features were obtained automatically using the Praat
program (Boersma & Weenink, 2006). The measures of F0 computed over portions of
the words (e.g., 1st quarter F0) were chosen in order to investigate how F0 changes across
the syllable might contribute to the differentiation of conditions.
Our first goal was to determine which of the 24 candidate acoustic features
mediated differences among conditions. We conducted a series of stepwise linear
discriminant analyses5 on all of the data collected in Experiments 1, 2 and 3 reported in
the current paper. In order to determine the features to be used in the analyses of all three
experiments, we performed a separate stepwise analysis on the data from each
experiment separately. For each analysis we entered all 24 acoustic features across each
5

Linear discriminant analysis (LDA) calculates a function, computed as a linear combination of all
predictors entered, which results in the best separation of two or more groups. For two groups, only one
function is computed. For three groups, the first function provides the best separation of group 1 from
groups 2 & 3; a second, orthogonal, function provides the best separation of groups 2 and 3, after
partialling out variance accounted for by the first function. Stepwise LDA is an iterative procedure which
adds predictors based on which of the candidate predictors provide the best discrimination.

Acoustic correlates of information structure 21
of the three sentence positions (subject, verb, and object) as possible predictors of the
seven experimental conditions, resulting in 72 predictors. Across the three analyses, the
acoustic features which consistently resulted in the best discrimination of conditions were
(1) duration + silence, (2) mean F0, (3) maximum F0, and (4) maximum intensity at the
positions of the (a) Subject, (b) Verb, and (c) Object. The fact that these 12 features (four
acoustic features across three sentence positions) consistently discriminated among
conditions across three independent sets of productions (from different speakers and
across somewhat different sets of materials) serves as evidence that these features are
underlying speaker- and material-independent differentiation of information structure.
Therefore, we use only these 12 features in the linear discriminant analyses reported for
the individual experiments in the paper.
Computing Residual Values
Because of differences among individuals, including age, gender, speech rate and
level of engagement with the task, speakers produce very different versions of the same
sentence even within the same experimental condition, thus adding variance to the
acoustic features of interest. Similarly, there is likely to be variability associated with
different items due to lexical and world knowledge factors. Researchers have previously
dealt with the issue of acoustic variability between speakers by normalizing pitch and/or
duration by speaker (e.g., Shriberg, Stolcke, Hakkani-Tur, & Tur, 2000; Shriberg et al.,
1998; Wightman, Shattuck-Hufnagel, Ostendorf, & Price, 1992). In order to remove
speaker- and item-related variance in the current studies, we computed linear regression
models in which speaker (n = 18) and item (n = 28) predicted each of the 12 acoustic
features identified in the stepwise discriminant analyses described in the previous section.
From each of these models, we calculated the predicted value of each acoustic feature for
a specific item from a specific speaker. We then subtracted this predicted value from

Acoustic correlates of information structure 22
every production. The differences among the resulting residual values should reflect
differences in the acoustic features due only to the experimental manipulations. All
subsequently reported analyses were performed on these residual values.

Focus Location
The extent to which a discriminant function analysis can separate data points into two or
more groups is calculated with a statistical test, Wilks’s lambda6.
To determine how well the acoustic features could differentiate focus location in
speakers’ productions, we computed a model where the 12 acoustic predictors were used
to discriminate among three focus locations: Subject, Verb or Object. In this analysis, we
are averaging across the contrastive and non-contrastive condition for each location.
The overall Wilks’s lambda of the model was significant, Λ = .46, χ2(24) = 271, p
< .001, indicating better-than-chance differentiation of subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .84, χ2(24) = 62.65,
p < .001, indicating that the acoustic predictors could also differentiate verb focus from
object focus (see Figure 2). Leave-one-out classification correctly classified 67% of the
productions. The model correctly classified subject focus 76% of the time, verb focus
58% of the time, and object focus 66% of the time. Table 3 presents the standardized
canonical discriminant function coefficients of the model.7

6

Wilks's lambda is a measure of the distance between groups on means of the independent variables, and is
computed for each function. It ranges in size from 0-1; lower values indicate a larger separation between
groups. The extent to which the model can effectively discriminate a new set of data is simulated by a
leave-one-out classification, in which the acoustic data from each production are iteratively removed from
the dataset, the model is computed, and the left-out case is classified by the resultant functions.
7
The coefficients in Table 3 indicate which acoustic features best discriminate focus location, such that
larger absolute values indicate a greater contribution of that feature to discrimination. For example,
inspection of the plot in Figure 2 and the coefficients in the Focus Location columns of Table 3 shows that
the acoustic features of Damon score around zero, or lower, on the first function (-0.002, 0.001, -0.01, and 0.06) and around zero on the second function (-0.003, 0.021, -0.016, -0.101). Fried shows a different
pattern; specifically, the acoustic features of fried have coefficients around zero for the first function, and
negative coefficients for function 2. Finally, omelet shows a third pattern: its acoustic correlates are
centered around zero on Function 1, but are high on Function 2.

Acoustic correlates of information structure 23
Figure 3 graphically presents the mean values of the four features, demonstrating
that across all three focus locations the intended focus location is produced with the
highest maximum intensity, the longest duration and silence, and the highest relative F0.

Function
1

Function
2

Subj
Focus

Verb
Focus

Obj
Focus

omelet

Focus
Breadth

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.001
-0.006
0.002
-0.037

0.004
0.011
0.001
0.181

0.008
0.011
-0.002
-0.137

0.003
-0.014
0.002
-0.026

0.004
-0.019
0.006
0.189

0.003
0.000
0.003
0.199

fried

Focus Type

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

0.007
0.024
0.002
0.094

-0.001
-0.003
-0.002
-0.010

0.007
0.000
0.004
-0.076

0.002
-0.040
-0.007
0.131

-0.001
-0.013
0.013
-0.043

0.005
-0.025
0.003
0.011

Damon

Focus Location

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.002
0.001
-0.010
-0.060

-0.003
0.021
-0.016
-0.101

0.005
-0.016
-0.012
0.087

-0.002
-0.007
0.020
0.056

0.005
-0.014
-0.011
-0.225

0.003
0.007
-0.005
-0.123

Acoustic correlates of information structure 24
Table 3: Standardized canonical coefficients of the discriminant functions computed for
Experiment 1.

Figure 2: Separation of focus locations on two discriminant functions in Experiment 1.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the upper left quadrant; productions of verb focus are
clustered in the lower half of the plot; productions of object focus are clustered in the
upper right quadrant.

Acoustic correlates of information structure 25

Damon
fried
omelet

Figure 3: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 1.
Focus type
To determine how well the acoustic features could differentiate the type of focus
(i.e. non-contrastive vs. contrastive) in speakers’ productions, we computed three models
in which the 12 acoustic predictors were used to discriminate between two focus type
groups. The three models investigated differences between non-contrastive and
contrastive focus at the three focus locations: subject, verb, and object.
Focus Type – Subject Position
The overall Wilks’s Lambda was not significant, Λ = .898, χ2(12) = 11.95 p = .45,
indicating that the acoustic features could not discriminate between non-contrastive and

Acoustic correlates of information structure 26
contrastive focus. Because the overall model is not significant, we do not present the
scores of the specific acoustic features or the classification statistics here or in the
analyses below.
Focus Type – Verb Position
The overall Wilks’s Lambda was not significant, Λ = .851, χ2(12) = 17.92 p = .12,
indicating that the acoustic features could not discriminate between non-contrastive and
contrastive focus.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .82, χ2(12) = 22.63 p < .05,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus above chance level. Leave-one-out classification correctly classified
59% of the productions. The model correctly classified non-contrastive focus 59% of the
time, and contrastive focus 59% of the time.
The coefficients in the Object Focus column of Table 3 indicate that intensity and
mean F0 contribute most to classification. Figure 4 graphically presents the mean values
of the four features, demonstrating that contrastive focus is produced with a higher
maximum intensity, a longer duration and silence, and higher maximum F0. Noncontrastive focus is produced with a higher mean F0.

Acoustic correlates of information structure 27

Damon
fried
omelet

Figure 4: Values for non-contrastive focus and contrastive focus type on the four
discriminating acoustic features when the direct object “omelet” is focused in
Experiment 1.
Wide Focus vs. Narrow Focus
To determine how well the acoustic features could differentiate focus breadth, we
computed a model in which the 12 critical predictors were used to discriminate between
productions where the entire sentence was focused and productions where the object was
non-contrastively or contrastively focused.
The overall Wilks’s Lambda was significant, Λ = .75, χ2(12) = 47.83, p < .001,
indicating that the acoustic features could successfully discriminate between conditions
where the entire event is focused and conditions where the object is narrowly focused.

Acoustic correlates of information structure 28
Leave-one-out classification correctly classified 72% of the productions. The model
correctly classified wide focus 67% of the time, and narrow focus 74% of the time.
The standardized canonical discriminant function coefficients in the Focus
Breadth column of Table 3 indicate that maximum intensity contributes most to focus
breadth classification. Figure 5 graphically presents the mean values of the four features,
demonstrating that wide focus is produced with a more uniform duration + silence and
maximum F0 across the sentence than object focus. Wide focus is also produced with a
more uniform, though overall greater, intensity than object focus.

Damon
fried
omelet

Figure 5: Values for wide focus vs. narrow object focus on the four discriminating
acoustic features in Experiment 3.
Discussion

Acoustic correlates of information structure 29
Focus Location
The results demonstrate that speakers consistently provide acoustic cues which
disambiguate focus location. Specifically speakers indicated focus with increased
duration, higher intensity, higher mean F0, and higher maximum F0. Furthermore, these
results are consistent with the pattern reported in Eady & Cooper (1986), such that the
word preceding a focused word is less prominent (produced with shorter duration, lower
intensity and lower F0) than the focused word, and the word following the focused word
is less prominent than the word preceding the focused word. Previous studies (Eady et
al., 1986; Rump and Collier, 1986) have reported this reduction in acoustic prominence
following focused elements as being mainly indicated by lower F0 on the post-focal
words, though in our data we also find evidence of this reduction in measures of duration
and intensity.
Focus Type
The results from Experiment 1 indicate that in semi-naturalistic productions
speakers do not systematically differentiate between different focus types (focused
elements which have explicit contrast sets in the discourse and those which do not).
Specifically, at two out of three sentence positions, a discriminant function analysis could
not successfully classify speakers’ productions of contrastively vs. non-contrastively
focused elements. The observation that speakers successfully discriminated contrastive
and non-contrastive focus in object position, but not in subject or verb positions, is
perhaps suggestive, but is likely due to a lack of experimental power, a limitation which
will be addressed in Experiment 2.
Focus Breadth
The results from Experiment 1 demonstrate that speakers do systematically mark
focus breadth prosodically. Narrow object focus is produced with the highest maximum
F0, longest duration, and maximum intensity of the object noun, relative to the other

Acoustic correlates of information structure 30
words in the sentence. For wide focus, the acoustic features are more similar across the
sentence; only intensity and mean F0 are higher on the object than on the other words in
the sentence. These differences are subtle, but sufficient for the model to successfully
discriminate the productions.
The fact that the model failed to systematically classify productions by focus type
(with the exception of the object position), while achieving high accuracy in focus
location and focus breadth indicates that speakers were not marking focus type with
prosody in Experiment 1. However, the method used to elicit productions did not require
that subjects be aware of the information structure ambiguity of the materials. Evidence
from other production studies suggests that speakers may not prosodically disambiguate
ambiguous productions if they are not aware of the ambiguity. Albritton, McKoon, and
Ratcliff (1996), for example, demonstrated that speakers did not disambiguate
syntactically ambiguous constructions like “Dave and Pat or Bob” unless they were
aware of the ambiguity (see also Snedeker and Trueswell, 2003, but cf. Kraljic and
Brennan, 2005, and Schafer, Speer, Warren, and White, 2000, for evidence that speakers
do disambiguate syntactically ambiguous structures even in the absence of ambiguity
awareness). Experiment 2 was designed to be a stronger test of speakers’ ability to
differentiate focus location, focus type, and focus breadth. We used materials similar to
those in Experiment 1, with two important methodological modifications. First, instead
of producing the answers to questions with no feedback, the speaker’s task now involved
trying to enable the answerer to choose the question that s/he was answering from a set of
possible questions. Moreover, we introduced feedback so that the speaker would always
know whether his/her partner had chosen the correct answer. Second, we changed the
design from a between- to a within-subjects manipulation. This ensured that speakers

Acoustic correlates of information structure 31
were aware of the manipulation, as they were producing the same answer seven times
with explicit instructions to differentiate their answers for their partner.
In addition to making the speaker’s task explicit, the new design also allowed us
to analyze the subset of the productions for which the listeners could successfully identify
the question-type and which therefore contain sufficient information for differentiating
utterances along the three relevant dimensions of information structure.

Experiment 2
Method
Participants
Seventeen pairs of participants were recorded for this experiment. Subjects were MIT
students or members of the surrounding community. All reported being native speakers
of American English. None had participated in Experiment 1. Participants were paid for
their participation.
Materials
The materials had the same structure as those from Experiment 1, though the
critical words differed. Specifically, a larger set of names and a wider variety of
temporal adverbs were used, and some verbs and objects differed from Experiment 1.
Unlike Experiment 1, each subject pair was presented with all seven versions of each of
14 items, according to a full within-subjects within-items design. All materials can be
found in Appendix B.
Procedure
Two participants sat at computers in the same room such that neither could see the
other’s screen. One participant was the speaker, and the other was the listener. Speakers
were told that they would be producing answers to questions out loud for their partners

Acoustic correlates of information structure 32
(the listeners), and that the listeners would be required to choose which question the
speaker was answering from a set of seven choices.
At the beginning of each trial, the speaker was presented with a question on the
computer screen to read silently. After pressing a button, the answer to the question
appeared below the question, accompanied by a reminder to the speaker that s/he would
only be producing the answer aloud, and not the question. Following this, the speaker
had one more chance to read the question and answer, and then he/she was instructed to
press a key to begin recording (after being told by the listener that he/she is ready), to
produce the answer, and then to press another key to stop recording.
The listener sat at another computer, and pressed a key to see the seven questions
that s/he would have to choose his/her answer from. When s/he felt familiar with the
questions, s/he told the speaker s/he was ready. After the speaker produced a sentence
out loud for the listener, the listener chose the question s/he thought the speaker was
answering. If the listener answered incorrectly, his/her computer produced a buzzer
sound, like the sound when a contestant makes an incorrect answer on a game show.
This cue was included to ensure that speakers knew when their productions did not
contain enough information for the listener to choose the correct answer.8
Results – Production
Two speaker-listener pairs were excluded as the Listener did not achieve
comprehension accuracy greater than 20%. One further pair was excluded as one
member was not a native speaker of American English. Finally, another pair of subjects
was excluded because they did not take the task seriously, and produced unnaturally
emphatic contrastive accents, often shouting the target word, and laughing while doing

8

In early pilots in which there was no feedback for incorrect responses, we observed that listeners were at
chance in choosing the correct question.

Acoustic correlates of information structure 33
so. These exclusions left a total of 13 pairs of participants whose responses were
analyzed.
Sixty-seven of the 1274 trials (5%) were excluded because (a) the speaker failed
to produce the correct words, (b) the speaker was disfluent, or (c) the production was
poorly recorded. Analyses were performed on all trials, and on the subset of trials for
which the listener correctly identified the question. The results were very similar in the
two analyses. For brevity of presentation, we present results from analyses conducted on
the correct trials (n = 660, 55%). The productions from Experiment 2 were analyzed
using the acoustic features chosen in the feature-selection procedure described in
Experiment 1. All analyses were performed on the residual values of these features, after
removing speaker and item variance with the method described in Experiment 1.
Focus Location
The overall Wilks’s lambda was significant, Λ = .085, χ2(24) = 1335, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .306, χ2(11) = 641, p
< .001, indicating that the acoustic features could also discriminate verb focus from
object focus (see Figure 6).
Leave-one-out classification correctly classified 93% of the productions. For
individual levels of focus location, the discriminant function correctly classified subject
focus 94% of the time, verb focus 90% of the time, and object focus 95% of the time.
The standardized canonical coefficients in the first two columns of Table 4
indicate that the acoustic features contributing most to the discrimination of focus
location are once again mean F0 and maximum intensity, though the other two features
are also contributing. In fact, inspection of the acoustic feature means in Figure 7

Acoustic correlates of information structure 34
demonstrate that the highest value of every acoustic feature is associated with the
intended focused item, with the exception of mean F0 when the subject is focused.

Figure 6: Separation of focus locations on two discriminant functions for Experiment 2.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the lower left quadrant of the plot; productions of verb
focus are clustered in the lower right quadrant; productions of object focus are clustered
in the lower half.
Focus Location

Focus Type

Focus Breadth

Function 2

Subject
Focus

Verb Focus

Object
Focus

Duration+ silence

omelet

Function 1
-0.001

0.004

0.004

0.006

0.003

0.003

Mean F0

-0.006

0.011

-0.003

0.005

-0.023

0.000

Maximum F0

0.002

0.001

0.004

-0.009

-0.003

0.003

Maximum Intensity

-0.025

0.183

-0.052

-0.171

0.012

0.199

Acoustic correlates of information structure 35
-0.002

0.006

0.002

-0.007

0.005

Mean F0

0.024

-0.005

0.001

-0.022

0.006

-0.025

Maximum F0

0.001

-0.002

-0.007

0.001

0.003

0.003

0.093

-0.016

-0.105

0.063

-0.084

0.011

Duration+ silence

Damon

0.007

Maximum Intensity

fried

Duration+ silence

-0.002

-0.002

0.002

0.005

0.009

0.003

Mean F0

0.003

0.021

-0.010

0.004

-0.009

0.007

Maximum F0

-0.011

-0.015

-0.014

-0.012

-0.006

-0.005

Maximum Intensity

-0.067

-0.097

0.094

-0.014

0.010

-0.123

Table 4: Standardized canonical coefficients of all discriminant functions computed for
Experiment 2.

Damon
fried
omelet

Figure 7: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 2.

Acoustic correlates of information structure 36
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .633, χ2(12) = 81.41, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
75% of the productions. The model correctly classified non-contrastive focus 78% of the
time, and contrastive focus 71% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that maximum intensity at all three locations (i.e. large intensity differences between the
subject and verb and the subject and object) contributes most to classification. Figure 8
graphically presents the mean values of the four features, demonstrating that, in addition
to intensity differences, contrastive focus is produced with longer duration and silence, as
well as lower mean and maximum F0.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .654, χ2(12) = 72.27, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
72% of the productions. The model correctly classified non-contrastive focus 70% of the
time, and contrastive focus 75% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that, once again maximum intensity contributes most to classification. Figure 9
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a higher maximum intensity, and a longer duration and silence,
than non-contrastive focus. Once again, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

Acoustic correlates of information structure 37
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .793, χ2(12) = 41.3, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
67% of the productions. The model correctly classified non-contrastive focus 69% of the
time, and contrastive focus 66% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that contrastive focus is most strongly associated with lower mean F0. Figure 10
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a lower mean and maximum F0 than non-contrastive focus.

Damon
fried
omelet

Figure 8. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 2.

Acoustic correlates of information structure 38

Damon
fried
omelet

Figure 9. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 2.

Acoustic correlates of information structure 39

Damon
fried
omelet

Figure 10. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “omelet” is focused in Experiment 2.
Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .59, χ2(12) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 84% of productions; wide
focus was correctly classified 77% of the time, and object focus was correctly classified
88% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 4
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Although intensity is contributing most strongly
to classification, inspection of the acoustic means in Figure 11 indicates that wide focus

Acoustic correlates of information structure 40
is marked by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is marked by greater prominence on the
object, reflected in longer duration, higher F0, and higher intensity.

Damon
fried
omelet

Figure 11: Values for wide vs. narrow object focus on the four discriminating acoustic
features in Experiment 2.

Acoustic correlates of information structure 41
Results – Perception

Figure 12. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 2.
Listeners’ choices of question sorted by the intended question are plotted in
Figure 12. Listeners’ overall accuracy was 55%. To determine whether listeners were
able to determine the speaker’s intended sentence meaning, we compared each subject's
responses to chance performance. Specifically we assessed, for focus location and focus
type, whether each subject's proportion of correct responses exceeded chance; wide
focus productions were excluded from the analysis, so that chance performance for focus

Acoustic correlates of information structure 42
location was .33, and chance performance for focus type was .5. Results demonstrated
that listeners were able to successfully identify focus location: all 13 subjects’
performance significantly exceeded chance performance, p = .05, two-tailed. However,
listeners were unable to successfully identify focus type: only three of 13 subjects
performed at above-chance levels (based on the binomial distribution), p = .05, twotailed. To investigate focus breadth, we assessed, for wide focus and narrow object focus
separately, whether each subject's proportion of correct responses exceeded chance. For
these analyses, we excluded subject and verb focus productions, so that chance
performance was .33 for wide focus, and .67 for narrow object focus. Results
demonstrated that listeners were moderately successful at identifying focus breadth: six
of 13 subjects identified wide focus at rates above chance, and nine out of 13 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.
Discussion
The production results replicated the two main findings from Experiment 1, and provided
evidence for acoustic discrimination of focus type across sentence positions as well.
First, these results demonstrated that focused elements have longer durations than nonfocused elements, incur larger F0 excursions, are more likely to be followed by silence,
and are produced with greater intensity. Second, speakers consistently differentiate
between wide and narrow focus by producing the object in the latter case with higher F0,
longer duration, and greater intensity. Specifically, although object focus was indicated
by increased duration, higher intensity, and higher F0 on the object than on the subject or
the verb, wide focus was indicated by comparatively greater duration, higher intensity,
and higher F0 on the subject and the verb, and shorter duration, lower intensity, and
lower F0 on the object. These results are consistent with those obtained by Baumann et

Acoustic correlates of information structure 43
al. (2006), who demonstrated that narrow focus on an element was indicated with longer
duration and a higher F0 peak than wide focus on an event encompassing that element.
Most importantly, although speakers in Experiment 1 did not differentiate
conditions with and without an explicit contrast set for the focused element (except for
the object position), these conditions were differentiated by speakers in Experiment 2, at
every syntactic position. There are two possible interpretations of this difference. First,
in Experiment 1, speakers produced only four versions of each of the seven conditions,
whereas speakers in Experiment 2 and 3, reported below, produced 14 versions of each of
the seven conditions, resulting in greater power in the latter two experiments. The fact
that, in Experiment 2, speakers successfully discriminated contrastive and noncontrastive focus in all three positions, suggests that the lack of such an effect in
Experiment 1 could be due to a lack of power.
As mentioned above, the difference in the findings between Experiments 1 and 2
is also consistent with results from Allbritton et al. (1996) and Snedeker and Trueswell
(2003) who demonstrated that speakers do not disambiguate syntactically ambiguous
sentences with prosody unless they are aware of the ambiguity. The current results
demonstrate a similar effect for acoustic prominence, such that speakers do not
differentiate two kinds of acoustically prominent elements (contrastively vs. noncontrastively focused elements) unless they are aware of the information structure
ambiguity in the structures they are producing.
The discriminant analyses indicated that contrastively focused words were
produced with longer durations and higher intensity than non-contrastively focused
words, but that non-contrastively focused words were produced with higher F0 than
contrastively focused words. This latter finding is surprising when compared to some
previous studies. For example, Ladd & Morton (1997) found that higher F0 and larger

Acoustic correlates of information structure 44
F0 range is perceived as more ‘emphatic’ or ‘contrastive’ by listeners. Similarly, Ito and
Speer (2008) demonstrated that contrastively focused words were produced with higher
F0 than non-contrastive ones. Given the unexpected results, we inspected individual
pitch tracks to more closely observe the F0 patterns across the entire utterances. The
pitch tracks presented in Figure 13 were generated from the productions of a typical
speaker, and they exemplify the higher F0 observed for non-contrastive focus than
contrastive focus in the subject position (A vs. B) and verb position (C vs. D).
Contrastive focus on the object is realized with the same F0 as non-contrastive focus on
the object (E vs. F).

300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Non-Contrastive

Given

an

omelet

yesterday

Given

0

1.433
Time (s)

Acoustic correlates of information structure 45
A. Non-contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Contrastive

an

Given

omelet

yesterday

Given

0

1.81
Time (s)

B. Contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Non-Contrastive

omelet

yesterday

Given

0

1.514
Time (s)

C. Non-contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Contrastive

an

omelet

<SIL>

yesterday

Given

0

2.377
Time (s)

Acoustic correlates of information structure 46
D. Contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Given

omelet

yesterday

Non-Contrastive
1.582

0
Time (s)

E. Non-contrastive Object Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Given

an

omelet

yesterday

Contrastive
2.188

0
Time (s)

F. Contrastive Object Focus
Figure 13. Pitch tracks for non-contrastive and contrastive subject focus, non-contrastive
and contrastive verb focus, and non-contrastive and contrastive object focus,
respectively, from a typical speaker from Experiment 2.
Note that our finding that non-contrastive focus is realized with higher F0 than
contrastive focus is still consistent with the claim that contrastive focus is more
prominent than non-contrastive focus. As the graphs in Figures 8-10, and the pitch tracks
in Figure 13 indicate, although contrastive elements were consistently produced with
lower pitch, they were also consistently produced with longer durations and greater

Acoustic correlates of information structure 47
intensity than non-contrastive elements. As reviewed in the introduction, there is
9

evidence that intensity and duration can convey prominence more effectively than higher
pitch (Fry, 1954, Lieberman, 1960, Beckman, 1986; Turk & Sawusch,1996; Kochanski et
al., 2005). Our data are therefore consistent with prior claims that contrastive focus is
produced with greater prominence than non-contrastive focus.
As discussed in the introduction, the production elicitation and analysis methods
used in the current experiment are more robust than methods used in many previous
studies, including those whose results are inconsistent with the current findings. In
particular, the current results are based on productions from naïve subjects in a
communication task, and the analyses were performed on data with speaker and item
variability removed. The current results are therefore more likely to reflect the
underlying generalizations about the relationship between acoustics and meaning.
The perception results only partially mirrored the production results. Consistent
with the production results, listeners were highly successful in discriminating among the
three focus locations. In contrast to the production results, however, listeners were only
moderately successful in identifying focus type (non-contrastive vs. contrastive) from the
speakers’ productions. In fact, listeners most often confused non-contrastive focus with
contrastive focus (see Figure 12). These results suggest that, even though speakers may
be consistently signaling focus type with their prosody, listeners are not able to exploit
those cues for comprehension.
With regard to focus breadth, the perception results are incompatible with a strong
version of the focus projection hypothesis (Selkirk, 1995). According to this hypothesis,
an acoustic prominence on the object NP can be interpreted as marking the entire clause
9

Importantly, the F0 results are not artifacts of the residualization procedure employed to remove variance
from the acoustic features due to speaker and item. The same numerical pattern of F0 values is observed
whether residualization is employed or not, though only the residualized acoustic features successfully
discriminate focus type.

Acoustic correlates of information structure 48
as focused. Listeners are therefore predicted to treat a production with an acoustically
prominent object NP as ambiguous between the narrow object focus reading and the wide
focus reading. However, as can be seen in Figure 12, listeners correctly identified narrow
object non-contrastive focus 57% of the time, interpreting it as wide focus only 13% of
the time, and correctly identified narrow object contrastive focus 49% of the time,
interpreting it as wide focus only 6% of the time. These results are not consistent with
Gussenhoven’s (1983) finding that listeners cannot reliably distinguish between narrow
objects focus and wide focus.
Experiments 1 and 2 provide evidence that speakers systematically indicate focus
location and focus breadth using a set of four acoustic features. These experiments
further suggest that speakers can, but don’t always, indicate focus type. In particular, the
results suggest that speakers only prosodically differentiate contrastive from noncontrastive focus when they are aware of the meaning ambiguity and/or when the task
involves conveying a particular meaning to a listener.
To further investigate the speakers’ ability to prosodically differentiate contrastive
from non-contrastive focus, we conducted an additional experiment. Acoustic analyses
in Experiments 1 and 2 were limited to three words (i.e. subject, verb, object) in the
sentence. However, in natural productions, speakers’ utterances are often prefaced by
attribution expressions (e.g., “I think” or “I heard”), or expressions of emotional attitudes
towards the described events (e.g., “Unfortunately”, or “Luckily”). It is therefore
possible that contrastive information might be partially conveyed by prosodically
manipulating these kinds of expressions. We explored this possibility in Experiment 3, in
which we had speakers produce target SVO constructions with a preamble. Experiment 3
was also intended to serve as a replication of the results of Experiment 2; in particular,

Acoustic correlates of information structure 49
the somewhat unexpected finding that non-contrastive focus is produced with higher F0
than contrastive focus.

Experiment 3
Method
Participants
Fourteen pairs of participants (speakers and listeners) were recorded for this
experiment. Subjects were MIT students or members of the surrounding community. All
reported being native speakers of American English. None had participated in
Experiments 1 or 2. Participants were paid for their participation.
Materials
The materials for Experiment 3 were identical to those from Experiment 1
described above with the exception that an attribution expression (“I heard that”) was
appended to the beginning of each target sentence.
Procedure
The procedure for Experiment 3 was identical to that for Experiment 2.
Results – Production
Four speaker-listener pairs were excluded as the listener did not achieve
comprehension accuracy greater than 20%. These exclusions left a total of 10 pairs of
participants whose responses were analyzed. Eighty-one of the 980 recorded trials (8%)
were excluded because (a) the speaker failed to produce the correct words, (b) the
speaker was disfluent, or (c) the production was poorly recorded. Analyses were
performed on all trials, and on the subset of trials for which the listener correctly
identified the question the speaker produced the sentence in response to. As in
Experiment 2, the results were very similar for the two analyses. For brevity of
presentation, we present results from analyses conducted on the correct trials (n = 632,
70%).

Acoustic correlates of information structure 50
Focus Location
In order to investigate the contribution of the prosody of “I heard that” to the
differentiation of the focus type in Experiment 2, we performed a stepwise discriminant
function analysis which included as predictors measures of the four acoustic features we
had selected initially (duration + silence, mean F0, maximum F0, maximum intensity) (1)
for the subject (“Damon”), verb (“fried”), and object (“omelet”), and (2) for each of the
first three words of the sentence (“I”, “heard”, “that”). Of the 24 predictors included in
the stepwise discriminant function analysis, the features which resulted in the best
discrimination of focus type were (1) the duration + silence of “I”, (2) the maximum F0
of “I”, and (3) the maximum intensity of “I”. Based on these results, we conducted an
additional analysis in which we included a subset of 16 predictors: the duration + silence,
mean F0, maximum F0, and maximum intensity of the subject, verb, object, and “I”.
As in Experiments 1 and 2, we conducted a discriminant analysis to determine
whether the measures of (1) duration + silence, (2) maximum F0, (3) mean F0, and (4)
maximum intensity of the four critical words in the sentence could predict focus location.
The overall Wilks’s lambda was significant, Λ = .058, χ2(32) = 1467.09, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .275, χ2(15) =
664.75, p < .001, indicating that the acoustic features could also discriminate verb focus
from object focus (Figure 14). Leave-one-out classification procedure correctly
classified 97% of the productions. At individual focus locations, the model correctly
classified subject focus 96% of the time, verb focus 97% of the time, and object focus
97% of the time.

Acoustic correlates of information structure 51

Figure 14. Separation of focus locations on two discriminant functions for Experiment 3.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the left half of the plot; productions of verb focus are
clustered in the lower right quadrant; productions of object focus are clustered in the
upper right quadrant.
Focus Location

Focus
Breadth

Focus Type

fried

omelet

Function
1

Function
2

Subject
Focus

Verb
Focus

Object
Focus

0.002

0.003

0.000

0.000

0.003

0.002

0.005

0.012

-0.013

-0.009

0.005

-0.010

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

0.003

0.000

0.005

0.006

-0.003

0.003

0.069

0.106

-0.037

-0.011

0.007

0.151

0.001

-0.003

0.000

0.002

0.001

0.005

0.025

-0.021

-0.001

-0.002

-0.001

-0.006

Maximum F0
Maximum
Intensity

-0.005

-0.002

-0.001

-0.005

0.000

-0.003

0.091

-0.077

-0.086

-0.015

0.027

-0.048

Duration+
silence
Mean F0

Acoustic correlates of information structure 52
Duration+
silence
Mean F0

Damon

0.000

0.002

0.000

0.000

0.002

0.011

0.011

-0.011

-0.020

0.019

-0.003

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

-0.014

-0.003

-0.001

0.007

-0.014

-0.008

-0.147

0.011

0.159

-0.006

-0.064

-0.123

0.000

0.000

0.004

0.005

0.005

-0.001

-0.005

0.000

-0.013

-0.008

-0.003

-0.009

Maximum F0
Maximum
Intensity

I

-0.003

0.004

-0.002

0.017

0.010

0.014

0.005

-0.021

-0.017

0.142

0.133

0.126

0.014

Table 5: Standardized canonical coefficients of all discriminant functions computed for
Experiment 3.

I
Damon
fried
omelet

Figure 15. Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 3.

Acoustic correlates of information structure 53
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .39, χ2(16) = 157.44, p<.001,
indicating that the acoustic features could successfully discriminate between noncontrastive and contrastive focus. Leave-one-out classification correctly classified 85%
of the productions. The model correctly classified non-contrastive focus 85% of the time,
and contrastive focus 85% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that maximum intensity overall, and specifically, maximum intensity on “I,” is
contributing most to classification. Figure 16 graphically presents the mean values of the
four features, demonstrating that, in addition to intensity differences, contrastive focus is
produced with longer duration and silence, and with lower mean and maximum F0.

Acoustic correlates of information structure 54

I
Damon
fried
omelet

Figure 16. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 3.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .46, χ2(16) = 139.28, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
80% of the productions. The model correctly classified non-contrastive focus 86% of the
time, and contrastive focus 74% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that, intensity on “I” is contributing the most to classification. Figure 17 graphically

Acoustic correlates of information structure 55
presents the mean values of the four features, demonstrating that contrastive focus is
produced with a higher maximum intensity, and a longer duration and silence, than noncontrastive focus. As in Experiment 2, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

I
Damon
fried
omelet

Figure 17. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 3.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .40, χ2(16) = 133.37, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and

Acoustic correlates of information structure 56
contrastive focus better than chance. Leave-one-out classification correctly classified
83% of the productions. The model correctly classified non-contrastive focus 89% of the
time, and contrastive focus 76% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that intensity and mean F0 on “I” are contributing the most to accurate classification.
Figure 18 graphically presents the mean values of the four features, demonstrating that
contrastive focus is produced with a higher mean and maximum F0 than non-contrastive
focus.

I
Damon
fried
omelet

Acoustic correlates of information structure 57
Figure 18: Values for non-contrastive vs. contrastive focus on the four discriminating
acoustic features when “omelet” is focused in Experiment 3.

Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .48, χ2(16) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 87% of productions; wide
focus was correctly classified 79% of the time, and object focus was correctly classified
92% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 5
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Specifically, greater intensity on the object is a
strong predictor of object focus; less intensity on the subject and the verb are strong
predictors of wide focus. Although intensity is contributing most strongly to
classification, inspection of the acoustic means in Figure 19 indicates that wide focus is
indicated by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is indicated by greater prominence on
the object, reflected in longer duration, higher F0, and higher intensity.

Acoustic correlates of information structure 58

I
Damon
fried
omelet

Figure 19. Values for wide vs narrow object focus on the four discriminating acoustic
features in Experiment 3.
Results – Perception
Listeners’ overall accuracy percentage by condition is plotted in Figure 20.
Listeners’ overall accuracy was 70%. As described in Experiment 2, we compared each
subject's responses to chance performance. Results demonstrated that listeners were able
to successfully identify focus location, as all 10 subjects’ performance significantly
exceeded chance performance, p = .05, two-tailed. Listeners were moderately successful
at discriminating focus type, as six of 10 subjects’ performance exceeded chance levels, p
= .05, two-tailed. Listeners successfully identified focus breadth as eight out of 10

Acoustic correlates of information structure 59
subjects identified wide focus at rates above chance, and eight out of 10 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.

Figure 20. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 3.

Discussion
Experiment 3 was conducted in order to (1) investigate whether speakers could
differentiate focus type with prosody if the sentences contained an attribution expression

Acoustic correlates of information structure 60
that could convey contrastive information, in addition to the elements that describe the
target event, and (2) replicate the results of Experiment 2.
With regard to the second goal, the production results of Experiment 3
successfully replicated the findings from Experiments 1 and 2. As in Experiments 1 and
2, speakers systematically differentiated focus location and focus breadth with a
combination of duration, intensity, and F0 cues. Furthermore, as in Experiment 2, noncontrastive focus was produced with higher F0 than contrastive focus (though only when
the subject or verb was focused), and contrastive focus was always produced with greater
duration and intensity. As discussed above, these F0 results contrast with prior findings
(Bartels & Kingston, 1994; Couper-Kuhlen, 1984; Ladd & Morton, 1997; Ito & Speer,
2008), but can be interpreted in light of more recent evidence that higher intensity is a
stronger cue to greater prominence than higher pitch (Kochanski et al., 2005).
In addition, results from Experiment 3 demonstrated that the strongest cues to
discrimination of focus type were the acoustics of “I” (from the attribution expression “I
heard that”). Specifically, in contrastive focus conditions, the word “I” was produced
with longer duration, higher intensity, and higher mean F0 and maximum F0. Indeed,
discrimination of focus type in Experiment 3 was far better than in Experiment 2. It
therefore appears that speakers can manipulate prosody on sentence elements outside of
the target clause (e.g., in attribution expressions) to convey contrastiveness.
The perception results demonstrated that listeners could accurately determine
focus location, similar to the results of Experiment 2. Furthermore, listeners were more
accurate in determining focus type than listeners in Experiment 2. This increase in
accuracy was likely due to speakers’ tendency to prosodically mark “I” in the contrastive
conditions.

Acoustic correlates of information structure 61

General Discussion
The three experiments reported in the current paper explored the ways in which focus
location, focus type, and focus breadth are conveyed with prosody. In each experiment,
naïve speakers and listeners engaged in tasks in which the information status of sentence
elements in SVO sentences was manipulated via preceding questions. The prosody of the
target sentences was analyzed using a series of classification models to select a subset
from the set of acoustic features that would best be able to discriminate among focus
locations and between focus types. In addition, in Experiments 2 and 3, the production
results were complemented by the perception results that demonstrated listeners’ ability
to use the prosodic cues in the speakers’ utterances to arrive at the intended meaning.
At the beginning of the paper, we posed three questions about the relationship
between acoustics and information structure: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? We are now in a position to answer these questions.
First, we have demonstrated that speakers systematically provide prosodic cues to
the location of focused material. Across all three experiments, speakers provided cues to
focus location whether or not the task explicitly demanded it, across subject, verb and
object positions. In addition, across all three experiments, speakers systematically
provided cues to focus breadth, such that wide focus was prosodically differentiated from
narrow object focus. Finally, we found that speakers can, but don’t always, prosodically
differentiate contrastive and non-contrastive focus. Specifically, speakers did not
prosodically differentiate focus type in Experiment 1, but they did so in Experiment 2
and, even more strongly, in Experiment 3. As discussed above, the fact that speakers did
not differentiate focus type in Experiment 1, where they were plausibly not aware of the

Acoustic correlates of information structure 62
meaning ambiguity, but did differentiate between contrastive and non-contrastive focus
conditions in Experiments 2 and 3, where the task made the meanings more salient, is
consistent with results from the literature on intonational boundary production
demonstrating that speakers only produce disambiguating boundaries when they are
aware of the syntactic ambiguity which could be resolved by the presence of a boundary
(Albritton et al., 1996; Snedeker & Trueswell, 2003; cf. Schafer, et al., 2000 and Kraljic
& Brennan, 2005). Furthermore, the results from Experiment 3, where the critical words
were preceded by the attribution expression “I heard that,” demonstrated even stronger
differentiation of focus type than in Experiment 2, suggesting that speakers are able to
convey contrastiveness using words outside of the clause containing the contrastivelyfocused element.
To answer the question of which acoustic features are associated with different
meaning categories of information structure, we conducted a series of discriminant
function analyses with the goal of objectively identifying which of 24 measures of
duration, intensity, and F0 allowed for the best discrimination of conditions. Across all
experiments, and across different sentence positions, the best differentiation among
conditions was achieved using the following four features: word duration, maximum
word intensity, mean F0, and maximum F0. These results are consistent with many
previous studies in the literature, implicating these features in conveying aspects of
information structure. An important contribution of the current studies is that these
results were obtained using a quantitative analysis across many naïve speakers and items,
and are therefore more likely to be generalizable.
These data also demonstrate how exactly these four features are used in
conveying different aspects of information structure. With regard to focus location,
focused material is produced with longer duration, higher F0, and greater intensity than

Acoustic correlates of information structure 63
non-focused material. With regard to focus type, non-contrastive focus is realized with
higher mean and maximum F0 on the focused word than contrastive focus, whereas
contrastive focus is realized with greater intensity on the focused word than noncontrastive focus. Finally, with regard to focus breadth, narrow focus on the object is
indicated by higher F0 and longer duration on the object, compared to wide focus, and
wide focus is conveyed by higher intensity and F0, and longer duration on pre-focal
words.
To answer the question of how well listeners can retrieve prosodic information
from the signal, we included a perception task in Experiments 2 and 3. When the
relevant acoustic cues were present in the input (as demonstrated by successful
classification by the models), listeners were also able to classify the utterances, although
not quite as successfully as the models. Furthermore, the fact that the model always
achieved high classification accuracy suggests that the utterances contained enough
acoustic information to make these discriminations, and that we did not leave any
particularly informative acoustic features out of the analyses.

Implications for theories of the mapping of acoustics to meaning
While our production and perception results are compatible with a direct
relationship between acoustics and meaning, they are also consistent with the existence of
mediating phonological categories, as in the intonational phonology framework. For
example, a standard assumption within intonational phonology is that there is a
phonological category “accent” mediating acoustics and semantic focus, such that a
focused element is accented, and an unfocused element is unaccented (e.g., Brown,
1983). Our production and perception results are compatible with this assumption. First,
if speakers are signaling focus location by means of placing acoustic features

Acoustic correlates of information structure 64
corresponding to a +accent category on focused elements, then we would expect to see
strong acoustic differences between focused and given elements, as we have observed.
Moreover, if listeners perceive accents categorically, then we would predict successful
discrimination of productions on the basis of focus location, as we have observed.
Second, when the object is focused, it will be accented, resulting in higher acoustic
measures on the object compared to other positions, as we have observed. Furthermore,
in the wide focus condition, the subject, verb, and object – all of which are focused –
would all receive accents, and would therefore be more acoustically similar to one
another than they are in the wide focus condition. This difference in accent placement
would lead to successful discrimination between wide and narrow focus by listeners, as
we have observed. Finally, there has been much debate in the intonational phonology
literature about whether there is a phonological category +/- contrastive. The results of
our experiments are perhaps best explained without such a category. In particular, if
speakers accent focused elements without differentiating between contrastive and noncontrastive focus, then we would expect similar acoustic results between productions
which differ only on focus type, which would lead to poor discrimination by the model.
Moreover, listeners would not be successful in discriminating focus type, as we have
observed. Our experimental results are thus compatible with an intonational
phonological approach which includes an accent category mediating acoustics and
meaning, but no category for contrastiveness. Importantly, although our results do not
support a categorical difference between non-contrastive and contrastive focus, they do
not exclude the possibility that speakers can mark these distinctions with relative
differences in prominence (Calhoun, 2006).

Acoustic correlates of information structure 65
Implications for semantic theories of information structure
The current results are relevant to two open questions in the semantics of
information structure: (1) whether contrastive and non-contrastive focus constitute two
distinct categories; and (2) whether focus on the object of a verb can project to the entire
verb phrase.
As described in the introduction, Rooth (1992) proposed an account of focus which
makes no distinction between non-contrastive focus and contrastive focus. (6) shows the
F-marking (focus-marking) that Rooth’s account would assign to the conditions in
Experiments 1 and 2. Importantly, words and phrases which evoke alternatives, either
explicit or implicit, are considered focused (i.e. F-marked).
(6)
a. Subject, Subject Contrast: DamonF fried an omelet last night.
b. Verb, Verb Contrast:

Damon friedF an omelet last night.

c. Object, Object Contrast: Damon fried an omeletF last night.
d. Wide:

[Damon fried an omelet] F last night.

Our results provide tentative support for Rooth’s proposal that F-marked constituents do
not differ substantively as a function of whether the alternatives they evoke are explicit
(our contrastive condition) or implicit (our non-contrastive condition). Although
speakers differentiated these two conditions acoustically, they only did so when the
contrast between the conditions was made salient (Experiments 2 and 3). Moreover, even
when speakers did mark this distinction, listeners were unable to consistently use this
information to recover the intended meaning (Experiment 2). These results suggest that
there are no consistent semantic differences between foci with explicit alternatives in the
discourse and those with implicit alternatives.

Acoustic correlates of information structure 66
The second semantic issue that these results bear upon is whether narrow focus on
the object can project to the entire verb phrase. According to the theory of focus
projection proposed in Selkirk (1984, 1995), an acoustic prominence on the direct object
(omelet) can project focus to the entire verb phrase (fried an omelet) and then up to the
entire clause/sentence. Gussenhoven (1983, 1999) makes a similar claim. Both Selkirk’s
and Gussenhoven’s accounts therefore predict that a verb phrase with a prominence on
the object would be ambiguous between a narrow object focus interpretation and a wide
focus interpretation. Neither the production nor the perception results were consistent
with this prediction. In production, speakers distinguished between narrow object focus
and wide focus, and in perception, listeners were able to distinguish these two conditions.
One aspect of the production results (the acoustic realization of the subject) for the
narrow object focus and wide focus conditions is, however, predicted by both Selkirk and
Gussenhoven’s accounts. In particular, in the wide focus condition, the subject
constitutes new information while in the narrow object focus condition the subject is
given. Selkirk & Gussenhoven both predict that the subject would be more acoustically
prominent in the wide focus condition than in the narrow object focus condition. This is
exactly what we observed (especially in Experiments 1 and 3). Nevertheless, as
discussed above, speakers also systematically disambiguated wide focus from narrow
object focus across all three experiments with their realization of the object and the verb.
Specifically, wide focus was produced with stable or increasing duration, intensity, and
F0 across the subject, verb, and object; narrow object focus, on the other hand, was
characterized by shorter duration and lower intensity and F0 on the subject and verb,
followed by a steep increase in each of these values on the object.
Similar to our production findings, Gussenhoven (1983) found that, at least in some
productions, wide focus differed from narrow object focus in that the verb was more

Acoustic correlates of information structure 67
prominent under wide focus. Listeners, however, were unable to use this acoustic
information to distinguish wide focus from narrow object focus. Gussenhoven took this
result as evidence that the two conditions are not reliably distinguished (consistent with
his theory). Our results did not replicate this production/perception asymmetry: Listeners
are able to successfully classify productions with a single prominence on omelet as
indicating narrow object focus and did not confuse these productions with those from the
wide focus condition.
Methodological contributions
A further contribution of the current research to investigations of prosody and
information structure is methodological. With regard to the methods used to elicit
productions, we utilized multiple, untrained speakers to ensure that our results are
generalizeable to all speakers and are not due to speakers’ prior beliefs about what pattern
of acoustic prominence signals a particular meaning (see Gibson & Fedorenko, in press,
for similar arguments with respect to linguistic judgments). Furthermore, unlike most
previous work in which productions were selected for analysis based on perceptual
differentiability or on ratings of the appropriateness of prosodic contours, we elicited and
selected for analysis productions using a meaning task. Thus our analyses were based on
the communicative function of language. Finally, we did not exclude speakers based on
our perceptions of their productions; speakers were excluded for failure to provide
information to their listeners.
The analyses used here also constitute an improvement over previous analyses.
First, using discriminant modeling, we were able to simultaneously investigate the
contribution of multiple sentence elements to acoustic differentiation of conditions.
Second, we demonstrated that residualization is a useful method for controlling for
variability among speakers and lexical items. For example, preliminary analyses

Acoustic correlates of information structure 68
performed on the productions from Experiment 2 without first computing residual values
of the acoustic features revealed a 13% average increase in values of Wilks’ lambda
(where lower values indicate better discrimination) and a 7% average decrease in
classification accuracy. Third, the discriminant modeling proved successful in
objectively determining which acoustic features were the biggest contributors to
differences among conditions. The success of the analyses used in the current studies is
encouraging for future investigations of prosodic phenomena previously considered too
variable for study in a laboratory setting with naïve speakers.
One question that arises from the current set of studies is, to what extent the
current results can be generalized to all speakers and all sentences. In production studies,
there is always a trade-off between (1) having enough control over what participants are
producing to ensure sufficient data for analysis, and (2) ensuring that the speech is as
natural as possible. In Experiment 1, we attempted to elicit natural productions, but
failed to find systematic differences between focus types. In making the speakers’ task—
to help their listeners choose the correct question-type—explicit, we may have also
encouraged speakers to produce these sentences with somewhat exaggerated prosody.
Further experiments will be necessary to determine whether speakers normally produce
contrastive meanings in this way.
In conclusion, the current studies used rigorous scientific methods to explore
several important questions about the acoustic correlates of information structure. By
providing some initial answers to these questions, along with some implications for
semantic theory, and by offering a novel, objective way to approach these and other
questions, these studies open the door to future investigations of the relationship between
acoustics and meaning.

Acoustic correlates of information structure 69

References
Albritton, D., McKoon, G., & Ratcliff, R. (1996) Reliability of Prosodic Cues for
Resolving Syntactic Ambiguity. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22, 714-735.
Bartels, C., Kingston, J., (1994). Salient pitch cues in the perception of contrastive focus.
In Boach, P., Van der Sandt, R. (Eds.), Focus & Natural Language Processing, Proc. of J.
Sem. conference on Focus. IBM Working Papers. TR-80, pp. 94–106.
Baumann, S., Grice, M., and Steindamm, S. (2006). Prosodic Marking of Focus Domains
- Categorical or Gradient? In Proceedings of Speech Prosody, Dresden, Germany, pp.
301-304.
Beckman, Mary E. (1986). Stress and Non-Stress Accent. Netherlands Phonetic Archives
Series No. 7. Foris.
Beckman, M., & Ayers Elam, G. (1997). Guidelines for ToBI labeling, version 3: Ohio
State University.
Beckman, M., Hirschberg, J., & Shattuck-Hufnagel, S. (2005). The original ToBI system
and the evolution of the ToBI framework. In S.-A. Jun (Ed.), Prosodic Typology: The
Phonology of Intonation and Phrasing (pp. 9-54): Oxford University Press.
Birch, S. and Clifton, C. (1995) Focus, accent, and argument structure: effects on
language comprehension. Language and Speech, 38 (4), 365-391.
Birner, B. (1994). Information status and Word Order: An Analysis of English Inversion.
Language, 70 (2), 233-259.
Boersma, Paul & Weenink, David (2006). Praat: doing phonetics by computer (Version
4.3.10) [Computer program]. Retrieved June 3, 2005, from http://www.praat.org/
Bolinger, D. (1961). Contrastive accent and contrastive stress. Language, 37, 83-96.
Breen, M., Dilley, L., Gibson, E., Bolivar, M., and Kraemer, J. (2006) Advances in
prosodic annotation: A test of inter-coder reliability for the RaP (Rhythm and Pitch) and
ToBI (Tones and Break Indices) transcription systems. Poster presented at the 19th
CUNY Conference on Human Sentence Processing, New York, NY. March, 2006.
Brown, G. (1983). Prosodic structures and the Given/New distinction. In D. R. Ladd &
A. Cutler (Eds.), Prosody:Models and measurements (pp. 67–77). Berlin: Springer.
Calhoun, S (2004). Phonetic Dimensions of Intonational Categories - the case of L+H*
and H*. In Proceedings of Speech Prosody, Nara, Japan, pp. 103-106.
Calhoun, S. (2005). It's the difference that matters: An argument for contextuallygrounded acoustic intonational phonology. In Linguistics Society of America Annual
Meeting, Oakland, California, January 2005.
Calhoun, S. (2006) Information Structure and the Prosodic Structure of English: a
Probabilistic Relationship. PhD thesis, University of Edinburgh.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics and points of
view. In Charles N. Li, editor, Subject and Topic, pages 27-- 55. Academic Press, 1976.
Clark, E. V., & Clark, H. H. (1978). Universals, relativity, and language processing. In: J.
H. Greenberg (Ed.), Universals of human language, Vol. I. (pp. 225–277). Stanford:
Stanford University Press.

Acoustic correlates of information structure 70
Cooper, W., Eady, S. & Mueller, P. (1985). Acoustical aspects of contrastive stress in
question-answer contexts. Journal of Acoustical Society of America, 77(6), 2142-2156.
Couper-Kuhlen, E. (1984). A new look at contrastive intonation., Modes of
Interpretation: Essays Presented to Ernst Leisi, Watts, R., Weidman, U. (Eds.) Gunter
Narr Verlag, 137–158.
Cutler, A. (1977). The Context-Independence of "Intonational Meaning". Chicago
Linguistic Society (CLS 13), 104-115.
Dilley, L. C. (2005). The phonetics and phonology of tonal systems. Unpublished Ph.D.
Dissertation, MIT.
Dilley, L. C., & Brown, M. (2005). The RaP (Rhythm and Pitch) Labeling System,
Version 1.0: Available at http://tedlab.mit.edu/rap.html.
Eady, S. J., & Cooper, W. E. (1986). Speech intonation and focus location in matched
statements and questions. Journal of the Acoustical Society of America, 80, 402-415.
Féry, C. and Krifka, M. (2008). Information Structure: Notional Distinctions, Ways of
Expression. In Piet van Sterkenburg (ed.), Unity and diversity of languages, Amsterdam:
John Benjamins, 123-136.
Fry, D. B. (1955). Duration and Intensity as Physical Correlates of Linguistic Stress.
Journal of the Acoustical Society of America, 27, 765–768.
Gibson, E. & Fedorenko, E. (In press). Weak quantitative standards in linguistics
research. Trends in Cognitive Sciences.
Gussenhoven, C. (1983). Testing the reality of focus domains. Language and Speech, 26,
61–80.
Gussenhoven, C. (1999). On the limits of focus projection in English. In P. Bosch & R.
van der Sandt (Eds.), Focus: Linguistic, cognitive, and computational perspectives (pp.
43 –55). Cambridge, U.K.: Cambridge University Press.
Gussenhoven, C., Repp, B. H., Rietveld, A., Rump, W. H. & J. Terken, J. (1997). The
perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society
of America, 102, 3009-3022.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hawkins, S. & Warren, P. (1991). Factors affecting the given-new distinction in speech.
In Proceedings of the 12th International Congress of Phonetic Sciences, Aix en
Provence. 66-69.
Ito, K & Speer, S. (2008). Anticipatory effects of intonation: Eye movements
during instructed visual search. Journal of Memory and Language, 58, 541-573.
Ito, K. Speer, S. R. and Beckman, M. E. (2004). Informational status and pitch accent
distribution in spontaneous dialogues in English, In Proceedings of the International
Conference on Spoken Language Processing, Nara: Japan, 279-282.
Jackendoff, R. (1972). Semantic interpretation in generative grammar. Cambridge: MIT
Press.
Jaeger, T. F. (2008). Categorical Data Analysis: Away from ANOVAs (transformation or
not) and towards Logit Mixed Models. Journal of Memory and Language. 59, 434–446.

Acoustic correlates of information structure 71
Kochanski, G., Grabe, E., Coleman, J., & Rosner, B. (2005) Loudness predicts
prominence: fundamental frequency lends little. The Journal of the Acoustical Society of
America, 118 (2), 1038-1054.
Krahmer, E., & Swerts, M. (2001). On the alleged existence of contrastive accents.
Speech Communication, 34, 391-405.
Kraljic, T. & Brennan, S. E. (2005). Prosodic disambiguation of syntactic structure: For
the speaker or for the addressee? Cognitive Psychology 50: 194-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge Studies in Linguistics 79.
Cambridge: Cambridge University Press.
Ladd, D. R. & Morton, R. (1997). The perception of intonational emphasis: continuous or
categorical? Journal of Phonetics, 25, 313–342.
Lambrecht, K. (2001). A framework for the analysis of cleft constructions. Linguistics,
39, 463–516.
Lieberman, P. (1960). Some acoustic correlates of word stress in American English. The
Journal of the Acoustical Society of America, 32(4), 451-454.
Molnar, V. (2002). Information Structure in a Cross-linguistic Perspective. In Hilde
Hasselgård, Stig Johansson, Bergljot Behrens, Cathrine Fabricius-Hansen (Eds.),
Language and Computers, Vol. 39, 147-161(15).
Paul, H. (1880), Prinzipien der Sprachgeschichte, Leipzig.
Pierrehumbert, J.B. (1980). The phonology and phonetics of English intonation.
Unpublished dissertation, MIT.
Pierrehumbert, J. & Hirschberg, J. (1990). The Meaning of Intonational Contours in the
Interpretation of Discourse. In P. R. Cohen & J. Morgan & M. E. Pollack (eds.).
Intentions in Communication. Cambridge/MA: MIT Press, 271-311.
Pierrehumbert, J. & Steele, S. (1989). Categories of tonal alignment in English.
Phonetica, 46, 181-196.
Pitrelli, J., Beckman, M. & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International
Conference on Spoken Language Processing, 123-126.
Rietveld, A. C. M., and Gussenhoven, C. (1985). On the relation between pitch excursion
size and prominence. Journal of Phonetics, 13, 299-308.
Rochemont, M. S. (1986). Focus in Generative Grammar. Amsterdam/Philadelphia: John
Benjamins.
Rooth, M. (1985). Association with Focus. PhD thesis, University of Massachusetts
Amherst.
Rooth, M. (1992). A theory of focus interpretation. Natural Language Semantics, 1, 75 –
116.
Rump, H. H., and Collier, R. (1996). ‘Focus conditions and the prominence of pitchaccented syllables. Language and Speech, 39, 1–17.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational disambiguation
in sentence production and comprehension. Journal of Psycholinguistic Research, 29,
169-182.

Acoustic correlates of information structure 72
Selkirk, E. (1984). Phonology and syntax: The relation between sound and structure.
Cambridge, MA: MIT.
Selkirk, E. (1995). Sentence Prosody: Intonation, Stress, and Phrasing. In: J.Goldsmith
(ed.). The Handbook of Phonological Theory. Oxford: Blackwell, 550-569.
Schwarzchild, R. (1999) GIVENness, AvoidF and other Constraints on the Placement of
Accent. Natural Language Semantics, 7, 141–177.
Shriberg, E., Stolcke, A., Hakkani-Tur, D. & Tur, G. (2000). Prosody-Based Automatic
Segmentation of Speech into Sentences and Topics. Speech Communication, 32, 127-154.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D., Ries, K., Coccaro, N.,
Martin, R., Meteer, M., & Van Ess-Dykema, C. (1998). Can Prosody Aid the Automatic
Classification of Dialog Acts in Conversational Speech? Language and Speech, 41:3-4,
439-487.
Silverman, K. E. A., Beckman, M., Pierrehumbert, J., Ostendorf, M., Wightman, C. W.
S., Price, P., et al. (1992). ToBI: A standard scheme for labeling prosody. In Proceedings
of the 2nd International Conference on Spoken Language Processing (pp. 867-879).
Banff.
Sluijter, A. and van Heuven, V. (1996). Spectral balance as an acoustic correlate of
linguistic stress. Journal of the Acoustical Society of America, 100, 2471–2485.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects of
speaker awareness and referential contest. Journal of Memory and Language, 48, 103–
130.
Stalnaker, R. (2002). Common ground. Linguistics and Philosophy, 25: 701–721.
Syrdal, A. and McGory, J. (2000). Inter-transcriber reliability of ToBI prosodic labeling.
In Proceedings of the International Conference on Spoken Language Processing,
Beijing: China, 235-238.
Terken, J. (1991). Fundamental frequency and perceived prominence accented syllables.
Journal of the Acoustical Society of America, 89, 1768–1776.
't Hart, J. Collier, R. & Cohen, A. (1990). A perceptual study of intonation. Cambridge
University Press, Cambridge.
Turk, A. & Sawusch, J. (1996) The processing of duration and intensity cues to
prominence. Journal of the Acoustical Society of America, 99, 3782-3790.
Welby, P. (2003). Effects of pitch accent position, type, and status on focus projection.
Language and Speech, 46, 53 – 81.
Wightman, C. W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P. J. (1992). Segmental
durations in the vicinity of prosodic phrase boundaries. Journal of the Acoustical Society
of America, 91(3), 1707-1717.
Xu, Y. & Xu, C. X. (2005). Phonetic realization of focus in English declarative
intonation, Journal of Phonetics, 33, 159–197.
Yoon, T., Chavarria, S., Cole, J., & Hasegawa-Johnson, M. (2004). Intertranscriber
reliability of prosodic labeling on telephone conversation using ToBI. In Proceedings of
the International Conference on Spoken Language Processing., Nara: Japan, 2729-2732.

Acoustic correlates of information structure 73

Appendix A
Experiment 1 items
Full items are recoverable as follows: Question A is always “What happened last night?”
Questions B, C, & D are wh-questions about the subject, verb, and object, respectively.
Questions E, F, & G are questions which introduce the explicit alternative subject, verb,
or object, indicated in parentheses.
1.

Question A: What happened last night?
Question B: Who fed a bunny last night?
Question C: What did Damon do to a bunny last night?
Question D: What did Damon feed last night?
Question E: Did Jenny feed a bunny last night?
Question F: Did Damon pet a bunny last night?
Question G: Did Damon feed a baby last night?
Response: Damon fed a bunny last night.

2. Damon (Lauren) caught (pet) a bunny (a squirrel) last night.
3. Damon (Molly) burned (break) a candle (a log) last night.
4. Darren (Lauren) cleaned (eat) a carrot (a chicken) last night.
5. Darren (Molly) peeled (eat) a carrot (a potato) last night.
6. Darren (Nora) found (buy) a diamond (a ring) last night.
7. Darren (Jenny) sold (lose) a diamond (a sapphire) last night.
8. Jenny (Damon) found (lose) a dollar (a quarter) last night.
9. Jenny (Darren) sewed (rip) a dolly (a blanket) last night.
10. Jenny (Logan) read (open) an email (a letter) last night.
11. Jenny (Nolan) smelled (plant) a flower (a skunk) last night.
12. Lauren (Darren) burned (write) a letter (a magazine) last night.
13. Lauren (Logan) mailed (open) a letter (a package) last night.
14. Lauren (Nolan) read (write) a novel (a newspaper) last night.
15. Lauren (Damon) fried (bake) an omelet (a chicken) last night.
16. Logan (Molly) peeled (chop) an onion (an apple) last night.
17. Logan (Nora) fried (chop) an onion (a potato) last night.
18. Logan (Jenny) cleaned (buy) a pillow (a rug) last night.
19. Molly (Logan) dried (wash) a platter (a bowl) last night.
20. Molly (Nolan) sold (find) a platter (a vase) last night.
21. Molly (Damon) poured (drink) a smoothie (a cocktail) last night.
22. Nolan (Nora) pulled (push) a stroller (a sled) last night.
23. Nolan (Jenny) bought (sell) a stroller (a wheelbarrow) last night.
24. Nolan (Lauren) sewed (knit) a sweater (a quilt) last night.
25. Nora (Nolan) killed (trap) a termite (a cockroach) last night.
26. Nora (Damon) changed (wash) a toddler (a baby) last night.
27. Nora (Darren) fed (dress) a toddler (a bunny) last night.
28. Nora (Logan) pulled (push) a wagon (a wheelbarrow) last night.

Acoustic correlates of information structure 74

Appendix B
Items used for Experiments 2-3
Full items are recoverable as follows: Question A always asks “What happened _____?”
where the blank corresponds to the temporal adverb. Questions B, C, & D are whquestions about the subject, verb, and object, respectively. Questions E, F, & G are
questions which introduce the explicit alternative subject, verb, or object, indicated in
parentheses.
1a.
1b.
1c.
1d.
1e.
1f.
1g.

Context: What happened yesterday?
Context: Who fried an omelet yesterday?
Context: What did Damon do to an omelet yesterday?
Context: What did Damon fry yesterday?
Context: Did Harry fry an omelet yesterday?
Context: Did Damon bake an omelet yesterday?
Context: Did Damon fry a chicken yesterday?
Target: No, Damon fried an omelet yesterday.

2. (I heard that) (No,) Megan (Jodi) sold (lose) her diamond (her sapphire) yesterday.
3. (I heard that) (No,) Mother (Daddy) dried (wash) a platter (a bowl) last night.
4. (I heard that) (No,) Norman (Kelly) read (write) an email (a letter) last night.
5. (I heard that) (No,) Lauren (Judy) poured (drink) a smoothie (a cocktail) this morning.
6. (I heard that) (No,) Nora (Jenny) sewed (rip) her dolly (her blanket) this morning.
7. (I heard that) (No,) Molly (Sarah) trimmed (wax) her eyebrows (her hair) on Tuesday.
8. (I heard that) (No,) Nolan (Steven) burned (break) a candle (a log) on Tuesday.
9. (I heard that) (No,) Logan (Billy) killed (trap) a termite (a cockroach) last week.
10. (I heard that) (No,) Radar (Fido) caught (lick) a bunny (a squirrel) last week.
11. (I heard that) (No,) Darren (Maggie) pulled (push) a stroller (a sled) on Sunday.
12. (I heard that) (No,) Brandon (Tommy) peeled (eat) a carrot (a potato) on Sunday.
13. (I heard that) (No,) Maren (Debbie) cleaned (buy) a pillow (a rug) on Friday.
14. (I heard that) (No,) Lindon (Kelly) fooled (fight) a bully (a teacher) on Friday.

Acoustic correlates of information structure 1

Acoustic correlates of information structure
Mara Breen1, Evelina Fedorenko2, Michael Wagner3, Edward Gibson2
1
2

University of Massachusetts Amherst
Massachusetts Institute of Technology
3
McGill University

June 7, 2010
Address correspondence to:
Mara Breen
522 Tobin Hall
University of Massachusetts
Amherst, MA
01003
mbreen@psych.umass.edu

Acoustic correlates of information structure 2
Abstract
This paper reports three studies aimed at addressing three questions about the acoustic
correlates of information structure in English: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? The information structure of subject-verb-object (SVO)
sentences was manipulated via the questions preceding those sentences: elements in the
target sentences were either focused (i.e. the answer to a wh-question) or given (i.e.
mentioned in prior discourse); furthermore, focused elements had either an implicit or an
explicit contrast set in the discourse; finally, either only the object was focused (narrow
object focus) or the entire event was focused (wide focus). The results across all three
experiments demonstrated that people reliably mark (a) focus location (subject, verb, or
object) using greater intensity, longer duration, and higher mean and maximum F0, and
(b) focus breadth, such that narrow object focus is marked with greater intensity, longer
duration, and higher mean and maximum F0 on the object than wide focus. Furthermore,
when participants are made aware of prosodic ambiguity present across different
information structures, they reliably mark focus type, so that contrastively-focused
elements are produced with higher intensity, longer duration, and lower mean and
maximum F0 than non-contrastively focused elements. In addition to having important
theoretical consequences for accounts of semantics and prosody, these experiments
demonstrate that linear residualization successfully removes individual differences in
people’s productions thereby revealing cross-speaker generalizations. Furthermore,
discriminant modeling allows us to objectively determine the acoustic features that
underlie meaning differences.

Acoustic correlates of information structure 3

Introduction
An important component of the meaning of a sentence is its relationship to the context in
which it is produced. Some parts of speakers’ sentences refer to information already
under discussion, while other parts convey information that the speaker is presenting as
new for the listener. Depending on the context, the same sentence can convey different
kinds of information to the listener. For example, consider the three contexts in (1a)-(1c)
for the sentence in (2):

(1) a. Who fried an omelet?
b. What did Damon do to an omelet?
c. What did Damon fry?

(2) Damon fried an omelet.

The event of frying an omelet is already made salient in the context in (1a), and
this part of the answer is therefore given. Consequently, the sentence Damon fried an
omelet conveys Damon as the new or focused information.1 Similarly, the verb fried is
the focused information relative to the context in (1b), and the object noun phrase an
omelet is the focused information relative to the context in (1c). This component of the
meaning of sentences - the differential contributions of different sentence elements to the

1

Numerous terms are used in the literature to refer to the distinction between the information that is old for
the listener and the information that the speaker is adding to the discourse: background and foreground;
given and new; topic and comment; theme and rheme, etc. In this paper, we will use the term given to refer
to the parts of the utterance which are old to the discourse, and focused to refer to the part of the utterance
which is new to the discourse.

Acoustic correlates of information structure 4
overall sentence meaning in its relation to the preceding discourse - is called information
structure.
Three components of information structure have been proposed in the literature:
givenness, focus, and topic (see e.g., Féry and Krifka, 2008, for a recent summary). The
current paper will be concerned with givenness and focus.2 Given material is material
that has been made salient in the discourse, either explicitly, like the event corresponding
to the verb fried and the object corresponding to the noun omelet in (1a), or implicitly, via
inferences based on world knowledge (e.g., mentioning omelet makes the notion of
“eggs” given, Schwarzchild, 1999).
Focused material is what is new to the discourse, or in the foreground. The focus
of a sentence can often be understood as the part that corresponds to the answer to the
wh-part of wh-questions, like Damon in (2) as an answer to (1a) (Paul, 1880; Jackendoff,
1972).
There are two dimensions along which focused elements can differ. The first is
contrastiveness. A contrastively focused element, like Damon in (3b), indicates that the
element in question is one of a set of explicit alternatives or serves to correct a specific
item already present in the discourse, as in the following:

(3) a. Did Harry fry an omelet yesterday?
b. Damon fried an omelet yesterday.

Unlike (1a), where there is no explicit set of individuals from which Damon is being
selected as the “omelet fryer”, in (3a) an explicit alternative “omelet fryer” is being

2

Topic, the third component of information structure, describes which discourse referent focused
information should be associated with, as in the mention of Damon in “As for Damon, he fried an omelet.”
The current studies do not address the prosodic realization of topic.

Acoustic correlates of information structure 5
introduced: Harry. The sentence (3b) in this context thus presents information (i.e.,
Damon) which explicitly contrasts with, or contradicts, some information which has been
introduced into the discourse.
There is no consensus in the literature regarding the relationship between noncontrastive focus and contrastive focus. Some researchers have treated non-contrastive
focus and contrastive focus as separate categories of information structure (Chafe, 1976;
Halliday, 1967; Rochemont, 1986; Molnar, 2002), whereas others have argued that there
is no principled difference between the two (e.g., Bolinger 1961, Rooth, 1985, Rooth,
1992). According to Rooth (1992), for example, each expression evokes two semantic
representations: the expression’s actual meaning, and a set of alternatives. If a
constituent in the expression is focused, then the alternative set contains the expression
itself and all expressions with an alternative substituted for the focus-marked constituent;
if there is no focus within the expression, the alternative set consists only of the
expression itself. Rooth would therefore argue that Damon in (1a) is focused and
introduces alternative propositions that differ only in the agent of the event ({Damon
fried an omelet, Harry fried an omelet, Ada fried an omelet, ...}), even if no alternatives
are explicitly mentioned. In (3a), Damon also evokes alternative omelet fryers, and
therefore has the same focus structure as (1a), but the context makes a specific alternative
(Harry) more salient than other potential alternatives. Importantly, from Rooth’s
standpoint, it does not matter whether the alternatives are explicit in the discourse or not:
the meaning of the expression is the same.
The second dimension along which focused elements can vary is focus breadth
(Selkirk, 1984; 1995; Gussenhoven, 1983; 1999), which refers to the size of the set of
focused elements. Narrow focus refers to cases where only a single aspect of an event
(e.g., the agent, the action, the patient, etc.) is focused, whereas wide focus focuses an

Acoustic correlates of information structure 6
entire event. Take, for example, the difference between (5) as an answer to (4a) versus as
an answer to (4b):

(4) a. What did Damon fry last night?
b. What happened last night?

(5) Damon fried an omelet last night.

(4a) narrowly focuses the patient of frying, omelet in (5), while (4b) widely focuses the
entire event of Damon frying an omelet.
The information status of a sentence element can be conveyed in at least three
ways: (1) using word order (i.e., given information generally precedes focused
information) (e.g., Birner, 1994, Clark & Clark, 1978); (2) using particular lexical items
and syntactic constructions (e.g., using cleft constructions such as “It was Damon who
fried an omelet”) (Lambrecht, 2001); and (3) using prosody. Prosody – which we focus
on in the current paper – refers to the way in which words are grouped in speech, the
relative acoustic prominence of words, and the overall tune of an utterance. Prosody is
comprised of acoustic features like fundamental frequency (F0), duration, and loudness,
the combinations of which give rise to the psychological percepts like phrasing
(grouping), stress (prominence), and tonal movement (intonation).
The goal of the current paper is to investigate the prosodic realization of
information structure in simple English subject-verb-object (SVO) sentences like (2),
with the goal of addressing the following questions:
1) First, do speakers prosodically distinguish focused and unfocused elements?
This question can be broken down into further questions:

Acoustic correlates of information structure 7
(1a) Do speakers distinguish focused elements that have an explicit contrast
set in the discourse from those that do not?
(1b) Do speakers distinguish sentences in which only the object is focused
from those in which the entire event is focused?
(2) What are the acoustic features associated with these different aspects of
information structure?
(3) How well can listeners retrieve this information from the signal?

Although the current experiments are all performed on English, the answers to
these questions will likely be similar for other West Germanic languages. However, the
relationship between prosodic features and information structure across different
languages and language groups remains an open question.
In the remainder of the introduction, we briefly lay out two approaches to the study
of the relationship between prosody and information structure, and summarize empirical
studies which have explored how information structure is realized acoustically and
prosodically. We then discuss methodological issues present in previous studies which
call into question the generalizeability of the reported findings, and outline how the
current methods were designed to better address these questions.
Empirical investigations of prosody and information structure
Two perspectives on the relationship between the acoustics of the speech signal and
the meaning associated with various aspects of information structure have been
articulated in the literature. According to the direct-relationship approach, sets of
acoustic features are directly associated with particular meanings (Fry, 1955; Lieberman,
1960; Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986; Pell, 2001; Xu & Xu,
2005). In contrast, according to the indirect-relationship approach (known as the

Acoustic correlates of information structure 8
intonational phonology framework), the relationship between acoustics and meaning is
mediated by phonological categories (Ladd, 1996; Gussenhoven, 1983; Pierrehumbert,
1980; Dilley, 2005; Hawkins & Warren, 1991). In particular, the phonetic prosodic cues
are hypothesized to be grouped into prosodic categories which are, in turn, associated
with particular meanings. The experiments in the current paper were not designed to
decide between these two approaches. However, In the current paper, we will initially
discuss our experiments in terms of the direct-relationship approach, because it is more
parsimonious. In the general discussion, we will show how the results are also
compatible with the indirect-relationship approach.
Turning now to previous empirical work on the relationship between prosody and
information structure, we start with studies of focused vs. given elements. Several
studies have demonstrated that focused elements are more acoustically prominent than
given elements. However, there has been some debate about which acoustic features
underlie a listener’s perception of acoustic prominence. Some features that have been
proposed to be associated with prominence include pitch (i.e. F0) (Lieberman, 1960;
Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986), duration (Fry, 1954; Beckman,
1986), loudness (i.e. intensity) (Kochanski, Grabe, Coleman, & Rosner, 2005; Beckman,
1986; Turk and Sawusch, 1996), and voice quality (Sluijter & van Heuven, 1996).
In early work on lexical stress, Fry (1954) and Liberman (1960) argued that
intensity and duration of the vowel of the stressed syllable contributed most strongly to
the percept of acoustic prominence, such that stressed vowels were produced with a
greater intensity and a longer duration than non-stressed vowels. In experiments on
phrase-level prominence, Cooper et al. (1985) and Eady and Cooper (1986) also noted
that more prominent syllables are longer than their non-prominent counterparts. Cooper
et al. (see also Liberman, 1960); Rietveld & Gussenhoven, 1985; Gussenhoven et al.,

Acoustic correlates of information structure 9
1997; and Terken, 1991) also argued that F0 was a highly important acoustic feature
underlying prominence. Others have argued that the strongest cue to prominence is
intensity (e.g., Beckman, 1986). More recently, Turk and Sawusch (1996) also found
that intensity (and duration) were better predictors of perceived prominence than pitch, in
a perception task. Finally, in a study of spoken corpora, Kochanski et al. (2005)
demonstrated that loudness (i.e. intensity) was a strong predictor of labelers’ annotations
of prominence, while pitch had very little predictive power.
The question of whether contrastively and non-contrastively focused elements are
prosodically differentiated by speakers, and perceptually differentiated by listeners has
also been extensively debated. Some have argued that there is no difference in the
acoustic features associated with contrastively vs. non-contrastively focused elements
(Cutler, 1977; Bolinger, 1961; t’Hart, Collier, & Cohen, 1990), while others have argued
that some acoustic features differ between contrastively vs. non-contrastively focused
elements (Couper-Kuhlen, 1984; Krahmer & Swerts, 2001; Bartels & Kingston, 1994;
Ito, Speer, & Beckman, 2004). For example, Couper-Kuhlen (1984) reported, on the
basis of corpus work, that speakers produce contrastive focus with a steep drop after a
high F0 target, while high F0 is sustained after non-contrastive focus (see also Krahmer
and Swerts, 2001). However, this finding is in contrast to Bartels and Kingston (1994),
who have argued, based on a series of production studies, that the most salient acoustic
cue to contrastiveness is the height of the peak on a contrastive word, such that a higher
peak is associated with a greater probability of an element being interpreted as
contrastive (see also Ladd and Morton, 1997). Finally, Ito, Speer, & Beckman (2004)
demonstrated that speakers are more likely to use a L+H* accent (i.e. a steep rise from a
low target to a high target), compared to a H* accent (i.e. a gradual rise to a high target),
to indicate an element that has an explicit contrast set in the discourse.

Acoustic correlates of information structure 10
Krahmer and Swerts (2001) observed that listeners were more likely to perceive a
contrastive adjective (e.g., red in red square preceded by blue square) as more prominent
than a new adjective when the adjective was presented with a noun compared to when it
was presented in isolation. They therefore hypothesized that the lack of a consensus in
the literature may be due to the failure of the earlier studies to investigate focused
elements in relation to the prosody of the surrounding elements. Consistent with this
idea, Calhoun (2005) demonstrated that a model’s ability to predict a word’s information
status is significantly improved when information about the acoustics of adjacent words
is included in the model. These results suggest that a more consistent picture of the
acoustic features associated with contrastively and non-contrastively-focused elements
may emerge if acoustic context is taken into account.
Finally, prior work has investigated whether speakers prosodically differentiate
narrow and wide focus. Selkirk (1995), for example, argued that, through a process
called focus projection, an acoustic prominence on the head of a phrase or its internal
argument can project to the entire phrase, thus making the entire phrase focused (see also
Selkirk, 1984; see Gussenhoven, 1983, 1999, for a similar claim). According to Selkirk
(1984) and Gussenhoven (1983) then a clause containing a transitive verb in which the
direct object is acoustically prominent is ambiguous between a reading where the object
alone is focused and a reading where the entire verb phrase is focused. This hypothesis
has been supported in several perception experiments (Welby, 2003; Birch & Clifton,
1995; Gussenhoven, 1983). Welby (2003), for example, demonstrated that listeners rated
a sentence like I read the DISPATCH with a single acoustic prominence on dispatch as a
similarly felicitous response to either a question narrowly focusing the object (i.e. “What
newspaper do you read?”), or a question widely focusing the entire event (i.e. “How do
you keep up with the news?”). However, Gussenhoven (1983) found that at least in some

Acoustic correlates of information structure 11
productions there is actually a perceptible difference between narrow and wide focus
although listeners cannot use this information to reliably tell in which context the
sentence was uttered (see Baumann et al., 2006, for evidence from German showing that
speakers do differentiate between narrow and wide focus, with prosodic cues varying
across speakers). In contrast to Gussenhoven’s perception results, Rump and Collier
(1986) found that listeners can accurately discriminate narrow and wide focus using pitch
cues.
Limitations of previous work
Although the studies summarized above provide evidence for some systematic
differences in the acoustic realization of different aspects of information structure, no
clear picture has yet emerged with regard to any of the three meaning distinctions
discussed above (i.e. focused vs. given elements, non-contrastively focused vs.
contrastively focused elements, and narrow vs. wide focus). Furthermore, previous
studies suffer from several methodological limitations that make the findings
inconclusive. Here, we discuss five limitations of previous studies which the current
studies seek to address in an effort to reveal a clearer picture of the relationship between
acoustic features and information structure.
First, instead of acoustic features, sometimes only ToBI3 annotations are
provided (e.g., Birch & Clifton, 1995; Ito et al., 2004). This includes work of researchers
who adopt the intonational phonology framework and who therefore believe that using
prosodic annotation offers a useful way to extrapolate away from potentially complex
interactions among acoustic features which give rise to the perception of specific
intonational patterns. One particular problem concerns H* and L+H* accents. As
defined in the ToBI system, these accents are meant to be explicit markers of non3

The (ToBI) Tones and Break Indices system was developed in the early 90s as the standard system for
annotation of prosodic features (Silverman et al., 1992).

Acoustic correlates of information structure 12
contrastive focus and contrastive focus, respectively (Beckman & Ayers-Elam, 1997).
However, H* and L+H* are often confused in ToBI annotations (Syrdal & McGory,
2000), and are, in fact, often collapsed in calculating inter-coder agreement (Pitrelli et al.,
1994; Yoon et al., 2004; Breen et al., 2006, submitted). Therefore, it is difficult to
interpret the results of studies which are based on the difference between H* and L+H*
without a discussion of the acoustic differences between these purported categories. In
the current studies, we report acoustic features in order to avoid confusion about what the
ToBI labels might mean and in order to not presuppose the existence of prosodic
categories associated with particular meaning categories of information structure.
A second limitation concerns the method used to generate and select productions
for analysis. A common practice involves eliciting productions from a small number of
speakers (e.g., Baumann et al., 2006; Krahmer & Swerts, 2001), which results in a
potential decrease in experimental power, and could therefore lead to a Type II error. In
addition, several previous experiments have excluded speakers’ data from analysis for not
producing accents consistently (e.g., Eady & Cooper, 1986; Cooper et al., 1985), which
could lead to a Type I error. For the current experiments, we recruited between 13 and
18 speakers. In addition, no speakers’ productions were excluded from the analyses
based on a priori predictions about potential behavior (e.g., placing accents in particular
locations).
A third limitation concerns the tasks used in perception studies. In particular,
some studies asked listeners to make judgments about which of two stimuli was more
prominent (Krahmer & Swerts, 2001), what accent is acceptable in a particular context
(Birch & Clifton, 1995; Welby, 2003), or with which of two questions a particular answer
sounded more natural (Gussenhoven, 1983). The problem with these meta-linguistic
judgments is that they lack a measure of the participants’ interpretation of the sentences.

Acoustic correlates of information structure 13
In the current studies we employ a more natural production-comprehension task, in which
speakers are trying to communicate a particular meaning of a semantically ambiguous
sentence and listeners are trying to understand the intended meaning.
A fourth limitation of previous studies is in how they have dealt with speaker
variability. Presenting data from individual subjects separately, as is commonly done, is
problematic because it fails to capture the shared aspects of individual productions (e.g.,
consistent use by most speakers of some set of acoustic features to mark focused
elements). In the current studies, we combine data across subjects while simultaneously
removing variance due to individual differences using linear regression modeling (e.g.,
Jaeger, 2008).
A fifth limitation is that many have reported differences between conditions based
only on individual acoustic features on single words (Eady & Cooper, 1986; Cooper et
al., 1985; Baumann et al, 2006). If acoustic prominence is perceived in a contextdependent manner, these single-feature/single-word analyses might find spurious
differences, or fail to find real differences. In the current studies, we used discriminant
modeling on the productions in order to simultaneously investigate the contribution of
multiple acoustic features from multiple words in an utterance to the interpretation of
information status of different sentence elements.

Experiments: Overview and general methods
The current paper presents results from three experiments. Experiment 1
investigated whether speakers prosodically disambiguate focus location (subject, verb,
object), focus type (contrastive vs. non-contrastive focus), and focus breadth (narrow vs.
wide) by eliciting semi-naturalistic productions like that in (3b) (e.g., Damon fried an
omelet this morning), whose information status was disambiguated by a preceding

Acoustic correlates of information structure 14
question. Experiment 2 investigated whether speakers disambiguate focus location and
focus type when the task explicitly required them to communicate a particular meaning to
their listeners. Finally, Experiment 3 served as a replication and extension of Experiment
2, in which speakers included an attribution expression (“I heard that”) before the critical
sentence.
The acoustic analysis of the productions elicited in all three experiments
proceeded in three steps. First, we automatically extracted a series of 24 acoustic features
(see Table 2) from the subject, verb, and object of the sentences elicited in Experiments
1, 2, and 3. Second, we subjected all of these features to a stepwise discriminant function
analysis in order to determine which features best discriminated the information status
conditions listed in Table 1 for each of the three experiments. This analysis resulted in a
subset of eight acoustic features. Finally, we used discriminant analyses to evaluate
whether this subset of eight features could effectively discriminate sets of 2 and 3
conditions for each of the three experiments. Specifically, we tested focus location by
comparing the features from productions in which Damon, fried, and omelet were
focused, respectively. We tested focus type by comparing the features from sentences in
which the focused element was contrastively or non-contrastively focused at each of the
three syntactic positions. Last, we tested focus breadth by comparing the features for
sentence with wide-focus to those with narrow object focus. In addition to the analysis of
acoustic features, in Experiments 2 and 3 we investigated whether listeners could
correctly determine the intended information status of the speaker.

Acoustic correlates of information structure 15

Experiment 1
Method
Participants
Nine pairs of participants were recorded. All participants were self-reported native
speakers of American English. All participants were MIT students or members of the
surrounding community. Participants were paid for their participation.
Materials
Each trial consisted of a set-up question and a target sentence, which always had an SVO
structure (e.g., Damon fried an omelet this morning). The target sentence could plausibly
answer any one of the seven set-up questions (see Table 1), which served to focus
different elements of the sentence or the entire event described in the sentence. The first
question focused the entire event (i.e. What happened?). In the remaining conditions,
two factors were manipulated: (1) the element in the target sentence that was focused by
the question (subject, verb, object); and (2) the presence of an explicit contrast set for the
focused element (non-contrastively focused, i.e. explicit contrast set absent, contrastively
focused, i.e. explicit contrast set present).
All subject and object noun phrases (NPs) in the target sentences were bi-syllabic
with first syllable stress, and all verbs were monosyllabic. All subject NPs were proper
names, and object NPs were mostly common inanimate objects, such that the events were
non-reversible. Furthermore, all words were comprised mostly of sonorant phonemes.
These constraints ensured that words could be more easily compared across items, and
facilitated the extraction of acoustic features (which is easier for vowels and sonorant
consonants). An adjunct prepositional phrase (PP) was included at the end of each
sentence so that differences in the production of the object NP due to the experimental
manipulations would be dissociable from prosodic effects on phrase-final, or in this case,

Acoustic correlates of information structure 16
sentence-final, words, which are typically lengthened and produced with lower F0
compared to phrase-medial words (e.g., Wightman et al., 1992).
We constructed 28 sets of materials. Participants saw one condition of each item,
following a Latin Square design. A sample item is presented in Table 1. The complete
set of materials can be found in Appendix A.

Condition

Focus Type

Focused
Argument

Setup Question

1

Non-contrastive

wide

What happened this morning?

2

Non-contrastive

S

Who fried an omelet this morning?

3

Non-contrastive

V

What did Damon do to an omelet this morning?

4

Non-contrastive

O

What did Damon fry this morning?

5

Contrastive

S

Did Harry fry an omelet this morning?

6

Contrastive

V

Did Damon bake an omelet this morning?

7

Contrastive

O

Did Damon fry a chicken this morning?

Table 1: Example item from Experiment 1. The target sentence is “Damon fried an
omelet this morning.”
Procedure
Productions were elicited and pre-screened in a two-part procedure. The first part
was a training session, where participants learned the intended names for pictures of
people, actions, and objects. In the second part, the pairs of participants produced
questions and answers for each other. The method was designed to maximize control
over what speakers were saying, but to also encourage natural-sounding productions.
Pilot testing revealed that having subjects simply read the target sentences resulted in
productions with low prosodic variability. After going through the experiment one time,
the participants switched roles.
Training session
In the training session, participants learned mappings between 96 pictures and
names, so that they could produce the names from memory during the second part of the

Acoustic correlates of information structure 17
experiment. In a PowerPoint presentation, each picture, corresponding to a person, an
action, an object, or a modifier, was presented with its intended name (see Figure 1, left).
The pictures consisted of eight names of people, which were repeated 3-4 items each in
the experimental materials, eight colors (which were used in a concurrently run filler
experiment), 34 verbs, 44 objects, and two temporal modifiers (this morning and last
night). The pictures were presented in alphabetical order, to facilitate memorization and
recall. Participants were instructed to learn the mappings by progressing through the
PowerPoint at their own pace.
When participants felt they had learned the mappings, they were given a picturenaming test, which consisted of 27 items from the full list of 96. The test was identical
for all participants. Participants were told of their mistakes, and, if they made four or
more errors, they were instructed to go back through the PowerPoint to improve their
memory of the picture-name mappings. Once participants could successfully name 23 or
more items on the test, which took between 1 and 3 rounds of testing, they continued with
the second part of the experiment. Early in pilot testing, we discovered that subjects had
poor recall for the names of the people in the pictures. Therefore, in the actual
experiment, subjects could refer to a sheet which had labeled pictures of the people.

Acoustic correlates of information structure 18

Figure 1: Left: Examples from the picture-training task for Experiment 1. Each square
represents a screen shot. Right: Examples of the procedure for the questioner (upper
squares) and answerer (lower squares) for Experiment 1. Two conditions are presented:
Non-contrastive, object (left) and contrastive, verb (right). The top squares represent
screen shots of what the questioner saw on a trial; the bottom squares represent what the
answerer saw on a trial.
Question-Answer Experiment
The experiment was conducted using Linger 2.92 (available at
http://telab.mit.edu/~dr/Linger/), a software platform designed by Doug Rohde for
language processing experiments. Participants were randomly paired and randomly
assigned to the role of questioner or answerer. Participants sat at computers in the same
room such that neither could see the other’s screen. On each trial, as illustrated in Figure
1 (right), the questioner saw a question (e.g., “What did Damon fry this morning?”)
which he/she was instructed to produce aloud for the answerer. The answerer was
instructed to produce an answer aloud using the information contained in the picture on
his/her screen (e.g., “Damon fried an omelet this morning”). The answerer was

Acoustic correlates of information structure 19
instructed to produce complete sentences, including the subject, verb, object, and
temporal abverb,4 and to emphasize the part of the sentence that the questioner had asked
about, or that he/she was correcting. On a random 20% of trials, the answerer was asked
a comprehension question about the answer s/he produced.
Productions were recorded in a quiet room with a head-mounted microphone at a
rate of 44kHz.
Acoustic Feature

Units

Description

duration

ms

Word duration excluding any silence before or after the word.

silence

ms

Duration of silence following the word, not due to stop closure.

duration+silence

ms

The sum of the duration of the word and any following silence.

mean F0

Hz

Mean F0 of the entire word

maximum F0

Hz

Maximum F0 value across the entire word

F0 peak location

0-1

The proportion of the way through the word where the maximum F0 occurs.

minimum F0

Hz

Minimum F0 across the entire word

F0 valley location

0-1

The proportion of the way through the word where the minimum F0 occurs.

initial F0

Hz

early F0

Hz

Mean F0 of the initial 5% of the word
Mean F0 value of 5% of the word centered at the point 25% of the way
through the word

center F0

Hz

late F0

Hz

Mean F0 value of 5% of the word centered on the midpoint of the word
Mean F0 value of 5% of the word centered on a point 75% of the way
through the word

final F0

Hz

Mean F0 of the last 5% of the word

1st quarter F0

The difference between initial F0 and early F0.

Hz

The difference between early F0 and center F0.

3rd quarter F0

Hz

The difference between center F0 and late F0.

4th quarter F0

Hz

The difference between late F0 and final F0.

mean intensity

dB

Mean intensity of the word

maximum intensity

dB

Maximum dB level in the word

minimum intensity
intensity peak
location
intensity valley
location

dB
0-1

Minimum dB level in the word
The proportion of the way through the word where the maximum intensity
occurs
The proportion of the way through the word where the minimum intensity
occurs

maximum amplitude

4

Hz

2nd quarter F0

Pascal

0-1

Maximum amplitude across the word

In the absence of explicit instruction to produce complete sentences, with a lexicalized subject, verb, and
object, speakers would likely resort to pronouns or would omit given elements altogether (e.g., “What did
Damon fry this morning?” “An omelet.”). A complete production account of information structure
meaning distinctions should include not just the prosodic cues used by the speakers, but also syntactic and
lexical production choices, as well as the interaction among these different production strategies. However,
because we focus on prosody in the current investigation, we wanted to be able to compare acoustic
features across identical words. Thus, we required that participants always produce a subject, verb, object
and adverb on every trial.

Acoustic correlates of information structure 20
energy

(Pascal)2 x
Duration

Table 2: Acoustic features extracted from each word in the target sentence for
Experiments 1-3. Stepwise discriminant analyses demonstrated that the measures in bold
provided the best discrimination among conditions and were used in all reported
analyses.
Results
Of the 504 speaker productions from the Question-Answer Experiment, 87 (17%) were
discarded because (a) the answerer failed to use the correct lexical items, (b) the answerer
was disfluent, or (c) the production was poorly recorded. The 417 remaining productions
were subjected to the acoustic analyses described below.
Acoustic Features
Based on previous investigations of prosody and information structure (Fry, 1955;
Lieberman, 1960; Eady et al., 1985; Cooper & Eady, 1986, Bartels & Kingston, 1994;
Krahmer & Swerts, 2001; Baumann et al., 2006), we chose a set of acoustic features to
analyze (see Table 2). These features were obtained automatically using the Praat
program (Boersma & Weenink, 2006). The measures of F0 computed over portions of
the words (e.g., 1st quarter F0) were chosen in order to investigate how F0 changes across
the syllable might contribute to the differentiation of conditions.
Our first goal was to determine which of the 24 candidate acoustic features
mediated differences among conditions. We conducted a series of stepwise linear
discriminant analyses5 on all of the data collected in Experiments 1, 2 and 3 reported in
the current paper. In order to determine the features to be used in the analyses of all three
experiments, we performed a separate stepwise analysis on the data from each
experiment separately. For each analysis we entered all 24 acoustic features across each
5

Linear discriminant analysis (LDA) calculates a function, computed as a linear combination of all
predictors entered, which results in the best separation of two or more groups. For two groups, only one
function is computed. For three groups, the first function provides the best separation of group 1 from
groups 2 & 3; a second, orthogonal, function provides the best separation of groups 2 and 3, after
partialling out variance accounted for by the first function. Stepwise LDA is an iterative procedure which
adds predictors based on which of the candidate predictors provide the best discrimination.

Acoustic correlates of information structure 21
of the three sentence positions (subject, verb, and object) as possible predictors of the
seven experimental conditions, resulting in 72 predictors. Across the three analyses, the
acoustic features which consistently resulted in the best discrimination of conditions were
(1) duration + silence, (2) mean F0, (3) maximum F0, and (4) maximum intensity at the
positions of the (a) Subject, (b) Verb, and (c) Object. The fact that these 12 features (four
acoustic features across three sentence positions) consistently discriminated among
conditions across three independent sets of productions (from different speakers and
across somewhat different sets of materials) serves as evidence that these features are
underlying speaker- and material-independent differentiation of information structure.
Therefore, we use only these 12 features in the linear discriminant analyses reported for
the individual experiments in the paper.
Computing Residual Values
Because of differences among individuals, including age, gender, speech rate and
level of engagement with the task, speakers produce very different versions of the same
sentence even within the same experimental condition, thus adding variance to the
acoustic features of interest. Similarly, there is likely to be variability associated with
different items due to lexical and world knowledge factors. Researchers have previously
dealt with the issue of acoustic variability between speakers by normalizing pitch and/or
duration by speaker (e.g., Shriberg, Stolcke, Hakkani-Tur, & Tur, 2000; Shriberg et al.,
1998; Wightman, Shattuck-Hufnagel, Ostendorf, & Price, 1992). In order to remove
speaker- and item-related variance in the current studies, we computed linear regression
models in which speaker (n = 18) and item (n = 28) predicted each of the 12 acoustic
features identified in the stepwise discriminant analyses described in the previous section.
From each of these models, we calculated the predicted value of each acoustic feature for
a specific item from a specific speaker. We then subtracted this predicted value from

Acoustic correlates of information structure 22
every production. The differences among the resulting residual values should reflect
differences in the acoustic features due only to the experimental manipulations. All
subsequently reported analyses were performed on these residual values.

Focus Location
The extent to which a discriminant function analysis can separate data points into two or
more groups is calculated with a statistical test, Wilks’s lambda6.
To determine how well the acoustic features could differentiate focus location in
speakers’ productions, we computed a model where the 12 acoustic predictors were used
to discriminate among three focus locations: Subject, Verb or Object. In this analysis, we
are averaging across the contrastive and non-contrastive condition for each location.
The overall Wilks’s lambda of the model was significant, Λ = .46, χ2(24) = 271, p
< .001, indicating better-than-chance differentiation of subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .84, χ2(24) = 62.65,
p < .001, indicating that the acoustic predictors could also differentiate verb focus from
object focus (see Figure 2). Leave-one-out classification correctly classified 67% of the
productions. The model correctly classified subject focus 76% of the time, verb focus
58% of the time, and object focus 66% of the time. Table 3 presents the standardized
canonical discriminant function coefficients of the model.7

6

Wilks's lambda is a measure of the distance between groups on means of the independent variables, and is
computed for each function. It ranges in size from 0-1; lower values indicate a larger separation between
groups. The extent to which the model can effectively discriminate a new set of data is simulated by a
leave-one-out classification, in which the acoustic data from each production are iteratively removed from
the dataset, the model is computed, and the left-out case is classified by the resultant functions.
7
The coefficients in Table 3 indicate which acoustic features best discriminate focus location, such that
larger absolute values indicate a greater contribution of that feature to discrimination. For example,
inspection of the plot in Figure 2 and the coefficients in the Focus Location columns of Table 3 shows that
the acoustic features of Damon score around zero, or lower, on the first function (-0.002, 0.001, -0.01, and 0.06) and around zero on the second function (-0.003, 0.021, -0.016, -0.101). Fried shows a different
pattern; specifically, the acoustic features of fried have coefficients around zero for the first function, and
negative coefficients for function 2. Finally, omelet shows a third pattern: its acoustic correlates are
centered around zero on Function 1, but are high on Function 2.

Acoustic correlates of information structure 23
Figure 3 graphically presents the mean values of the four features, demonstrating
that across all three focus locations the intended focus location is produced with the
highest maximum intensity, the longest duration and silence, and the highest relative F0.

Function
1

Function
2

Subj
Focus

Verb
Focus

Obj
Focus

omelet

Focus
Breadth

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.001
-0.006
0.002
-0.037

0.004
0.011
0.001
0.181

0.008
0.011
-0.002
-0.137

0.003
-0.014
0.002
-0.026

0.004
-0.019
0.006
0.189

0.003
0.000
0.003
0.199

fried

Focus Type

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

0.007
0.024
0.002
0.094

-0.001
-0.003
-0.002
-0.010

0.007
0.000
0.004
-0.076

0.002
-0.040
-0.007
0.131

-0.001
-0.013
0.013
-0.043

0.005
-0.025
0.003
0.011

Damon

Focus Location

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.002
0.001
-0.010
-0.060

-0.003
0.021
-0.016
-0.101

0.005
-0.016
-0.012
0.087

-0.002
-0.007
0.020
0.056

0.005
-0.014
-0.011
-0.225

0.003
0.007
-0.005
-0.123

Acoustic correlates of information structure 24
Table 3: Standardized canonical coefficients of the discriminant functions computed for
Experiment 1.

Figure 2: Separation of focus locations on two discriminant functions in Experiment 1.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the upper left quadrant; productions of verb focus are
clustered in the lower half of the plot; productions of object focus are clustered in the
upper right quadrant.

Acoustic correlates of information structure 25

Damon
fried
omelet

Figure 3: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 1.
Focus type
To determine how well the acoustic features could differentiate the type of focus
(i.e. non-contrastive vs. contrastive) in speakers’ productions, we computed three models
in which the 12 acoustic predictors were used to discriminate between two focus type
groups. The three models investigated differences between non-contrastive and
contrastive focus at the three focus locations: subject, verb, and object.
Focus Type – Subject Position
The overall Wilks’s Lambda was not significant, Λ = .898, χ2(12) = 11.95 p = .45,
indicating that the acoustic features could not discriminate between non-contrastive and

Acoustic correlates of information structure 26
contrastive focus. Because the overall model is not significant, we do not present the
scores of the specific acoustic features or the classification statistics here or in the
analyses below.
Focus Type – Verb Position
The overall Wilks’s Lambda was not significant, Λ = .851, χ2(12) = 17.92 p = .12,
indicating that the acoustic features could not discriminate between non-contrastive and
contrastive focus.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .82, χ2(12) = 22.63 p < .05,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus above chance level. Leave-one-out classification correctly classified
59% of the productions. The model correctly classified non-contrastive focus 59% of the
time, and contrastive focus 59% of the time.
The coefficients in the Object Focus column of Table 3 indicate that intensity and
mean F0 contribute most to classification. Figure 4 graphically presents the mean values
of the four features, demonstrating that contrastive focus is produced with a higher
maximum intensity, a longer duration and silence, and higher maximum F0. Noncontrastive focus is produced with a higher mean F0.

Acoustic correlates of information structure 27

Damon
fried
omelet

Figure 4: Values for non-contrastive focus and contrastive focus type on the four
discriminating acoustic features when the direct object “omelet” is focused in
Experiment 1.
Wide Focus vs. Narrow Focus
To determine how well the acoustic features could differentiate focus breadth, we
computed a model in which the 12 critical predictors were used to discriminate between
productions where the entire sentence was focused and productions where the object was
non-contrastively or contrastively focused.
The overall Wilks’s Lambda was significant, Λ = .75, χ2(12) = 47.83, p < .001,
indicating that the acoustic features could successfully discriminate between conditions
where the entire event is focused and conditions where the object is narrowly focused.

Acoustic correlates of information structure 28
Leave-one-out classification correctly classified 72% of the productions. The model
correctly classified wide focus 67% of the time, and narrow focus 74% of the time.
The standardized canonical discriminant function coefficients in the Focus
Breadth column of Table 3 indicate that maximum intensity contributes most to focus
breadth classification. Figure 5 graphically presents the mean values of the four features,
demonstrating that wide focus is produced with a more uniform duration + silence and
maximum F0 across the sentence than object focus. Wide focus is also produced with a
more uniform, though overall greater, intensity than object focus.

Damon
fried
omelet

Figure 5: Values for wide focus vs. narrow object focus on the four discriminating
acoustic features in Experiment 3.
Discussion

Acoustic correlates of information structure 29
Focus Location
The results demonstrate that speakers consistently provide acoustic cues which
disambiguate focus location. Specifically speakers indicated focus with increased
duration, higher intensity, higher mean F0, and higher maximum F0. Furthermore, these
results are consistent with the pattern reported in Eady & Cooper (1986), such that the
word preceding a focused word is less prominent (produced with shorter duration, lower
intensity and lower F0) than the focused word, and the word following the focused word
is less prominent than the word preceding the focused word. Previous studies (Eady et
al., 1986; Rump and Collier, 1986) have reported this reduction in acoustic prominence
following focused elements as being mainly indicated by lower F0 on the post-focal
words, though in our data we also find evidence of this reduction in measures of duration
and intensity.
Focus Type
The results from Experiment 1 indicate that in semi-naturalistic productions
speakers do not systematically differentiate between different focus types (focused
elements which have explicit contrast sets in the discourse and those which do not).
Specifically, at two out of three sentence positions, a discriminant function analysis could
not successfully classify speakers’ productions of contrastively vs. non-contrastively
focused elements. The observation that speakers successfully discriminated contrastive
and non-contrastive focus in object position, but not in subject or verb positions, is
perhaps suggestive, but is likely due to a lack of experimental power, a limitation which
will be addressed in Experiment 2.
Focus Breadth
The results from Experiment 1 demonstrate that speakers do systematically mark
focus breadth prosodically. Narrow object focus is produced with the highest maximum
F0, longest duration, and maximum intensity of the object noun, relative to the other

Acoustic correlates of information structure 30
words in the sentence. For wide focus, the acoustic features are more similar across the
sentence; only intensity and mean F0 are higher on the object than on the other words in
the sentence. These differences are subtle, but sufficient for the model to successfully
discriminate the productions.
The fact that the model failed to systematically classify productions by focus type
(with the exception of the object position), while achieving high accuracy in focus
location and focus breadth indicates that speakers were not marking focus type with
prosody in Experiment 1. However, the method used to elicit productions did not require
that subjects be aware of the information structure ambiguity of the materials. Evidence
from other production studies suggests that speakers may not prosodically disambiguate
ambiguous productions if they are not aware of the ambiguity. Albritton, McKoon, and
Ratcliff (1996), for example, demonstrated that speakers did not disambiguate
syntactically ambiguous constructions like “Dave and Pat or Bob” unless they were
aware of the ambiguity (see also Snedeker and Trueswell, 2003, but cf. Kraljic and
Brennan, 2005, and Schafer, Speer, Warren, and White, 2000, for evidence that speakers
do disambiguate syntactically ambiguous structures even in the absence of ambiguity
awareness). Experiment 2 was designed to be a stronger test of speakers’ ability to
differentiate focus location, focus type, and focus breadth. We used materials similar to
those in Experiment 1, with two important methodological modifications. First, instead
of producing the answers to questions with no feedback, the speaker’s task now involved
trying to enable the answerer to choose the question that s/he was answering from a set of
possible questions. Moreover, we introduced feedback so that the speaker would always
know whether his/her partner had chosen the correct answer. Second, we changed the
design from a between- to a within-subjects manipulation. This ensured that speakers

Acoustic correlates of information structure 31
were aware of the manipulation, as they were producing the same answer seven times
with explicit instructions to differentiate their answers for their partner.
In addition to making the speaker’s task explicit, the new design also allowed us
to analyze the subset of the productions for which the listeners could successfully identify
the question-type and which therefore contain sufficient information for differentiating
utterances along the three relevant dimensions of information structure.

Experiment 2
Method
Participants
Seventeen pairs of participants were recorded for this experiment. Subjects were MIT
students or members of the surrounding community. All reported being native speakers
of American English. None had participated in Experiment 1. Participants were paid for
their participation.
Materials
The materials had the same structure as those from Experiment 1, though the
critical words differed. Specifically, a larger set of names and a wider variety of
temporal adverbs were used, and some verbs and objects differed from Experiment 1.
Unlike Experiment 1, each subject pair was presented with all seven versions of each of
14 items, according to a full within-subjects within-items design. All materials can be
found in Appendix B.
Procedure
Two participants sat at computers in the same room such that neither could see the
other’s screen. One participant was the speaker, and the other was the listener. Speakers
were told that they would be producing answers to questions out loud for their partners

Acoustic correlates of information structure 32
(the listeners), and that the listeners would be required to choose which question the
speaker was answering from a set of seven choices.
At the beginning of each trial, the speaker was presented with a question on the
computer screen to read silently. After pressing a button, the answer to the question
appeared below the question, accompanied by a reminder to the speaker that s/he would
only be producing the answer aloud, and not the question. Following this, the speaker
had one more chance to read the question and answer, and then he/she was instructed to
press a key to begin recording (after being told by the listener that he/she is ready), to
produce the answer, and then to press another key to stop recording.
The listener sat at another computer, and pressed a key to see the seven questions
that s/he would have to choose his/her answer from. When s/he felt familiar with the
questions, s/he told the speaker s/he was ready. After the speaker produced a sentence
out loud for the listener, the listener chose the question s/he thought the speaker was
answering. If the listener answered incorrectly, his/her computer produced a buzzer
sound, like the sound when a contestant makes an incorrect answer on a game show.
This cue was included to ensure that speakers knew when their productions did not
contain enough information for the listener to choose the correct answer.8
Results – Production
Two speaker-listener pairs were excluded as the Listener did not achieve
comprehension accuracy greater than 20%. One further pair was excluded as one
member was not a native speaker of American English. Finally, another pair of subjects
was excluded because they did not take the task seriously, and produced unnaturally
emphatic contrastive accents, often shouting the target word, and laughing while doing

8

In early pilots in which there was no feedback for incorrect responses, we observed that listeners were at
chance in choosing the correct question.

Acoustic correlates of information structure 33
so. These exclusions left a total of 13 pairs of participants whose responses were
analyzed.
Sixty-seven of the 1274 trials (5%) were excluded because (a) the speaker failed
to produce the correct words, (b) the speaker was disfluent, or (c) the production was
poorly recorded. Analyses were performed on all trials, and on the subset of trials for
which the listener correctly identified the question. The results were very similar in the
two analyses. For brevity of presentation, we present results from analyses conducted on
the correct trials (n = 660, 55%). The productions from Experiment 2 were analyzed
using the acoustic features chosen in the feature-selection procedure described in
Experiment 1. All analyses were performed on the residual values of these features, after
removing speaker and item variance with the method described in Experiment 1.
Focus Location
The overall Wilks’s lambda was significant, Λ = .085, χ2(24) = 1335, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .306, χ2(11) = 641, p
< .001, indicating that the acoustic features could also discriminate verb focus from
object focus (see Figure 6).
Leave-one-out classification correctly classified 93% of the productions. For
individual levels of focus location, the discriminant function correctly classified subject
focus 94% of the time, verb focus 90% of the time, and object focus 95% of the time.
The standardized canonical coefficients in the first two columns of Table 4
indicate that the acoustic features contributing most to the discrimination of focus
location are once again mean F0 and maximum intensity, though the other two features
are also contributing. In fact, inspection of the acoustic feature means in Figure 7

Acoustic correlates of information structure 34
demonstrate that the highest value of every acoustic feature is associated with the
intended focused item, with the exception of mean F0 when the subject is focused.

Figure 6: Separation of focus locations on two discriminant functions for Experiment 2.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the lower left quadrant of the plot; productions of verb
focus are clustered in the lower right quadrant; productions of object focus are clustered
in the lower half.
Focus Location

Focus Type

Focus Breadth

Function 2

Subject
Focus

Verb Focus

Object
Focus

Duration+ silence

omelet

Function 1
-0.001

0.004

0.004

0.006

0.003

0.003

Mean F0

-0.006

0.011

-0.003

0.005

-0.023

0.000

Maximum F0

0.002

0.001

0.004

-0.009

-0.003

0.003

Maximum Intensity

-0.025

0.183

-0.052

-0.171

0.012

0.199

Acoustic correlates of information structure 35
-0.002

0.006

0.002

-0.007

0.005

Mean F0

0.024

-0.005

0.001

-0.022

0.006

-0.025

Maximum F0

0.001

-0.002

-0.007

0.001

0.003

0.003

0.093

-0.016

-0.105

0.063

-0.084

0.011

Duration+ silence

Damon

0.007

Maximum Intensity

fried

Duration+ silence

-0.002

-0.002

0.002

0.005

0.009

0.003

Mean F0

0.003

0.021

-0.010

0.004

-0.009

0.007

Maximum F0

-0.011

-0.015

-0.014

-0.012

-0.006

-0.005

Maximum Intensity

-0.067

-0.097

0.094

-0.014

0.010

-0.123

Table 4: Standardized canonical coefficients of all discriminant functions computed for
Experiment 2.

Damon
fried
omelet

Figure 7: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 2.

Acoustic correlates of information structure 36
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .633, χ2(12) = 81.41, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
75% of the productions. The model correctly classified non-contrastive focus 78% of the
time, and contrastive focus 71% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that maximum intensity at all three locations (i.e. large intensity differences between the
subject and verb and the subject and object) contributes most to classification. Figure 8
graphically presents the mean values of the four features, demonstrating that, in addition
to intensity differences, contrastive focus is produced with longer duration and silence, as
well as lower mean and maximum F0.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .654, χ2(12) = 72.27, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
72% of the productions. The model correctly classified non-contrastive focus 70% of the
time, and contrastive focus 75% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that, once again maximum intensity contributes most to classification. Figure 9
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a higher maximum intensity, and a longer duration and silence,
than non-contrastive focus. Once again, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

Acoustic correlates of information structure 37
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .793, χ2(12) = 41.3, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
67% of the productions. The model correctly classified non-contrastive focus 69% of the
time, and contrastive focus 66% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that contrastive focus is most strongly associated with lower mean F0. Figure 10
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a lower mean and maximum F0 than non-contrastive focus.

Damon
fried
omelet

Figure 8. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 2.

Acoustic correlates of information structure 38

Damon
fried
omelet

Figure 9. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 2.

Acoustic correlates of information structure 39

Damon
fried
omelet

Figure 10. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “omelet” is focused in Experiment 2.
Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .59, χ2(12) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 84% of productions; wide
focus was correctly classified 77% of the time, and object focus was correctly classified
88% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 4
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Although intensity is contributing most strongly
to classification, inspection of the acoustic means in Figure 11 indicates that wide focus

Acoustic correlates of information structure 40
is marked by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is marked by greater prominence on the
object, reflected in longer duration, higher F0, and higher intensity.

Damon
fried
omelet

Figure 11: Values for wide vs. narrow object focus on the four discriminating acoustic
features in Experiment 2.

Acoustic correlates of information structure 41
Results – Perception

Figure 12. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 2.
Listeners’ choices of question sorted by the intended question are plotted in
Figure 12. Listeners’ overall accuracy was 55%. To determine whether listeners were
able to determine the speaker’s intended sentence meaning, we compared each subject's
responses to chance performance. Specifically we assessed, for focus location and focus
type, whether each subject's proportion of correct responses exceeded chance; wide
focus productions were excluded from the analysis, so that chance performance for focus

Acoustic correlates of information structure 42
location was .33, and chance performance for focus type was .5. Results demonstrated
that listeners were able to successfully identify focus location: all 13 subjects’
performance significantly exceeded chance performance, p = .05, two-tailed. However,
listeners were unable to successfully identify focus type: only three of 13 subjects
performed at above-chance levels (based on the binomial distribution), p = .05, twotailed. To investigate focus breadth, we assessed, for wide focus and narrow object focus
separately, whether each subject's proportion of correct responses exceeded chance. For
these analyses, we excluded subject and verb focus productions, so that chance
performance was .33 for wide focus, and .67 for narrow object focus. Results
demonstrated that listeners were moderately successful at identifying focus breadth: six
of 13 subjects identified wide focus at rates above chance, and nine out of 13 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.
Discussion
The production results replicated the two main findings from Experiment 1, and provided
evidence for acoustic discrimination of focus type across sentence positions as well.
First, these results demonstrated that focused elements have longer durations than nonfocused elements, incur larger F0 excursions, are more likely to be followed by silence,
and are produced with greater intensity. Second, speakers consistently differentiate
between wide and narrow focus by producing the object in the latter case with higher F0,
longer duration, and greater intensity. Specifically, although object focus was indicated
by increased duration, higher intensity, and higher F0 on the object than on the subject or
the verb, wide focus was indicated by comparatively greater duration, higher intensity,
and higher F0 on the subject and the verb, and shorter duration, lower intensity, and
lower F0 on the object. These results are consistent with those obtained by Baumann et

Acoustic correlates of information structure 43
al. (2006), who demonstrated that narrow focus on an element was indicated with longer
duration and a higher F0 peak than wide focus on an event encompassing that element.
Most importantly, although speakers in Experiment 1 did not differentiate
conditions with and without an explicit contrast set for the focused element (except for
the object position), these conditions were differentiated by speakers in Experiment 2, at
every syntactic position. There are two possible interpretations of this difference. First,
in Experiment 1, speakers produced only four versions of each of the seven conditions,
whereas speakers in Experiment 2 and 3, reported below, produced 14 versions of each of
the seven conditions, resulting in greater power in the latter two experiments. The fact
that, in Experiment 2, speakers successfully discriminated contrastive and noncontrastive focus in all three positions, suggests that the lack of such an effect in
Experiment 1 could be due to a lack of power.
As mentioned above, the difference in the findings between Experiments 1 and 2
is also consistent with results from Allbritton et al. (1996) and Snedeker and Trueswell
(2003) who demonstrated that speakers do not disambiguate syntactically ambiguous
sentences with prosody unless they are aware of the ambiguity. The current results
demonstrate a similar effect for acoustic prominence, such that speakers do not
differentiate two kinds of acoustically prominent elements (contrastively vs. noncontrastively focused elements) unless they are aware of the information structure
ambiguity in the structures they are producing.
The discriminant analyses indicated that contrastively focused words were
produced with longer durations and higher intensity than non-contrastively focused
words, but that non-contrastively focused words were produced with higher F0 than
contrastively focused words. This latter finding is surprising when compared to some
previous studies. For example, Ladd & Morton (1997) found that higher F0 and larger

Acoustic correlates of information structure 44
F0 range is perceived as more ‘emphatic’ or ‘contrastive’ by listeners. Similarly, Ito and
Speer (2008) demonstrated that contrastively focused words were produced with higher
F0 than non-contrastive ones. Given the unexpected results, we inspected individual
pitch tracks to more closely observe the F0 patterns across the entire utterances. The
pitch tracks presented in Figure 13 were generated from the productions of a typical
speaker, and they exemplify the higher F0 observed for non-contrastive focus than
contrastive focus in the subject position (A vs. B) and verb position (C vs. D).
Contrastive focus on the object is realized with the same F0 as non-contrastive focus on
the object (E vs. F).

300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Non-Contrastive

Given

an

omelet

yesterday

Given

0

1.433
Time (s)

Acoustic correlates of information structure 45
A. Non-contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Contrastive

an

Given

omelet

yesterday

Given

0

1.81
Time (s)

B. Contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Non-Contrastive

omelet

yesterday

Given

0

1.514
Time (s)

C. Non-contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Contrastive

an

omelet

<SIL>

yesterday

Given

0

2.377
Time (s)

Acoustic correlates of information structure 46
D. Contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Given

omelet

yesterday

Non-Contrastive
1.582

0
Time (s)

E. Non-contrastive Object Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Given

an

omelet

yesterday

Contrastive
2.188

0
Time (s)

F. Contrastive Object Focus
Figure 13. Pitch tracks for non-contrastive and contrastive subject focus, non-contrastive
and contrastive verb focus, and non-contrastive and contrastive object focus,
respectively, from a typical speaker from Experiment 2.
Note that our finding that non-contrastive focus is realized with higher F0 than
contrastive focus is still consistent with the claim that contrastive focus is more
prominent than non-contrastive focus. As the graphs in Figures 8-10, and the pitch tracks
in Figure 13 indicate, although contrastive elements were consistently produced with
lower pitch, they were also consistently produced with longer durations and greater

Acoustic correlates of information structure 47
intensity than non-contrastive elements. As reviewed in the introduction, there is
9

evidence that intensity and duration can convey prominence more effectively than higher
pitch (Fry, 1954, Lieberman, 1960, Beckman, 1986; Turk & Sawusch,1996; Kochanski et
al., 2005). Our data are therefore consistent with prior claims that contrastive focus is
produced with greater prominence than non-contrastive focus.
As discussed in the introduction, the production elicitation and analysis methods
used in the current experiment are more robust than methods used in many previous
studies, including those whose results are inconsistent with the current findings. In
particular, the current results are based on productions from naïve subjects in a
communication task, and the analyses were performed on data with speaker and item
variability removed. The current results are therefore more likely to reflect the
underlying generalizations about the relationship between acoustics and meaning.
The perception results only partially mirrored the production results. Consistent
with the production results, listeners were highly successful in discriminating among the
three focus locations. In contrast to the production results, however, listeners were only
moderately successful in identifying focus type (non-contrastive vs. contrastive) from the
speakers’ productions. In fact, listeners most often confused non-contrastive focus with
contrastive focus (see Figure 12). These results suggest that, even though speakers may
be consistently signaling focus type with their prosody, listeners are not able to exploit
those cues for comprehension.
With regard to focus breadth, the perception results are incompatible with a strong
version of the focus projection hypothesis (Selkirk, 1995). According to this hypothesis,
an acoustic prominence on the object NP can be interpreted as marking the entire clause
9

Importantly, the F0 results are not artifacts of the residualization procedure employed to remove variance
from the acoustic features due to speaker and item. The same numerical pattern of F0 values is observed
whether residualization is employed or not, though only the residualized acoustic features successfully
discriminate focus type.

Acoustic correlates of information structure 48
as focused. Listeners are therefore predicted to treat a production with an acoustically
prominent object NP as ambiguous between the narrow object focus reading and the wide
focus reading. However, as can be seen in Figure 12, listeners correctly identified narrow
object non-contrastive focus 57% of the time, interpreting it as wide focus only 13% of
the time, and correctly identified narrow object contrastive focus 49% of the time,
interpreting it as wide focus only 6% of the time. These results are not consistent with
Gussenhoven’s (1983) finding that listeners cannot reliably distinguish between narrow
objects focus and wide focus.
Experiments 1 and 2 provide evidence that speakers systematically indicate focus
location and focus breadth using a set of four acoustic features. These experiments
further suggest that speakers can, but don’t always, indicate focus type. In particular, the
results suggest that speakers only prosodically differentiate contrastive from noncontrastive focus when they are aware of the meaning ambiguity and/or when the task
involves conveying a particular meaning to a listener.
To further investigate the speakers’ ability to prosodically differentiate contrastive
from non-contrastive focus, we conducted an additional experiment. Acoustic analyses
in Experiments 1 and 2 were limited to three words (i.e. subject, verb, object) in the
sentence. However, in natural productions, speakers’ utterances are often prefaced by
attribution expressions (e.g., “I think” or “I heard”), or expressions of emotional attitudes
towards the described events (e.g., “Unfortunately”, or “Luckily”). It is therefore
possible that contrastive information might be partially conveyed by prosodically
manipulating these kinds of expressions. We explored this possibility in Experiment 3, in
which we had speakers produce target SVO constructions with a preamble. Experiment 3
was also intended to serve as a replication of the results of Experiment 2; in particular,

Acoustic correlates of information structure 49
the somewhat unexpected finding that non-contrastive focus is produced with higher F0
than contrastive focus.

Experiment 3
Method
Participants
Fourteen pairs of participants (speakers and listeners) were recorded for this
experiment. Subjects were MIT students or members of the surrounding community. All
reported being native speakers of American English. None had participated in
Experiments 1 or 2. Participants were paid for their participation.
Materials
The materials for Experiment 3 were identical to those from Experiment 1
described above with the exception that an attribution expression (“I heard that”) was
appended to the beginning of each target sentence.
Procedure
The procedure for Experiment 3 was identical to that for Experiment 2.
Results – Production
Four speaker-listener pairs were excluded as the listener did not achieve
comprehension accuracy greater than 20%. These exclusions left a total of 10 pairs of
participants whose responses were analyzed. Eighty-one of the 980 recorded trials (8%)
were excluded because (a) the speaker failed to produce the correct words, (b) the
speaker was disfluent, or (c) the production was poorly recorded. Analyses were
performed on all trials, and on the subset of trials for which the listener correctly
identified the question the speaker produced the sentence in response to. As in
Experiment 2, the results were very similar for the two analyses. For brevity of
presentation, we present results from analyses conducted on the correct trials (n = 632,
70%).

Acoustic correlates of information structure 50
Focus Location
In order to investigate the contribution of the prosody of “I heard that” to the
differentiation of the focus type in Experiment 2, we performed a stepwise discriminant
function analysis which included as predictors measures of the four acoustic features we
had selected initially (duration + silence, mean F0, maximum F0, maximum intensity) (1)
for the subject (“Damon”), verb (“fried”), and object (“omelet”), and (2) for each of the
first three words of the sentence (“I”, “heard”, “that”). Of the 24 predictors included in
the stepwise discriminant function analysis, the features which resulted in the best
discrimination of focus type were (1) the duration + silence of “I”, (2) the maximum F0
of “I”, and (3) the maximum intensity of “I”. Based on these results, we conducted an
additional analysis in which we included a subset of 16 predictors: the duration + silence,
mean F0, maximum F0, and maximum intensity of the subject, verb, object, and “I”.
As in Experiments 1 and 2, we conducted a discriminant analysis to determine
whether the measures of (1) duration + silence, (2) maximum F0, (3) mean F0, and (4)
maximum intensity of the four critical words in the sentence could predict focus location.
The overall Wilks’s lambda was significant, Λ = .058, χ2(32) = 1467.09, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .275, χ2(15) =
664.75, p < .001, indicating that the acoustic features could also discriminate verb focus
from object focus (Figure 14). Leave-one-out classification procedure correctly
classified 97% of the productions. At individual focus locations, the model correctly
classified subject focus 96% of the time, verb focus 97% of the time, and object focus
97% of the time.

Acoustic correlates of information structure 51

Figure 14. Separation of focus locations on two discriminant functions for Experiment 3.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the left half of the plot; productions of verb focus are
clustered in the lower right quadrant; productions of object focus are clustered in the
upper right quadrant.
Focus Location

Focus
Breadth

Focus Type

fried

omelet

Function
1

Function
2

Subject
Focus

Verb
Focus

Object
Focus

0.002

0.003

0.000

0.000

0.003

0.002

0.005

0.012

-0.013

-0.009

0.005

-0.010

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

0.003

0.000

0.005

0.006

-0.003

0.003

0.069

0.106

-0.037

-0.011

0.007

0.151

0.001

-0.003

0.000

0.002

0.001

0.005

0.025

-0.021

-0.001

-0.002

-0.001

-0.006

Maximum F0
Maximum
Intensity

-0.005

-0.002

-0.001

-0.005

0.000

-0.003

0.091

-0.077

-0.086

-0.015

0.027

-0.048

Duration+
silence
Mean F0

Acoustic correlates of information structure 52
Duration+
silence
Mean F0

Damon

0.000

0.002

0.000

0.000

0.002

0.011

0.011

-0.011

-0.020

0.019

-0.003

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

-0.014

-0.003

-0.001

0.007

-0.014

-0.008

-0.147

0.011

0.159

-0.006

-0.064

-0.123

0.000

0.000

0.004

0.005

0.005

-0.001

-0.005

0.000

-0.013

-0.008

-0.003

-0.009

Maximum F0
Maximum
Intensity

I

-0.003

0.004

-0.002

0.017

0.010

0.014

0.005

-0.021

-0.017

0.142

0.133

0.126

0.014

Table 5: Standardized canonical coefficients of all discriminant functions computed for
Experiment 3.

I
Damon
fried
omelet

Figure 15. Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 3.

Acoustic correlates of information structure 53
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .39, χ2(16) = 157.44, p<.001,
indicating that the acoustic features could successfully discriminate between noncontrastive and contrastive focus. Leave-one-out classification correctly classified 85%
of the productions. The model correctly classified non-contrastive focus 85% of the time,
and contrastive focus 85% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that maximum intensity overall, and specifically, maximum intensity on “I,” is
contributing most to classification. Figure 16 graphically presents the mean values of the
four features, demonstrating that, in addition to intensity differences, contrastive focus is
produced with longer duration and silence, and with lower mean and maximum F0.

Acoustic correlates of information structure 54

I
Damon
fried
omelet

Figure 16. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 3.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .46, χ2(16) = 139.28, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
80% of the productions. The model correctly classified non-contrastive focus 86% of the
time, and contrastive focus 74% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that, intensity on “I” is contributing the most to classification. Figure 17 graphically

Acoustic correlates of information structure 55
presents the mean values of the four features, demonstrating that contrastive focus is
produced with a higher maximum intensity, and a longer duration and silence, than noncontrastive focus. As in Experiment 2, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

I
Damon
fried
omelet

Figure 17. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 3.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .40, χ2(16) = 133.37, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and

Acoustic correlates of information structure 56
contrastive focus better than chance. Leave-one-out classification correctly classified
83% of the productions. The model correctly classified non-contrastive focus 89% of the
time, and contrastive focus 76% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that intensity and mean F0 on “I” are contributing the most to accurate classification.
Figure 18 graphically presents the mean values of the four features, demonstrating that
contrastive focus is produced with a higher mean and maximum F0 than non-contrastive
focus.

I
Damon
fried
omelet

Acoustic correlates of information structure 57
Figure 18: Values for non-contrastive vs. contrastive focus on the four discriminating
acoustic features when “omelet” is focused in Experiment 3.

Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .48, χ2(16) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 87% of productions; wide
focus was correctly classified 79% of the time, and object focus was correctly classified
92% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 5
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Specifically, greater intensity on the object is a
strong predictor of object focus; less intensity on the subject and the verb are strong
predictors of wide focus. Although intensity is contributing most strongly to
classification, inspection of the acoustic means in Figure 19 indicates that wide focus is
indicated by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is indicated by greater prominence on
the object, reflected in longer duration, higher F0, and higher intensity.

Acoustic correlates of information structure 58

I
Damon
fried
omelet

Figure 19. Values for wide vs narrow object focus on the four discriminating acoustic
features in Experiment 3.
Results – Perception
Listeners’ overall accuracy percentage by condition is plotted in Figure 20.
Listeners’ overall accuracy was 70%. As described in Experiment 2, we compared each
subject's responses to chance performance. Results demonstrated that listeners were able
to successfully identify focus location, as all 10 subjects’ performance significantly
exceeded chance performance, p = .05, two-tailed. Listeners were moderately successful
at discriminating focus type, as six of 10 subjects’ performance exceeded chance levels, p
= .05, two-tailed. Listeners successfully identified focus breadth as eight out of 10

Acoustic correlates of information structure 59
subjects identified wide focus at rates above chance, and eight out of 10 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.

Figure 20. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 3.

Discussion
Experiment 3 was conducted in order to (1) investigate whether speakers could
differentiate focus type with prosody if the sentences contained an attribution expression

Acoustic correlates of information structure 60
that could convey contrastive information, in addition to the elements that describe the
target event, and (2) replicate the results of Experiment 2.
With regard to the second goal, the production results of Experiment 3
successfully replicated the findings from Experiments 1 and 2. As in Experiments 1 and
2, speakers systematically differentiated focus location and focus breadth with a
combination of duration, intensity, and F0 cues. Furthermore, as in Experiment 2, noncontrastive focus was produced with higher F0 than contrastive focus (though only when
the subject or verb was focused), and contrastive focus was always produced with greater
duration and intensity. As discussed above, these F0 results contrast with prior findings
(Bartels & Kingston, 1994; Couper-Kuhlen, 1984; Ladd & Morton, 1997; Ito & Speer,
2008), but can be interpreted in light of more recent evidence that higher intensity is a
stronger cue to greater prominence than higher pitch (Kochanski et al., 2005).
In addition, results from Experiment 3 demonstrated that the strongest cues to
discrimination of focus type were the acoustics of “I” (from the attribution expression “I
heard that”). Specifically, in contrastive focus conditions, the word “I” was produced
with longer duration, higher intensity, and higher mean F0 and maximum F0. Indeed,
discrimination of focus type in Experiment 3 was far better than in Experiment 2. It
therefore appears that speakers can manipulate prosody on sentence elements outside of
the target clause (e.g., in attribution expressions) to convey contrastiveness.
The perception results demonstrated that listeners could accurately determine
focus location, similar to the results of Experiment 2. Furthermore, listeners were more
accurate in determining focus type than listeners in Experiment 2. This increase in
accuracy was likely due to speakers’ tendency to prosodically mark “I” in the contrastive
conditions.

Acoustic correlates of information structure 61

General Discussion
The three experiments reported in the current paper explored the ways in which focus
location, focus type, and focus breadth are conveyed with prosody. In each experiment,
naïve speakers and listeners engaged in tasks in which the information status of sentence
elements in SVO sentences was manipulated via preceding questions. The prosody of the
target sentences was analyzed using a series of classification models to select a subset
from the set of acoustic features that would best be able to discriminate among focus
locations and between focus types. In addition, in Experiments 2 and 3, the production
results were complemented by the perception results that demonstrated listeners’ ability
to use the prosodic cues in the speakers’ utterances to arrive at the intended meaning.
At the beginning of the paper, we posed three questions about the relationship
between acoustics and information structure: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? We are now in a position to answer these questions.
First, we have demonstrated that speakers systematically provide prosodic cues to
the location of focused material. Across all three experiments, speakers provided cues to
focus location whether or not the task explicitly demanded it, across subject, verb and
object positions. In addition, across all three experiments, speakers systematically
provided cues to focus breadth, such that wide focus was prosodically differentiated from
narrow object focus. Finally, we found that speakers can, but don’t always, prosodically
differentiate contrastive and non-contrastive focus. Specifically, speakers did not
prosodically differentiate focus type in Experiment 1, but they did so in Experiment 2
and, even more strongly, in Experiment 3. As discussed above, the fact that speakers did
not differentiate focus type in Experiment 1, where they were plausibly not aware of the

Acoustic correlates of information structure 62
meaning ambiguity, but did differentiate between contrastive and non-contrastive focus
conditions in Experiments 2 and 3, where the task made the meanings more salient, is
consistent with results from the literature on intonational boundary production
demonstrating that speakers only produce disambiguating boundaries when they are
aware of the syntactic ambiguity which could be resolved by the presence of a boundary
(Albritton et al., 1996; Snedeker & Trueswell, 2003; cf. Schafer, et al., 2000 and Kraljic
& Brennan, 2005). Furthermore, the results from Experiment 3, where the critical words
were preceded by the attribution expression “I heard that,” demonstrated even stronger
differentiation of focus type than in Experiment 2, suggesting that speakers are able to
convey contrastiveness using words outside of the clause containing the contrastivelyfocused element.
To answer the question of which acoustic features are associated with different
meaning categories of information structure, we conducted a series of discriminant
function analyses with the goal of objectively identifying which of 24 measures of
duration, intensity, and F0 allowed for the best discrimination of conditions. Across all
experiments, and across different sentence positions, the best differentiation among
conditions was achieved using the following four features: word duration, maximum
word intensity, mean F0, and maximum F0. These results are consistent with many
previous studies in the literature, implicating these features in conveying aspects of
information structure. An important contribution of the current studies is that these
results were obtained using a quantitative analysis across many naïve speakers and items,
and are therefore more likely to be generalizable.
These data also demonstrate how exactly these four features are used in
conveying different aspects of information structure. With regard to focus location,
focused material is produced with longer duration, higher F0, and greater intensity than

Acoustic correlates of information structure 63
non-focused material. With regard to focus type, non-contrastive focus is realized with
higher mean and maximum F0 on the focused word than contrastive focus, whereas
contrastive focus is realized with greater intensity on the focused word than noncontrastive focus. Finally, with regard to focus breadth, narrow focus on the object is
indicated by higher F0 and longer duration on the object, compared to wide focus, and
wide focus is conveyed by higher intensity and F0, and longer duration on pre-focal
words.
To answer the question of how well listeners can retrieve prosodic information
from the signal, we included a perception task in Experiments 2 and 3. When the
relevant acoustic cues were present in the input (as demonstrated by successful
classification by the models), listeners were also able to classify the utterances, although
not quite as successfully as the models. Furthermore, the fact that the model always
achieved high classification accuracy suggests that the utterances contained enough
acoustic information to make these discriminations, and that we did not leave any
particularly informative acoustic features out of the analyses.

Implications for theories of the mapping of acoustics to meaning
While our production and perception results are compatible with a direct
relationship between acoustics and meaning, they are also consistent with the existence of
mediating phonological categories, as in the intonational phonology framework. For
example, a standard assumption within intonational phonology is that there is a
phonological category “accent” mediating acoustics and semantic focus, such that a
focused element is accented, and an unfocused element is unaccented (e.g., Brown,
1983). Our production and perception results are compatible with this assumption. First,
if speakers are signaling focus location by means of placing acoustic features

Acoustic correlates of information structure 64
corresponding to a +accent category on focused elements, then we would expect to see
strong acoustic differences between focused and given elements, as we have observed.
Moreover, if listeners perceive accents categorically, then we would predict successful
discrimination of productions on the basis of focus location, as we have observed.
Second, when the object is focused, it will be accented, resulting in higher acoustic
measures on the object compared to other positions, as we have observed. Furthermore,
in the wide focus condition, the subject, verb, and object – all of which are focused –
would all receive accents, and would therefore be more acoustically similar to one
another than they are in the wide focus condition. This difference in accent placement
would lead to successful discrimination between wide and narrow focus by listeners, as
we have observed. Finally, there has been much debate in the intonational phonology
literature about whether there is a phonological category +/- contrastive. The results of
our experiments are perhaps best explained without such a category. In particular, if
speakers accent focused elements without differentiating between contrastive and noncontrastive focus, then we would expect similar acoustic results between productions
which differ only on focus type, which would lead to poor discrimination by the model.
Moreover, listeners would not be successful in discriminating focus type, as we have
observed. Our experimental results are thus compatible with an intonational
phonological approach which includes an accent category mediating acoustics and
meaning, but no category for contrastiveness. Importantly, although our results do not
support a categorical difference between non-contrastive and contrastive focus, they do
not exclude the possibility that speakers can mark these distinctions with relative
differences in prominence (Calhoun, 2006).

Acoustic correlates of information structure 65
Implications for semantic theories of information structure
The current results are relevant to two open questions in the semantics of
information structure: (1) whether contrastive and non-contrastive focus constitute two
distinct categories; and (2) whether focus on the object of a verb can project to the entire
verb phrase.
As described in the introduction, Rooth (1992) proposed an account of focus which
makes no distinction between non-contrastive focus and contrastive focus. (6) shows the
F-marking (focus-marking) that Rooth’s account would assign to the conditions in
Experiments 1 and 2. Importantly, words and phrases which evoke alternatives, either
explicit or implicit, are considered focused (i.e. F-marked).
(6)
a. Subject, Subject Contrast: DamonF fried an omelet last night.
b. Verb, Verb Contrast:

Damon friedF an omelet last night.

c. Object, Object Contrast: Damon fried an omeletF last night.
d. Wide:

[Damon fried an omelet] F last night.

Our results provide tentative support for Rooth’s proposal that F-marked constituents do
not differ substantively as a function of whether the alternatives they evoke are explicit
(our contrastive condition) or implicit (our non-contrastive condition). Although
speakers differentiated these two conditions acoustically, they only did so when the
contrast between the conditions was made salient (Experiments 2 and 3). Moreover, even
when speakers did mark this distinction, listeners were unable to consistently use this
information to recover the intended meaning (Experiment 2). These results suggest that
there are no consistent semantic differences between foci with explicit alternatives in the
discourse and those with implicit alternatives.

Acoustic correlates of information structure 66
The second semantic issue that these results bear upon is whether narrow focus on
the object can project to the entire verb phrase. According to the theory of focus
projection proposed in Selkirk (1984, 1995), an acoustic prominence on the direct object
(omelet) can project focus to the entire verb phrase (fried an omelet) and then up to the
entire clause/sentence. Gussenhoven (1983, 1999) makes a similar claim. Both Selkirk’s
and Gussenhoven’s accounts therefore predict that a verb phrase with a prominence on
the object would be ambiguous between a narrow object focus interpretation and a wide
focus interpretation. Neither the production nor the perception results were consistent
with this prediction. In production, speakers distinguished between narrow object focus
and wide focus, and in perception, listeners were able to distinguish these two conditions.
One aspect of the production results (the acoustic realization of the subject) for the
narrow object focus and wide focus conditions is, however, predicted by both Selkirk and
Gussenhoven’s accounts. In particular, in the wide focus condition, the subject
constitutes new information while in the narrow object focus condition the subject is
given. Selkirk & Gussenhoven both predict that the subject would be more acoustically
prominent in the wide focus condition than in the narrow object focus condition. This is
exactly what we observed (especially in Experiments 1 and 3). Nevertheless, as
discussed above, speakers also systematically disambiguated wide focus from narrow
object focus across all three experiments with their realization of the object and the verb.
Specifically, wide focus was produced with stable or increasing duration, intensity, and
F0 across the subject, verb, and object; narrow object focus, on the other hand, was
characterized by shorter duration and lower intensity and F0 on the subject and verb,
followed by a steep increase in each of these values on the object.
Similar to our production findings, Gussenhoven (1983) found that, at least in some
productions, wide focus differed from narrow object focus in that the verb was more

Acoustic correlates of information structure 67
prominent under wide focus. Listeners, however, were unable to use this acoustic
information to distinguish wide focus from narrow object focus. Gussenhoven took this
result as evidence that the two conditions are not reliably distinguished (consistent with
his theory). Our results did not replicate this production/perception asymmetry: Listeners
are able to successfully classify productions with a single prominence on omelet as
indicating narrow object focus and did not confuse these productions with those from the
wide focus condition.
Methodological contributions
A further contribution of the current research to investigations of prosody and
information structure is methodological. With regard to the methods used to elicit
productions, we utilized multiple, untrained speakers to ensure that our results are
generalizeable to all speakers and are not due to speakers’ prior beliefs about what pattern
of acoustic prominence signals a particular meaning (see Gibson & Fedorenko, in press,
for similar arguments with respect to linguistic judgments). Furthermore, unlike most
previous work in which productions were selected for analysis based on perceptual
differentiability or on ratings of the appropriateness of prosodic contours, we elicited and
selected for analysis productions using a meaning task. Thus our analyses were based on
the communicative function of language. Finally, we did not exclude speakers based on
our perceptions of their productions; speakers were excluded for failure to provide
information to their listeners.
The analyses used here also constitute an improvement over previous analyses.
First, using discriminant modeling, we were able to simultaneously investigate the
contribution of multiple sentence elements to acoustic differentiation of conditions.
Second, we demonstrated that residualization is a useful method for controlling for
variability among speakers and lexical items. For example, preliminary analyses

Acoustic correlates of information structure 68
performed on the productions from Experiment 2 without first computing residual values
of the acoustic features revealed a 13% average increase in values of Wilks’ lambda
(where lower values indicate better discrimination) and a 7% average decrease in
classification accuracy. Third, the discriminant modeling proved successful in
objectively determining which acoustic features were the biggest contributors to
differences among conditions. The success of the analyses used in the current studies is
encouraging for future investigations of prosodic phenomena previously considered too
variable for study in a laboratory setting with naïve speakers.
One question that arises from the current set of studies is, to what extent the
current results can be generalized to all speakers and all sentences. In production studies,
there is always a trade-off between (1) having enough control over what participants are
producing to ensure sufficient data for analysis, and (2) ensuring that the speech is as
natural as possible. In Experiment 1, we attempted to elicit natural productions, but
failed to find systematic differences between focus types. In making the speakers’ task—
to help their listeners choose the correct question-type—explicit, we may have also
encouraged speakers to produce these sentences with somewhat exaggerated prosody.
Further experiments will be necessary to determine whether speakers normally produce
contrastive meanings in this way.
In conclusion, the current studies used rigorous scientific methods to explore
several important questions about the acoustic correlates of information structure. By
providing some initial answers to these questions, along with some implications for
semantic theory, and by offering a novel, objective way to approach these and other
questions, these studies open the door to future investigations of the relationship between
acoustics and meaning.

Acoustic correlates of information structure 69

References
Albritton, D., McKoon, G., & Ratcliff, R. (1996) Reliability of Prosodic Cues for
Resolving Syntactic Ambiguity. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22, 714-735.
Bartels, C., Kingston, J., (1994). Salient pitch cues in the perception of contrastive focus.
In Boach, P., Van der Sandt, R. (Eds.), Focus & Natural Language Processing, Proc. of J.
Sem. conference on Focus. IBM Working Papers. TR-80, pp. 94–106.
Baumann, S., Grice, M., and Steindamm, S. (2006). Prosodic Marking of Focus Domains
- Categorical or Gradient? In Proceedings of Speech Prosody, Dresden, Germany, pp.
301-304.
Beckman, Mary E. (1986). Stress and Non-Stress Accent. Netherlands Phonetic Archives
Series No. 7. Foris.
Beckman, M., & Ayers Elam, G. (1997). Guidelines for ToBI labeling, version 3: Ohio
State University.
Beckman, M., Hirschberg, J., & Shattuck-Hufnagel, S. (2005). The original ToBI system
and the evolution of the ToBI framework. In S.-A. Jun (Ed.), Prosodic Typology: The
Phonology of Intonation and Phrasing (pp. 9-54): Oxford University Press.
Birch, S. and Clifton, C. (1995) Focus, accent, and argument structure: effects on
language comprehension. Language and Speech, 38 (4), 365-391.
Birner, B. (1994). Information status and Word Order: An Analysis of English Inversion.
Language, 70 (2), 233-259.
Boersma, Paul & Weenink, David (2006). Praat: doing phonetics by computer (Version
4.3.10) [Computer program]. Retrieved June 3, 2005, from http://www.praat.org/
Bolinger, D. (1961). Contrastive accent and contrastive stress. Language, 37, 83-96.
Breen, M., Dilley, L., Gibson, E., Bolivar, M., and Kraemer, J. (2006) Advances in
prosodic annotation: A test of inter-coder reliability for the RaP (Rhythm and Pitch) and
ToBI (Tones and Break Indices) transcription systems. Poster presented at the 19th
CUNY Conference on Human Sentence Processing, New York, NY. March, 2006.
Brown, G. (1983). Prosodic structures and the Given/New distinction. In D. R. Ladd &
A. Cutler (Eds.), Prosody:Models and measurements (pp. 67–77). Berlin: Springer.
Calhoun, S (2004). Phonetic Dimensions of Intonational Categories - the case of L+H*
and H*. In Proceedings of Speech Prosody, Nara, Japan, pp. 103-106.
Calhoun, S. (2005). It's the difference that matters: An argument for contextuallygrounded acoustic intonational phonology. In Linguistics Society of America Annual
Meeting, Oakland, California, January 2005.
Calhoun, S. (2006) Information Structure and the Prosodic Structure of English: a
Probabilistic Relationship. PhD thesis, University of Edinburgh.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics and points of
view. In Charles N. Li, editor, Subject and Topic, pages 27-- 55. Academic Press, 1976.
Clark, E. V., & Clark, H. H. (1978). Universals, relativity, and language processing. In: J.
H. Greenberg (Ed.), Universals of human language, Vol. I. (pp. 225–277). Stanford:
Stanford University Press.

Acoustic correlates of information structure 70
Cooper, W., Eady, S. & Mueller, P. (1985). Acoustical aspects of contrastive stress in
question-answer contexts. Journal of Acoustical Society of America, 77(6), 2142-2156.
Couper-Kuhlen, E. (1984). A new look at contrastive intonation., Modes of
Interpretation: Essays Presented to Ernst Leisi, Watts, R., Weidman, U. (Eds.) Gunter
Narr Verlag, 137–158.
Cutler, A. (1977). The Context-Independence of "Intonational Meaning". Chicago
Linguistic Society (CLS 13), 104-115.
Dilley, L. C. (2005). The phonetics and phonology of tonal systems. Unpublished Ph.D.
Dissertation, MIT.
Dilley, L. C., & Brown, M. (2005). The RaP (Rhythm and Pitch) Labeling System,
Version 1.0: Available at http://tedlab.mit.edu/rap.html.
Eady, S. J., & Cooper, W. E. (1986). Speech intonation and focus location in matched
statements and questions. Journal of the Acoustical Society of America, 80, 402-415.
Féry, C. and Krifka, M. (2008). Information Structure: Notional Distinctions, Ways of
Expression. In Piet van Sterkenburg (ed.), Unity and diversity of languages, Amsterdam:
John Benjamins, 123-136.
Fry, D. B. (1955). Duration and Intensity as Physical Correlates of Linguistic Stress.
Journal of the Acoustical Society of America, 27, 765–768.
Gibson, E. & Fedorenko, E. (In press). Weak quantitative standards in linguistics
research. Trends in Cognitive Sciences.
Gussenhoven, C. (1983). Testing the reality of focus domains. Language and Speech, 26,
61–80.
Gussenhoven, C. (1999). On the limits of focus projection in English. In P. Bosch & R.
van der Sandt (Eds.), Focus: Linguistic, cognitive, and computational perspectives (pp.
43 –55). Cambridge, U.K.: Cambridge University Press.
Gussenhoven, C., Repp, B. H., Rietveld, A., Rump, W. H. & J. Terken, J. (1997). The
perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society
of America, 102, 3009-3022.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hawkins, S. & Warren, P. (1991). Factors affecting the given-new distinction in speech.
In Proceedings of the 12th International Congress of Phonetic Sciences, Aix en
Provence. 66-69.
Ito, K & Speer, S. (2008). Anticipatory effects of intonation: Eye movements
during instructed visual search. Journal of Memory and Language, 58, 541-573.
Ito, K. Speer, S. R. and Beckman, M. E. (2004). Informational status and pitch accent
distribution in spontaneous dialogues in English, In Proceedings of the International
Conference on Spoken Language Processing, Nara: Japan, 279-282.
Jackendoff, R. (1972). Semantic interpretation in generative grammar. Cambridge: MIT
Press.
Jaeger, T. F. (2008). Categorical Data Analysis: Away from ANOVAs (transformation or
not) and towards Logit Mixed Models. Journal of Memory and Language. 59, 434–446.

Acoustic correlates of information structure 71
Kochanski, G., Grabe, E., Coleman, J., & Rosner, B. (2005) Loudness predicts
prominence: fundamental frequency lends little. The Journal of the Acoustical Society of
America, 118 (2), 1038-1054.
Krahmer, E., & Swerts, M. (2001). On the alleged existence of contrastive accents.
Speech Communication, 34, 391-405.
Kraljic, T. & Brennan, S. E. (2005). Prosodic disambiguation of syntactic structure: For
the speaker or for the addressee? Cognitive Psychology 50: 194-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge Studies in Linguistics 79.
Cambridge: Cambridge University Press.
Ladd, D. R. & Morton, R. (1997). The perception of intonational emphasis: continuous or
categorical? Journal of Phonetics, 25, 313–342.
Lambrecht, K. (2001). A framework for the analysis of cleft constructions. Linguistics,
39, 463–516.
Lieberman, P. (1960). Some acoustic correlates of word stress in American English. The
Journal of the Acoustical Society of America, 32(4), 451-454.
Molnar, V. (2002). Information Structure in a Cross-linguistic Perspective. In Hilde
Hasselgård, Stig Johansson, Bergljot Behrens, Cathrine Fabricius-Hansen (Eds.),
Language and Computers, Vol. 39, 147-161(15).
Paul, H. (1880), Prinzipien der Sprachgeschichte, Leipzig.
Pierrehumbert, J.B. (1980). The phonology and phonetics of English intonation.
Unpublished dissertation, MIT.
Pierrehumbert, J. & Hirschberg, J. (1990). The Meaning of Intonational Contours in the
Interpretation of Discourse. In P. R. Cohen & J. Morgan & M. E. Pollack (eds.).
Intentions in Communication. Cambridge/MA: MIT Press, 271-311.
Pierrehumbert, J. & Steele, S. (1989). Categories of tonal alignment in English.
Phonetica, 46, 181-196.
Pitrelli, J., Beckman, M. & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International
Conference on Spoken Language Processing, 123-126.
Rietveld, A. C. M., and Gussenhoven, C. (1985). On the relation between pitch excursion
size and prominence. Journal of Phonetics, 13, 299-308.
Rochemont, M. S. (1986). Focus in Generative Grammar. Amsterdam/Philadelphia: John
Benjamins.
Rooth, M. (1985). Association with Focus. PhD thesis, University of Massachusetts
Amherst.
Rooth, M. (1992). A theory of focus interpretation. Natural Language Semantics, 1, 75 –
116.
Rump, H. H., and Collier, R. (1996). ‘Focus conditions and the prominence of pitchaccented syllables. Language and Speech, 39, 1–17.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational disambiguation
in sentence production and comprehension. Journal of Psycholinguistic Research, 29,
169-182.

Acoustic correlates of information structure 72
Selkirk, E. (1984). Phonology and syntax: The relation between sound and structure.
Cambridge, MA: MIT.
Selkirk, E. (1995). Sentence Prosody: Intonation, Stress, and Phrasing. In: J.Goldsmith
(ed.). The Handbook of Phonological Theory. Oxford: Blackwell, 550-569.
Schwarzchild, R. (1999) GIVENness, AvoidF and other Constraints on the Placement of
Accent. Natural Language Semantics, 7, 141–177.
Shriberg, E., Stolcke, A., Hakkani-Tur, D. & Tur, G. (2000). Prosody-Based Automatic
Segmentation of Speech into Sentences and Topics. Speech Communication, 32, 127-154.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D., Ries, K., Coccaro, N.,
Martin, R., Meteer, M., & Van Ess-Dykema, C. (1998). Can Prosody Aid the Automatic
Classification of Dialog Acts in Conversational Speech? Language and Speech, 41:3-4,
439-487.
Silverman, K. E. A., Beckman, M., Pierrehumbert, J., Ostendorf, M., Wightman, C. W.
S., Price, P., et al. (1992). ToBI: A standard scheme for labeling prosody. In Proceedings
of the 2nd International Conference on Spoken Language Processing (pp. 867-879).
Banff.
Sluijter, A. and van Heuven, V. (1996). Spectral balance as an acoustic correlate of
linguistic stress. Journal of the Acoustical Society of America, 100, 2471–2485.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects of
speaker awareness and referential contest. Journal of Memory and Language, 48, 103–
130.
Stalnaker, R. (2002). Common ground. Linguistics and Philosophy, 25: 701–721.
Syrdal, A. and McGory, J. (2000). Inter-transcriber reliability of ToBI prosodic labeling.
In Proceedings of the International Conference on Spoken Language Processing,
Beijing: China, 235-238.
Terken, J. (1991). Fundamental frequency and perceived prominence accented syllables.
Journal of the Acoustical Society of America, 89, 1768–1776.
't Hart, J. Collier, R. & Cohen, A. (1990). A perceptual study of intonation. Cambridge
University Press, Cambridge.
Turk, A. & Sawusch, J. (1996) The processing of duration and intensity cues to
prominence. Journal of the Acoustical Society of America, 99, 3782-3790.
Welby, P. (2003). Effects of pitch accent position, type, and status on focus projection.
Language and Speech, 46, 53 – 81.
Wightman, C. W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P. J. (1992). Segmental
durations in the vicinity of prosodic phrase boundaries. Journal of the Acoustical Society
of America, 91(3), 1707-1717.
Xu, Y. & Xu, C. X. (2005). Phonetic realization of focus in English declarative
intonation, Journal of Phonetics, 33, 159–197.
Yoon, T., Chavarria, S., Cole, J., & Hasegawa-Johnson, M. (2004). Intertranscriber
reliability of prosodic labeling on telephone conversation using ToBI. In Proceedings of
the International Conference on Spoken Language Processing., Nara: Japan, 2729-2732.

Acoustic correlates of information structure 73

Appendix A
Experiment 1 items
Full items are recoverable as follows: Question A is always “What happened last night?”
Questions B, C, & D are wh-questions about the subject, verb, and object, respectively.
Questions E, F, & G are questions which introduce the explicit alternative subject, verb,
or object, indicated in parentheses.
1.

Question A: What happened last night?
Question B: Who fed a bunny last night?
Question C: What did Damon do to a bunny last night?
Question D: What did Damon feed last night?
Question E: Did Jenny feed a bunny last night?
Question F: Did Damon pet a bunny last night?
Question G: Did Damon feed a baby last night?
Response: Damon fed a bunny last night.

2. Damon (Lauren) caught (pet) a bunny (a squirrel) last night.
3. Damon (Molly) burned (break) a candle (a log) last night.
4. Darren (Lauren) cleaned (eat) a carrot (a chicken) last night.
5. Darren (Molly) peeled (eat) a carrot (a potato) last night.
6. Darren (Nora) found (buy) a diamond (a ring) last night.
7. Darren (Jenny) sold (lose) a diamond (a sapphire) last night.
8. Jenny (Damon) found (lose) a dollar (a quarter) last night.
9. Jenny (Darren) sewed (rip) a dolly (a blanket) last night.
10. Jenny (Logan) read (open) an email (a letter) last night.
11. Jenny (Nolan) smelled (plant) a flower (a skunk) last night.
12. Lauren (Darren) burned (write) a letter (a magazine) last night.
13. Lauren (Logan) mailed (open) a letter (a package) last night.
14. Lauren (Nolan) read (write) a novel (a newspaper) last night.
15. Lauren (Damon) fried (bake) an omelet (a chicken) last night.
16. Logan (Molly) peeled (chop) an onion (an apple) last night.
17. Logan (Nora) fried (chop) an onion (a potato) last night.
18. Logan (Jenny) cleaned (buy) a pillow (a rug) last night.
19. Molly (Logan) dried (wash) a platter (a bowl) last night.
20. Molly (Nolan) sold (find) a platter (a vase) last night.
21. Molly (Damon) poured (drink) a smoothie (a cocktail) last night.
22. Nolan (Nora) pulled (push) a stroller (a sled) last night.
23. Nolan (Jenny) bought (sell) a stroller (a wheelbarrow) last night.
24. Nolan (Lauren) sewed (knit) a sweater (a quilt) last night.
25. Nora (Nolan) killed (trap) a termite (a cockroach) last night.
26. Nora (Damon) changed (wash) a toddler (a baby) last night.
27. Nora (Darren) fed (dress) a toddler (a bunny) last night.
28. Nora (Logan) pulled (push) a wagon (a wheelbarrow) last night.

Acoustic correlates of information structure 74

Appendix B
Items used for Experiments 2-3
Full items are recoverable as follows: Question A always asks “What happened _____?”
where the blank corresponds to the temporal adverb. Questions B, C, & D are whquestions about the subject, verb, and object, respectively. Questions E, F, & G are
questions which introduce the explicit alternative subject, verb, or object, indicated in
parentheses.
1a.
1b.
1c.
1d.
1e.
1f.
1g.

Context: What happened yesterday?
Context: Who fried an omelet yesterday?
Context: What did Damon do to an omelet yesterday?
Context: What did Damon fry yesterday?
Context: Did Harry fry an omelet yesterday?
Context: Did Damon bake an omelet yesterday?
Context: Did Damon fry a chicken yesterday?
Target: No, Damon fried an omelet yesterday.

2. (I heard that) (No,) Megan (Jodi) sold (lose) her diamond (her sapphire) yesterday.
3. (I heard that) (No,) Mother (Daddy) dried (wash) a platter (a bowl) last night.
4. (I heard that) (No,) Norman (Kelly) read (write) an email (a letter) last night.
5. (I heard that) (No,) Lauren (Judy) poured (drink) a smoothie (a cocktail) this morning.
6. (I heard that) (No,) Nora (Jenny) sewed (rip) her dolly (her blanket) this morning.
7. (I heard that) (No,) Molly (Sarah) trimmed (wax) her eyebrows (her hair) on Tuesday.
8. (I heard that) (No,) Nolan (Steven) burned (break) a candle (a log) on Tuesday.
9. (I heard that) (No,) Logan (Billy) killed (trap) a termite (a cockroach) last week.
10. (I heard that) (No,) Radar (Fido) caught (lick) a bunny (a squirrel) last week.
11. (I heard that) (No,) Darren (Maggie) pulled (push) a stroller (a sled) on Sunday.
12. (I heard that) (No,) Brandon (Tommy) peeled (eat) a carrot (a potato) on Sunday.
13. (I heard that) (No,) Maren (Debbie) cleaned (buy) a pillow (a rug) on Friday.
14. (I heard that) (No,) Lindon (Kelly) fooled (fight) a bully (a teacher) on Friday.

Acoustic correlates of information structure 1

Acoustic correlates of information structure
Mara Breen1, Evelina Fedorenko2, Michael Wagner3, Edward Gibson2
1
2

University of Massachusetts Amherst
Massachusetts Institute of Technology
3
McGill University

June 7, 2010
Address correspondence to:
Mara Breen
522 Tobin Hall
University of Massachusetts
Amherst, MA
01003
mbreen@psych.umass.edu

Acoustic correlates of information structure 2
Abstract
This paper reports three studies aimed at addressing three questions about the acoustic
correlates of information structure in English: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? The information structure of subject-verb-object (SVO)
sentences was manipulated via the questions preceding those sentences: elements in the
target sentences were either focused (i.e. the answer to a wh-question) or given (i.e.
mentioned in prior discourse); furthermore, focused elements had either an implicit or an
explicit contrast set in the discourse; finally, either only the object was focused (narrow
object focus) or the entire event was focused (wide focus). The results across all three
experiments demonstrated that people reliably mark (a) focus location (subject, verb, or
object) using greater intensity, longer duration, and higher mean and maximum F0, and
(b) focus breadth, such that narrow object focus is marked with greater intensity, longer
duration, and higher mean and maximum F0 on the object than wide focus. Furthermore,
when participants are made aware of prosodic ambiguity present across different
information structures, they reliably mark focus type, so that contrastively-focused
elements are produced with higher intensity, longer duration, and lower mean and
maximum F0 than non-contrastively focused elements. In addition to having important
theoretical consequences for accounts of semantics and prosody, these experiments
demonstrate that linear residualization successfully removes individual differences in
people’s productions thereby revealing cross-speaker generalizations. Furthermore,
discriminant modeling allows us to objectively determine the acoustic features that
underlie meaning differences.

Acoustic correlates of information structure 3

Introduction
An important component of the meaning of a sentence is its relationship to the context in
which it is produced. Some parts of speakers’ sentences refer to information already
under discussion, while other parts convey information that the speaker is presenting as
new for the listener. Depending on the context, the same sentence can convey different
kinds of information to the listener. For example, consider the three contexts in (1a)-(1c)
for the sentence in (2):

(1) a. Who fried an omelet?
b. What did Damon do to an omelet?
c. What did Damon fry?

(2) Damon fried an omelet.

The event of frying an omelet is already made salient in the context in (1a), and
this part of the answer is therefore given. Consequently, the sentence Damon fried an
omelet conveys Damon as the new or focused information.1 Similarly, the verb fried is
the focused information relative to the context in (1b), and the object noun phrase an
omelet is the focused information relative to the context in (1c). This component of the
meaning of sentences - the differential contributions of different sentence elements to the

1

Numerous terms are used in the literature to refer to the distinction between the information that is old for
the listener and the information that the speaker is adding to the discourse: background and foreground;
given and new; topic and comment; theme and rheme, etc. In this paper, we will use the term given to refer
to the parts of the utterance which are old to the discourse, and focused to refer to the part of the utterance
which is new to the discourse.

Acoustic correlates of information structure 4
overall sentence meaning in its relation to the preceding discourse - is called information
structure.
Three components of information structure have been proposed in the literature:
givenness, focus, and topic (see e.g., Féry and Krifka, 2008, for a recent summary). The
current paper will be concerned with givenness and focus.2 Given material is material
that has been made salient in the discourse, either explicitly, like the event corresponding
to the verb fried and the object corresponding to the noun omelet in (1a), or implicitly, via
inferences based on world knowledge (e.g., mentioning omelet makes the notion of
“eggs” given, Schwarzchild, 1999).
Focused material is what is new to the discourse, or in the foreground. The focus
of a sentence can often be understood as the part that corresponds to the answer to the
wh-part of wh-questions, like Damon in (2) as an answer to (1a) (Paul, 1880; Jackendoff,
1972).
There are two dimensions along which focused elements can differ. The first is
contrastiveness. A contrastively focused element, like Damon in (3b), indicates that the
element in question is one of a set of explicit alternatives or serves to correct a specific
item already present in the discourse, as in the following:

(3) a. Did Harry fry an omelet yesterday?
b. Damon fried an omelet yesterday.

Unlike (1a), where there is no explicit set of individuals from which Damon is being
selected as the “omelet fryer”, in (3a) an explicit alternative “omelet fryer” is being

2

Topic, the third component of information structure, describes which discourse referent focused
information should be associated with, as in the mention of Damon in “As for Damon, he fried an omelet.”
The current studies do not address the prosodic realization of topic.

Acoustic correlates of information structure 5
introduced: Harry. The sentence (3b) in this context thus presents information (i.e.,
Damon) which explicitly contrasts with, or contradicts, some information which has been
introduced into the discourse.
There is no consensus in the literature regarding the relationship between noncontrastive focus and contrastive focus. Some researchers have treated non-contrastive
focus and contrastive focus as separate categories of information structure (Chafe, 1976;
Halliday, 1967; Rochemont, 1986; Molnar, 2002), whereas others have argued that there
is no principled difference between the two (e.g., Bolinger 1961, Rooth, 1985, Rooth,
1992). According to Rooth (1992), for example, each expression evokes two semantic
representations: the expression’s actual meaning, and a set of alternatives. If a
constituent in the expression is focused, then the alternative set contains the expression
itself and all expressions with an alternative substituted for the focus-marked constituent;
if there is no focus within the expression, the alternative set consists only of the
expression itself. Rooth would therefore argue that Damon in (1a) is focused and
introduces alternative propositions that differ only in the agent of the event ({Damon
fried an omelet, Harry fried an omelet, Ada fried an omelet, ...}), even if no alternatives
are explicitly mentioned. In (3a), Damon also evokes alternative omelet fryers, and
therefore has the same focus structure as (1a), but the context makes a specific alternative
(Harry) more salient than other potential alternatives. Importantly, from Rooth’s
standpoint, it does not matter whether the alternatives are explicit in the discourse or not:
the meaning of the expression is the same.
The second dimension along which focused elements can vary is focus breadth
(Selkirk, 1984; 1995; Gussenhoven, 1983; 1999), which refers to the size of the set of
focused elements. Narrow focus refers to cases where only a single aspect of an event
(e.g., the agent, the action, the patient, etc.) is focused, whereas wide focus focuses an

Acoustic correlates of information structure 6
entire event. Take, for example, the difference between (5) as an answer to (4a) versus as
an answer to (4b):

(4) a. What did Damon fry last night?
b. What happened last night?

(5) Damon fried an omelet last night.

(4a) narrowly focuses the patient of frying, omelet in (5), while (4b) widely focuses the
entire event of Damon frying an omelet.
The information status of a sentence element can be conveyed in at least three
ways: (1) using word order (i.e., given information generally precedes focused
information) (e.g., Birner, 1994, Clark & Clark, 1978); (2) using particular lexical items
and syntactic constructions (e.g., using cleft constructions such as “It was Damon who
fried an omelet”) (Lambrecht, 2001); and (3) using prosody. Prosody – which we focus
on in the current paper – refers to the way in which words are grouped in speech, the
relative acoustic prominence of words, and the overall tune of an utterance. Prosody is
comprised of acoustic features like fundamental frequency (F0), duration, and loudness,
the combinations of which give rise to the psychological percepts like phrasing
(grouping), stress (prominence), and tonal movement (intonation).
The goal of the current paper is to investigate the prosodic realization of
information structure in simple English subject-verb-object (SVO) sentences like (2),
with the goal of addressing the following questions:
1) First, do speakers prosodically distinguish focused and unfocused elements?
This question can be broken down into further questions:

Acoustic correlates of information structure 7
(1a) Do speakers distinguish focused elements that have an explicit contrast
set in the discourse from those that do not?
(1b) Do speakers distinguish sentences in which only the object is focused
from those in which the entire event is focused?
(2) What are the acoustic features associated with these different aspects of
information structure?
(3) How well can listeners retrieve this information from the signal?

Although the current experiments are all performed on English, the answers to
these questions will likely be similar for other West Germanic languages. However, the
relationship between prosodic features and information structure across different
languages and language groups remains an open question.
In the remainder of the introduction, we briefly lay out two approaches to the study
of the relationship between prosody and information structure, and summarize empirical
studies which have explored how information structure is realized acoustically and
prosodically. We then discuss methodological issues present in previous studies which
call into question the generalizeability of the reported findings, and outline how the
current methods were designed to better address these questions.
Empirical investigations of prosody and information structure
Two perspectives on the relationship between the acoustics of the speech signal and
the meaning associated with various aspects of information structure have been
articulated in the literature. According to the direct-relationship approach, sets of
acoustic features are directly associated with particular meanings (Fry, 1955; Lieberman,
1960; Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986; Pell, 2001; Xu & Xu,
2005). In contrast, according to the indirect-relationship approach (known as the

Acoustic correlates of information structure 8
intonational phonology framework), the relationship between acoustics and meaning is
mediated by phonological categories (Ladd, 1996; Gussenhoven, 1983; Pierrehumbert,
1980; Dilley, 2005; Hawkins & Warren, 1991). In particular, the phonetic prosodic cues
are hypothesized to be grouped into prosodic categories which are, in turn, associated
with particular meanings. The experiments in the current paper were not designed to
decide between these two approaches. However, In the current paper, we will initially
discuss our experiments in terms of the direct-relationship approach, because it is more
parsimonious. In the general discussion, we will show how the results are also
compatible with the indirect-relationship approach.
Turning now to previous empirical work on the relationship between prosody and
information structure, we start with studies of focused vs. given elements. Several
studies have demonstrated that focused elements are more acoustically prominent than
given elements. However, there has been some debate about which acoustic features
underlie a listener’s perception of acoustic prominence. Some features that have been
proposed to be associated with prominence include pitch (i.e. F0) (Lieberman, 1960;
Cooper, Eady & Mueller, 1985; Eady and Cooper, 1986), duration (Fry, 1954; Beckman,
1986), loudness (i.e. intensity) (Kochanski, Grabe, Coleman, & Rosner, 2005; Beckman,
1986; Turk and Sawusch, 1996), and voice quality (Sluijter & van Heuven, 1996).
In early work on lexical stress, Fry (1954) and Liberman (1960) argued that
intensity and duration of the vowel of the stressed syllable contributed most strongly to
the percept of acoustic prominence, such that stressed vowels were produced with a
greater intensity and a longer duration than non-stressed vowels. In experiments on
phrase-level prominence, Cooper et al. (1985) and Eady and Cooper (1986) also noted
that more prominent syllables are longer than their non-prominent counterparts. Cooper
et al. (see also Liberman, 1960); Rietveld & Gussenhoven, 1985; Gussenhoven et al.,

Acoustic correlates of information structure 9
1997; and Terken, 1991) also argued that F0 was a highly important acoustic feature
underlying prominence. Others have argued that the strongest cue to prominence is
intensity (e.g., Beckman, 1986). More recently, Turk and Sawusch (1996) also found
that intensity (and duration) were better predictors of perceived prominence than pitch, in
a perception task. Finally, in a study of spoken corpora, Kochanski et al. (2005)
demonstrated that loudness (i.e. intensity) was a strong predictor of labelers’ annotations
of prominence, while pitch had very little predictive power.
The question of whether contrastively and non-contrastively focused elements are
prosodically differentiated by speakers, and perceptually differentiated by listeners has
also been extensively debated. Some have argued that there is no difference in the
acoustic features associated with contrastively vs. non-contrastively focused elements
(Cutler, 1977; Bolinger, 1961; t’Hart, Collier, & Cohen, 1990), while others have argued
that some acoustic features differ between contrastively vs. non-contrastively focused
elements (Couper-Kuhlen, 1984; Krahmer & Swerts, 2001; Bartels & Kingston, 1994;
Ito, Speer, & Beckman, 2004). For example, Couper-Kuhlen (1984) reported, on the
basis of corpus work, that speakers produce contrastive focus with a steep drop after a
high F0 target, while high F0 is sustained after non-contrastive focus (see also Krahmer
and Swerts, 2001). However, this finding is in contrast to Bartels and Kingston (1994),
who have argued, based on a series of production studies, that the most salient acoustic
cue to contrastiveness is the height of the peak on a contrastive word, such that a higher
peak is associated with a greater probability of an element being interpreted as
contrastive (see also Ladd and Morton, 1997). Finally, Ito, Speer, & Beckman (2004)
demonstrated that speakers are more likely to use a L+H* accent (i.e. a steep rise from a
low target to a high target), compared to a H* accent (i.e. a gradual rise to a high target),
to indicate an element that has an explicit contrast set in the discourse.

Acoustic correlates of information structure 10
Krahmer and Swerts (2001) observed that listeners were more likely to perceive a
contrastive adjective (e.g., red in red square preceded by blue square) as more prominent
than a new adjective when the adjective was presented with a noun compared to when it
was presented in isolation. They therefore hypothesized that the lack of a consensus in
the literature may be due to the failure of the earlier studies to investigate focused
elements in relation to the prosody of the surrounding elements. Consistent with this
idea, Calhoun (2005) demonstrated that a model’s ability to predict a word’s information
status is significantly improved when information about the acoustics of adjacent words
is included in the model. These results suggest that a more consistent picture of the
acoustic features associated with contrastively and non-contrastively-focused elements
may emerge if acoustic context is taken into account.
Finally, prior work has investigated whether speakers prosodically differentiate
narrow and wide focus. Selkirk (1995), for example, argued that, through a process
called focus projection, an acoustic prominence on the head of a phrase or its internal
argument can project to the entire phrase, thus making the entire phrase focused (see also
Selkirk, 1984; see Gussenhoven, 1983, 1999, for a similar claim). According to Selkirk
(1984) and Gussenhoven (1983) then a clause containing a transitive verb in which the
direct object is acoustically prominent is ambiguous between a reading where the object
alone is focused and a reading where the entire verb phrase is focused. This hypothesis
has been supported in several perception experiments (Welby, 2003; Birch & Clifton,
1995; Gussenhoven, 1983). Welby (2003), for example, demonstrated that listeners rated
a sentence like I read the DISPATCH with a single acoustic prominence on dispatch as a
similarly felicitous response to either a question narrowly focusing the object (i.e. “What
newspaper do you read?”), or a question widely focusing the entire event (i.e. “How do
you keep up with the news?”). However, Gussenhoven (1983) found that at least in some

Acoustic correlates of information structure 11
productions there is actually a perceptible difference between narrow and wide focus
although listeners cannot use this information to reliably tell in which context the
sentence was uttered (see Baumann et al., 2006, for evidence from German showing that
speakers do differentiate between narrow and wide focus, with prosodic cues varying
across speakers). In contrast to Gussenhoven’s perception results, Rump and Collier
(1986) found that listeners can accurately discriminate narrow and wide focus using pitch
cues.
Limitations of previous work
Although the studies summarized above provide evidence for some systematic
differences in the acoustic realization of different aspects of information structure, no
clear picture has yet emerged with regard to any of the three meaning distinctions
discussed above (i.e. focused vs. given elements, non-contrastively focused vs.
contrastively focused elements, and narrow vs. wide focus). Furthermore, previous
studies suffer from several methodological limitations that make the findings
inconclusive. Here, we discuss five limitations of previous studies which the current
studies seek to address in an effort to reveal a clearer picture of the relationship between
acoustic features and information structure.
First, instead of acoustic features, sometimes only ToBI3 annotations are
provided (e.g., Birch & Clifton, 1995; Ito et al., 2004). This includes work of researchers
who adopt the intonational phonology framework and who therefore believe that using
prosodic annotation offers a useful way to extrapolate away from potentially complex
interactions among acoustic features which give rise to the perception of specific
intonational patterns. One particular problem concerns H* and L+H* accents. As
defined in the ToBI system, these accents are meant to be explicit markers of non3

The (ToBI) Tones and Break Indices system was developed in the early 90s as the standard system for
annotation of prosodic features (Silverman et al., 1992).

Acoustic correlates of information structure 12
contrastive focus and contrastive focus, respectively (Beckman & Ayers-Elam, 1997).
However, H* and L+H* are often confused in ToBI annotations (Syrdal & McGory,
2000), and are, in fact, often collapsed in calculating inter-coder agreement (Pitrelli et al.,
1994; Yoon et al., 2004; Breen et al., 2006, submitted). Therefore, it is difficult to
interpret the results of studies which are based on the difference between H* and L+H*
without a discussion of the acoustic differences between these purported categories. In
the current studies, we report acoustic features in order to avoid confusion about what the
ToBI labels might mean and in order to not presuppose the existence of prosodic
categories associated with particular meaning categories of information structure.
A second limitation concerns the method used to generate and select productions
for analysis. A common practice involves eliciting productions from a small number of
speakers (e.g., Baumann et al., 2006; Krahmer & Swerts, 2001), which results in a
potential decrease in experimental power, and could therefore lead to a Type II error. In
addition, several previous experiments have excluded speakers’ data from analysis for not
producing accents consistently (e.g., Eady & Cooper, 1986; Cooper et al., 1985), which
could lead to a Type I error. For the current experiments, we recruited between 13 and
18 speakers. In addition, no speakers’ productions were excluded from the analyses
based on a priori predictions about potential behavior (e.g., placing accents in particular
locations).
A third limitation concerns the tasks used in perception studies. In particular,
some studies asked listeners to make judgments about which of two stimuli was more
prominent (Krahmer & Swerts, 2001), what accent is acceptable in a particular context
(Birch & Clifton, 1995; Welby, 2003), or with which of two questions a particular answer
sounded more natural (Gussenhoven, 1983). The problem with these meta-linguistic
judgments is that they lack a measure of the participants’ interpretation of the sentences.

Acoustic correlates of information structure 13
In the current studies we employ a more natural production-comprehension task, in which
speakers are trying to communicate a particular meaning of a semantically ambiguous
sentence and listeners are trying to understand the intended meaning.
A fourth limitation of previous studies is in how they have dealt with speaker
variability. Presenting data from individual subjects separately, as is commonly done, is
problematic because it fails to capture the shared aspects of individual productions (e.g.,
consistent use by most speakers of some set of acoustic features to mark focused
elements). In the current studies, we combine data across subjects while simultaneously
removing variance due to individual differences using linear regression modeling (e.g.,
Jaeger, 2008).
A fifth limitation is that many have reported differences between conditions based
only on individual acoustic features on single words (Eady & Cooper, 1986; Cooper et
al., 1985; Baumann et al, 2006). If acoustic prominence is perceived in a contextdependent manner, these single-feature/single-word analyses might find spurious
differences, or fail to find real differences. In the current studies, we used discriminant
modeling on the productions in order to simultaneously investigate the contribution of
multiple acoustic features from multiple words in an utterance to the interpretation of
information status of different sentence elements.

Experiments: Overview and general methods
The current paper presents results from three experiments. Experiment 1
investigated whether speakers prosodically disambiguate focus location (subject, verb,
object), focus type (contrastive vs. non-contrastive focus), and focus breadth (narrow vs.
wide) by eliciting semi-naturalistic productions like that in (3b) (e.g., Damon fried an
omelet this morning), whose information status was disambiguated by a preceding

Acoustic correlates of information structure 14
question. Experiment 2 investigated whether speakers disambiguate focus location and
focus type when the task explicitly required them to communicate a particular meaning to
their listeners. Finally, Experiment 3 served as a replication and extension of Experiment
2, in which speakers included an attribution expression (“I heard that”) before the critical
sentence.
The acoustic analysis of the productions elicited in all three experiments
proceeded in three steps. First, we automatically extracted a series of 24 acoustic features
(see Table 2) from the subject, verb, and object of the sentences elicited in Experiments
1, 2, and 3. Second, we subjected all of these features to a stepwise discriminant function
analysis in order to determine which features best discriminated the information status
conditions listed in Table 1 for each of the three experiments. This analysis resulted in a
subset of eight acoustic features. Finally, we used discriminant analyses to evaluate
whether this subset of eight features could effectively discriminate sets of 2 and 3
conditions for each of the three experiments. Specifically, we tested focus location by
comparing the features from productions in which Damon, fried, and omelet were
focused, respectively. We tested focus type by comparing the features from sentences in
which the focused element was contrastively or non-contrastively focused at each of the
three syntactic positions. Last, we tested focus breadth by comparing the features for
sentence with wide-focus to those with narrow object focus. In addition to the analysis of
acoustic features, in Experiments 2 and 3 we investigated whether listeners could
correctly determine the intended information status of the speaker.

Acoustic correlates of information structure 15

Experiment 1
Method
Participants
Nine pairs of participants were recorded. All participants were self-reported native
speakers of American English. All participants were MIT students or members of the
surrounding community. Participants were paid for their participation.
Materials
Each trial consisted of a set-up question and a target sentence, which always had an SVO
structure (e.g., Damon fried an omelet this morning). The target sentence could plausibly
answer any one of the seven set-up questions (see Table 1), which served to focus
different elements of the sentence or the entire event described in the sentence. The first
question focused the entire event (i.e. What happened?). In the remaining conditions,
two factors were manipulated: (1) the element in the target sentence that was focused by
the question (subject, verb, object); and (2) the presence of an explicit contrast set for the
focused element (non-contrastively focused, i.e. explicit contrast set absent, contrastively
focused, i.e. explicit contrast set present).
All subject and object noun phrases (NPs) in the target sentences were bi-syllabic
with first syllable stress, and all verbs were monosyllabic. All subject NPs were proper
names, and object NPs were mostly common inanimate objects, such that the events were
non-reversible. Furthermore, all words were comprised mostly of sonorant phonemes.
These constraints ensured that words could be more easily compared across items, and
facilitated the extraction of acoustic features (which is easier for vowels and sonorant
consonants). An adjunct prepositional phrase (PP) was included at the end of each
sentence so that differences in the production of the object NP due to the experimental
manipulations would be dissociable from prosodic effects on phrase-final, or in this case,

Acoustic correlates of information structure 16
sentence-final, words, which are typically lengthened and produced with lower F0
compared to phrase-medial words (e.g., Wightman et al., 1992).
We constructed 28 sets of materials. Participants saw one condition of each item,
following a Latin Square design. A sample item is presented in Table 1. The complete
set of materials can be found in Appendix A.

Condition

Focus Type

Focused
Argument

Setup Question

1

Non-contrastive

wide

What happened this morning?

2

Non-contrastive

S

Who fried an omelet this morning?

3

Non-contrastive

V

What did Damon do to an omelet this morning?

4

Non-contrastive

O

What did Damon fry this morning?

5

Contrastive

S

Did Harry fry an omelet this morning?

6

Contrastive

V

Did Damon bake an omelet this morning?

7

Contrastive

O

Did Damon fry a chicken this morning?

Table 1: Example item from Experiment 1. The target sentence is “Damon fried an
omelet this morning.”
Procedure
Productions were elicited and pre-screened in a two-part procedure. The first part
was a training session, where participants learned the intended names for pictures of
people, actions, and objects. In the second part, the pairs of participants produced
questions and answers for each other. The method was designed to maximize control
over what speakers were saying, but to also encourage natural-sounding productions.
Pilot testing revealed that having subjects simply read the target sentences resulted in
productions with low prosodic variability. After going through the experiment one time,
the participants switched roles.
Training session
In the training session, participants learned mappings between 96 pictures and
names, so that they could produce the names from memory during the second part of the

Acoustic correlates of information structure 17
experiment. In a PowerPoint presentation, each picture, corresponding to a person, an
action, an object, or a modifier, was presented with its intended name (see Figure 1, left).
The pictures consisted of eight names of people, which were repeated 3-4 items each in
the experimental materials, eight colors (which were used in a concurrently run filler
experiment), 34 verbs, 44 objects, and two temporal modifiers (this morning and last
night). The pictures were presented in alphabetical order, to facilitate memorization and
recall. Participants were instructed to learn the mappings by progressing through the
PowerPoint at their own pace.
When participants felt they had learned the mappings, they were given a picturenaming test, which consisted of 27 items from the full list of 96. The test was identical
for all participants. Participants were told of their mistakes, and, if they made four or
more errors, they were instructed to go back through the PowerPoint to improve their
memory of the picture-name mappings. Once participants could successfully name 23 or
more items on the test, which took between 1 and 3 rounds of testing, they continued with
the second part of the experiment. Early in pilot testing, we discovered that subjects had
poor recall for the names of the people in the pictures. Therefore, in the actual
experiment, subjects could refer to a sheet which had labeled pictures of the people.

Acoustic correlates of information structure 18

Figure 1: Left: Examples from the picture-training task for Experiment 1. Each square
represents a screen shot. Right: Examples of the procedure for the questioner (upper
squares) and answerer (lower squares) for Experiment 1. Two conditions are presented:
Non-contrastive, object (left) and contrastive, verb (right). The top squares represent
screen shots of what the questioner saw on a trial; the bottom squares represent what the
answerer saw on a trial.
Question-Answer Experiment
The experiment was conducted using Linger 2.92 (available at
http://telab.mit.edu/~dr/Linger/), a software platform designed by Doug Rohde for
language processing experiments. Participants were randomly paired and randomly
assigned to the role of questioner or answerer. Participants sat at computers in the same
room such that neither could see the other’s screen. On each trial, as illustrated in Figure
1 (right), the questioner saw a question (e.g., “What did Damon fry this morning?”)
which he/she was instructed to produce aloud for the answerer. The answerer was
instructed to produce an answer aloud using the information contained in the picture on
his/her screen (e.g., “Damon fried an omelet this morning”). The answerer was

Acoustic correlates of information structure 19
instructed to produce complete sentences, including the subject, verb, object, and
temporal abverb,4 and to emphasize the part of the sentence that the questioner had asked
about, or that he/she was correcting. On a random 20% of trials, the answerer was asked
a comprehension question about the answer s/he produced.
Productions were recorded in a quiet room with a head-mounted microphone at a
rate of 44kHz.
Acoustic Feature

Units

Description

duration

ms

Word duration excluding any silence before or after the word.

silence

ms

Duration of silence following the word, not due to stop closure.

duration+silence

ms

The sum of the duration of the word and any following silence.

mean F0

Hz

Mean F0 of the entire word

maximum F0

Hz

Maximum F0 value across the entire word

F0 peak location

0-1

The proportion of the way through the word where the maximum F0 occurs.

minimum F0

Hz

Minimum F0 across the entire word

F0 valley location

0-1

The proportion of the way through the word where the minimum F0 occurs.

initial F0

Hz

early F0

Hz

Mean F0 of the initial 5% of the word
Mean F0 value of 5% of the word centered at the point 25% of the way
through the word

center F0

Hz

late F0

Hz

Mean F0 value of 5% of the word centered on the midpoint of the word
Mean F0 value of 5% of the word centered on a point 75% of the way
through the word

final F0

Hz

Mean F0 of the last 5% of the word

1st quarter F0

The difference between initial F0 and early F0.

Hz

The difference between early F0 and center F0.

3rd quarter F0

Hz

The difference between center F0 and late F0.

4th quarter F0

Hz

The difference between late F0 and final F0.

mean intensity

dB

Mean intensity of the word

maximum intensity

dB

Maximum dB level in the word

minimum intensity
intensity peak
location
intensity valley
location

dB
0-1

Minimum dB level in the word
The proportion of the way through the word where the maximum intensity
occurs
The proportion of the way through the word where the minimum intensity
occurs

maximum amplitude

4

Hz

2nd quarter F0

Pascal

0-1

Maximum amplitude across the word

In the absence of explicit instruction to produce complete sentences, with a lexicalized subject, verb, and
object, speakers would likely resort to pronouns or would omit given elements altogether (e.g., “What did
Damon fry this morning?” “An omelet.”). A complete production account of information structure
meaning distinctions should include not just the prosodic cues used by the speakers, but also syntactic and
lexical production choices, as well as the interaction among these different production strategies. However,
because we focus on prosody in the current investigation, we wanted to be able to compare acoustic
features across identical words. Thus, we required that participants always produce a subject, verb, object
and adverb on every trial.

Acoustic correlates of information structure 20
energy

(Pascal)2 x
Duration

Table 2: Acoustic features extracted from each word in the target sentence for
Experiments 1-3. Stepwise discriminant analyses demonstrated that the measures in bold
provided the best discrimination among conditions and were used in all reported
analyses.
Results
Of the 504 speaker productions from the Question-Answer Experiment, 87 (17%) were
discarded because (a) the answerer failed to use the correct lexical items, (b) the answerer
was disfluent, or (c) the production was poorly recorded. The 417 remaining productions
were subjected to the acoustic analyses described below.
Acoustic Features
Based on previous investigations of prosody and information structure (Fry, 1955;
Lieberman, 1960; Eady et al., 1985; Cooper & Eady, 1986, Bartels & Kingston, 1994;
Krahmer & Swerts, 2001; Baumann et al., 2006), we chose a set of acoustic features to
analyze (see Table 2). These features were obtained automatically using the Praat
program (Boersma & Weenink, 2006). The measures of F0 computed over portions of
the words (e.g., 1st quarter F0) were chosen in order to investigate how F0 changes across
the syllable might contribute to the differentiation of conditions.
Our first goal was to determine which of the 24 candidate acoustic features
mediated differences among conditions. We conducted a series of stepwise linear
discriminant analyses5 on all of the data collected in Experiments 1, 2 and 3 reported in
the current paper. In order to determine the features to be used in the analyses of all three
experiments, we performed a separate stepwise analysis on the data from each
experiment separately. For each analysis we entered all 24 acoustic features across each
5

Linear discriminant analysis (LDA) calculates a function, computed as a linear combination of all
predictors entered, which results in the best separation of two or more groups. For two groups, only one
function is computed. For three groups, the first function provides the best separation of group 1 from
groups 2 & 3; a second, orthogonal, function provides the best separation of groups 2 and 3, after
partialling out variance accounted for by the first function. Stepwise LDA is an iterative procedure which
adds predictors based on which of the candidate predictors provide the best discrimination.

Acoustic correlates of information structure 21
of the three sentence positions (subject, verb, and object) as possible predictors of the
seven experimental conditions, resulting in 72 predictors. Across the three analyses, the
acoustic features which consistently resulted in the best discrimination of conditions were
(1) duration + silence, (2) mean F0, (3) maximum F0, and (4) maximum intensity at the
positions of the (a) Subject, (b) Verb, and (c) Object. The fact that these 12 features (four
acoustic features across three sentence positions) consistently discriminated among
conditions across three independent sets of productions (from different speakers and
across somewhat different sets of materials) serves as evidence that these features are
underlying speaker- and material-independent differentiation of information structure.
Therefore, we use only these 12 features in the linear discriminant analyses reported for
the individual experiments in the paper.
Computing Residual Values
Because of differences among individuals, including age, gender, speech rate and
level of engagement with the task, speakers produce very different versions of the same
sentence even within the same experimental condition, thus adding variance to the
acoustic features of interest. Similarly, there is likely to be variability associated with
different items due to lexical and world knowledge factors. Researchers have previously
dealt with the issue of acoustic variability between speakers by normalizing pitch and/or
duration by speaker (e.g., Shriberg, Stolcke, Hakkani-Tur, & Tur, 2000; Shriberg et al.,
1998; Wightman, Shattuck-Hufnagel, Ostendorf, & Price, 1992). In order to remove
speaker- and item-related variance in the current studies, we computed linear regression
models in which speaker (n = 18) and item (n = 28) predicted each of the 12 acoustic
features identified in the stepwise discriminant analyses described in the previous section.
From each of these models, we calculated the predicted value of each acoustic feature for
a specific item from a specific speaker. We then subtracted this predicted value from

Acoustic correlates of information structure 22
every production. The differences among the resulting residual values should reflect
differences in the acoustic features due only to the experimental manipulations. All
subsequently reported analyses were performed on these residual values.

Focus Location
The extent to which a discriminant function analysis can separate data points into two or
more groups is calculated with a statistical test, Wilks’s lambda6.
To determine how well the acoustic features could differentiate focus location in
speakers’ productions, we computed a model where the 12 acoustic predictors were used
to discriminate among three focus locations: Subject, Verb or Object. In this analysis, we
are averaging across the contrastive and non-contrastive condition for each location.
The overall Wilks’s lambda of the model was significant, Λ = .46, χ2(24) = 271, p
< .001, indicating better-than-chance differentiation of subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .84, χ2(24) = 62.65,
p < .001, indicating that the acoustic predictors could also differentiate verb focus from
object focus (see Figure 2). Leave-one-out classification correctly classified 67% of the
productions. The model correctly classified subject focus 76% of the time, verb focus
58% of the time, and object focus 66% of the time. Table 3 presents the standardized
canonical discriminant function coefficients of the model.7

6

Wilks's lambda is a measure of the distance between groups on means of the independent variables, and is
computed for each function. It ranges in size from 0-1; lower values indicate a larger separation between
groups. The extent to which the model can effectively discriminate a new set of data is simulated by a
leave-one-out classification, in which the acoustic data from each production are iteratively removed from
the dataset, the model is computed, and the left-out case is classified by the resultant functions.
7
The coefficients in Table 3 indicate which acoustic features best discriminate focus location, such that
larger absolute values indicate a greater contribution of that feature to discrimination. For example,
inspection of the plot in Figure 2 and the coefficients in the Focus Location columns of Table 3 shows that
the acoustic features of Damon score around zero, or lower, on the first function (-0.002, 0.001, -0.01, and 0.06) and around zero on the second function (-0.003, 0.021, -0.016, -0.101). Fried shows a different
pattern; specifically, the acoustic features of fried have coefficients around zero for the first function, and
negative coefficients for function 2. Finally, omelet shows a third pattern: its acoustic correlates are
centered around zero on Function 1, but are high on Function 2.

Acoustic correlates of information structure 23
Figure 3 graphically presents the mean values of the four features, demonstrating
that across all three focus locations the intended focus location is produced with the
highest maximum intensity, the longest duration and silence, and the highest relative F0.

Function
1

Function
2

Subj
Focus

Verb
Focus

Obj
Focus

omelet

Focus
Breadth

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.001
-0.006
0.002
-0.037

0.004
0.011
0.001
0.181

0.008
0.011
-0.002
-0.137

0.003
-0.014
0.002
-0.026

0.004
-0.019
0.006
0.189

0.003
0.000
0.003
0.199

fried

Focus Type

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

0.007
0.024
0.002
0.094

-0.001
-0.003
-0.002
-0.010

0.007
0.000
0.004
-0.076

0.002
-0.040
-0.007
0.131

-0.001
-0.013
0.013
-0.043

0.005
-0.025
0.003
0.011

Damon

Focus Location

Duration+ silence
Mean F0
Maximum F0
Maximum
Intensity

-0.002
0.001
-0.010
-0.060

-0.003
0.021
-0.016
-0.101

0.005
-0.016
-0.012
0.087

-0.002
-0.007
0.020
0.056

0.005
-0.014
-0.011
-0.225

0.003
0.007
-0.005
-0.123

Acoustic correlates of information structure 24
Table 3: Standardized canonical coefficients of the discriminant functions computed for
Experiment 1.

Figure 2: Separation of focus locations on two discriminant functions in Experiment 1.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the upper left quadrant; productions of verb focus are
clustered in the lower half of the plot; productions of object focus are clustered in the
upper right quadrant.

Acoustic correlates of information structure 25

Damon
fried
omelet

Figure 3: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 1.
Focus type
To determine how well the acoustic features could differentiate the type of focus
(i.e. non-contrastive vs. contrastive) in speakers’ productions, we computed three models
in which the 12 acoustic predictors were used to discriminate between two focus type
groups. The three models investigated differences between non-contrastive and
contrastive focus at the three focus locations: subject, verb, and object.
Focus Type – Subject Position
The overall Wilks’s Lambda was not significant, Λ = .898, χ2(12) = 11.95 p = .45,
indicating that the acoustic features could not discriminate between non-contrastive and

Acoustic correlates of information structure 26
contrastive focus. Because the overall model is not significant, we do not present the
scores of the specific acoustic features or the classification statistics here or in the
analyses below.
Focus Type – Verb Position
The overall Wilks’s Lambda was not significant, Λ = .851, χ2(12) = 17.92 p = .12,
indicating that the acoustic features could not discriminate between non-contrastive and
contrastive focus.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .82, χ2(12) = 22.63 p < .05,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus above chance level. Leave-one-out classification correctly classified
59% of the productions. The model correctly classified non-contrastive focus 59% of the
time, and contrastive focus 59% of the time.
The coefficients in the Object Focus column of Table 3 indicate that intensity and
mean F0 contribute most to classification. Figure 4 graphically presents the mean values
of the four features, demonstrating that contrastive focus is produced with a higher
maximum intensity, a longer duration and silence, and higher maximum F0. Noncontrastive focus is produced with a higher mean F0.

Acoustic correlates of information structure 27

Damon
fried
omelet

Figure 4: Values for non-contrastive focus and contrastive focus type on the four
discriminating acoustic features when the direct object “omelet” is focused in
Experiment 1.
Wide Focus vs. Narrow Focus
To determine how well the acoustic features could differentiate focus breadth, we
computed a model in which the 12 critical predictors were used to discriminate between
productions where the entire sentence was focused and productions where the object was
non-contrastively or contrastively focused.
The overall Wilks’s Lambda was significant, Λ = .75, χ2(12) = 47.83, p < .001,
indicating that the acoustic features could successfully discriminate between conditions
where the entire event is focused and conditions where the object is narrowly focused.

Acoustic correlates of information structure 28
Leave-one-out classification correctly classified 72% of the productions. The model
correctly classified wide focus 67% of the time, and narrow focus 74% of the time.
The standardized canonical discriminant function coefficients in the Focus
Breadth column of Table 3 indicate that maximum intensity contributes most to focus
breadth classification. Figure 5 graphically presents the mean values of the four features,
demonstrating that wide focus is produced with a more uniform duration + silence and
maximum F0 across the sentence than object focus. Wide focus is also produced with a
more uniform, though overall greater, intensity than object focus.

Damon
fried
omelet

Figure 5: Values for wide focus vs. narrow object focus on the four discriminating
acoustic features in Experiment 3.
Discussion

Acoustic correlates of information structure 29
Focus Location
The results demonstrate that speakers consistently provide acoustic cues which
disambiguate focus location. Specifically speakers indicated focus with increased
duration, higher intensity, higher mean F0, and higher maximum F0. Furthermore, these
results are consistent with the pattern reported in Eady & Cooper (1986), such that the
word preceding a focused word is less prominent (produced with shorter duration, lower
intensity and lower F0) than the focused word, and the word following the focused word
is less prominent than the word preceding the focused word. Previous studies (Eady et
al., 1986; Rump and Collier, 1986) have reported this reduction in acoustic prominence
following focused elements as being mainly indicated by lower F0 on the post-focal
words, though in our data we also find evidence of this reduction in measures of duration
and intensity.
Focus Type
The results from Experiment 1 indicate that in semi-naturalistic productions
speakers do not systematically differentiate between different focus types (focused
elements which have explicit contrast sets in the discourse and those which do not).
Specifically, at two out of three sentence positions, a discriminant function analysis could
not successfully classify speakers’ productions of contrastively vs. non-contrastively
focused elements. The observation that speakers successfully discriminated contrastive
and non-contrastive focus in object position, but not in subject or verb positions, is
perhaps suggestive, but is likely due to a lack of experimental power, a limitation which
will be addressed in Experiment 2.
Focus Breadth
The results from Experiment 1 demonstrate that speakers do systematically mark
focus breadth prosodically. Narrow object focus is produced with the highest maximum
F0, longest duration, and maximum intensity of the object noun, relative to the other

Acoustic correlates of information structure 30
words in the sentence. For wide focus, the acoustic features are more similar across the
sentence; only intensity and mean F0 are higher on the object than on the other words in
the sentence. These differences are subtle, but sufficient for the model to successfully
discriminate the productions.
The fact that the model failed to systematically classify productions by focus type
(with the exception of the object position), while achieving high accuracy in focus
location and focus breadth indicates that speakers were not marking focus type with
prosody in Experiment 1. However, the method used to elicit productions did not require
that subjects be aware of the information structure ambiguity of the materials. Evidence
from other production studies suggests that speakers may not prosodically disambiguate
ambiguous productions if they are not aware of the ambiguity. Albritton, McKoon, and
Ratcliff (1996), for example, demonstrated that speakers did not disambiguate
syntactically ambiguous constructions like “Dave and Pat or Bob” unless they were
aware of the ambiguity (see also Snedeker and Trueswell, 2003, but cf. Kraljic and
Brennan, 2005, and Schafer, Speer, Warren, and White, 2000, for evidence that speakers
do disambiguate syntactically ambiguous structures even in the absence of ambiguity
awareness). Experiment 2 was designed to be a stronger test of speakers’ ability to
differentiate focus location, focus type, and focus breadth. We used materials similar to
those in Experiment 1, with two important methodological modifications. First, instead
of producing the answers to questions with no feedback, the speaker’s task now involved
trying to enable the answerer to choose the question that s/he was answering from a set of
possible questions. Moreover, we introduced feedback so that the speaker would always
know whether his/her partner had chosen the correct answer. Second, we changed the
design from a between- to a within-subjects manipulation. This ensured that speakers

Acoustic correlates of information structure 31
were aware of the manipulation, as they were producing the same answer seven times
with explicit instructions to differentiate their answers for their partner.
In addition to making the speaker’s task explicit, the new design also allowed us
to analyze the subset of the productions for which the listeners could successfully identify
the question-type and which therefore contain sufficient information for differentiating
utterances along the three relevant dimensions of information structure.

Experiment 2
Method
Participants
Seventeen pairs of participants were recorded for this experiment. Subjects were MIT
students or members of the surrounding community. All reported being native speakers
of American English. None had participated in Experiment 1. Participants were paid for
their participation.
Materials
The materials had the same structure as those from Experiment 1, though the
critical words differed. Specifically, a larger set of names and a wider variety of
temporal adverbs were used, and some verbs and objects differed from Experiment 1.
Unlike Experiment 1, each subject pair was presented with all seven versions of each of
14 items, according to a full within-subjects within-items design. All materials can be
found in Appendix B.
Procedure
Two participants sat at computers in the same room such that neither could see the
other’s screen. One participant was the speaker, and the other was the listener. Speakers
were told that they would be producing answers to questions out loud for their partners

Acoustic correlates of information structure 32
(the listeners), and that the listeners would be required to choose which question the
speaker was answering from a set of seven choices.
At the beginning of each trial, the speaker was presented with a question on the
computer screen to read silently. After pressing a button, the answer to the question
appeared below the question, accompanied by a reminder to the speaker that s/he would
only be producing the answer aloud, and not the question. Following this, the speaker
had one more chance to read the question and answer, and then he/she was instructed to
press a key to begin recording (after being told by the listener that he/she is ready), to
produce the answer, and then to press another key to stop recording.
The listener sat at another computer, and pressed a key to see the seven questions
that s/he would have to choose his/her answer from. When s/he felt familiar with the
questions, s/he told the speaker s/he was ready. After the speaker produced a sentence
out loud for the listener, the listener chose the question s/he thought the speaker was
answering. If the listener answered incorrectly, his/her computer produced a buzzer
sound, like the sound when a contestant makes an incorrect answer on a game show.
This cue was included to ensure that speakers knew when their productions did not
contain enough information for the listener to choose the correct answer.8
Results – Production
Two speaker-listener pairs were excluded as the Listener did not achieve
comprehension accuracy greater than 20%. One further pair was excluded as one
member was not a native speaker of American English. Finally, another pair of subjects
was excluded because they did not take the task seriously, and produced unnaturally
emphatic contrastive accents, often shouting the target word, and laughing while doing

8

In early pilots in which there was no feedback for incorrect responses, we observed that listeners were at
chance in choosing the correct question.

Acoustic correlates of information structure 33
so. These exclusions left a total of 13 pairs of participants whose responses were
analyzed.
Sixty-seven of the 1274 trials (5%) were excluded because (a) the speaker failed
to produce the correct words, (b) the speaker was disfluent, or (c) the production was
poorly recorded. Analyses were performed on all trials, and on the subset of trials for
which the listener correctly identified the question. The results were very similar in the
two analyses. For brevity of presentation, we present results from analyses conducted on
the correct trials (n = 660, 55%). The productions from Experiment 2 were analyzed
using the acoustic features chosen in the feature-selection procedure described in
Experiment 1. All analyses were performed on the residual values of these features, after
removing speaker and item variance with the method described in Experiment 1.
Focus Location
The overall Wilks’s lambda was significant, Λ = .085, χ2(24) = 1335, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .306, χ2(11) = 641, p
< .001, indicating that the acoustic features could also discriminate verb focus from
object focus (see Figure 6).
Leave-one-out classification correctly classified 93% of the productions. For
individual levels of focus location, the discriminant function correctly classified subject
focus 94% of the time, verb focus 90% of the time, and object focus 95% of the time.
The standardized canonical coefficients in the first two columns of Table 4
indicate that the acoustic features contributing most to the discrimination of focus
location are once again mean F0 and maximum intensity, though the other two features
are also contributing. In fact, inspection of the acoustic feature means in Figure 7

Acoustic correlates of information structure 34
demonstrate that the highest value of every acoustic feature is associated with the
intended focused item, with the exception of mean F0 when the subject is focused.

Figure 6: Separation of focus locations on two discriminant functions for Experiment 2.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the lower left quadrant of the plot; productions of verb
focus are clustered in the lower right quadrant; productions of object focus are clustered
in the lower half.
Focus Location

Focus Type

Focus Breadth

Function 2

Subject
Focus

Verb Focus

Object
Focus

Duration+ silence

omelet

Function 1
-0.001

0.004

0.004

0.006

0.003

0.003

Mean F0

-0.006

0.011

-0.003

0.005

-0.023

0.000

Maximum F0

0.002

0.001

0.004

-0.009

-0.003

0.003

Maximum Intensity

-0.025

0.183

-0.052

-0.171

0.012

0.199

Acoustic correlates of information structure 35
-0.002

0.006

0.002

-0.007

0.005

Mean F0

0.024

-0.005

0.001

-0.022

0.006

-0.025

Maximum F0

0.001

-0.002

-0.007

0.001

0.003

0.003

0.093

-0.016

-0.105

0.063

-0.084

0.011

Duration+ silence

Damon

0.007

Maximum Intensity

fried

Duration+ silence

-0.002

-0.002

0.002

0.005

0.009

0.003

Mean F0

0.003

0.021

-0.010

0.004

-0.009

0.007

Maximum F0

-0.011

-0.015

-0.014

-0.012

-0.006

-0.005

Maximum Intensity

-0.067

-0.097

0.094

-0.014

0.010

-0.123

Table 4: Standardized canonical coefficients of all discriminant functions computed for
Experiment 2.

Damon
fried
omelet

Figure 7: Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 2.

Acoustic correlates of information structure 36
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .633, χ2(12) = 81.41, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
75% of the productions. The model correctly classified non-contrastive focus 78% of the
time, and contrastive focus 71% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that maximum intensity at all three locations (i.e. large intensity differences between the
subject and verb and the subject and object) contributes most to classification. Figure 8
graphically presents the mean values of the four features, demonstrating that, in addition
to intensity differences, contrastive focus is produced with longer duration and silence, as
well as lower mean and maximum F0.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .654, χ2(12) = 72.27, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
72% of the productions. The model correctly classified non-contrastive focus 70% of the
time, and contrastive focus 75% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that, once again maximum intensity contributes most to classification. Figure 9
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a higher maximum intensity, and a longer duration and silence,
than non-contrastive focus. Once again, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

Acoustic correlates of information structure 37
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .793, χ2(12) = 41.3, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
67% of the productions. The model correctly classified non-contrastive focus 69% of the
time, and contrastive focus 66% of the time.
The standardized canonical discriminant function coefficients in Table 4 indicate
that contrastive focus is most strongly associated with lower mean F0. Figure 10
graphically presents the mean values of the four features, demonstrating that contrastive
focus is produced with a lower mean and maximum F0 than non-contrastive focus.

Damon
fried
omelet

Figure 8. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 2.

Acoustic correlates of information structure 38

Damon
fried
omelet

Figure 9. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 2.

Acoustic correlates of information structure 39

Damon
fried
omelet

Figure 10. Values for non-contrastive focus vs. contrastive focus on the four
discriminating acoustic features when “omelet” is focused in Experiment 2.
Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .59, χ2(12) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 84% of productions; wide
focus was correctly classified 77% of the time, and object focus was correctly classified
88% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 4
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Although intensity is contributing most strongly
to classification, inspection of the acoustic means in Figure 11 indicates that wide focus

Acoustic correlates of information structure 40
is marked by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is marked by greater prominence on the
object, reflected in longer duration, higher F0, and higher intensity.

Damon
fried
omelet

Figure 11: Values for wide vs. narrow object focus on the four discriminating acoustic
features in Experiment 2.

Acoustic correlates of information structure 41
Results – Perception

Figure 12. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 2.
Listeners’ choices of question sorted by the intended question are plotted in
Figure 12. Listeners’ overall accuracy was 55%. To determine whether listeners were
able to determine the speaker’s intended sentence meaning, we compared each subject's
responses to chance performance. Specifically we assessed, for focus location and focus
type, whether each subject's proportion of correct responses exceeded chance; wide
focus productions were excluded from the analysis, so that chance performance for focus

Acoustic correlates of information structure 42
location was .33, and chance performance for focus type was .5. Results demonstrated
that listeners were able to successfully identify focus location: all 13 subjects’
performance significantly exceeded chance performance, p = .05, two-tailed. However,
listeners were unable to successfully identify focus type: only three of 13 subjects
performed at above-chance levels (based on the binomial distribution), p = .05, twotailed. To investigate focus breadth, we assessed, for wide focus and narrow object focus
separately, whether each subject's proportion of correct responses exceeded chance. For
these analyses, we excluded subject and verb focus productions, so that chance
performance was .33 for wide focus, and .67 for narrow object focus. Results
demonstrated that listeners were moderately successful at identifying focus breadth: six
of 13 subjects identified wide focus at rates above chance, and nine out of 13 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.
Discussion
The production results replicated the two main findings from Experiment 1, and provided
evidence for acoustic discrimination of focus type across sentence positions as well.
First, these results demonstrated that focused elements have longer durations than nonfocused elements, incur larger F0 excursions, are more likely to be followed by silence,
and are produced with greater intensity. Second, speakers consistently differentiate
between wide and narrow focus by producing the object in the latter case with higher F0,
longer duration, and greater intensity. Specifically, although object focus was indicated
by increased duration, higher intensity, and higher F0 on the object than on the subject or
the verb, wide focus was indicated by comparatively greater duration, higher intensity,
and higher F0 on the subject and the verb, and shorter duration, lower intensity, and
lower F0 on the object. These results are consistent with those obtained by Baumann et

Acoustic correlates of information structure 43
al. (2006), who demonstrated that narrow focus on an element was indicated with longer
duration and a higher F0 peak than wide focus on an event encompassing that element.
Most importantly, although speakers in Experiment 1 did not differentiate
conditions with and without an explicit contrast set for the focused element (except for
the object position), these conditions were differentiated by speakers in Experiment 2, at
every syntactic position. There are two possible interpretations of this difference. First,
in Experiment 1, speakers produced only four versions of each of the seven conditions,
whereas speakers in Experiment 2 and 3, reported below, produced 14 versions of each of
the seven conditions, resulting in greater power in the latter two experiments. The fact
that, in Experiment 2, speakers successfully discriminated contrastive and noncontrastive focus in all three positions, suggests that the lack of such an effect in
Experiment 1 could be due to a lack of power.
As mentioned above, the difference in the findings between Experiments 1 and 2
is also consistent with results from Allbritton et al. (1996) and Snedeker and Trueswell
(2003) who demonstrated that speakers do not disambiguate syntactically ambiguous
sentences with prosody unless they are aware of the ambiguity. The current results
demonstrate a similar effect for acoustic prominence, such that speakers do not
differentiate two kinds of acoustically prominent elements (contrastively vs. noncontrastively focused elements) unless they are aware of the information structure
ambiguity in the structures they are producing.
The discriminant analyses indicated that contrastively focused words were
produced with longer durations and higher intensity than non-contrastively focused
words, but that non-contrastively focused words were produced with higher F0 than
contrastively focused words. This latter finding is surprising when compared to some
previous studies. For example, Ladd & Morton (1997) found that higher F0 and larger

Acoustic correlates of information structure 44
F0 range is perceived as more ‘emphatic’ or ‘contrastive’ by listeners. Similarly, Ito and
Speer (2008) demonstrated that contrastively focused words were produced with higher
F0 than non-contrastive ones. Given the unexpected results, we inspected individual
pitch tracks to more closely observe the F0 patterns across the entire utterances. The
pitch tracks presented in Figure 13 were generated from the productions of a typical
speaker, and they exemplify the higher F0 observed for non-contrastive focus than
contrastive focus in the subject position (A vs. B) and verb position (C vs. D).
Contrastive focus on the object is realized with the same F0 as non-contrastive focus on
the object (E vs. F).

300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Non-Contrastive

Given

an

omelet

yesterday

Given

0

1.433
Time (s)

Acoustic correlates of information structure 45
A. Non-contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Contrastive

an

Given

omelet

yesterday

Given

0

1.81
Time (s)

B. Contrastive Subject Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Non-Contrastive

omelet

yesterday

Given

0

1.514
Time (s)

C. Non-contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Contrastive

an

omelet

<SIL>

yesterday

Given

0

2.377
Time (s)

Acoustic correlates of information structure 46
D. Contrastive Verb Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

an

Given

omelet

yesterday

Non-Contrastive
1.582

0
Time (s)

E. Non-contrastive Object Focus
300
250

Pitch (Hz)

200
150
100
50
Damon

fried

Given

Given

an

omelet

yesterday

Contrastive
2.188

0
Time (s)

F. Contrastive Object Focus
Figure 13. Pitch tracks for non-contrastive and contrastive subject focus, non-contrastive
and contrastive verb focus, and non-contrastive and contrastive object focus,
respectively, from a typical speaker from Experiment 2.
Note that our finding that non-contrastive focus is realized with higher F0 than
contrastive focus is still consistent with the claim that contrastive focus is more
prominent than non-contrastive focus. As the graphs in Figures 8-10, and the pitch tracks
in Figure 13 indicate, although contrastive elements were consistently produced with
lower pitch, they were also consistently produced with longer durations and greater

Acoustic correlates of information structure 47
intensity than non-contrastive elements. As reviewed in the introduction, there is
9

evidence that intensity and duration can convey prominence more effectively than higher
pitch (Fry, 1954, Lieberman, 1960, Beckman, 1986; Turk & Sawusch,1996; Kochanski et
al., 2005). Our data are therefore consistent with prior claims that contrastive focus is
produced with greater prominence than non-contrastive focus.
As discussed in the introduction, the production elicitation and analysis methods
used in the current experiment are more robust than methods used in many previous
studies, including those whose results are inconsistent with the current findings. In
particular, the current results are based on productions from naïve subjects in a
communication task, and the analyses were performed on data with speaker and item
variability removed. The current results are therefore more likely to reflect the
underlying generalizations about the relationship between acoustics and meaning.
The perception results only partially mirrored the production results. Consistent
with the production results, listeners were highly successful in discriminating among the
three focus locations. In contrast to the production results, however, listeners were only
moderately successful in identifying focus type (non-contrastive vs. contrastive) from the
speakers’ productions. In fact, listeners most often confused non-contrastive focus with
contrastive focus (see Figure 12). These results suggest that, even though speakers may
be consistently signaling focus type with their prosody, listeners are not able to exploit
those cues for comprehension.
With regard to focus breadth, the perception results are incompatible with a strong
version of the focus projection hypothesis (Selkirk, 1995). According to this hypothesis,
an acoustic prominence on the object NP can be interpreted as marking the entire clause
9

Importantly, the F0 results are not artifacts of the residualization procedure employed to remove variance
from the acoustic features due to speaker and item. The same numerical pattern of F0 values is observed
whether residualization is employed or not, though only the residualized acoustic features successfully
discriminate focus type.

Acoustic correlates of information structure 48
as focused. Listeners are therefore predicted to treat a production with an acoustically
prominent object NP as ambiguous between the narrow object focus reading and the wide
focus reading. However, as can be seen in Figure 12, listeners correctly identified narrow
object non-contrastive focus 57% of the time, interpreting it as wide focus only 13% of
the time, and correctly identified narrow object contrastive focus 49% of the time,
interpreting it as wide focus only 6% of the time. These results are not consistent with
Gussenhoven’s (1983) finding that listeners cannot reliably distinguish between narrow
objects focus and wide focus.
Experiments 1 and 2 provide evidence that speakers systematically indicate focus
location and focus breadth using a set of four acoustic features. These experiments
further suggest that speakers can, but don’t always, indicate focus type. In particular, the
results suggest that speakers only prosodically differentiate contrastive from noncontrastive focus when they are aware of the meaning ambiguity and/or when the task
involves conveying a particular meaning to a listener.
To further investigate the speakers’ ability to prosodically differentiate contrastive
from non-contrastive focus, we conducted an additional experiment. Acoustic analyses
in Experiments 1 and 2 were limited to three words (i.e. subject, verb, object) in the
sentence. However, in natural productions, speakers’ utterances are often prefaced by
attribution expressions (e.g., “I think” or “I heard”), or expressions of emotional attitudes
towards the described events (e.g., “Unfortunately”, or “Luckily”). It is therefore
possible that contrastive information might be partially conveyed by prosodically
manipulating these kinds of expressions. We explored this possibility in Experiment 3, in
which we had speakers produce target SVO constructions with a preamble. Experiment 3
was also intended to serve as a replication of the results of Experiment 2; in particular,

Acoustic correlates of information structure 49
the somewhat unexpected finding that non-contrastive focus is produced with higher F0
than contrastive focus.

Experiment 3
Method
Participants
Fourteen pairs of participants (speakers and listeners) were recorded for this
experiment. Subjects were MIT students or members of the surrounding community. All
reported being native speakers of American English. None had participated in
Experiments 1 or 2. Participants were paid for their participation.
Materials
The materials for Experiment 3 were identical to those from Experiment 1
described above with the exception that an attribution expression (“I heard that”) was
appended to the beginning of each target sentence.
Procedure
The procedure for Experiment 3 was identical to that for Experiment 2.
Results – Production
Four speaker-listener pairs were excluded as the listener did not achieve
comprehension accuracy greater than 20%. These exclusions left a total of 10 pairs of
participants whose responses were analyzed. Eighty-one of the 980 recorded trials (8%)
were excluded because (a) the speaker failed to produce the correct words, (b) the
speaker was disfluent, or (c) the production was poorly recorded. Analyses were
performed on all trials, and on the subset of trials for which the listener correctly
identified the question the speaker produced the sentence in response to. As in
Experiment 2, the results were very similar for the two analyses. For brevity of
presentation, we present results from analyses conducted on the correct trials (n = 632,
70%).

Acoustic correlates of information structure 50
Focus Location
In order to investigate the contribution of the prosody of “I heard that” to the
differentiation of the focus type in Experiment 2, we performed a stepwise discriminant
function analysis which included as predictors measures of the four acoustic features we
had selected initially (duration + silence, mean F0, maximum F0, maximum intensity) (1)
for the subject (“Damon”), verb (“fried”), and object (“omelet”), and (2) for each of the
first three words of the sentence (“I”, “heard”, “that”). Of the 24 predictors included in
the stepwise discriminant function analysis, the features which resulted in the best
discrimination of focus type were (1) the duration + silence of “I”, (2) the maximum F0
of “I”, and (3) the maximum intensity of “I”. Based on these results, we conducted an
additional analysis in which we included a subset of 16 predictors: the duration + silence,
mean F0, maximum F0, and maximum intensity of the subject, verb, object, and “I”.
As in Experiments 1 and 2, we conducted a discriminant analysis to determine
whether the measures of (1) duration + silence, (2) maximum F0, (3) mean F0, and (4)
maximum intensity of the four critical words in the sentence could predict focus location.
The overall Wilks’s lambda was significant, Λ = .058, χ2(32) = 1467.09, p < .001,
indicating that the acoustic features could differentiate subject focus from verb and object
focus. In addition, the residual Wilks’s lambda was significant, Λ = .275, χ2(15) =
664.75, p < .001, indicating that the acoustic features could also discriminate verb focus
from object focus (Figure 14). Leave-one-out classification procedure correctly
classified 97% of the productions. At individual focus locations, the model correctly
classified subject focus 96% of the time, verb focus 97% of the time, and object focus
97% of the time.

Acoustic correlates of information structure 51

Figure 14. Separation of focus locations on two discriminant functions for Experiment 3.
The figure illustrates an effective discrimination among the three groups. Productions of
subject focus are clustered in the left half of the plot; productions of verb focus are
clustered in the lower right quadrant; productions of object focus are clustered in the
upper right quadrant.
Focus Location

Focus
Breadth

Focus Type

fried

omelet

Function
1

Function
2

Subject
Focus

Verb
Focus

Object
Focus

0.002

0.003

0.000

0.000

0.003

0.002

0.005

0.012

-0.013

-0.009

0.005

-0.010

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

0.003

0.000

0.005

0.006

-0.003

0.003

0.069

0.106

-0.037

-0.011

0.007

0.151

0.001

-0.003

0.000

0.002

0.001

0.005

0.025

-0.021

-0.001

-0.002

-0.001

-0.006

Maximum F0
Maximum
Intensity

-0.005

-0.002

-0.001

-0.005

0.000

-0.003

0.091

-0.077

-0.086

-0.015

0.027

-0.048

Duration+
silence
Mean F0

Acoustic correlates of information structure 52
Duration+
silence
Mean F0

Damon

0.000

0.002

0.000

0.000

0.002

0.011

0.011

-0.011

-0.020

0.019

-0.003

Maximum F0
Maximum
Intensity
Duration+
silence
Mean F0

-0.014

-0.003

-0.001

0.007

-0.014

-0.008

-0.147

0.011

0.159

-0.006

-0.064

-0.123

0.000

0.000

0.004

0.005

0.005

-0.001

-0.005

0.000

-0.013

-0.008

-0.003

-0.009

Maximum F0
Maximum
Intensity

I

-0.003

0.004

-0.002

0.017

0.010

0.014

0.005

-0.021

-0.017

0.142

0.133

0.126

0.014

Table 5: Standardized canonical coefficients of all discriminant functions computed for
Experiment 3.

I
Damon
fried
omelet

Figure 15. Means of the four discriminating acoustic features of productions of Subject,
Verb, and Object focus for Experiment 3.

Acoustic correlates of information structure 53
Focus Type
Focus Type – Subject Position
The overall Wilks’s Lambda was significant, Λ = .39, χ2(16) = 157.44, p<.001,
indicating that the acoustic features could successfully discriminate between noncontrastive and contrastive focus. Leave-one-out classification correctly classified 85%
of the productions. The model correctly classified non-contrastive focus 85% of the time,
and contrastive focus 85% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that maximum intensity overall, and specifically, maximum intensity on “I,” is
contributing most to classification. Figure 16 graphically presents the mean values of the
four features, demonstrating that, in addition to intensity differences, contrastive focus is
produced with longer duration and silence, and with lower mean and maximum F0.

Acoustic correlates of information structure 54

I
Damon
fried
omelet

Figure 16. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “Damon” is focused in Experiment 3.
Focus Type – Verb Position
The overall Wilks’s Lambda was significant, Λ = .46, χ2(16) = 139.28, p< .001,
indicating that the acoustic features could discriminate between non-contrastive and
contrastive focus better than chance. Leave-one-out classification correctly classified
80% of the productions. The model correctly classified non-contrastive focus 86% of the
time, and contrastive focus 74% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that, intensity on “I” is contributing the most to classification. Figure 17 graphically

Acoustic correlates of information structure 55
presents the mean values of the four features, demonstrating that contrastive focus is
produced with a higher maximum intensity, and a longer duration and silence, than noncontrastive focus. As in Experiment 2, non-contrastive focus is produced with higher
mean and maximum F0 than contrastive focus.

I
Damon
fried
omelet

Figure 17. Values for non-contrastive focus vs contrastive focus on the four
discriminating acoustic features when “fried” is focused in Experiment 3.
Focus Type – Object Position
The overall Wilks’s Lambda was significant, Λ = .40, χ2(16) = 133.37, p<.001,
indicating that the acoustic features could discriminate between non-contrastive and

Acoustic correlates of information structure 56
contrastive focus better than chance. Leave-one-out classification correctly classified
83% of the productions. The model correctly classified non-contrastive focus 89% of the
time, and contrastive focus 76% of the time.
The standardized canonical discriminant function coefficients in Table 5 indicate
that intensity and mean F0 on “I” are contributing the most to accurate classification.
Figure 18 graphically presents the mean values of the four features, demonstrating that
contrastive focus is produced with a higher mean and maximum F0 than non-contrastive
focus.

I
Damon
fried
omelet

Acoustic correlates of information structure 57
Figure 18: Values for non-contrastive vs. contrastive focus on the four discriminating
acoustic features when “omelet” is focused in Experiment 3.

Wide Focus vs. Narrow Focus
The overall Wilks’s Lambda was significant, Λ = .48, χ2(16) = 148, p < .001,
indicating that the acoustic features could differentiate between wide focus and narrow
object focus. Leave-one-out classification correctly classified 87% of productions; wide
focus was correctly classified 79% of the time, and object focus was correctly classified
92% of the time.
The standard canonical coefficients in the “Focus Breadth” column of Table 5
indicate that the maximum intensity of each of the target words contributes most strongly
to the discrimination of focus breadth. Specifically, greater intensity on the object is a
strong predictor of object focus; less intensity on the subject and the verb are strong
predictors of wide focus. Although intensity is contributing most strongly to
classification, inspection of the acoustic means in Figure 19 indicates that wide focus is
indicated by lesser prominence on the object, reflected in shorter duration, lower F0, and
lower intensity; conversely, narrow object focus is indicated by greater prominence on
the object, reflected in longer duration, higher F0, and higher intensity.

Acoustic correlates of information structure 58

I
Damon
fried
omelet

Figure 19. Values for wide vs narrow object focus on the four discriminating acoustic
features in Experiment 3.
Results – Perception
Listeners’ overall accuracy percentage by condition is plotted in Figure 20.
Listeners’ overall accuracy was 70%. As described in Experiment 2, we compared each
subject's responses to chance performance. Results demonstrated that listeners were able
to successfully identify focus location, as all 10 subjects’ performance significantly
exceeded chance performance, p = .05, two-tailed. Listeners were moderately successful
at discriminating focus type, as six of 10 subjects’ performance exceeded chance levels, p
= .05, two-tailed. Listeners successfully identified focus breadth as eight out of 10

Acoustic correlates of information structure 59
subjects identified wide focus at rates above chance, and eight out of 10 subjects
identified narrow object focus at levels above chance p = .05, two-tailed.

Figure 20. Percentage of Listeners’ condition choice by intended sentence type for
Experiment 3.

Discussion
Experiment 3 was conducted in order to (1) investigate whether speakers could
differentiate focus type with prosody if the sentences contained an attribution expression

Acoustic correlates of information structure 60
that could convey contrastive information, in addition to the elements that describe the
target event, and (2) replicate the results of Experiment 2.
With regard to the second goal, the production results of Experiment 3
successfully replicated the findings from Experiments 1 and 2. As in Experiments 1 and
2, speakers systematically differentiated focus location and focus breadth with a
combination of duration, intensity, and F0 cues. Furthermore, as in Experiment 2, noncontrastive focus was produced with higher F0 than contrastive focus (though only when
the subject or verb was focused), and contrastive focus was always produced with greater
duration and intensity. As discussed above, these F0 results contrast with prior findings
(Bartels & Kingston, 1994; Couper-Kuhlen, 1984; Ladd & Morton, 1997; Ito & Speer,
2008), but can be interpreted in light of more recent evidence that higher intensity is a
stronger cue to greater prominence than higher pitch (Kochanski et al., 2005).
In addition, results from Experiment 3 demonstrated that the strongest cues to
discrimination of focus type were the acoustics of “I” (from the attribution expression “I
heard that”). Specifically, in contrastive focus conditions, the word “I” was produced
with longer duration, higher intensity, and higher mean F0 and maximum F0. Indeed,
discrimination of focus type in Experiment 3 was far better than in Experiment 2. It
therefore appears that speakers can manipulate prosody on sentence elements outside of
the target clause (e.g., in attribution expressions) to convey contrastiveness.
The perception results demonstrated that listeners could accurately determine
focus location, similar to the results of Experiment 2. Furthermore, listeners were more
accurate in determining focus type than listeners in Experiment 2. This increase in
accuracy was likely due to speakers’ tendency to prosodically mark “I” in the contrastive
conditions.

Acoustic correlates of information structure 61

General Discussion
The three experiments reported in the current paper explored the ways in which focus
location, focus type, and focus breadth are conveyed with prosody. In each experiment,
naïve speakers and listeners engaged in tasks in which the information status of sentence
elements in SVO sentences was manipulated via preceding questions. The prosody of the
target sentences was analyzed using a series of classification models to select a subset
from the set of acoustic features that would best be able to discriminate among focus
locations and between focus types. In addition, in Experiments 2 and 3, the production
results were complemented by the perception results that demonstrated listeners’ ability
to use the prosodic cues in the speakers’ utterances to arrive at the intended meaning.
At the beginning of the paper, we posed three questions about the relationship
between acoustics and information structure: (1) do speakers mark information structure
prosodically, and, to the extent they do, (2) what are the acoustic features associated with
different aspects of information structure, and (3) how well can listeners retrieve this
information from the signal? We are now in a position to answer these questions.
First, we have demonstrated that speakers systematically provide prosodic cues to
the location of focused material. Across all three experiments, speakers provided cues to
focus location whether or not the task explicitly demanded it, across subject, verb and
object positions. In addition, across all three experiments, speakers systematically
provided cues to focus breadth, such that wide focus was prosodically differentiated from
narrow object focus. Finally, we found that speakers can, but don’t always, prosodically
differentiate contrastive and non-contrastive focus. Specifically, speakers did not
prosodically differentiate focus type in Experiment 1, but they did so in Experiment 2
and, even more strongly, in Experiment 3. As discussed above, the fact that speakers did
not differentiate focus type in Experiment 1, where they were plausibly not aware of the

Acoustic correlates of information structure 62
meaning ambiguity, but did differentiate between contrastive and non-contrastive focus
conditions in Experiments 2 and 3, where the task made the meanings more salient, is
consistent with results from the literature on intonational boundary production
demonstrating that speakers only produce disambiguating boundaries when they are
aware of the syntactic ambiguity which could be resolved by the presence of a boundary
(Albritton et al., 1996; Snedeker & Trueswell, 2003; cf. Schafer, et al., 2000 and Kraljic
& Brennan, 2005). Furthermore, the results from Experiment 3, where the critical words
were preceded by the attribution expression “I heard that,” demonstrated even stronger
differentiation of focus type than in Experiment 2, suggesting that speakers are able to
convey contrastiveness using words outside of the clause containing the contrastivelyfocused element.
To answer the question of which acoustic features are associated with different
meaning categories of information structure, we conducted a series of discriminant
function analyses with the goal of objectively identifying which of 24 measures of
duration, intensity, and F0 allowed for the best discrimination of conditions. Across all
experiments, and across different sentence positions, the best differentiation among
conditions was achieved using the following four features: word duration, maximum
word intensity, mean F0, and maximum F0. These results are consistent with many
previous studies in the literature, implicating these features in conveying aspects of
information structure. An important contribution of the current studies is that these
results were obtained using a quantitative analysis across many naïve speakers and items,
and are therefore more likely to be generalizable.
These data also demonstrate how exactly these four features are used in
conveying different aspects of information structure. With regard to focus location,
focused material is produced with longer duration, higher F0, and greater intensity than

Acoustic correlates of information structure 63
non-focused material. With regard to focus type, non-contrastive focus is realized with
higher mean and maximum F0 on the focused word than contrastive focus, whereas
contrastive focus is realized with greater intensity on the focused word than noncontrastive focus. Finally, with regard to focus breadth, narrow focus on the object is
indicated by higher F0 and longer duration on the object, compared to wide focus, and
wide focus is conveyed by higher intensity and F0, and longer duration on pre-focal
words.
To answer the question of how well listeners can retrieve prosodic information
from the signal, we included a perception task in Experiments 2 and 3. When the
relevant acoustic cues were present in the input (as demonstrated by successful
classification by the models), listeners were also able to classify the utterances, although
not quite as successfully as the models. Furthermore, the fact that the model always
achieved high classification accuracy suggests that the utterances contained enough
acoustic information to make these discriminations, and that we did not leave any
particularly informative acoustic features out of the analyses.

Implications for theories of the mapping of acoustics to meaning
While our production and perception results are compatible with a direct
relationship between acoustics and meaning, they are also consistent with the existence of
mediating phonological categories, as in the intonational phonology framework. For
example, a standard assumption within intonational phonology is that there is a
phonological category “accent” mediating acoustics and semantic focus, such that a
focused element is accented, and an unfocused element is unaccented (e.g., Brown,
1983). Our production and perception results are compatible with this assumption. First,
if speakers are signaling focus location by means of placing acoustic features

Acoustic correlates of information structure 64
corresponding to a +accent category on focused elements, then we would expect to see
strong acoustic differences between focused and given elements, as we have observed.
Moreover, if listeners perceive accents categorically, then we would predict successful
discrimination of productions on the basis of focus location, as we have observed.
Second, when the object is focused, it will be accented, resulting in higher acoustic
measures on the object compared to other positions, as we have observed. Furthermore,
in the wide focus condition, the subject, verb, and object – all of which are focused –
would all receive accents, and would therefore be more acoustically similar to one
another than they are in the wide focus condition. This difference in accent placement
would lead to successful discrimination between wide and narrow focus by listeners, as
we have observed. Finally, there has been much debate in the intonational phonology
literature about whether there is a phonological category +/- contrastive. The results of
our experiments are perhaps best explained without such a category. In particular, if
speakers accent focused elements without differentiating between contrastive and noncontrastive focus, then we would expect similar acoustic results between productions
which differ only on focus type, which would lead to poor discrimination by the model.
Moreover, listeners would not be successful in discriminating focus type, as we have
observed. Our experimental results are thus compatible with an intonational
phonological approach which includes an accent category mediating acoustics and
meaning, but no category for contrastiveness. Importantly, although our results do not
support a categorical difference between non-contrastive and contrastive focus, they do
not exclude the possibility that speakers can mark these distinctions with relative
differences in prominence (Calhoun, 2006).

Acoustic correlates of information structure 65
Implications for semantic theories of information structure
The current results are relevant to two open questions in the semantics of
information structure: (1) whether contrastive and non-contrastive focus constitute two
distinct categories; and (2) whether focus on the object of a verb can project to the entire
verb phrase.
As described in the introduction, Rooth (1992) proposed an account of focus which
makes no distinction between non-contrastive focus and contrastive focus. (6) shows the
F-marking (focus-marking) that Rooth’s account would assign to the conditions in
Experiments 1 and 2. Importantly, words and phrases which evoke alternatives, either
explicit or implicit, are considered focused (i.e. F-marked).
(6)
a. Subject, Subject Contrast: DamonF fried an omelet last night.
b. Verb, Verb Contrast:

Damon friedF an omelet last night.

c. Object, Object Contrast: Damon fried an omeletF last night.
d. Wide:

[Damon fried an omelet] F last night.

Our results provide tentative support for Rooth’s proposal that F-marked constituents do
not differ substantively as a function of whether the alternatives they evoke are explicit
(our contrastive condition) or implicit (our non-contrastive condition). Although
speakers differentiated these two conditions acoustically, they only did so when the
contrast between the conditions was made salient (Experiments 2 and 3). Moreover, even
when speakers did mark this distinction, listeners were unable to consistently use this
information to recover the intended meaning (Experiment 2). These results suggest that
there are no consistent semantic differences between foci with explicit alternatives in the
discourse and those with implicit alternatives.

Acoustic correlates of information structure 66
The second semantic issue that these results bear upon is whether narrow focus on
the object can project to the entire verb phrase. According to the theory of focus
projection proposed in Selkirk (1984, 1995), an acoustic prominence on the direct object
(omelet) can project focus to the entire verb phrase (fried an omelet) and then up to the
entire clause/sentence. Gussenhoven (1983, 1999) makes a similar claim. Both Selkirk’s
and Gussenhoven’s accounts therefore predict that a verb phrase with a prominence on
the object would be ambiguous between a narrow object focus interpretation and a wide
focus interpretation. Neither the production nor the perception results were consistent
with this prediction. In production, speakers distinguished between narrow object focus
and wide focus, and in perception, listeners were able to distinguish these two conditions.
One aspect of the production results (the acoustic realization of the subject) for the
narrow object focus and wide focus conditions is, however, predicted by both Selkirk and
Gussenhoven’s accounts. In particular, in the wide focus condition, the subject
constitutes new information while in the narrow object focus condition the subject is
given. Selkirk & Gussenhoven both predict that the subject would be more acoustically
prominent in the wide focus condition than in the narrow object focus condition. This is
exactly what we observed (especially in Experiments 1 and 3). Nevertheless, as
discussed above, speakers also systematically disambiguated wide focus from narrow
object focus across all three experiments with their realization of the object and the verb.
Specifically, wide focus was produced with stable or increasing duration, intensity, and
F0 across the subject, verb, and object; narrow object focus, on the other hand, was
characterized by shorter duration and lower intensity and F0 on the subject and verb,
followed by a steep increase in each of these values on the object.
Similar to our production findings, Gussenhoven (1983) found that, at least in some
productions, wide focus differed from narrow object focus in that the verb was more

Acoustic correlates of information structure 67
prominent under wide focus. Listeners, however, were unable to use this acoustic
information to distinguish wide focus from narrow object focus. Gussenhoven took this
result as evidence that the two conditions are not reliably distinguished (consistent with
his theory). Our results did not replicate this production/perception asymmetry: Listeners
are able to successfully classify productions with a single prominence on omelet as
indicating narrow object focus and did not confuse these productions with those from the
wide focus condition.
Methodological contributions
A further contribution of the current research to investigations of prosody and
information structure is methodological. With regard to the methods used to elicit
productions, we utilized multiple, untrained speakers to ensure that our results are
generalizeable to all speakers and are not due to speakers’ prior beliefs about what pattern
of acoustic prominence signals a particular meaning (see Gibson & Fedorenko, in press,
for similar arguments with respect to linguistic judgments). Furthermore, unlike most
previous work in which productions were selected for analysis based on perceptual
differentiability or on ratings of the appropriateness of prosodic contours, we elicited and
selected for analysis productions using a meaning task. Thus our analyses were based on
the communicative function of language. Finally, we did not exclude speakers based on
our perceptions of their productions; speakers were excluded for failure to provide
information to their listeners.
The analyses used here also constitute an improvement over previous analyses.
First, using discriminant modeling, we were able to simultaneously investigate the
contribution of multiple sentence elements to acoustic differentiation of conditions.
Second, we demonstrated that residualization is a useful method for controlling for
variability among speakers and lexical items. For example, preliminary analyses

Acoustic correlates of information structure 68
performed on the productions from Experiment 2 without first computing residual values
of the acoustic features revealed a 13% average increase in values of Wilks’ lambda
(where lower values indicate better discrimination) and a 7% average decrease in
classification accuracy. Third, the discriminant modeling proved successful in
objectively determining which acoustic features were the biggest contributors to
differences among conditions. The success of the analyses used in the current studies is
encouraging for future investigations of prosodic phenomena previously considered too
variable for study in a laboratory setting with naïve speakers.
One question that arises from the current set of studies is, to what extent the
current results can be generalized to all speakers and all sentences. In production studies,
there is always a trade-off between (1) having enough control over what participants are
producing to ensure sufficient data for analysis, and (2) ensuring that the speech is as
natural as possible. In Experiment 1, we attempted to elicit natural productions, but
failed to find systematic differences between focus types. In making the speakers’ task—
to help their listeners choose the correct question-type—explicit, we may have also
encouraged speakers to produce these sentences with somewhat exaggerated prosody.
Further experiments will be necessary to determine whether speakers normally produce
contrastive meanings in this way.
In conclusion, the current studies used rigorous scientific methods to explore
several important questions about the acoustic correlates of information structure. By
providing some initial answers to these questions, along with some implications for
semantic theory, and by offering a novel, objective way to approach these and other
questions, these studies open the door to future investigations of the relationship between
acoustics and meaning.

Acoustic correlates of information structure 69

References
Albritton, D., McKoon, G., & Ratcliff, R. (1996) Reliability of Prosodic Cues for
Resolving Syntactic Ambiguity. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 22, 714-735.
Bartels, C., Kingston, J., (1994). Salient pitch cues in the perception of contrastive focus.
In Boach, P., Van der Sandt, R. (Eds.), Focus & Natural Language Processing, Proc. of J.
Sem. conference on Focus. IBM Working Papers. TR-80, pp. 94–106.
Baumann, S., Grice, M., and Steindamm, S. (2006). Prosodic Marking of Focus Domains
- Categorical or Gradient? In Proceedings of Speech Prosody, Dresden, Germany, pp.
301-304.
Beckman, Mary E. (1986). Stress and Non-Stress Accent. Netherlands Phonetic Archives
Series No. 7. Foris.
Beckman, M., & Ayers Elam, G. (1997). Guidelines for ToBI labeling, version 3: Ohio
State University.
Beckman, M., Hirschberg, J., & Shattuck-Hufnagel, S. (2005). The original ToBI system
and the evolution of the ToBI framework. In S.-A. Jun (Ed.), Prosodic Typology: The
Phonology of Intonation and Phrasing (pp. 9-54): Oxford University Press.
Birch, S. and Clifton, C. (1995) Focus, accent, and argument structure: effects on
language comprehension. Language and Speech, 38 (4), 365-391.
Birner, B. (1994). Information status and Word Order: An Analysis of English Inversion.
Language, 70 (2), 233-259.
Boersma, Paul & Weenink, David (2006). Praat: doing phonetics by computer (Version
4.3.10) [Computer program]. Retrieved June 3, 2005, from http://www.praat.org/
Bolinger, D. (1961). Contrastive accent and contrastive stress. Language, 37, 83-96.
Breen, M., Dilley, L., Gibson, E., Bolivar, M., and Kraemer, J. (2006) Advances in
prosodic annotation: A test of inter-coder reliability for the RaP (Rhythm and Pitch) and
ToBI (Tones and Break Indices) transcription systems. Poster presented at the 19th
CUNY Conference on Human Sentence Processing, New York, NY. March, 2006.
Brown, G. (1983). Prosodic structures and the Given/New distinction. In D. R. Ladd &
A. Cutler (Eds.), Prosody:Models and measurements (pp. 67–77). Berlin: Springer.
Calhoun, S (2004). Phonetic Dimensions of Intonational Categories - the case of L+H*
and H*. In Proceedings of Speech Prosody, Nara, Japan, pp. 103-106.
Calhoun, S. (2005). It's the difference that matters: An argument for contextuallygrounded acoustic intonational phonology. In Linguistics Society of America Annual
Meeting, Oakland, California, January 2005.
Calhoun, S. (2006) Information Structure and the Prosodic Structure of English: a
Probabilistic Relationship. PhD thesis, University of Edinburgh.
Chafe, W. (1976). Givenness, contrastiveness, definiteness, subjects, topics and points of
view. In Charles N. Li, editor, Subject and Topic, pages 27-- 55. Academic Press, 1976.
Clark, E. V., & Clark, H. H. (1978). Universals, relativity, and language processing. In: J.
H. Greenberg (Ed.), Universals of human language, Vol. I. (pp. 225–277). Stanford:
Stanford University Press.

Acoustic correlates of information structure 70
Cooper, W., Eady, S. & Mueller, P. (1985). Acoustical aspects of contrastive stress in
question-answer contexts. Journal of Acoustical Society of America, 77(6), 2142-2156.
Couper-Kuhlen, E. (1984). A new look at contrastive intonation., Modes of
Interpretation: Essays Presented to Ernst Leisi, Watts, R., Weidman, U. (Eds.) Gunter
Narr Verlag, 137–158.
Cutler, A. (1977). The Context-Independence of "Intonational Meaning". Chicago
Linguistic Society (CLS 13), 104-115.
Dilley, L. C. (2005). The phonetics and phonology of tonal systems. Unpublished Ph.D.
Dissertation, MIT.
Dilley, L. C., & Brown, M. (2005). The RaP (Rhythm and Pitch) Labeling System,
Version 1.0: Available at http://tedlab.mit.edu/rap.html.
Eady, S. J., & Cooper, W. E. (1986). Speech intonation and focus location in matched
statements and questions. Journal of the Acoustical Society of America, 80, 402-415.
Féry, C. and Krifka, M. (2008). Information Structure: Notional Distinctions, Ways of
Expression. In Piet van Sterkenburg (ed.), Unity and diversity of languages, Amsterdam:
John Benjamins, 123-136.
Fry, D. B. (1955). Duration and Intensity as Physical Correlates of Linguistic Stress.
Journal of the Acoustical Society of America, 27, 765–768.
Gibson, E. & Fedorenko, E. (In press). Weak quantitative standards in linguistics
research. Trends in Cognitive Sciences.
Gussenhoven, C. (1983). Testing the reality of focus domains. Language and Speech, 26,
61–80.
Gussenhoven, C. (1999). On the limits of focus projection in English. In P. Bosch & R.
van der Sandt (Eds.), Focus: Linguistic, cognitive, and computational perspectives (pp.
43 –55). Cambridge, U.K.: Cambridge University Press.
Gussenhoven, C., Repp, B. H., Rietveld, A., Rump, W. H. & J. Terken, J. (1997). The
perceptual prominence of fundamental frequency peaks. Journal of the Acoustical Society
of America, 102, 3009-3022.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hawkins, S. & Warren, P. (1991). Factors affecting the given-new distinction in speech.
In Proceedings of the 12th International Congress of Phonetic Sciences, Aix en
Provence. 66-69.
Ito, K & Speer, S. (2008). Anticipatory effects of intonation: Eye movements
during instructed visual search. Journal of Memory and Language, 58, 541-573.
Ito, K. Speer, S. R. and Beckman, M. E. (2004). Informational status and pitch accent
distribution in spontaneous dialogues in English, In Proceedings of the International
Conference on Spoken Language Processing, Nara: Japan, 279-282.
Jackendoff, R. (1972). Semantic interpretation in generative grammar. Cambridge: MIT
Press.
Jaeger, T. F. (2008). Categorical Data Analysis: Away from ANOVAs (transformation or
not) and towards Logit Mixed Models. Journal of Memory and Language. 59, 434–446.

Acoustic correlates of information structure 71
Kochanski, G., Grabe, E., Coleman, J., & Rosner, B. (2005) Loudness predicts
prominence: fundamental frequency lends little. The Journal of the Acoustical Society of
America, 118 (2), 1038-1054.
Krahmer, E., & Swerts, M. (2001). On the alleged existence of contrastive accents.
Speech Communication, 34, 391-405.
Kraljic, T. & Brennan, S. E. (2005). Prosodic disambiguation of syntactic structure: For
the speaker or for the addressee? Cognitive Psychology 50: 194-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge Studies in Linguistics 79.
Cambridge: Cambridge University Press.
Ladd, D. R. & Morton, R. (1997). The perception of intonational emphasis: continuous or
categorical? Journal of Phonetics, 25, 313–342.
Lambrecht, K. (2001). A framework for the analysis of cleft constructions. Linguistics,
39, 463–516.
Lieberman, P. (1960). Some acoustic correlates of word stress in American English. The
Journal of the Acoustical Society of America, 32(4), 451-454.
Molnar, V. (2002). Information Structure in a Cross-linguistic Perspective. In Hilde
Hasselgård, Stig Johansson, Bergljot Behrens, Cathrine Fabricius-Hansen (Eds.),
Language and Computers, Vol. 39, 147-161(15).
Paul, H. (1880), Prinzipien der Sprachgeschichte, Leipzig.
Pierrehumbert, J.B. (1980). The phonology and phonetics of English intonation.
Unpublished dissertation, MIT.
Pierrehumbert, J. & Hirschberg, J. (1990). The Meaning of Intonational Contours in the
Interpretation of Discourse. In P. R. Cohen & J. Morgan & M. E. Pollack (eds.).
Intentions in Communication. Cambridge/MA: MIT Press, 271-311.
Pierrehumbert, J. & Steele, S. (1989). Categories of tonal alignment in English.
Phonetica, 46, 181-196.
Pitrelli, J., Beckman, M. & Hirschberg, J. (1994). Evaluation of prosodic transcription
labeling reliability in the ToBI framework. In Proceedings of the International
Conference on Spoken Language Processing, 123-126.
Rietveld, A. C. M., and Gussenhoven, C. (1985). On the relation between pitch excursion
size and prominence. Journal of Phonetics, 13, 299-308.
Rochemont, M. S. (1986). Focus in Generative Grammar. Amsterdam/Philadelphia: John
Benjamins.
Rooth, M. (1985). Association with Focus. PhD thesis, University of Massachusetts
Amherst.
Rooth, M. (1992). A theory of focus interpretation. Natural Language Semantics, 1, 75 –
116.
Rump, H. H., and Collier, R. (1996). ‘Focus conditions and the prominence of pitchaccented syllables. Language and Speech, 39, 1–17.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational disambiguation
in sentence production and comprehension. Journal of Psycholinguistic Research, 29,
169-182.

Acoustic correlates of information structure 72
Selkirk, E. (1984). Phonology and syntax: The relation between sound and structure.
Cambridge, MA: MIT.
Selkirk, E. (1995). Sentence Prosody: Intonation, Stress, and Phrasing. In: J.Goldsmith
(ed.). The Handbook of Phonological Theory. Oxford: Blackwell, 550-569.
Schwarzchild, R. (1999) GIVENness, AvoidF and other Constraints on the Placement of
Accent. Natural Language Semantics, 7, 141–177.
Shriberg, E., Stolcke, A., Hakkani-Tur, D. & Tur, G. (2000). Prosody-Based Automatic
Segmentation of Speech into Sentences and Topics. Speech Communication, 32, 127-154.
Shriberg, E., Bates, R., Taylor, P., Stolcke, A., Jurafsky, D., Ries, K., Coccaro, N.,
Martin, R., Meteer, M., & Van Ess-Dykema, C. (1998). Can Prosody Aid the Automatic
Classification of Dialog Acts in Conversational Speech? Language and Speech, 41:3-4,
439-487.
Silverman, K. E. A., Beckman, M., Pierrehumbert, J., Ostendorf, M., Wightman, C. W.
S., Price, P., et al. (1992). ToBI: A standard scheme for labeling prosody. In Proceedings
of the 2nd International Conference on Spoken Language Processing (pp. 867-879).
Banff.
Sluijter, A. and van Heuven, V. (1996). Spectral balance as an acoustic correlate of
linguistic stress. Journal of the Acoustical Society of America, 100, 2471–2485.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects of
speaker awareness and referential contest. Journal of Memory and Language, 48, 103–
130.
Stalnaker, R. (2002). Common ground. Linguistics and Philosophy, 25: 701–721.
Syrdal, A. and McGory, J. (2000). Inter-transcriber reliability of ToBI prosodic labeling.
In Proceedings of the International Conference on Spoken Language Processing,
Beijing: China, 235-238.
Terken, J. (1991). Fundamental frequency and perceived prominence accented syllables.
Journal of the Acoustical Society of America, 89, 1768–1776.
't Hart, J. Collier, R. & Cohen, A. (1990). A perceptual study of intonation. Cambridge
University Press, Cambridge.
Turk, A. & Sawusch, J. (1996) The processing of duration and intensity cues to
prominence. Journal of the Acoustical Society of America, 99, 3782-3790.
Welby, P. (2003). Effects of pitch accent position, type, and status on focus projection.
Language and Speech, 46, 53 – 81.
Wightman, C. W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P. J. (1992). Segmental
durations in the vicinity of prosodic phrase boundaries. Journal of the Acoustical Society
of America, 91(3), 1707-1717.
Xu, Y. & Xu, C. X. (2005). Phonetic realization of focus in English declarative
intonation, Journal of Phonetics, 33, 159–197.
Yoon, T., Chavarria, S., Cole, J., & Hasegawa-Johnson, M. (2004). Intertranscriber
reliability of prosodic labeling on telephone conversation using ToBI. In Proceedings of
the International Conference on Spoken Language Processing., Nara: Japan, 2729-2732.

Acoustic correlates of information structure 73

Appendix A
Experiment 1 items
Full items are recoverable as follows: Question A is always “What happened last night?”
Questions B, C, & D are wh-questions about the subject, verb, and object, respectively.
Questions E, F, & G are questions which introduce the explicit alternative subject, verb,
or object, indicated in parentheses.
1.

Question A: What happened last night?
Question B: Who fed a bunny last night?
Question C: What did Damon do to a bunny last night?
Question D: What did Damon feed last night?
Question E: Did Jenny feed a bunny last night?
Question F: Did Damon pet a bunny last night?
Question G: Did Damon feed a baby last night?
Response: Damon fed a bunny last night.

2. Damon (Lauren) caught (pet) a bunny (a squirrel) last night.
3. Damon (Molly) burned (break) a candle (a log) last night.
4. Darren (Lauren) cleaned (eat) a carrot (a chicken) last night.
5. Darren (Molly) peeled (eat) a carrot (a potato) last night.
6. Darren (Nora) found (buy) a diamond (a ring) last night.
7. Darren (Jenny) sold (lose) a diamond (a sapphire) last night.
8. Jenny (Damon) found (lose) a dollar (a quarter) last night.
9. Jenny (Darren) sewed (rip) a dolly (a blanket) last night.
10. Jenny (Logan) read (open) an email (a letter) last night.
11. Jenny (Nolan) smelled (plant) a flower (a skunk) last night.
12. Lauren (Darren) burned (write) a letter (a magazine) last night.
13. Lauren (Logan) mailed (open) a letter (a package) last night.
14. Lauren (Nolan) read (write) a novel (a newspaper) last night.
15. Lauren (Damon) fried (bake) an omelet (a chicken) last night.
16. Logan (Molly) peeled (chop) an onion (an apple) last night.
17. Logan (Nora) fried (chop) an onion (a potato) last night.
18. Logan (Jenny) cleaned (buy) a pillow (a rug) last night.
19. Molly (Logan) dried (wash) a platter (a bowl) last night.
20. Molly (Nolan) sold (find) a platter (a vase) last night.
21. Molly (Damon) poured (drink) a smoothie (a cocktail) last night.
22. Nolan (Nora) pulled (push) a stroller (a sled) last night.
23. Nolan (Jenny) bought (sell) a stroller (a wheelbarrow) last night.
24. Nolan (Lauren) sewed (knit) a sweater (a quilt) last night.
25. Nora (Nolan) killed (trap) a termite (a cockroach) last night.
26. Nora (Damon) changed (wash) a toddler (a baby) last night.
27. Nora (Darren) fed (dress) a toddler (a bunny) last night.
28. Nora (Logan) pulled (push) a wagon (a wheelbarrow) last night.

Acoustic correlates of information structure 74

Appendix B
Items used for Experiments 2-3
Full items are recoverable as follows: Question A always asks “What happened _____?”
where the blank corresponds to the temporal adverb. Questions B, C, & D are whquestions about the subject, verb, and object, respectively. Questions E, F, & G are
questions which introduce the explicit alternative subject, verb, or object, indicated in
parentheses.
1a.
1b.
1c.
1d.
1e.
1f.
1g.

Context: What happened yesterday?
Context: Who fried an omelet yesterday?
Context: What did Damon do to an omelet yesterday?
Context: What did Damon fry yesterday?
Context: Did Harry fry an omelet yesterday?
Context: Did Damon bake an omelet yesterday?
Context: Did Damon fry a chicken yesterday?
Target: No, Damon fried an omelet yesterday.

2. (I heard that) (No,) Megan (Jodi) sold (lose) her diamond (her sapphire) yesterday.
3. (I heard that) (No,) Mother (Daddy) dried (wash) a platter (a bowl) last night.
4. (I heard that) (No,) Norman (Kelly) read (write) an email (a letter) last night.
5. (I heard that) (No,) Lauren (Judy) poured (drink) a smoothie (a cocktail) this morning.
6. (I heard that) (No,) Nora (Jenny) sewed (rip) her dolly (her blanket) this morning.
7. (I heard that) (No,) Molly (Sarah) trimmed (wax) her eyebrows (her hair) on Tuesday.
8. (I heard that) (No,) Nolan (Steven) burned (break) a candle (a log) on Tuesday.
9. (I heard that) (No,) Logan (Billy) killed (trap) a termite (a cockroach) last week.
10. (I heard that) (No,) Radar (Fido) caught (lick) a bunny (a squirrel) last week.
11. (I heard that) (No,) Darren (Maggie) pulled (push) a stroller (a sled) on Sunday.
12. (I heard that) (No,) Brandon (Tommy) peeled (eat) a carrot (a potato) on Sunday.
13. (I heard that) (No,) Maren (Debbie) cleaned (buy) a pillow (a rug) on Friday.
14. (I heard that) (No,) Lindon (Kelly) fooled (fight) a bully (a teacher) on Friday.

A Statistical Model for Lost Language Decipherment
Benjamin Snyder and Regina Barzilay
CSAIL
Massachusetts Institute of Technology
{bsnyder,regina}@csail.mit.edu

Abstract
In this paper we propose a method for the
automatic decipherment of lost languages.
Given a non-parallel corpus in a known related language, our model produces both
alphabetic mappings and translations of
words into their corresponding cognates.
We employ a non-parametric Bayesian
framework to simultaneously capture both
low-level character mappings and highlevel morphemic correspondences. This
formulation enables us to encode some of
the linguistic intuitions that have guided
human decipherers. When applied to
the ancient Semitic language Ugaritic, the
model correctly maps 29 of 30 letters to
their Hebrew counterparts, and deduces
the correct Hebrew cognate for 60% of
the Ugaritic words which have cognates in
Hebrew.

1 Introduction
Dozens of lost languages have been deciphered
by humans in the last two centuries. In each
case, the decipherment has been considered a major intellectual breakthrough, often the culmination of decades of scholarly efforts. Computers
have played no role in the decipherment any of
these languages. In fact, skeptics argue that computers do not possess the “logic and intuition” required to unravel the mysteries of ancient scripts.1
In this paper, we demonstrate that at least some of
this logic and intuition can be successfully modeled, allowing computational tools to be used in
the decipherment process.
1

“Successful archaeological decipherment has turned out
to require a synthesis of logic and intuition . . . that computers do not (and presumably cannot) possess.” A. Robinson,
“Lost Languages: The Enigma of the World’s Undeciphered
Scripts” (2002)

Kevin Knight
ISI
University of Southern California
knight@isi.edu

Our deﬁnition of the computational decipherment task closely follows the setup typically faced
by human decipherers (Robinson, 2002). Our input consists of texts in a lost language and a corpus
of non-parallel data in a known related language.
The decipherment itself involves two related subtasks: (i) ﬁnding the mapping between alphabets
of the known and lost languages, and (ii) translating words in the lost language into corresponding
cognates of the known language.
While there is no single formula that human decipherers have employed, manual efforts have focused on several guiding principles. A common
starting point is to compare letter and word frequencies between the lost and known languages.
In the presence of cognates the correct mapping
between the languages will reveal similarities in
frequency, both at the character and lexical level.
In addition, morphological analysis plays a crucial role here, as highly frequent morpheme correspondences can be particularly revealing. In
fact, these three strands of analysis (character frequency, morphology, and lexical frequency) are
intertwined throughout the human decipherment
process. Partial knowledge of each drives discovery in the others.
We capture these intuitions in a generative
Bayesian model. This model assumes that each
word in the lost language is composed of morphemes which were generated with latent counterparts in the known language. We model bilingual morpheme pairs as arising through a series
of Dirichlet processes. This allows us to assign
probabilities based both on character-level correspondences (using a character-edit base distribution) as well as higher-level morpheme correspondences. In addition, our model carries out an implicit morphological analysis of the lost language,
utilizing the known morphological structure of the
related language. This model structure allows us
to capture the interplay between the character-

and morpheme-level correspondences that humans
have used in the manual decipherment process.
In addition, we introduce a novel technique
for imposing structural sparsity constraints on
character-level mappings. We assume that an accurate alphabetic mapping between related languages will be sparse in the following way: each
letter will map to a very limited subset of letters
in the other language. We capture this intuition
by adapting the so-called “spike and slab” prior to
the Dirichlet-multinomial setting. For each pair
of characters in the two languages, we posit an
indicator variable which controls the prior likelihood of character substitutions. We deﬁne a joint
prior over these indicator variables which encourages sparse settings.
We applied our model to a corpus of Ugaritic,
an ancient Semitic language discovered in 1928.
Ugaritic was manually deciphered in 1932, using knowledge of Hebrew, a related language.
We compare our method against the only existing
decipherment baseline, an HMM-based character
substitution cipher (Knight and Yamada, 1999;
Knight et al., 2006). The baseline correctly maps
the majority of letters — 22 out of 30 — to their
correct Hebrew counterparts, but only correctly
translates 29% of all cognates. In comparison, our
method yields correct mappings for 29 of 30 letters, and correctly translates 60.4% of all cognates.

2 Related Work
Our work on decipherment has connections to
three lines of work in statistical NLP. First, our
work relates to research on cognate identiﬁcation (Lowe and Mazaudon, 1994; Guy, 1994;
Kondrak, 2001; Bouchard et al., 2007; Kondrak,
2009). These methods typically rely on information that is unknown in a typical deciphering scenario (while being readily available for living languages). For instance, some methods employ a
hand-coded similarity function (Kondrak, 2001),
while others assume knowledge of the phonetic
mapping or require parallel cognate pairs to learn
a similarity function (Bouchard et al., 2007).
A second related line of work is lexicon induction from non-parallel corpora. While this
research has similar goals, it typically builds on
information or resources unavailable for ancient
texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002;

Haghighi et al., 2008). Moreover, distributional
methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language.
Finally, Knight and Yamada (1999) and Knight
et al. (2006) describe a computational HMMbased method for deciphering an unknown script
that represents a known spoken language. This
method “makes the text speak” by gleaning
character-to-sound mappings from non-parallel
character and sound sequences. It does not relate
words in different languages, thus it cannot encode
deciphering constraints similar to the ones considered in this paper. More importantly, this method
had not been applied to archaeological data. While
lost languages are gaining increasing interest in
the NLP community (Knight and Sproat, 2009),
there have been no successful attempts of their automatic decipherment.

3 Background on Ugaritic
Manual Decipherment of Ugaritic Ugaritic
tablets were ﬁrst found in Syria in 1929 (Smith,
1955; Watson and Wyatt, 1999). At the time, the
cuneiform writing on the tablets was of an unknown type. Charles Virolleaud, who lead the initial decipherment effort, recognized that the script
was likely alphabetic, since the inscribed words
consisted of only thirty distinct symbols. The location of the tablets discovery further suggested
that Ugaritic was likely to have been a Semitic
language from the Western branch, with properties similar to Hebrew and Aramaic. This realization was crucial for deciphering the Ugaritic
script. In fact, German cryptographer and Semitic
scholar Hans Bauer decoded the ﬁrst two Ugaritic
letters—mem and lambda—by mapping them to
Hebrew letters with similar occurrence patterns
in preﬁxes and sufﬁxes. Bootstrapping from this
ﬁnding, Bauer found words in the tablets that were
likely to serve as cognates to Hebrew words—
e.g., the Ugaritic word for king matches its Hebrew equivalent. Through this process a few
more letters were decoded, but the Ugaritic texts
were still unreadable. What made the ﬁnal decipherment possible was a sheer stroke of luck—
Bauer guessed that a word inscribed on an ax discovered in the Ras Shamra excavations was the
Ugaritic word for ax. Bauer’s guess was correct, though he selected the wrong phonetic sequence. Edouard Dhorme, another cryptographer

and Semitic scholar, later corrected the reading,
expanding a set of translated words. Discoveries
of additional tablets allowed Bauer, Dhorme and
Virolleaud to revise their hypothesis, successfully
completing the decipherment.
Linguistic Features of Ugaritic Ugaritic
shares many features with other ancient Semitic
languages, following the same word order, gender,
number, and case structure (Hetzron, 1997). It is a
morphologically rich language, with triliteral roots
and many preﬁxes and sufﬁxes.
At the same time, it exhibits a number of features that distinguish it from Hebrew. Ugaritic has
a bigger phonemic inventory than Hebrew, yielding a bigger alphabet – 30 letters vs. 22 in Hebrew. Another distinguishing feature of Ugaritic
is that vowels are only written with glottal stops
while in Hebrew many long vowels are written using homorganic consonants. Ugaritic also does not
have articles, while Hebrew nouns and adjectives
take deﬁnite articles which are realized as preﬁxes.
These differences result in signiﬁcant divergence
between Hebrew and Ugaritic cognates, thereby
complicating the decipherment process.

rect morphological analysis of words in the lost
language must be learned, we assume that the inventory and frequencies of preﬁxes and sufﬁxes in
the known language are given.
In summary, the observed input to the model
consists of two elements: (i) a list of unanalyzed
word types derived from a corpus in the lost language, and (ii) a morphologically analyzed lexicon
in a known related language derived from a separate corpus, in our case non-parallel.

4 Problem Formulation

Analyzing the undeciphered corpus, we might ﬁrst
notice a pair of endings, -34, and -5, which both
occur after the initial sequence 152- (and may likewise occur at the end of a variety of words in
the corpus). If we know this lost language to be
closely related to English, we can surmise that
these two endings correspond to the English verbal sufﬁxes -ed and -s. Using this knowledge,
we can hypothesize the following character correspondences: (3 = e), (4 = d), (5 = s). We now know
that (4252 = des2) and we can use our knowledge of the English lexicon to hypothesize that this
word is desk, thereby learning the correspondence
(2 = k). Finally, we can use similar reasoning to
reveal that the initial character sequence 152- corresponds to the English verb ask.
As this example illustrates, human decipherment efforts proceed by discovering both
character-level and morpheme-level correspondences. This interplay implicitly relies on a
morphological analysis of words in the lost language, while utilizing knowledge of the known
language’s lexicon and morphology.
One ﬁnal intuition our model should capture is
the sparsity of the alphabetic correspondence between related languages. We know from comparative linguistics that the correct mapping will pre-

We are given a corpus in a lost language and a nonparallel corpus in a related language from the same
language family. Our primary goal is to translate
words in the unknown language by mapping them
to cognates in the known language. As part of this
process, we induce a lower-level mapping between
the letters of the two alphabets, capturing the regular phonetic correspondences found in cognates.
We make several assumptions about the writing system of the lost language. First, we assume
that the writing system is alphabetic in nature. In
general, this assumption can be easily validated by
counting the number of symbols found in the written record. Next, we assume that the corpus has
been transcribed into electronic format, where the
graphemes present in the physical text have been
unambiguously identiﬁed. Finally, we assume that
words are explicitly separated in the text, either by
white space or a special symbol.
We also make a mild assumption about the morphology of the lost language. We posit that each
word consists of a stem, preﬁx, and sufﬁx, where
the latter two may be omitted. This assumption
captures a wide range of human languages and a
variety of morphological systems. While the cor-

5 Model
5.1 Intuitions
Our goal is to incorporate the logic and intuition
used by human decipherers in an unsupervised statistical model. To make these intuitions concrete,
consider the following toy example, consisting of
a lost language much like English, but written using numerals:
• 15234 (asked)
• 1525 (asks)
• 4352 (desk)

serve regular phonetic relationships between the
two languages (as exempliﬁed by cognates). As a
result, each character in one language will map to
a small number of characters in the other language
(typically one, but sometimes two or three). By
incorporating this structural sparsity intuition, we
can allow the model to focus on on a smaller set of
linguistically valid hypotheses.
Below we give an overview of our model, which
is designed to capture these linguistic intuitions.
5.2 Model Structure
Our model posits that every observed word in the
lost language is composed of a sequence of morphemes (preﬁx, stem, sufﬁx). Furthermore we
posit that each morpheme was probabilistically
generated jointly with a latent counterpart in the
known language.
Our goal is to ﬁnd those counterparts that lead to
high frequency correspondences both at the character and morpheme level. The technical challenge is that each level of correspondence (character and morpheme) can completely describe the
observed data. A probabilistic mechanism based
simply on one leaves no room for the other to play
a role. We resolve this tension by employing a
non-parametric Bayesian model: the distributions
over bilingual morpheme pairs assign probability based on recurrent patterns at the morpheme
level. These distributions are themselves drawn
from a prior probabilistic process which favors
distributions with consistent character-level correspondences.
We now give a formal description of the model
(see Figure 1 for a graphical overview). There are
four basic layers in the generative process:
1. Structural sparsity: draw a set of indicator
variables ⃗ corresponding to character-edit
λ
operations.
2. Character-edit distribution: draw a base
distribution G0 parameterized by weights on
character-edit operations.
3. Morpheme-pair distributions: draw a set
of distributions on bilingual morpheme pairs
Gstm , Gpre|stm , Gsuf |stm .
4. Word generation: draw pairs of cognates
in the lost and known language, as well as
words in the lost language with no cognate
counterpart.

λ

v
G0

Gpre|stm

Gstm

Gsuf |stm
stm

stm

upre
hpre

ustm

hstm

usuf

hsuf

word

Figure 1: Plate diagram of the decipherment
model. The structural sparsity indicator variables
⃗ determine the values of the base distribution hyλ
perparameters ⃗ . The base distribution G0 dev
ﬁnes probabilities over string-pairs based solely on
character-level edits. The morpheme-pair distributions Gstm , Gpre|stm , Gsuf |stm directly assign
probabilities to highly frequent morpheme pairs.

We now go through each step in more detail.
Structural Sparsity The ﬁrst step of the generative process provides a control on the sparsity of
edit-operation probabilities, encoding the linguistic intuition that the correct character-level mappings should be sparse. The set of edit operations includes character substitutions, insertions,
and deletions, as well as a special end symbol: {(u, h), (ϵ, h), (u, ϵ), EN D} (where u and h
range over characters in the lost and known languages, respectively). For each edit operation e we
posit a corresponding indicator variable λe . The
set of character substitutions with indicators set to
one, {(u, h) : λ(u,h) = 1}) conveys the set of
phonetically valid correspondences. We deﬁne a
joint prior over these variables to encourage sparse
character mappings. This prior can be viewed as a
distribution over binary matrices and is deﬁned to
encourage rows and columns to sum to low integer
values (typically 1). More precisely, for each character u in the lost language, we count the number
∑
of mappings c(u) =
h λ(u,h) . We then deﬁne
a set of features which count how many of these
characters map to i other characters beyond some
budget bi : fi = max (0, |{u : c(u) = i}| − bi ).
Likewise, we deﬁne corresponding features fi′ and
budgets b′ for the characters h in the known lani

guage. The prior over ⃗ is then deﬁned as
λ
(
)
⃗ ⃗ ⃗′ ⃗
⃗ = exp f · w + f · w
P (λ)
Z

(1)

where the feature weight vector w is set to encour⃗
age sparse mappings, and Z is a corresponding
normalizing constant, which we never need compute. We set w so that each character must map to
⃗
at least one other character, and so that mappings
to more than one other character are discouraged 2
Character-edit Distribution The next step in
the generative process is drawing a base distribution G0 over character edit sequences (each of
which yields a bilingual pair of morphemes). This
⃗
distribution is parameterized by a set of weights ϕ
on edit operations, where the weights over substitutions, insertions, and deletions each individually
sum to one. In addition, G0 provides a ﬁxed distribution q over the number of insertions and deletions occurring in any single edit sequence. Probabilities over edit sequences (and consequently on
bilingual morpheme pairs) are then deﬁned according to G0 as:
∏
P (⃗) =
e
ϕei · q (#ins (⃗), #del (⃗))
e
e
i

We observe that the average Ugaritic word is over
two letters longer than the average Hebrew word.
Thus, occurrences of Hebrew character insertions
are a priori likely, and Ugaritic character deletions
are very unlikely. In our experiments, we set q
to disallow Ugaritic deletions, and to allow one
Hebrew insertion per morpheme (with probability
0.4).
The prior on the base distribution G0 is a
Dirichlet distribution with hyperparameters ⃗ , i.e.,
v
⃗ ∼ Dirichlet(⃗ ). Each value ve thus correϕ
v
sponds to a character edit operation e. Crucially,
the value of each ve depends deterministically on
its corresponding indicator variable:
{
1 if λe = 0,
ve =
K if λe = 1.
1.3

where K is some constant value >
The overall
effect is that when λe = 0, the marginal prior density of the corresponding edit weight ϕe spikes at
We set w0 = −∞, w1 = 0, w2 = −50, w>2 = −∞,
with budgets b′ = 7, b′ = 1 (otherwise zero), reﬂecting the
2
3
knowledge that there are eight more Ugaritic than Hebrew
letters.
3
Set to 50 in our experiments.
2

0. When λe = 1, the corresponding marginal prior
density remains relatively ﬂat and unconstrained.
See (Ishwaran and Rao, 2005) for a similar application of “spike-and-slab” priors in the regression
scenario.
Morpheme-pair Distributions Next we draw a
series of distributions which directly assign probability to morpheme pairs. The previously drawn
base distribution G0 along with a ﬁxed concentration parameter α deﬁne a Dirichlet process (Antoniak, 1974): DP (G0 , α), which provides probabilities over morpheme-pair distributions. The
resulting distributions are likely to be skewed in
favor of a few frequently occurring morphemepairs, while remaining sensitive to the characterlevel probabilities of the base distribution.
Our model distinguishes between three types of
morphemes: preﬁxes, stems, and sufﬁxes. As a
result, we model each morpheme type as arising
from distinct Dirichlet processes, that share a single base distribution:
Gstm

∼ DP (G0 , αstm )

Gpre|stm

∼ DP (G0 , αpre )

Gsuf |stm

∼ DP (G0 , αsuf )

We model preﬁx and sufﬁx distributions as conditionally dependent on the part-of-speech of the
stem morpheme-pair. This choice capture the linguistic fact that different parts-of-speech bear distinct afﬁx frequencies. Thus, while we draw a single distribution Gstm , we maintain separate distributions Gpre|stm and Gsuf |stm for each possible
stem part-of-speech.
Word Generation Once the morpheme-pair
distributions have been drawn, actual word pairs
may now be generated. First the model draws a
boolean variable ci to determine whether word i in
the lost language has a cognate in the known language, according to some prior P (ci ). If ci = 1,
then a cognate word pair (u, h) is produced:
(ustm , hstm )

∼ Gstm

(upre , hpre )

∼ Gpre|stm

(usuf , hsuf )

∼ Gsuf |stm

u = upre ustm usuf
h = hpre hstm hsuf
Otherwise, a lone word u is generated, according
a uniform character-level language model.

In summary, this model structure captures both
character and lexical level correspondences, while
utilizing morphological knowledge of the known
language. An additional feature of this multilayered model structure is that each distribution
over morpheme pairs is derived from the single
character-level base distribution G0 . As a result, any character-level mappings learned from
one type of morphological correspondence will be
propagated to all other morpheme distributions.
Finally, the character-level mappings discovered
by the model are encouraged to obey linguistically
motivated structural sparsity constraints.

6 Inference
For each word ui in our undeciphered language we predict a morphological segmentation
(upre ustm usuf )i and corresponding cognate in the
known language (hpre hstm hsuf )i . Ideally we
would like to predict the analysis with highest
marginal probability under our model given the
observed undeciphered corpus and related language lexicon. In order to do so, we need to
integrate out all the other latent variables in our
model. As these integrals are intractable to compute exactly, we resort to the standard Monte Carlo
approximation. We collect samples of the variables over which we wish to marginalize but for
which we cannot compute closed-form integrals.
We then approximate the marginal probabilities
for undeciphered word ui by summing over all the
samples, and predicting the analysis with highest
probability.
In our sampling algorithm, we avoid sampling the base distribution G0 and the derived
morpheme-pair distributions (Gstm etc.), instead
using analytical closed forms. We explicitly sample the sparsity indicator variables ⃗ the cognate
λ,
indicator variables ci , and latent word analyses
(segmentations and Hebrew counterparts). To do
so tractably, we use Gibbs sampling to draw each
latent variable conditioned on our current sample
of the others. Although the samples are no longer
independent, they form a Markov chain whose stationary distribution is the true joint distribution deﬁned by the model (Geman and Geman, 1984).
6.1 Sampling Word Analyses
For each undeciphered word, we need to sample
a morphological segmentation (upre , ustm , usuf )i
along with latent morphemes in the known lan-

guage (hpre , hstm , hsuf )i . More precisely, we
need to sample three character-edit sequences
⃗pre , ⃗stm , ⃗suf which together yield the observed
e
e
e
word ui .
We break this into two sampling steps. First
we sample the morphological segmentation of ui ,
along with the part-of-speech pos of the latent
stem cognate. To do so, we enumerate each possible segmentation and part-of-speech and calculate its joint conditional probability (for notational
clarity, we leave implicit the conditioning on the
other samples in the corpus):
P (upre , ustm , usuf , pos) =
∑
∑
∑
P (⃗stm )
e
P (⃗pre |pos)
e
P (⃗suf |pos)
e
⃗stm
e

⃗pre
e

⃗suf
e

(2)
where the summations over character-edit sequences are restricted to those which yield the segmentation (upre , ustm , usuf ) and a latent cognate
with part-of-speech pos.
For a particular stem edit-sequence ⃗stm , we
e
compute its conditional probability in closed form
according to a Chinese Restaurant Process (Antoniak, 1974). To do so, we use counts from
the other sampled word analyses: countstm (⃗stm )
e
gives the number of times that the entire editsequence ⃗stm has been observed:
e
∏
countstm (⃗stm ) + α i p(ei )
e
P (⃗stm ) ∝
e
n+α
where n is the number of other word analyses sampled, and α is a ﬁxed concentration parameter. The
∏
product i p(ei ) gives the probability of ⃗stm ace
cording to the base distribution G0 . Since the
parameters of G0 are left unsampled, we use the
marginalized form:
ve + count(e)
p(e) = ∑
e′ ve′ + k

(3)

where count(e) is the number of times that
character-edit e appears in distinct edit-sequences
(across preﬁxes, stems, and sufﬁxes), and k is the
sum of these counts across all character-edits. Recall that ve is a hyperparameter for the Dirichlet
prior on G0 and depends on the value of the corresponding indicator variable λe .
Once the segmentation (upre , ustm , usuf ) and
part-of-speech pos have been sampled, we proceed to sample the actual edit-sequences (and thus

latent morphemes counterparts). Now, instead of
summing over the values in Equation 2, we instead
sample from them.
6.2 Sampling Sparsity Indicators
Recall that each sparsity indicator λe determines
the value of the corresponding hyperparameter ve
of the Dirichlet prior for the character-edit base
distribution G0 . In addition, we have an unnormal⃗
ized joint prior P (⃗ = g(λ) which encourages a
λ)
Z
sparse setting of these variables. To sample a particular λe , we consider the set ⃗ in which λe = 0
λ
⃗′ in which λe = 1. We then compute:
and λ
[count(e)]

ve
P (⃗ ∝ g(⃗ · ∑
λ)
λ)

e′

[k]

ve′

where k is the sum of counts for all edit operations, and the notation a[b] indicates the ascending
factorial. Likewise, we can compute a probability
′
⃗
for λ′ with corresponding values ve .
6.3 Sampling Cognate Indicators
Finally, for each word ui , we sample a corresponding indicator variable ci . To do so, we calculate Equation 2 for all possible segmentations and
parts-of-speech and sum the resulting values to obtain the conditional likelihood P (ui |ci = 1). We
also calculate P (ui |ci = 0) using a uniform unigram character-level language model (and thus depends only on the number of characters in ui ). We
then sample from among the two values:
P (ui |ci = 1) · P (ci = 1)
P (ui |ci = 0) · P (ci = 0)

6.5 Implementation Details
Many of the steps detailed above involve the consideration of all possible edit-sequences consistent with (i) a particular undeciphered word ui and
(ii) the entire lexicon of words in the known language (or some subset of words with a particular part-of-speech). In particular, we need to both
sample from and sum over this space of possibilities repeatedly. Doing so by simple enumeration
would needlessly repeat many sub-computations.
Instead we use ﬁnite-state acceptors to compactly
represent both the entire Hebrew lexicon as well
as potential Hebrew word forms for each Ugaritic
word. By intersecting two such FSAs and minimizing the result we can efﬁciently represent all
potential Hebrew words for a particular Ugaritic
word. We weight the edges in the FSA according
to the base distribution probabilities (in Equation 3
above). Although these intersected acceptors have
to be constantly reweighted to reﬂect changing
probabilities, their topologies need only be computed once. One weighted correctly, marginals
and samples can be computed using dynamic programming.
Even with a large number of sampling rounds, it
is difﬁcult to fully explore the latent variable space
for complex unsupervised models. Thus a clever
initialization is usually required to start the sampler in a high probability region. We initialize our
model with the results of the HMM-based baseline
(see section 8), and rule out character substitutions
with probability < 0.05 according to the baseline.

7 Experiments

6.4 High-level Resampling

7.1 Corpus and Annotations

Besides the individual sampling steps detailed
above, we also consider several larger sampling
moves in order to speed convergence. For example, for each type of edit-sequence ⃗ which has
e
been sampled (and may now occur many times
throughout the data), we consider a single joint
⃗
move to another edit-sequence e′ (both of which
yield the same lost language morpheme u). The
details are much the same as above, and as before
the set of possible edit-sequences is limited by the
string u and the known language lexicon.
We also resample groups of the sparsity indicator variables ⃗ in tandem, to allow a more rapid exλ
ploration of the probability space. For each character u, we block sample the entire set {λ(u,h) }h ,
and likewise for each character h.

We apply our model to the ancient Ugaritic language (see Section 3 for background). Our undeciphered corpus consists of an electronic transcription of the Ugaritic tablets (Cunchillos et al.,
2002). This corpus contains 7,386 unique word
types. As our known language corpus, we use the
Hebrew Bible, which is both geographically and
temporally close to Ugaritic. To extract a Hebrew
morphological lexicon we assume the existence
of manual morphological and part-of-speech annotations (Groves and Lowery, 2006). We divide
Hebrew stems into four main part-of-speech categories each with a distinct afﬁx proﬁle: Noun,
Verb, Pronoun, and Particle. For each part-ofspeech category, we determine the set of allowable
afﬁxes using the annotated Bible corpus.

Baseline
Our Model
No Sparsity

Words
type
token
28.82% 46.00%
60.42% 66.71%
46.08% 54.01%

Morphemes
type
token
N/A
N/A
75.07% 81.25%
69.48% 76.10%

Table 1: Accuracy of cognate translations, measured with respect to complete word-forms and
morphemes, for the HMM-based substitution cipher baseline, our complete model, and our model
without the structural sparsity priors. Note that the
baseline does not provide per-morpheme results,
as it does not predict morpheme boundaries.
To evaluate the output of our model, we annotated the words in the Ugaritic lexicon with the
corresponding Hebrew cognates found in the standard reference dictionary (del Olo Lete and Sanmart´n, 2004). In addition, manual morphological
ı
segmentation was carried out with the guidance of
a standard Ugaritic grammar (Schniedewind and
Hunt, 2007). Although Ugaritic is an inﬂectional
rather than agglutinative language, in its written
form (which lacks vowels) words can easily be
segmented (e.g. wypltn becomes wy-plt-n).
.
.
Overall, we identiﬁed Hebrew cognates for
2,155 word forms, covering almost 1/3 of the
Ugaritic vocabulary.4

8 Evaluation Tasks and Results
We evaluate our model on four separate decipherment tasks: (i) Learning alphabetic mappings,
(ii) translating cognates, (iii) identifying cognates,
and (iv) morphological segmentation.
As a baseline for the ﬁrst three of these tasks
(learning alphabetic mappings and translating and
identifying cognates), we adapt the HMM-based
method of Knight et al. (2006) for learning letter substitution ciphers. In its original setting, this
model was used to map written texts to spoken language, under the assumption that each character
was emitted from a hidden phonemic state. In our
adaptation, we assume instead that each Ugaritic
character was generated by a hidden Hebrew letter. Hebrew character trigram transition probabilities are estimated using the Hebrew Bible, and Hebrew to Ugaritic character emission probabilities
are learned using EM. Finally, the highest prob4

We are conﬁdent that a large majority of Ugaritic words
with known Hebrew cognates were thus identiﬁed. The
remaining Ugaritic words include many personal and geographic names, words with cognates in other Semitic languages, and words whose etymology is uncertain.

ability sequence of latent Hebrew letters is predicted for each Ugaritic word-form, using Viterbi
decoding.
Alphabetic Mapping The ﬁrst essential step towards successful decipherment is recovering the
mapping between the symbols of the lost language
and the alphabet of a known language. As a gold
standard for this comparison, we use the wellestablished relationship between the Ugaritic and
Hebrew alphabets (Hetzron, 1997). This mapping
is not one-to-one but is generally quite sparse. Of
the 30 Ugaritic symbols, 28 map predominantly
to a single Hebrew letter, and the remaining two
map to two different letters. As the Hebrew alphabet contains only 22 letters, six map to two distinct Ugaritic letters and two map to three distinct
Ugaritic letters.
We recover our model’s predicted alphabetic
mappings by simply examining the sampled values of the binary indicator variables λu,h for each
Ugaritic-Hebrew letter pair (u, h). Due to our
structural sparsity prior P (⃗ the predicted mapλ),
pings are sparse: each Ugaritic letter maps to only
a single Hebrew letter, and most Hebrew letters
map to only a single Ugaritic letter. To recover
alphabetic mappings from the HMM substitution
cipher baseline, we predict the Hebrew letter h
which maximizes the model’s probability P (h|u),
for each Ugaritic letter u.
To evaluate these mappings, we simply count
the number of Ugaritic letters that are correctly
mapped to one of their Hebrew reﬂexes. By this
measure, the baseline recovers correct mappings
for 22 out of 30 Ugaritic characters (73.3%). Our
model recovers correct mappings for all but one
(very low frequency) Ugaritic characters, yielding
96.67% accuracy.
Cognate Decipherment We compare the decipherment accuracy for Ugaritic words that have
corresponding Hebrew cognates. We evaluate
our model’s predictions on each distinct Ugaritic
word-form at both the type and token level. As
Table 1 shows, our method correctly translates
over 60% of all distinct Ugaritic word-forms with
Hebrew cognates and over 71% of the individual morphemes that compose them, outperforming the baseline by signiﬁcant margins. Accuracy improves when the frequency of the wordforms is taken into account (token-level evaluation), indicating that the model is able to decipher frequent words more accurately than infre-

1

Morfessor
Our Model

True positive rate

0.8

recall
67.48%
90.53%

f-measure
76.71%
88.53%

Table 2: Morphological segmentation accuracy for
a standard unsupervised baseline and our model.

0.6

0.4

Our Model
Baseline
Random

0.2

0

precision
88.87%
86.62%

0

0.2

0.4

0.6

0.8

those Ugaritic word-forms which are very unlikely
to have Hebrew cognates.
1

False positive rate

Figure 2: ROC curve for cognate identiﬁcation.
quent words. We also measure the average Levenshtein distance between predicted and actual cognate word-forms. On average, our model’s predictions lie 0.52 edit operations from the true cognate, whereas the baseline’s predictions average a
distance of 1.26 edit operations.
Finally, we evaluated the performance of our
model when the structural sparsity constraints are
not used. As Table 1 shows, performance degrades
signiﬁcantly in the absence of these priors, indicating the importance of modeling the sparsity of
character mappings.
Cognate identiﬁcation We evaluate our
model’s ability to identify cognates using the
sampled indicator variables ci . As before, we
compare our performance against the HMM
substitution cipher baseline. To produce baseline
cognate identiﬁcation predictions, we calculate
the probability of each latent Hebrew letter sequence predicted by the HMM, and compare it to
a uniform character-level Ugaritic language model
(as done by our model, to avoid automatically
assigning higher cognate probability to shorter
Ugaritic words). For both our model and the
baseline, we can vary the threshold for cognate
identiﬁcation by raising or lowering the cognate
prior P (ci ). As the prior is set higher, we detect
more true cognates, but the false positive rate
increases as well.
Figure 2 shows the ROC curve obtained by
varying this prior both for our model and the baseline. At all operating points, our model outperforms the baseline, and both models always predict better than chance. In practice for our model,
we use a high cognate prior, thus only ruling out

Morphological segmentation Finally, we evaluate the accuracy of our model’s morphological
segmentation for Ugaritic words. As a baseline
for this comparison, we use Morfessor CategoriesMAP (Creutz and Lagus, 2007). As Table 2
shows, our model provides a signiﬁcant boost in
performance, especially for recall. This result is
consistent with previous work showing that morphological annotations can be projected to new
languages lacking annotation (Yarowsky et al.,
2000; Snyder and Barzilay, 2008), but generalizes
those results to the case where parallel data is unavailable.

9 Conclusion and Future Work
In this paper we proposed a method for the automatic decipherment of lost languages. The key
strength of our model lies in its ability to incorporate a range of linguistic intuitions in a statistical
framework.
We hope to address several issues in future
work. Our model fails to take into account
the known frequency of Hebrew words and morphemes. In fact, the most common error is incorrectly translating the masculine plural sufﬁx (-m)
as the third person plural possessive sufﬁx (-m)
rather than the correct and much more common
plural sufﬁx (-ym). Also, even with the correct alphabetic mapping, many words can only be deciphered by examining their literary context. Our
model currently operates purely on the vocabulary
level and thus fails to take this contextual information into account. Finally, we intend to explore
our model’s predictive power when the family of
the lost language is unknown.5
5
The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0835445, and grant IIS0835652) and the Microsoft Research New Faculty Fellowship. Thanks to Michael Collins, Tommi Jaakkola, and
the MIT NLP group for their suggestions and comments.
Any opinions, ﬁndings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reﬂect the views of the funding organizations.

References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to bayesian nonparametric
problems. The Annals of Statistics, 2:1152–1174,
November.
Alexandre Bouchard, Percy Liang, Thomas Grifﬁths,
and Dan Klein. 2007. A probabilistic approach to
diachronic phonology. In Proceedings of EMNLP,
pages 887–896.
Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and
Language Processing, 4(1).
Jesus-Luis Cunchillos, Juan-Pablo Vita, and Jose´
Angel Zamora. 2002. Ugaritic data bank. CDROM.
Gregoria del Olo Lete and Joaqu´n Sanmart´n. 2004.
ı
ı
A Dictionary of the Ugaritic Language in the Alphabetic Tradition. Number 67 in Handbook of Oriental
Studies. Section 1 The Near and Middle East. Brill.
Pascale Fung and Kathleen McKeown. 1997. Finding terminology translations from non-parallel corpora. In Proceedings of the Annual Workshop on
Very Large Corpora, pages 192–202.
S. Geman and D. Geman. 1984. Stochastic relaxation,
gibbs distributions and the bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12:609–628.
Alan Groves and Kirk Lowery, editors. 2006. The
Westminster Hebrew Bible Morphology Database.
Westminster Hebrew Institute, Philadelphia, PA,
USA.
Jacques B. M. Guy. 1994. An algorithm for identifying
cognates in bilingual wordlists and its applicability
to machine translation. Journal of Quantitative Linguistics, 1(1):35–42.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
ACL/HLT, pages 771–779.
Robert Hetzron, editor. 1997. The Semitic Languages.
Routledge.
H. Ishwaran and J.S. Rao. 2005. Spike and slab variable selection: frequentist and Bayesian strategies.
The Annals of Statistics, 33(2):730–773.
Kevin Knight and Richard Sproat. 2009. Writing systems, transliteration and decipherment. NAACL Tutorial.
K. Knight and K. Yamada. 1999. A computational approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natural Language Processing.

Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of the COLING/ACL, pages 499–506.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition, pages 9–16.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceeding of
NAACL, pages 1–8.
Grzegorz Kondrak. 2009. Identiﬁcation of cognates
and recurrent sound correspondences in word lists.
Traitement Automatique des Langues, 50(2):201–
235.
John B. Lowe and Martine Mazaudon. 1994. The reconstruction engine: a computer implementation of
the comparative method. Computational Linguistics, 20(3):381–417.
Reinhard Rapp. 1999. Automatic identiﬁcation of
word translations from unrelated english and german
corpora. In Proceedings of the ACL, pages 519–526.
Andrew Robinson. 2002. Lost Languages: The
Enigma of the World’s Undeciphered Scripts.
McGraw-Hill.
William M. Schniedewind and Joel H. Hunt. 2007. A
Primer on Ugaritic: Language, Culture and Literature. Cambridge University Press.
Mark S. Smith, editor. 1955. Untold Stories: The Bible
and Ugaritic Studies in the Twentieth Century. Hendrickson Publishers.
Benjamin Snyder and Regina Barzilay. 2008. Crosslingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848–854.
Wilfred Watson and Nicolas Wyatt, editors.
Handbook of Ugaritic Studies. Brill.

1999.

David Yarowsky, Grace Ngai, and Richard Wicentowski. 2000. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of HLT, pages 161–168.

A Statistical Model for Lost Language Decipherment
Benjamin Snyder and Regina Barzilay
CSAIL
Massachusetts Institute of Technology
{bsnyder,regina}@csail.mit.edu

Abstract
In this paper we propose a method for the
automatic decipherment of lost languages.
Given a non-parallel corpus in a known related language, our model produces both
alphabetic mappings and translations of
words into their corresponding cognates.
We employ a non-parametric Bayesian
framework to simultaneously capture both
low-level character mappings and highlevel morphemic correspondences. This
formulation enables us to encode some of
the linguistic intuitions that have guided
human decipherers. When applied to
the ancient Semitic language Ugaritic, the
model correctly maps 29 of 30 letters to
their Hebrew counterparts, and deduces
the correct Hebrew cognate for 60% of
the Ugaritic words which have cognates in
Hebrew.

1 Introduction
Dozens of lost languages have been deciphered
by humans in the last two centuries. In each
case, the decipherment has been considered a major intellectual breakthrough, often the culmination of decades of scholarly efforts. Computers
have played no role in the decipherment any of
these languages. In fact, skeptics argue that computers do not possess the “logic and intuition” required to unravel the mysteries of ancient scripts.1
In this paper, we demonstrate that at least some of
this logic and intuition can be successfully modeled, allowing computational tools to be used in
the decipherment process.
1

“Successful archaeological decipherment has turned out
to require a synthesis of logic and intuition . . . that computers do not (and presumably cannot) possess.” A. Robinson,
“Lost Languages: The Enigma of the World’s Undeciphered
Scripts” (2002)

Kevin Knight
ISI
University of Southern California
knight@isi.edu

Our deﬁnition of the computational decipherment task closely follows the setup typically faced
by human decipherers (Robinson, 2002). Our input consists of texts in a lost language and a corpus
of non-parallel data in a known related language.
The decipherment itself involves two related subtasks: (i) ﬁnding the mapping between alphabets
of the known and lost languages, and (ii) translating words in the lost language into corresponding
cognates of the known language.
While there is no single formula that human decipherers have employed, manual efforts have focused on several guiding principles. A common
starting point is to compare letter and word frequencies between the lost and known languages.
In the presence of cognates the correct mapping
between the languages will reveal similarities in
frequency, both at the character and lexical level.
In addition, morphological analysis plays a crucial role here, as highly frequent morpheme correspondences can be particularly revealing. In
fact, these three strands of analysis (character frequency, morphology, and lexical frequency) are
intertwined throughout the human decipherment
process. Partial knowledge of each drives discovery in the others.
We capture these intuitions in a generative
Bayesian model. This model assumes that each
word in the lost language is composed of morphemes which were generated with latent counterparts in the known language. We model bilingual morpheme pairs as arising through a series
of Dirichlet processes. This allows us to assign
probabilities based both on character-level correspondences (using a character-edit base distribution) as well as higher-level morpheme correspondences. In addition, our model carries out an implicit morphological analysis of the lost language,
utilizing the known morphological structure of the
related language. This model structure allows us
to capture the interplay between the character-

and morpheme-level correspondences that humans
have used in the manual decipherment process.
In addition, we introduce a novel technique
for imposing structural sparsity constraints on
character-level mappings. We assume that an accurate alphabetic mapping between related languages will be sparse in the following way: each
letter will map to a very limited subset of letters
in the other language. We capture this intuition
by adapting the so-called “spike and slab” prior to
the Dirichlet-multinomial setting. For each pair
of characters in the two languages, we posit an
indicator variable which controls the prior likelihood of character substitutions. We deﬁne a joint
prior over these indicator variables which encourages sparse settings.
We applied our model to a corpus of Ugaritic,
an ancient Semitic language discovered in 1928.
Ugaritic was manually deciphered in 1932, using knowledge of Hebrew, a related language.
We compare our method against the only existing
decipherment baseline, an HMM-based character
substitution cipher (Knight and Yamada, 1999;
Knight et al., 2006). The baseline correctly maps
the majority of letters — 22 out of 30 — to their
correct Hebrew counterparts, but only correctly
translates 29% of all cognates. In comparison, our
method yields correct mappings for 29 of 30 letters, and correctly translates 60.4% of all cognates.

2 Related Work
Our work on decipherment has connections to
three lines of work in statistical NLP. First, our
work relates to research on cognate identiﬁcation (Lowe and Mazaudon, 1994; Guy, 1994;
Kondrak, 2001; Bouchard et al., 2007; Kondrak,
2009). These methods typically rely on information that is unknown in a typical deciphering scenario (while being readily available for living languages). For instance, some methods employ a
hand-coded similarity function (Kondrak, 2001),
while others assume knowledge of the phonetic
mapping or require parallel cognate pairs to learn
a similarity function (Bouchard et al., 2007).
A second related line of work is lexicon induction from non-parallel corpora. While this
research has similar goals, it typically builds on
information or resources unavailable for ancient
texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002;

Haghighi et al., 2008). Moreover, distributional
methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language.
Finally, Knight and Yamada (1999) and Knight
et al. (2006) describe a computational HMMbased method for deciphering an unknown script
that represents a known spoken language. This
method “makes the text speak” by gleaning
character-to-sound mappings from non-parallel
character and sound sequences. It does not relate
words in different languages, thus it cannot encode
deciphering constraints similar to the ones considered in this paper. More importantly, this method
had not been applied to archaeological data. While
lost languages are gaining increasing interest in
the NLP community (Knight and Sproat, 2009),
there have been no successful attempts of their automatic decipherment.

3 Background on Ugaritic
Manual Decipherment of Ugaritic Ugaritic
tablets were ﬁrst found in Syria in 1929 (Smith,
1955; Watson and Wyatt, 1999). At the time, the
cuneiform writing on the tablets was of an unknown type. Charles Virolleaud, who lead the initial decipherment effort, recognized that the script
was likely alphabetic, since the inscribed words
consisted of only thirty distinct symbols. The location of the tablets discovery further suggested
that Ugaritic was likely to have been a Semitic
language from the Western branch, with properties similar to Hebrew and Aramaic. This realization was crucial for deciphering the Ugaritic
script. In fact, German cryptographer and Semitic
scholar Hans Bauer decoded the ﬁrst two Ugaritic
letters—mem and lambda—by mapping them to
Hebrew letters with similar occurrence patterns
in preﬁxes and sufﬁxes. Bootstrapping from this
ﬁnding, Bauer found words in the tablets that were
likely to serve as cognates to Hebrew words—
e.g., the Ugaritic word for king matches its Hebrew equivalent. Through this process a few
more letters were decoded, but the Ugaritic texts
were still unreadable. What made the ﬁnal decipherment possible was a sheer stroke of luck—
Bauer guessed that a word inscribed on an ax discovered in the Ras Shamra excavations was the
Ugaritic word for ax. Bauer’s guess was correct, though he selected the wrong phonetic sequence. Edouard Dhorme, another cryptographer

and Semitic scholar, later corrected the reading,
expanding a set of translated words. Discoveries
of additional tablets allowed Bauer, Dhorme and
Virolleaud to revise their hypothesis, successfully
completing the decipherment.
Linguistic Features of Ugaritic Ugaritic
shares many features with other ancient Semitic
languages, following the same word order, gender,
number, and case structure (Hetzron, 1997). It is a
morphologically rich language, with triliteral roots
and many preﬁxes and sufﬁxes.
At the same time, it exhibits a number of features that distinguish it from Hebrew. Ugaritic has
a bigger phonemic inventory than Hebrew, yielding a bigger alphabet – 30 letters vs. 22 in Hebrew. Another distinguishing feature of Ugaritic
is that vowels are only written with glottal stops
while in Hebrew many long vowels are written using homorganic consonants. Ugaritic also does not
have articles, while Hebrew nouns and adjectives
take deﬁnite articles which are realized as preﬁxes.
These differences result in signiﬁcant divergence
between Hebrew and Ugaritic cognates, thereby
complicating the decipherment process.

rect morphological analysis of words in the lost
language must be learned, we assume that the inventory and frequencies of preﬁxes and sufﬁxes in
the known language are given.
In summary, the observed input to the model
consists of two elements: (i) a list of unanalyzed
word types derived from a corpus in the lost language, and (ii) a morphologically analyzed lexicon
in a known related language derived from a separate corpus, in our case non-parallel.

4 Problem Formulation

Analyzing the undeciphered corpus, we might ﬁrst
notice a pair of endings, -34, and -5, which both
occur after the initial sequence 152- (and may likewise occur at the end of a variety of words in
the corpus). If we know this lost language to be
closely related to English, we can surmise that
these two endings correspond to the English verbal sufﬁxes -ed and -s. Using this knowledge,
we can hypothesize the following character correspondences: (3 = e), (4 = d), (5 = s). We now know
that (4252 = des2) and we can use our knowledge of the English lexicon to hypothesize that this
word is desk, thereby learning the correspondence
(2 = k). Finally, we can use similar reasoning to
reveal that the initial character sequence 152- corresponds to the English verb ask.
As this example illustrates, human decipherment efforts proceed by discovering both
character-level and morpheme-level correspondences. This interplay implicitly relies on a
morphological analysis of words in the lost language, while utilizing knowledge of the known
language’s lexicon and morphology.
One ﬁnal intuition our model should capture is
the sparsity of the alphabetic correspondence between related languages. We know from comparative linguistics that the correct mapping will pre-

We are given a corpus in a lost language and a nonparallel corpus in a related language from the same
language family. Our primary goal is to translate
words in the unknown language by mapping them
to cognates in the known language. As part of this
process, we induce a lower-level mapping between
the letters of the two alphabets, capturing the regular phonetic correspondences found in cognates.
We make several assumptions about the writing system of the lost language. First, we assume
that the writing system is alphabetic in nature. In
general, this assumption can be easily validated by
counting the number of symbols found in the written record. Next, we assume that the corpus has
been transcribed into electronic format, where the
graphemes present in the physical text have been
unambiguously identiﬁed. Finally, we assume that
words are explicitly separated in the text, either by
white space or a special symbol.
We also make a mild assumption about the morphology of the lost language. We posit that each
word consists of a stem, preﬁx, and sufﬁx, where
the latter two may be omitted. This assumption
captures a wide range of human languages and a
variety of morphological systems. While the cor-

5 Model
5.1 Intuitions
Our goal is to incorporate the logic and intuition
used by human decipherers in an unsupervised statistical model. To make these intuitions concrete,
consider the following toy example, consisting of
a lost language much like English, but written using numerals:
• 15234 (asked)
• 1525 (asks)
• 4352 (desk)

serve regular phonetic relationships between the
two languages (as exempliﬁed by cognates). As a
result, each character in one language will map to
a small number of characters in the other language
(typically one, but sometimes two or three). By
incorporating this structural sparsity intuition, we
can allow the model to focus on on a smaller set of
linguistically valid hypotheses.
Below we give an overview of our model, which
is designed to capture these linguistic intuitions.
5.2 Model Structure
Our model posits that every observed word in the
lost language is composed of a sequence of morphemes (preﬁx, stem, sufﬁx). Furthermore we
posit that each morpheme was probabilistically
generated jointly with a latent counterpart in the
known language.
Our goal is to ﬁnd those counterparts that lead to
high frequency correspondences both at the character and morpheme level. The technical challenge is that each level of correspondence (character and morpheme) can completely describe the
observed data. A probabilistic mechanism based
simply on one leaves no room for the other to play
a role. We resolve this tension by employing a
non-parametric Bayesian model: the distributions
over bilingual morpheme pairs assign probability based on recurrent patterns at the morpheme
level. These distributions are themselves drawn
from a prior probabilistic process which favors
distributions with consistent character-level correspondences.
We now give a formal description of the model
(see Figure 1 for a graphical overview). There are
four basic layers in the generative process:
1. Structural sparsity: draw a set of indicator
variables ⃗ corresponding to character-edit
λ
operations.
2. Character-edit distribution: draw a base
distribution G0 parameterized by weights on
character-edit operations.
3. Morpheme-pair distributions: draw a set
of distributions on bilingual morpheme pairs
Gstm , Gpre|stm , Gsuf |stm .
4. Word generation: draw pairs of cognates
in the lost and known language, as well as
words in the lost language with no cognate
counterpart.

λ

v
G0

Gpre|stm

Gstm

Gsuf |stm
stm

stm

upre
hpre

ustm

hstm

usuf

hsuf

word

Figure 1: Plate diagram of the decipherment
model. The structural sparsity indicator variables
⃗ determine the values of the base distribution hyλ
perparameters ⃗ . The base distribution G0 dev
ﬁnes probabilities over string-pairs based solely on
character-level edits. The morpheme-pair distributions Gstm , Gpre|stm , Gsuf |stm directly assign
probabilities to highly frequent morpheme pairs.

We now go through each step in more detail.
Structural Sparsity The ﬁrst step of the generative process provides a control on the sparsity of
edit-operation probabilities, encoding the linguistic intuition that the correct character-level mappings should be sparse. The set of edit operations includes character substitutions, insertions,
and deletions, as well as a special end symbol: {(u, h), (ϵ, h), (u, ϵ), EN D} (where u and h
range over characters in the lost and known languages, respectively). For each edit operation e we
posit a corresponding indicator variable λe . The
set of character substitutions with indicators set to
one, {(u, h) : λ(u,h) = 1}) conveys the set of
phonetically valid correspondences. We deﬁne a
joint prior over these variables to encourage sparse
character mappings. This prior can be viewed as a
distribution over binary matrices and is deﬁned to
encourage rows and columns to sum to low integer
values (typically 1). More precisely, for each character u in the lost language, we count the number
∑
of mappings c(u) =
h λ(u,h) . We then deﬁne
a set of features which count how many of these
characters map to i other characters beyond some
budget bi : fi = max (0, |{u : c(u) = i}| − bi ).
Likewise, we deﬁne corresponding features fi′ and
budgets b′ for the characters h in the known lani

guage. The prior over ⃗ is then deﬁned as
λ
(
)
⃗ ⃗ ⃗′ ⃗
⃗ = exp f · w + f · w
P (λ)
Z

(1)

where the feature weight vector w is set to encour⃗
age sparse mappings, and Z is a corresponding
normalizing constant, which we never need compute. We set w so that each character must map to
⃗
at least one other character, and so that mappings
to more than one other character are discouraged 2
Character-edit Distribution The next step in
the generative process is drawing a base distribution G0 over character edit sequences (each of
which yields a bilingual pair of morphemes). This
⃗
distribution is parameterized by a set of weights ϕ
on edit operations, where the weights over substitutions, insertions, and deletions each individually
sum to one. In addition, G0 provides a ﬁxed distribution q over the number of insertions and deletions occurring in any single edit sequence. Probabilities over edit sequences (and consequently on
bilingual morpheme pairs) are then deﬁned according to G0 as:
∏
P (⃗) =
e
ϕei · q (#ins (⃗), #del (⃗))
e
e
i

We observe that the average Ugaritic word is over
two letters longer than the average Hebrew word.
Thus, occurrences of Hebrew character insertions
are a priori likely, and Ugaritic character deletions
are very unlikely. In our experiments, we set q
to disallow Ugaritic deletions, and to allow one
Hebrew insertion per morpheme (with probability
0.4).
The prior on the base distribution G0 is a
Dirichlet distribution with hyperparameters ⃗ , i.e.,
v
⃗ ∼ Dirichlet(⃗ ). Each value ve thus correϕ
v
sponds to a character edit operation e. Crucially,
the value of each ve depends deterministically on
its corresponding indicator variable:
{
1 if λe = 0,
ve =
K if λe = 1.
1.3

where K is some constant value >
The overall
effect is that when λe = 0, the marginal prior density of the corresponding edit weight ϕe spikes at
We set w0 = −∞, w1 = 0, w2 = −50, w>2 = −∞,
with budgets b′ = 7, b′ = 1 (otherwise zero), reﬂecting the
2
3
knowledge that there are eight more Ugaritic than Hebrew
letters.
3
Set to 50 in our experiments.
2

0. When λe = 1, the corresponding marginal prior
density remains relatively ﬂat and unconstrained.
See (Ishwaran and Rao, 2005) for a similar application of “spike-and-slab” priors in the regression
scenario.
Morpheme-pair Distributions Next we draw a
series of distributions which directly assign probability to morpheme pairs. The previously drawn
base distribution G0 along with a ﬁxed concentration parameter α deﬁne a Dirichlet process (Antoniak, 1974): DP (G0 , α), which provides probabilities over morpheme-pair distributions. The
resulting distributions are likely to be skewed in
favor of a few frequently occurring morphemepairs, while remaining sensitive to the characterlevel probabilities of the base distribution.
Our model distinguishes between three types of
morphemes: preﬁxes, stems, and sufﬁxes. As a
result, we model each morpheme type as arising
from distinct Dirichlet processes, that share a single base distribution:
Gstm

∼ DP (G0 , αstm )

Gpre|stm

∼ DP (G0 , αpre )

Gsuf |stm

∼ DP (G0 , αsuf )

We model preﬁx and sufﬁx distributions as conditionally dependent on the part-of-speech of the
stem morpheme-pair. This choice capture the linguistic fact that different parts-of-speech bear distinct afﬁx frequencies. Thus, while we draw a single distribution Gstm , we maintain separate distributions Gpre|stm and Gsuf |stm for each possible
stem part-of-speech.
Word Generation Once the morpheme-pair
distributions have been drawn, actual word pairs
may now be generated. First the model draws a
boolean variable ci to determine whether word i in
the lost language has a cognate in the known language, according to some prior P (ci ). If ci = 1,
then a cognate word pair (u, h) is produced:
(ustm , hstm )

∼ Gstm

(upre , hpre )

∼ Gpre|stm

(usuf , hsuf )

∼ Gsuf |stm

u = upre ustm usuf
h = hpre hstm hsuf
Otherwise, a lone word u is generated, according
a uniform character-level language model.

In summary, this model structure captures both
character and lexical level correspondences, while
utilizing morphological knowledge of the known
language. An additional feature of this multilayered model structure is that each distribution
over morpheme pairs is derived from the single
character-level base distribution G0 . As a result, any character-level mappings learned from
one type of morphological correspondence will be
propagated to all other morpheme distributions.
Finally, the character-level mappings discovered
by the model are encouraged to obey linguistically
motivated structural sparsity constraints.

6 Inference
For each word ui in our undeciphered language we predict a morphological segmentation
(upre ustm usuf )i and corresponding cognate in the
known language (hpre hstm hsuf )i . Ideally we
would like to predict the analysis with highest
marginal probability under our model given the
observed undeciphered corpus and related language lexicon. In order to do so, we need to
integrate out all the other latent variables in our
model. As these integrals are intractable to compute exactly, we resort to the standard Monte Carlo
approximation. We collect samples of the variables over which we wish to marginalize but for
which we cannot compute closed-form integrals.
We then approximate the marginal probabilities
for undeciphered word ui by summing over all the
samples, and predicting the analysis with highest
probability.
In our sampling algorithm, we avoid sampling the base distribution G0 and the derived
morpheme-pair distributions (Gstm etc.), instead
using analytical closed forms. We explicitly sample the sparsity indicator variables ⃗ the cognate
λ,
indicator variables ci , and latent word analyses
(segmentations and Hebrew counterparts). To do
so tractably, we use Gibbs sampling to draw each
latent variable conditioned on our current sample
of the others. Although the samples are no longer
independent, they form a Markov chain whose stationary distribution is the true joint distribution deﬁned by the model (Geman and Geman, 1984).
6.1 Sampling Word Analyses
For each undeciphered word, we need to sample
a morphological segmentation (upre , ustm , usuf )i
along with latent morphemes in the known lan-

guage (hpre , hstm , hsuf )i . More precisely, we
need to sample three character-edit sequences
⃗pre , ⃗stm , ⃗suf which together yield the observed
e
e
e
word ui .
We break this into two sampling steps. First
we sample the morphological segmentation of ui ,
along with the part-of-speech pos of the latent
stem cognate. To do so, we enumerate each possible segmentation and part-of-speech and calculate its joint conditional probability (for notational
clarity, we leave implicit the conditioning on the
other samples in the corpus):
P (upre , ustm , usuf , pos) =
∑
∑
∑
P (⃗stm )
e
P (⃗pre |pos)
e
P (⃗suf |pos)
e
⃗stm
e

⃗pre
e

⃗suf
e

(2)
where the summations over character-edit sequences are restricted to those which yield the segmentation (upre , ustm , usuf ) and a latent cognate
with part-of-speech pos.
For a particular stem edit-sequence ⃗stm , we
e
compute its conditional probability in closed form
according to a Chinese Restaurant Process (Antoniak, 1974). To do so, we use counts from
the other sampled word analyses: countstm (⃗stm )
e
gives the number of times that the entire editsequence ⃗stm has been observed:
e
∏
countstm (⃗stm ) + α i p(ei )
e
P (⃗stm ) ∝
e
n+α
where n is the number of other word analyses sampled, and α is a ﬁxed concentration parameter. The
∏
product i p(ei ) gives the probability of ⃗stm ace
cording to the base distribution G0 . Since the
parameters of G0 are left unsampled, we use the
marginalized form:
ve + count(e)
p(e) = ∑
e′ ve′ + k

(3)

where count(e) is the number of times that
character-edit e appears in distinct edit-sequences
(across preﬁxes, stems, and sufﬁxes), and k is the
sum of these counts across all character-edits. Recall that ve is a hyperparameter for the Dirichlet
prior on G0 and depends on the value of the corresponding indicator variable λe .
Once the segmentation (upre , ustm , usuf ) and
part-of-speech pos have been sampled, we proceed to sample the actual edit-sequences (and thus

latent morphemes counterparts). Now, instead of
summing over the values in Equation 2, we instead
sample from them.
6.2 Sampling Sparsity Indicators
Recall that each sparsity indicator λe determines
the value of the corresponding hyperparameter ve
of the Dirichlet prior for the character-edit base
distribution G0 . In addition, we have an unnormal⃗
ized joint prior P (⃗ = g(λ) which encourages a
λ)
Z
sparse setting of these variables. To sample a particular λe , we consider the set ⃗ in which λe = 0
λ
⃗′ in which λe = 1. We then compute:
and λ
[count(e)]

ve
P (⃗ ∝ g(⃗ · ∑
λ)
λ)

e′

[k]

ve′

where k is the sum of counts for all edit operations, and the notation a[b] indicates the ascending
factorial. Likewise, we can compute a probability
′
⃗
for λ′ with corresponding values ve .
6.3 Sampling Cognate Indicators
Finally, for each word ui , we sample a corresponding indicator variable ci . To do so, we calculate Equation 2 for all possible segmentations and
parts-of-speech and sum the resulting values to obtain the conditional likelihood P (ui |ci = 1). We
also calculate P (ui |ci = 0) using a uniform unigram character-level language model (and thus depends only on the number of characters in ui ). We
then sample from among the two values:
P (ui |ci = 1) · P (ci = 1)
P (ui |ci = 0) · P (ci = 0)

6.5 Implementation Details
Many of the steps detailed above involve the consideration of all possible edit-sequences consistent with (i) a particular undeciphered word ui and
(ii) the entire lexicon of words in the known language (or some subset of words with a particular part-of-speech). In particular, we need to both
sample from and sum over this space of possibilities repeatedly. Doing so by simple enumeration
would needlessly repeat many sub-computations.
Instead we use ﬁnite-state acceptors to compactly
represent both the entire Hebrew lexicon as well
as potential Hebrew word forms for each Ugaritic
word. By intersecting two such FSAs and minimizing the result we can efﬁciently represent all
potential Hebrew words for a particular Ugaritic
word. We weight the edges in the FSA according
to the base distribution probabilities (in Equation 3
above). Although these intersected acceptors have
to be constantly reweighted to reﬂect changing
probabilities, their topologies need only be computed once. One weighted correctly, marginals
and samples can be computed using dynamic programming.
Even with a large number of sampling rounds, it
is difﬁcult to fully explore the latent variable space
for complex unsupervised models. Thus a clever
initialization is usually required to start the sampler in a high probability region. We initialize our
model with the results of the HMM-based baseline
(see section 8), and rule out character substitutions
with probability < 0.05 according to the baseline.

7 Experiments

6.4 High-level Resampling

7.1 Corpus and Annotations

Besides the individual sampling steps detailed
above, we also consider several larger sampling
moves in order to speed convergence. For example, for each type of edit-sequence ⃗ which has
e
been sampled (and may now occur many times
throughout the data), we consider a single joint
⃗
move to another edit-sequence e′ (both of which
yield the same lost language morpheme u). The
details are much the same as above, and as before
the set of possible edit-sequences is limited by the
string u and the known language lexicon.
We also resample groups of the sparsity indicator variables ⃗ in tandem, to allow a more rapid exλ
ploration of the probability space. For each character u, we block sample the entire set {λ(u,h) }h ,
and likewise for each character h.

We apply our model to the ancient Ugaritic language (see Section 3 for background). Our undeciphered corpus consists of an electronic transcription of the Ugaritic tablets (Cunchillos et al.,
2002). This corpus contains 7,386 unique word
types. As our known language corpus, we use the
Hebrew Bible, which is both geographically and
temporally close to Ugaritic. To extract a Hebrew
morphological lexicon we assume the existence
of manual morphological and part-of-speech annotations (Groves and Lowery, 2006). We divide
Hebrew stems into four main part-of-speech categories each with a distinct afﬁx proﬁle: Noun,
Verb, Pronoun, and Particle. For each part-ofspeech category, we determine the set of allowable
afﬁxes using the annotated Bible corpus.

Baseline
Our Model
No Sparsity

Words
type
token
28.82% 46.00%
60.42% 66.71%
46.08% 54.01%

Morphemes
type
token
N/A
N/A
75.07% 81.25%
69.48% 76.10%

Table 1: Accuracy of cognate translations, measured with respect to complete word-forms and
morphemes, for the HMM-based substitution cipher baseline, our complete model, and our model
without the structural sparsity priors. Note that the
baseline does not provide per-morpheme results,
as it does not predict morpheme boundaries.
To evaluate the output of our model, we annotated the words in the Ugaritic lexicon with the
corresponding Hebrew cognates found in the standard reference dictionary (del Olo Lete and Sanmart´n, 2004). In addition, manual morphological
ı
segmentation was carried out with the guidance of
a standard Ugaritic grammar (Schniedewind and
Hunt, 2007). Although Ugaritic is an inﬂectional
rather than agglutinative language, in its written
form (which lacks vowels) words can easily be
segmented (e.g. wypltn becomes wy-plt-n).
.
.
Overall, we identiﬁed Hebrew cognates for
2,155 word forms, covering almost 1/3 of the
Ugaritic vocabulary.4

8 Evaluation Tasks and Results
We evaluate our model on four separate decipherment tasks: (i) Learning alphabetic mappings,
(ii) translating cognates, (iii) identifying cognates,
and (iv) morphological segmentation.
As a baseline for the ﬁrst three of these tasks
(learning alphabetic mappings and translating and
identifying cognates), we adapt the HMM-based
method of Knight et al. (2006) for learning letter substitution ciphers. In its original setting, this
model was used to map written texts to spoken language, under the assumption that each character
was emitted from a hidden phonemic state. In our
adaptation, we assume instead that each Ugaritic
character was generated by a hidden Hebrew letter. Hebrew character trigram transition probabilities are estimated using the Hebrew Bible, and Hebrew to Ugaritic character emission probabilities
are learned using EM. Finally, the highest prob4

We are conﬁdent that a large majority of Ugaritic words
with known Hebrew cognates were thus identiﬁed. The
remaining Ugaritic words include many personal and geographic names, words with cognates in other Semitic languages, and words whose etymology is uncertain.

ability sequence of latent Hebrew letters is predicted for each Ugaritic word-form, using Viterbi
decoding.
Alphabetic Mapping The ﬁrst essential step towards successful decipherment is recovering the
mapping between the symbols of the lost language
and the alphabet of a known language. As a gold
standard for this comparison, we use the wellestablished relationship between the Ugaritic and
Hebrew alphabets (Hetzron, 1997). This mapping
is not one-to-one but is generally quite sparse. Of
the 30 Ugaritic symbols, 28 map predominantly
to a single Hebrew letter, and the remaining two
map to two different letters. As the Hebrew alphabet contains only 22 letters, six map to two distinct Ugaritic letters and two map to three distinct
Ugaritic letters.
We recover our model’s predicted alphabetic
mappings by simply examining the sampled values of the binary indicator variables λu,h for each
Ugaritic-Hebrew letter pair (u, h). Due to our
structural sparsity prior P (⃗ the predicted mapλ),
pings are sparse: each Ugaritic letter maps to only
a single Hebrew letter, and most Hebrew letters
map to only a single Ugaritic letter. To recover
alphabetic mappings from the HMM substitution
cipher baseline, we predict the Hebrew letter h
which maximizes the model’s probability P (h|u),
for each Ugaritic letter u.
To evaluate these mappings, we simply count
the number of Ugaritic letters that are correctly
mapped to one of their Hebrew reﬂexes. By this
measure, the baseline recovers correct mappings
for 22 out of 30 Ugaritic characters (73.3%). Our
model recovers correct mappings for all but one
(very low frequency) Ugaritic characters, yielding
96.67% accuracy.
Cognate Decipherment We compare the decipherment accuracy for Ugaritic words that have
corresponding Hebrew cognates. We evaluate
our model’s predictions on each distinct Ugaritic
word-form at both the type and token level. As
Table 1 shows, our method correctly translates
over 60% of all distinct Ugaritic word-forms with
Hebrew cognates and over 71% of the individual morphemes that compose them, outperforming the baseline by signiﬁcant margins. Accuracy improves when the frequency of the wordforms is taken into account (token-level evaluation), indicating that the model is able to decipher frequent words more accurately than infre-

1

Morfessor
Our Model

True positive rate

0.8

recall
67.48%
90.53%

f-measure
76.71%
88.53%

Table 2: Morphological segmentation accuracy for
a standard unsupervised baseline and our model.

0.6

0.4

Our Model
Baseline
Random

0.2

0

precision
88.87%
86.62%

0

0.2

0.4

0.6

0.8

those Ugaritic word-forms which are very unlikely
to have Hebrew cognates.
1

False positive rate

Figure 2: ROC curve for cognate identiﬁcation.
quent words. We also measure the average Levenshtein distance between predicted and actual cognate word-forms. On average, our model’s predictions lie 0.52 edit operations from the true cognate, whereas the baseline’s predictions average a
distance of 1.26 edit operations.
Finally, we evaluated the performance of our
model when the structural sparsity constraints are
not used. As Table 1 shows, performance degrades
signiﬁcantly in the absence of these priors, indicating the importance of modeling the sparsity of
character mappings.
Cognate identiﬁcation We evaluate our
model’s ability to identify cognates using the
sampled indicator variables ci . As before, we
compare our performance against the HMM
substitution cipher baseline. To produce baseline
cognate identiﬁcation predictions, we calculate
the probability of each latent Hebrew letter sequence predicted by the HMM, and compare it to
a uniform character-level Ugaritic language model
(as done by our model, to avoid automatically
assigning higher cognate probability to shorter
Ugaritic words). For both our model and the
baseline, we can vary the threshold for cognate
identiﬁcation by raising or lowering the cognate
prior P (ci ). As the prior is set higher, we detect
more true cognates, but the false positive rate
increases as well.
Figure 2 shows the ROC curve obtained by
varying this prior both for our model and the baseline. At all operating points, our model outperforms the baseline, and both models always predict better than chance. In practice for our model,
we use a high cognate prior, thus only ruling out

Morphological segmentation Finally, we evaluate the accuracy of our model’s morphological
segmentation for Ugaritic words. As a baseline
for this comparison, we use Morfessor CategoriesMAP (Creutz and Lagus, 2007). As Table 2
shows, our model provides a signiﬁcant boost in
performance, especially for recall. This result is
consistent with previous work showing that morphological annotations can be projected to new
languages lacking annotation (Yarowsky et al.,
2000; Snyder and Barzilay, 2008), but generalizes
those results to the case where parallel data is unavailable.

9 Conclusion and Future Work
In this paper we proposed a method for the automatic decipherment of lost languages. The key
strength of our model lies in its ability to incorporate a range of linguistic intuitions in a statistical
framework.
We hope to address several issues in future
work. Our model fails to take into account
the known frequency of Hebrew words and morphemes. In fact, the most common error is incorrectly translating the masculine plural sufﬁx (-m)
as the third person plural possessive sufﬁx (-m)
rather than the correct and much more common
plural sufﬁx (-ym). Also, even with the correct alphabetic mapping, many words can only be deciphered by examining their literary context. Our
model currently operates purely on the vocabulary
level and thus fails to take this contextual information into account. Finally, we intend to explore
our model’s predictive power when the family of
the lost language is unknown.5
5
The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0835445, and grant IIS0835652) and the Microsoft Research New Faculty Fellowship. Thanks to Michael Collins, Tommi Jaakkola, and
the MIT NLP group for their suggestions and comments.
Any opinions, ﬁndings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reﬂect the views of the funding organizations.

References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to bayesian nonparametric
problems. The Annals of Statistics, 2:1152–1174,
November.
Alexandre Bouchard, Percy Liang, Thomas Grifﬁths,
and Dan Klein. 2007. A probabilistic approach to
diachronic phonology. In Proceedings of EMNLP,
pages 887–896.
Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and
Language Processing, 4(1).
Jesus-Luis Cunchillos, Juan-Pablo Vita, and Jose´
Angel Zamora. 2002. Ugaritic data bank. CDROM.
Gregoria del Olo Lete and Joaqu´n Sanmart´n. 2004.
ı
ı
A Dictionary of the Ugaritic Language in the Alphabetic Tradition. Number 67 in Handbook of Oriental
Studies. Section 1 The Near and Middle East. Brill.
Pascale Fung and Kathleen McKeown. 1997. Finding terminology translations from non-parallel corpora. In Proceedings of the Annual Workshop on
Very Large Corpora, pages 192–202.
S. Geman and D. Geman. 1984. Stochastic relaxation,
gibbs distributions and the bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12:609–628.
Alan Groves and Kirk Lowery, editors. 2006. The
Westminster Hebrew Bible Morphology Database.
Westminster Hebrew Institute, Philadelphia, PA,
USA.
Jacques B. M. Guy. 1994. An algorithm for identifying
cognates in bilingual wordlists and its applicability
to machine translation. Journal of Quantitative Linguistics, 1(1):35–42.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the
ACL/HLT, pages 771–779.
Robert Hetzron, editor. 1997. The Semitic Languages.
Routledge.
H. Ishwaran and J.S. Rao. 2005. Spike and slab variable selection: frequentist and Bayesian strategies.
The Annals of Statistics, 33(2):730–773.
Kevin Knight and Richard Sproat. 2009. Writing systems, transliteration and decipherment. NAACL Tutorial.
K. Knight and K. Yamada. 1999. A computational approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natural Language Processing.

Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of the COLING/ACL, pages 499–506.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition, pages 9–16.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceeding of
NAACL, pages 1–8.
Grzegorz Kondrak. 2009. Identiﬁcation of cognates
and recurrent sound correspondences in word lists.
Traitement Automatique des Langues, 50(2):201–
235.
John B. Lowe and Martine Mazaudon. 1994. The reconstruction engine: a computer implementation of
the comparative method. Computational Linguistics, 20(3):381–417.
Reinhard Rapp. 1999. Automatic identiﬁcation of
word translations from unrelated english and german
corpora. In Proceedings of the ACL, pages 519–526.
Andrew Robinson. 2002. Lost Languages: The
Enigma of the World’s Undeciphered Scripts.
McGraw-Hill.
William M. Schniedewind and Joel H. Hunt. 2007. A
Primer on Ugaritic: Language, Culture and Literature. Cambridge University Press.
Mark S. Smith, editor. 1955. Untold Stories: The Bible
and Ugaritic Studies in the Twentieth Century. Hendrickson Publishers.
Benjamin Snyder and Regina Barzilay. 2008. Crosslingual propagation for morphological analysis. In
Proceedings of the AAAI, pages 848–854.
Wilfred Watson and Nicolas Wyatt, editors.
Handbook of Ugaritic Studies. Brill.

1999.

David Yarowsky, Grace Ngai, and Richard Wicentowski. 2000. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of HLT, pages 161–168.

Why do we Accent Words?
The Processing of Information Structure and Prosodic Structure
Sasha Calhoun
School of Philosophy, Psychology and Language Sciences
University of Edinburgh
Submission to
Sasha.Calhoun@ed.ac.uk
Experimental and Theoretical Advances in Prosody
Cornell, April 2008
One of the main functions of prosody is to signal information structure: i.e. to “chunk” the
information in a spoken utterance into coherent units; and to “highlight” the important, or
focussed (as per Rooth 1992), element(s) in each unit (e.g. Halliday 1968). However, as much
work has shown, it is problematic to directly equate prosodic phrases with information units,
and pitch accents with semantic focus (e.g. Ladd 1996, Truckenbrodt 1995, Wagner 2006). I
propose that, rather than a direct relationship; information structure acts as a strong
probabilistic constraint on the alignment of words with metrical prosodic structure (see also
Calhoun 2006). Foci try to align with nuclear accents; however realisation is also affected by
other factors, including lexical predictability and syntactic structure, constraints on prosodic
structure itself, as well as pragmatic distinctions. In support of this model, I present an
analysis of accent prediction results using a substantial corpus of annotated spontaneous
speech. Finally, I seek to motivate the model in terms of language processing. Why should we
compute a separate prosodic structure rather than just using the phonetic cues to prominence
and phrasing directly? Does the “highlighting” function of prosody really separate into both
semantic focus and lexical predictability? Why?
Experimental evidence comes from accent prediction models developed using the NXT
Switchboard corpus (Calhoun et al. 2005). A subset of 8915 words from 18 conversations in
the corpus were annotated for prosodic features, including nuclear and non-nuclear accents
and phrase breaks; focus status; and lexical and syntactic features including part-of-speech,
syntactic clauses and accent ratio (a measure of a word’s “accentability” shown to perform
better than other standard lexical predictability measures, Nenkova et al. 2007). Multiple
regression models were trained to predict the accent status (nuclear, non-nuclear or
unaccented) of each word, using different feature sets, i.e. phrasal (e.g. position in the phrase,
words since last accent), semantic/syntactic (e.g. focus status, position in clause, accent ratio)
and combined (phrasal + semantic/syntactic). The phrasal feature model performed better
than the semantic/syntactic model and only slightly worse than the combined model. This is
consistent with the view that accents are manifestations of metrical structure, rather than
appearing independently to mark semantic or lexical features. Crucially, both the
semantic/syntactic and combined models could reliably predict the occurrence of nuclear
accents, but not non-nuclear accents. Focus status was more strongly predictive of nuclear
than non-nuclear accents, and vice versa for accent ratio. These results suggest that focus acts
mainly as a constraint on the position of the nuclear accent, while the appearance of nonnuclear accents are determined by other factors, such as the inherent accentability of the word
and rhythm.
My model and accent prediction results offer support for a coherent, metrical prominence
structure; rather than direct use of phonetic cues to prominence in processing. It has been

proposed that prosodic phrasing acts to “unify” different levels of linguistic representation of
an utterance (e.g. Frazier et al. 2006). I suggest this is also achieved through the hierarchical
patterns of metrical prominence. The highly rhythmical, and predictable, nature of these
patterns may facilitate this. Further, these results seem to show that the “highlighting”
function of prosodic prominence needs to be separated into the marking of semantic focus and
lexical predictability, as these have distinct effects on accent realisation. I suggest this is
because the two operate at quite different levels of language processing. The signalling of
semantic focus, which affects truth-conditional and pragmatic interpretation, is part of the
high-level planning of an utterance; concurrently with the overall planning of prosodic
structure. Lexical predictability, on the other hand, seems strongly related to lexical retrieval;
and therefore principally affects low-level within-phrase prominence patterns. It is hoped that
this discussion, along with my theoretical model and experimental results, will help in the
move towards more sophisticated use of both prosody and information structure in models of
language processing.

References
Calhoun, S. 2006. Information Structure and the Prosodic Structure of English: a Probabilistic Relationship, PhD
thesis, University of Edinburgh.
Calhoun, S., M. Nissim, M. Steedman & J. Brenier 2005. A Framework for Annotating Information Structure in
Discourse. In ‘Frontiers in Corpus Annotation II’, ACL Workshop, Ann Arbor, MI.
Frazier, L., K. Carlson & C. Clifton Jr. 2006. Prosodic phrasing is central to language comprehension, Trends in
Cognitive Science 10, 6, 244-249.
Halliday, M. 1968. Notes on transitivity and theme in English: Part 3, Journal of Linguistics 4, 179–215.
Ladd, D. R. 1996. Intonational Phonology, Cambridge University Press, Cambridge, UK.
Nenkova, A., J. Brenier, A. Kothari, S. Calhoun, L. Whitton, D. Beaver & D. Jurafsky 2007. To memorize or to
predict: Prominence labelling in conversational speech. In HLT:NAACL, Rochester, NY.
Rooth, M. 1992. A theory of focus intepretation, Natural Language Semantics 1, 75–116.
Truckenbrodt, H. 1995. Phonological Phrases: Their Relation to Syntax, Focus and Prominence, PhD thesis,
MIT.
Wagner, M. 2006, Givenness and locality, in M. Gibson & J. Howell, eds, ‘Proceedings of SALT XVI’, CLC
Publications, Ithaca, NY.

Why do we Accent Words?
The Processing of Information Structure and Prosodic Structure
Sasha Calhoun
School of Philosophy, Psychology and Language Sciences
University of Edinburgh
Submission to
Sasha.Calhoun@ed.ac.uk
Experimental and Theoretical Advances in Prosody
Cornell, April 2008
One of the main functions of prosody is to signal information structure: i.e. to “chunk” the
information in a spoken utterance into coherent units; and to “highlight” the important, or
focussed (as per Rooth 1992), element(s) in each unit (e.g. Halliday 1968). However, as much
work has shown, it is problematic to directly equate prosodic phrases with information units,
and pitch accents with semantic focus (e.g. Ladd 1996, Truckenbrodt 1995, Wagner 2006). I
propose that, rather than a direct relationship; information structure acts as a strong
probabilistic constraint on the alignment of words with metrical prosodic structure (see also
Calhoun 2006). Foci try to align with nuclear accents; however realisation is also affected by
other factors, including lexical predictability and syntactic structure, constraints on prosodic
structure itself, as well as pragmatic distinctions. In support of this model, I present an
analysis of accent prediction results using a substantial corpus of annotated spontaneous
speech. Finally, I seek to motivate the model in terms of language processing. Why should we
compute a separate prosodic structure rather than just using the phonetic cues to prominence
and phrasing directly? Does the “highlighting” function of prosody really separate into both
semantic focus and lexical predictability? Why?
Experimental evidence comes from accent prediction models developed using the NXT
Switchboard corpus (Calhoun et al. 2005). A subset of 8915 words from 18 conversations in
the corpus were annotated for prosodic features, including nuclear and non-nuclear accents
and phrase breaks; focus status; and lexical and syntactic features including part-of-speech,
syntactic clauses and accent ratio (a measure of a word’s “accentability” shown to perform
better than other standard lexical predictability measures, Nenkova et al. 2007). Multiple
regression models were trained to predict the accent status (nuclear, non-nuclear or
unaccented) of each word, using different feature sets, i.e. phrasal (e.g. position in the phrase,
words since last accent), semantic/syntactic (e.g. focus status, position in clause, accent ratio)
and combined (phrasal + semantic/syntactic). The phrasal feature model performed better
than the semantic/syntactic model and only slightly worse than the combined model. This is
consistent with the view that accents are manifestations of metrical structure, rather than
appearing independently to mark semantic or lexical features. Crucially, both the
semantic/syntactic and combined models could reliably predict the occurrence of nuclear
accents, but not non-nuclear accents. Focus status was more strongly predictive of nuclear
than non-nuclear accents, and vice versa for accent ratio. These results suggest that focus acts
mainly as a constraint on the position of the nuclear accent, while the appearance of nonnuclear accents are determined by other factors, such as the inherent accentability of the word
and rhythm.
My model and accent prediction results offer support for a coherent, metrical prominence
structure; rather than direct use of phonetic cues to prominence in processing. It has been

proposed that prosodic phrasing acts to “unify” different levels of linguistic representation of
an utterance (e.g. Frazier et al. 2006). I suggest this is also achieved through the hierarchical
patterns of metrical prominence. The highly rhythmical, and predictable, nature of these
patterns may facilitate this. Further, these results seem to show that the “highlighting”
function of prosodic prominence needs to be separated into the marking of semantic focus and
lexical predictability, as these have distinct effects on accent realisation. I suggest this is
because the two operate at quite different levels of language processing. The signalling of
semantic focus, which affects truth-conditional and pragmatic interpretation, is part of the
high-level planning of an utterance; concurrently with the overall planning of prosodic
structure. Lexical predictability, on the other hand, seems strongly related to lexical retrieval;
and therefore principally affects low-level within-phrase prominence patterns. It is hoped that
this discussion, along with my theoretical model and experimental results, will help in the
move towards more sophisticated use of both prosody and information structure in models of
language processing.

References
Calhoun, S. 2006. Information Structure and the Prosodic Structure of English: a Probabilistic Relationship, PhD
thesis, University of Edinburgh.
Calhoun, S., M. Nissim, M. Steedman & J. Brenier 2005. A Framework for Annotating Information Structure in
Discourse. In ‘Frontiers in Corpus Annotation II’, ACL Workshop, Ann Arbor, MI.
Frazier, L., K. Carlson & C. Clifton Jr. 2006. Prosodic phrasing is central to language comprehension, Trends in
Cognitive Science 10, 6, 244-249.
Halliday, M. 1968. Notes on transitivity and theme in English: Part 3, Journal of Linguistics 4, 179–215.
Ladd, D. R. 1996. Intonational Phonology, Cambridge University Press, Cambridge, UK.
Nenkova, A., J. Brenier, A. Kothari, S. Calhoun, L. Whitton, D. Beaver & D. Jurafsky 2007. To memorize or to
predict: Prominence labelling in conversational speech. In HLT:NAACL, Rochester, NY.
Rooth, M. 1992. A theory of focus intepretation, Natural Language Semantics 1, 75–116.
Truckenbrodt, H. 1995. Phonological Phrases: Their Relation to Syntax, Focus and Prominence, PhD thesis,
MIT.
Wagner, M. 2006, Givenness and locality, in M. Gibson & J. Howell, eds, ‘Proceedings of SALT XVI’, CLC
Publications, Ithaca, NY.

Behavior Research Methods
2009, 41 (2), 385-390
doi:10.3758/BRM.41.2.385

Praat script to detect syllable nuclei
and measure speech rate automatically
Nivja H. de Jong

University of Amsterdam, Amsterdam, The Netherlands
and Utrecht University, Utrecht, The Netherlands
and

Ton Wempe

University of Amsterdam, Amsterdam, The Netherlands
In this article, we describe a method for automatically detecting syllable nuclei in order to measure speech rate
without the need for a transcription. A script written in the software program Praat (Boersma & Weenink, 2007)
detects syllables in running speech. Peaks in intensity (dB) that are preceded and followed by dips in intensity are
considered to be potential syllable nuclei. The script subsequently discards peaks that are not voiced. Testing the
resulting syllable counts of this script on two corpora of spoken Dutch, we obtained high correlations between
speech rate calculated from human syllable counts and speech rate calculated from automatically determined
syllable counts. We conclude that a syllable count measured in this automatic fashion suffices to reliably assess
and compare speech rates between participants and tasks.

Speech rate is used as a measure of fluency in second
language acquisition and bilingualism research (e.g.,
O’Brien, Segalowitz, Freed, & Collentine, 2007; Riggenbach, 1991; Towell, Hawkins, & Bazergui, 1996); in research diagnosing speech and language disorders (e.g.,
Feyereisen, Pillon, & de Partz, 1991; Redmond, 2004;
Shenker, 2006); and in research on speech of pathological populations (e.g., Cannizzaro, Harel, Reilly, Chappell,
& Snyder, 2004; Covington et al., 2005). However, calculating speech rate by hand is a tedious task, which is
therefore often not carried out. In this article, we present a
script written in the software program Praat (Boersma &
Weenink, 2007) that automatically detects syllable nuclei
in order to calculate speech rate. The nucleus of a syllable,
also called the peak, is the central part of a syllable (most
commonly, the vowel in the syllable). Locating these syllable nuclei allows for a computation of number of syllables, which can be used to calculate speech rate.
According to Tavakoli and Skehan (2005), fluency is
multifaceted in nature. They distinguish three different
facets of fluency: breakdown fluency (number and length
of pauses), speed and density per time unit (speech rate),
and repair fluency (false starts and repetitions). In second language testing practice, fluency is usually a score
awarded by human judges who use several aspects of
fluency in their judgment. However, Cucchiarini, Strik,
and Boves (2002) have shown that of several objectively
measured aspects of fluency, speech rate (as measured
by phonemes per time unit) is the best predictor of sub-

jective fluency. Kormos and Dénes (2004) likewise have
shown that speech rate (in terms of number of syllables
per time unit) is a good predictor of subjective fluency.
We conclude that, for researchers wanting to include a
measure of fluency, speech rate is an important factor to
take into account. However, because of time constraints,
this measure is often impossible to carry out. For instance,
the script presented in this article was written in order to
be able to measure the speech rate of 250 participants in
a corpus of over 45 h of speech, a task that would take at
least 8 months of full-time work for one person to measure
by hand. In the context of a large-scale research project on
the correlates of speaking proficiency carried out at the
University of Amsterdam (What Is Speaking Proficiency:
www.hum.uva.nl/wisp), we developed two tools to measure fluency automatically. For the purpose of measuring
pauses in running speech, we wrote a script in the software
program Praat to automatically detect silence in speech
[a simplified version of which is now incorporated in the
button To TextGrid (silences) in the Praat software]. For
the purpose of estimating the speech rate of speech performances, we wrote a script in Praat that automatically
detects syllable nuclei to compute speech rate in terms of
syllables per time unit. In this article, we will present and
validate the script for detecting syllable nuclei.
For automatic speech recognition, speech rate is an important factor as well. Human listeners are able to understand
both fast and slow speech. Speech recognizers implemented
in computers, however, perform relatively poorly when

N. H. de Jong, nivja.dejong@let.uu.nl

	

385	

© 2009 The Psychonomic Society, Inc.

386     de Jong and Wempe
speech rate is very fast or very slow. In order to improve
computer performance, several researchers have proposed
that measuring speech rate prior to speech recognition will
result in higher success rates of automatic speech recognizers (Pfau, Faltlhauser, & Ruske, 2000), and several ways to
automatically measure speech rate in terms of phones and/
or syllables per time unit have been put forward.
Mermelstein (1975) developed an algorithm with which
to segment speech into syllables by finding minima in loudness that serve as possible syllable boundaries. Verhasselt
and Martens (1996) presented an automatic speech detector
that measures phone boundaries and thus calculates rate of
speech as phone rate. The phone boundaries are provided
by a multilayer perceptron that is trained on a subset of the
data that must be hand-segmented at the phone level. Pfitzinger (1999) used a combination of syllable rate and phone
rate for correlations with perceptual speech rate. Syllable
rate was calculated by counting peaks in the energy contour,
whereas phone rate was calculated by use of transcription.
The syllable, phone, and perceptual speech rates were measured over (very) short stimuli (625 msec). Hunt (1993)
used recurrent neural networks to detect syllables. Pfau and
Ruske (1998) determined syllable nuclei by detecting vowels on smoothed modified loudness, and then calculated
speech rate. Pellegrino and Andre-Obrecht (2000) used a
vowel-detection algorithm based on the spectral analysis
to detect formant–antiformant structure that is specific
for vowels (and some nasals). In their study, the voweldetection algorithm was used for automatic language identification. In the study by Pellegrino, Farinas, and Rouas
(2004), the same vowel-detection algorithm was used to
estimate speech rate. They compared hand-­ easured and
m
automatically measured speech rates, calculated for speech
performances of around 45 sec with speakers of several
languages. Finally, Wang and Narayanan (2007) also developed a method for speech-rate estimation; they made
use of spectral subband correlation including temporal correlation, prominent spectral subbands, and pitch information to estimate syllable nuclei.
All of these different automatic ways to measure syllable and/or phone rate are quite successful. It is difficult,
however, to compare the success of these automatic measurers, because they were all used on different corpora,
and their success was reported in different ways. Some researchers have reported correlations between human and
automatic speech rate; others, a percentage of syllables (or
phones) undetected and falsely detected as compared with
human-measured syllables (or phones); and still others,
a correlation between the number of manually measured
and automatically measured syllables (or phones). The
difference in corpora used should also be noted, because
some studies have used large corpora with many different
speakers and others have used quite small corpora with few
speakers; some have used corpora of speech read aloud,
whereas others have used (semi-) spontaneous speech
corpora. Finally, a noteworthy difference between studies
concerns the length of the spurt on which speech rate was
calculated. Some studies have used extremely short time
windows to calculate speech rate, and others have used
much longer windows. Perhaps the most obvious reason

why we cannot compare success of these different automatic speech-rate measurers is that the length of the time
window (or spurt) as well as the variance in spurt length
will strongly influence calculations of speech rate.
Many of the proposed speech-rate measurers need to
be trained on a subset of the data that is transcribed or
preprocessed by hand (Hunt, 1993; Pfau & Ruske, 1998;
Pfitzinger, 1999; Verhasselt & Martens, 1996). In this article, we will present an easy way to automatically measure speech rate without the use of preprocessing or the
need for transcriptions and test it on two different corpora
of spontaneously spoken Dutch. To be able to compare
the success of the script over these two different corpora,
spurt length was controlled. We wrote a script in Praat
using a combination of intensity (similar to Pfitzinger,
1999) and voicedness (similar to Pfau & Ruske, 1998) to
find syllable nuclei.
The Praat Script
In what follows, we describe the sequence of actions
that the script completes to find syllable nuclei using intensity (dB) and voicedness. We use intensity first to be
able to find peaks in the energy contour, since a vowel
within a syllable (the syllable nucleus) has higher energy
than do surrounding sounds (described in Steps 1 and 2,
below). We then use the intensity contour to make sure
that the intensity between the current peak and the preceding peak is sufficiently low. With this procedure, we delete
multiple peaks within one syllable (described in Step 3,
below). Finally, we use voicedness to exclude peaks that
are unvoiced, which is required to delete surrounding
voiceless consonants that have high intensity (described
in Step 4, below). Before the script is run, sound files that
are quite noisy should be filtered so that the frequency
range is speech-band limited.
Step 1. We extract the intensity, with the parameter
“minimum pitch” set to 50 Hz, using autocorrelation. With
this parameter setting, we extract intensity smoothed over
a time window of 64 msec, with 16-msec time steps.
Step 2. We consider all peaks above a certain threshold
in intensity to be potential syllables. We set the threshold
to 0 or 2 dB above the median intensity measured over the
total sound file (0 dB if the sound is not filtered; 2 dB if
the sound is filtered). We use the median, rather than the
mean, to calculate the threshold in order to avoid including extreme peaks in the calculation of the threshold.
Step 3. We inspect the preceding dip in intensity and consider only a peak with a preceding dip of at least 2 or 4 dB
with respect to the current peak as a potential syllable (2 dB
if the sound is not filtered; 4 dB if the sound is filtered).
Step 4. We extract the pitch contour, this time using
a window size of 100 msec and 20-msec time steps, and
exclude all peaks that are unvoiced.
Step 5. The remaining peaks are considered syllable
nuclei and are saved in a TextGrid (point tier).
Figure 1 shows a part of a sound file together with the
output as TextGrid made by the script. The speech utterance depicted here is dat uh was wel goed bevallen toen
(“that uhm was quite well liked then”), totaling nine syllables, including “uh.” The script is available at sites.google

Praat Script To Detect Syllable Nuclei     387

Figure 1. Part of a speech file in Praat with intensity (white line in spectrum)
and pitch (dark line in spectrum) shown. The points in the tier are the syllable
nuclei as detected by the script.

.com/site/speechrate under the terms of the GNU General
Public License (de Jong & Wempe, 2008).
Performance
As a part of the project “What Is Speaking Proficiency”
(WISP), conducted at the Amsterdam Center for Language
and Communication at the University of Amsterdam, we
have collected speech data from 258 ­ articipants—200
p
nonnative speakers of Dutch (with various first languages) and 58 native speakers of Dutch. Each participant performed 8 speaking tasks, resulting in a total of approximately 46 h of speech. See de Jong, Steinel, Florijn,
Schoonen, and Hulstijn (2007) for a description of the
speaking tasks and an application of the fluency measures.
In order to be able to include measures of fluency in our
research, we made two scripts written in Praat. The first
script automatically detects pauses [a modified version
is now incorporated in Praat in the TextGrid (to silences)
button], and the second script automatically detects syllables. The second script is described in this article. In
what follows, we report a validation of the computation
of syllables per time unit as generated by the script in two
different corpora. First, we randomly selected 50 out of
the total of 258 * 8 speaking tasks and measured syllables
by hand. This corpus comprised 75 min of speech. Second, we tested the script on a subset of the IFA corpus that
was comparable to the speaking tasks in the WISP study
(van Son, Binnenpoorte, van den Heuvel, & Pols, 2001).
This part of the corpus comprised 125 min of speech
summed over 8 participants.

Speech Data for the WISP Study
We counted the syllables of 50 speech files. Pauses longer than 0.4 sec were considered possible spurt boundaries. We used all spurts of 5 sec or more to calculate speech
rate, and combined consecutive shorter spurts to get to
5 sec (excluding pauses). We thus avoided calculating
speech rate over very short periods of time. We then automatically detected syllables using the Praat script. Many
sound files in this corpus were moderately noisy; therefore, we filtered all sounds prior to the syllable measuring,
using 100 Hz as the lower edge of the pass band, 5000 Hz
as the upper edge of the pass band, and 50 Hz as the width
of the smoothing region. With these settings, we attenuate
nonspeech frequency components and keep all possible
voice-related information about intensity and voicedness,
across all formants. Measuring peaks in intensity (dB),
we used 2 dB above the median intensity per sound file
as threshold, and 4 dB as minimum dip between peaks,
excluding peaks that are unvoiced.
We computed speech rate by hand and from the script
per spurt by dividing the number of syllables per spurt for
both measures by the spurt length in seconds. Figure 2
shows the scatterplot of the human and automatic speechrate calculations per spurt; the correlation was .71. For
our purposes, of comparing speakers and/or tasks, however, we needed a less refined calculation of speech rate.
Figure 3 shows the scatterplot when we calculated speech
rate over the total speech file: total number of syllables
per speech file divided by total speaking time (correlation
.88). In other words, the automatically measured speech

388     de Jong and Wempe

Syllables/sec by Human

6

5

4

3

2

1

2

3

4

5

Syllables/sec From Script
Figure 2. Scatterplot of WISP speech data of 50 participants;
441 spurts. The number of syllables per second is calculated per
spurt by hand and automatically.

rate correlates well with the human-measured speech rate.
However, with these data and these parameters, it seems
to be the case that the script tended to miss syllables that
were actually present. Upon inspection of the TextGrids
made by the script, we concluded that the script missed
mostly unstressed syllables that were detected by hand.
Speech Data for the IFA Corpus
The IFA corpus is an open source database of handsegmented Dutch speech. Eight participants (4 female,
4 male) performed several speech tasks, ranging from
reading aloud lists of syllables to informal storytelling.
To validate the script on another corpus of Dutch, we selected the three tasks that were similar to the tasks used
in the WISP study, eliciting (semi-) spontaneous speech.
The three tasks were informal storytelling, face to face, to
an “interviewer,” retelling of the story previously told, and
retelling of a story previously read (van Son et al., 2001).
For this corpus, we decided not to use a filter because
the speech data of this corpus were not as noisy as the
above-described speech data. Furthermore, filtering long
sounds takes a lot of time and uses up a lot of computer
memory (too much for the computer this script was run on
at the time). As a result, we decided to lower the threshold
and minimum preceding dip in intensity. We used the median intensity per sound file as threshold and 2 dB as the
minimum preceding dip in intensity. In this corpus, sentences are defined on the basis of pauses as well as syntax,
and the number of hand-measured syllables could therefore be counted per sentence. Because sentences were also
defined on syntax, many sentences were very short. Such
sentences comprised a single word, such as uh or en (“uh”
or “and”), mostly as beginnings of unfinished sentences.

To test the automatic measures against these existing
human-made measures, and to be able to compare success of speech-rate measures across the two corpora, we
redefined spurts in this corpus as stretches of speech (including pauses) of at least 5 sec (except when the end of
the speech file was reached, in which case the remaining
shorter spurt was selected). We then counted the number of syllables using the human transcripts and counted
the number of syllables measured automatically for the
same time period. For this corpus, we had 8 participants
for whom human-measured information was available in
speech tasks quite comparable to those in the WISP study.
In Figure 4, we show, for all 8 participants, the scatterplot
of human-measured speech rate per spurt with automatically measured speech rate for that same spurt.
Again, for the purpose of comparing tasks and speakers, we need a calculation of speech rate computed per
task. Figure 5 shows the correlation of the 8 speakers in
three different tasks (r 5 .8). As with the speech data for
the WISP study, the script missed syllables that were detected by hand. An inspection of the TextGrids produced
by the script revealed that most of the undetected syllables
were unstressed syllables. We think that many of these
unstressed syllables might be phonological syllables and
therefore can be detected when measured by hand, but
probably not all are also phonetic syllables in the sense
that they are present in the signal in any detectable way.
For example, Ernestus (2000, pp. 129–132) explains that
unstressed vowels and even adjacent consonants may be
absent—in particular, for casual Dutch. Therefore, we
may conclude that the algorithm picks up on prominent
syllables. As shown by the correlations between human

r = .88

4

Syllables/sec by Human

r = .71

3

2

1

1.0

1.5

2.0

2.5

3.0

Syllables/sec From Script
Figure 3. Scatterplot of WISP speech data of 50 participants.
The number of syllables per second is calculated per task (participant) by hand and automatically.

Praat Script To Detect Syllable Nuclei     389

7

Syllables/sec by Human

6
5
4
3
2
1
0
0

1

2

3

4

5

Syllables/sec From Script
Figure 4. Scatterplot of the IFA corpus; 8 participants, 1,171
spurts. The speech rate and number of syllables per second are
counted per spurt by hand and automatically.

measures and automatic measures, missing such unprominent syllables is not problematic for researchers who want
to compare speech rate between speakers and tasks, since
the underestimation of speech rate is consistent. The script
will miss syllable nuclei that are very soft in comparison
with most syllables in the speech signal (since they do not
exceed the set threshold of intensity) and syllables that
are not present in the speech signal in any detectable way,
as explained above. To determine how the speech rate as
detected by hand relates to the speech rate as computed
from the script, we fitted linear regression models. When
we fix the intercept of these models to zero, we can compare the slopes of the two regression models of the two
corpora (using the speech rates calculated over tasks).1
For the speech rates in the WISP corpus, we found a slope
of 1.27, and in the IFA corpus, the slope in the model was
1.29. In other words, to predict hand-measured speech
rate using the speech rate as derived from the script, you
should multiply by approximately 1.28.
Research by Kormos and Dénes (2004) suggests that
in fact it is the number of stressed syllables that correlates
best with subjective fluency. Perhaps it is the case that
number of prominent syllables better reflects speech rate in
the sense that it measures density of content per time unit.
Future research is needed to further explore this thesis.

correlations between human-measured speech rate and
automatically measured speech rate. Although the script
misses (mostly unstressed) syllables that are detected by
human judges, the correlations suggest that the algorithm
works well in predicting the actual number of syllables.
We conclude that for the purpose of measuring speech
rate as number of syllables per time unit when comparing
speakers and tasks, this script suffices.
In second language testing (see, e.g., the speaking rubrics of the TOEFL test as reported on the ETS Web site
[Educational Testing Service, 2004]) and second language
research (e.g., Kormos & Dénes, 2004), as well as in the
diagnosis of different language and speech disorders
(­ eyereisen et al., 1991; Redmond, 2004; Shenker, 2006),
F
fluency is an important factor to take into account. The
script described and validated in this article may be useful to easily and objectively measure speech rate in terms
of syllables per  second without the need to transcribe
speech beforehand. For research investigating differences
in speech rate for pathological populations, this script may
also be useful. For instance, Cannizzaro et al. (2004) investigated the relation between ratings of major depression and speech rate (measured by hand) and found strong
correlations. Depressed and schizophrenic patients may
speak monotonously (see, e.g., Covington et al., 2005, for
an overview). Even though the script uses pitch (along
with intensity) to detect syllable nuclei, monotonous
speech will not pose a problem since the pitch contour is
used only to ascertain voicedness of syllable nuclei, and
not to measure any changes in pitch. In any case, the recorded speech should not be too soft, since for soft speech
the signal/noise ratio is too low for syllable nuclei to be
reliably detected as a function of intensity (dB).
r = .8

4.5

Syllables/sec by Human

r = .77

4.0

3.5

3.0

2.5

2.0

Discussion
In this article, we have described a script written in
Praat that automatically detects syllables in sound files
of speech. No transcription of the speech data is necessary to run this script. The script takes sound files as input
and writes a TextGrid file with syllable nuclei marked
in a point tier. In two validation studies, we found high

2.0

2.5

3.0

3.5

Syllables/sec From Script
Figure 5. Scatterplot of the IFA corpus; 8 participants in three
tasks. The speech rate and number of syllables per second are
calculated per task by hand and automatically; N 5 24.

390     de Jong and Wempe
As yet, it is impossible to directly compare the amount
of success of the different syllable measurers described in
the introduction. First of all, other syllable measurers have
been developed to detect syllables in spoken English or
German, which might be different from detecting syllables
in Dutch. For instance, Pellegrino et al. (2004) compared
success of speech-rate estimation for several languages and
found that the correlations between hand-measured speech
rate and automatically measured speech rate (calculated
over approximately 45 sec) differed between languages.
For German speakers, the Pearson correlation was r 5 .73,
but for English it was r 5 .82. Furthermore, the different
corpora on which the existing syllable measurers have been
tested have been transcribed by different criteria. Finally,
most researchers report Pearson correlations for speech
rate or for number of syllables per spurt. However, comparisons are confounded if spurt length is uncontrolled.
For longer spurts, a count of one or two extra or fewer syllables will not result in a large deviation of the calculated
speech rate. For short spurts, a count of a single extra or
fewer syllable will result in an enormous difference in the
calculated speech rate. Future research should take these
mathematical issues into account when comparing different methods that automatically measure speech rate. In the
present article, we opted for choosing at least 5 sec as a
constant spurt length. In this way, we were able to compare
success in syllable detection across corpora.
AUTHOR NOTE
Research was funded by the Netherlands Organization of Scientific
Research (NWO) under Grant 254-70-030, with the project title, “Unraveling Second Language Proficiency.” Project leaders were Jan H.
Hulstijn and Rob Schoonen. We thank Renske Berns for her help in
counting syllables in speech. We also thank Jan Hulstijn, Rob Schoonen,
and two anonymous reviewers for comments on earlier versions of this
article. Correspondence relating to this article should be addressed to
N. H. de Jong, Department of Dutch Language and Culture, Utrecht
University, Trans 10, 3512 JK Utrecht, The Netherlands (e-mail: nivja
.dejong@let.uu.nl).
References
Boersma, P., & Weenink, D. (2007). Praat (Version 4.5.25) [Software].
Latest version available for download from www.praat.org.
Cannizzaro, M., Harel, B., Reilly, N., Chappell, P., & Snyder, P. J. (2004). Voice acoustical measurement of the severity of
major depression. Brain & Cognition, 56, 30-35. doi:10.1016/j
.bandc.2004.05.003
Covington, M. A., He, C., Brown, C., Naçi, L., McClain, J. T.,
Fjordbak, B. S., et al. (2005). Schizophrenia and the structure of
language: The linguist’s view. Schizophrenia Research, 77, 85-98.
doi:10.1016/j.schres.2005.01.016
Cucchiarini, C., Strik, H., & Boves, L. (2002). Quantitative assessment of second language learners’ fluency: Comparisons between
read and spontaneous speech. Journal of the Acoustical Society of
America, 111, 2862-2873. doi:10.1121/1.1471894
de Jong, N. H., Steinel, M. P., Florijn, A. F., Schoonen, R., &
Hulstijn, J. H. (2007). The effect of task complexity on fluency
and functional adequacy of speaking performance. In S. Van Daele,
A. Housen, M. Pierrard, F. Kuiken, & I. Vedder (Eds.), Complexity,
accuracy and fluency in second language use, learning and teaching (pp. 53-63). Brussels: Koninklijke Vlaamse Academie van België
voor Wetenschappen en Kunsten.
de Jong, N. H., & Wempe, T. (2008). Praat script speech rate. Retrieved
October 14, 2008, from sites.google.com/site/speechrate/.
Educational Testing Service (2004). iBT/Next Generation TOEFL

Test: Independent Speaking Rubrics. Retrieved December 10, 2007,
from www.ets.org/Media/Tests/TOEFL/pdf /Speaking_Rubrics.pdf.
Ernestus, M. T. C. (2000). Voice assimilation and segment reduction in casual Dutch: A corpus-based study of the phonology–phonetics interface.
Ph.D. dissertation, Vrije Universiteit, Amsterdam (LOT Series 36).
Feyereisen, P., Pillon, A., & de Partz, M.-P. (1991). On the measures
of fluency in the assessment of spontaneous speech production by aphasic subjects. Aphasiology, 5, 1-21. doi:10.1080/02687039108248516
Hunt, A. (1993). Recurrent neural networks for syllabification. Speech
Communication, 13, 323-332. doi:10.1016/0167-6393(93)90031-F
Kormos, J., & Dénes, M. (2004). Exploring measures and perceptions
of fluency in the speech of second language learners. System, 32, 145164. doi:10.1016/j.system.2004.01.001
Mermelstein, P. (1975). Automatic segmentation of speech into syllabic units. Journal of the Acoustical Society of America, 58, 880-883.
doi:10.1121/1.380738
O’Brien, I., Segalowitz, N., Freed, B., & Collentine, J. (2007).
Phonological memory predicts second language oral fluency gains
in adults. Studies in Second Language Acquisition, 29, 557-582.
doi:10.1017/S027226310707043X
Pellegrino, F., & Andre-Obrecht, R. (2000). Automatic language
identification: An alternative approach to phonetic modelling. Signal
Processing, 80, 1231-1244. doi:10.1016/S0165-1684(00)00032-3
Pellegrino, F., Farinas, J., & Rouas, J.-L. (2004). Automatic estimation of speaking rate in multilingual spontaneous speech. Proceedings
of Speech Prosody 2004, Nara, Japan (pp. 517-520).
Pfau, T., Faltlhauser, R., & Ruske, G. (2000). A combination of speaker
normalization and speech rate normalization for automatic speech recognition. Proceedings of ICSLP 2000, Peking, China, 4, 362-365.
Pfau, T., & Ruske, G. (1998). Estimating the speaking rate by vowel
detection. Acoustics, Speech, and Signal Processing (ICASSP 2005
Proceedings), 2, 945-948. doi:10.1109/ICASSP.1998.675422
Pfitzinger, H. R. (1999). Local speech rate perception in German
speech. Proceedings of the XIVth International Congress of Phonetic
Sciences, 2, 893-896.
Redmond, S. (2004). Conversational profiles of children with ADHD,
SLI and typical development. Clinical Linguistics & Phonetics, 18,
107-125. doi:10.1080/02699200310001611612
Riggenbach, H. (1991). Toward an understanding of fluency: A microanalysis of nonnative speaker conversations. Discourse Processes,
14, 423-441.
Shenker, R. C. (2006). Connecting stuttering management and measurement: I. Core speech measures of clinical process and outcome.
International Journal of Language & Communication Disorders, 41,
355-364. doi:10.1080/13682820600623861
Tavakoli, P., & Skehan, P. (2005). Strategic planning, task structure, and
performance testing. In R. Ellis (Ed.), Planning and task performance in
a second language (pp. 239-276). Amsterdam: John Benjamins.
Towell, R., Hawkins, R., & Bazergui, N. (1996). The development
of fluency in advanced learners of French. Applied Linguistics, 17,
84-119. doi:10.1093/applin/17.1.84
van Son, R. J. J. H., Binnenpoorte, D., van den Heuvel, H., & Pols,
L. C. W. (2001). The IFA corpus: A phonemically segmented Dutch
“open source” speech database. EUROSPEECH 2001, 2051-2054.
Verhasselt, J. P., & Martens, J. P. (1996). A fast and reliable rate of
speech detector. Spoken Language (ICSLP 96 Proceedings), 4, 22582261. doi:10.1109/ICSLP.1996.607256
Wang, D., & Narayanan, S. (2007). Robust speech rate estimation for
spontaneous speech. IEEE Transactions on Speech, Audio and Language Processing, 15, 2190-2201. doi:10.1109/TASL.2007.905178
Note
1. For the IFA corpus, this simplification does not lead to loss of fit (R2
for both models is .645). For the WISP corpus, R2 changes from .77 (intercept is 2.9) to .73 (intercept fixed to 0). However, for the present goal
of comparing slopes for the two models, we can assume the intercept to
be 0. Theoretically, this simplification is warranted because, if nothing is
said, both automatically calculated speech rate and speech rate measured
by hand should be 0.
(Manuscript received October 14, 2008;
revision accepted for publication January 6, 2009.)

Behavior Research Methods
2009, 41 (2), 385-390
doi:10.3758/BRM.41.2.385

Praat script to detect syllable nuclei
and measure speech rate automatically
Nivja H. de Jong

University of Amsterdam, Amsterdam, The Netherlands
and Utrecht University, Utrecht, The Netherlands
and

Ton Wempe

University of Amsterdam, Amsterdam, The Netherlands
In this article, we describe a method for automatically detecting syllable nuclei in order to measure speech rate
without the need for a transcription. A script written in the software program Praat (Boersma & Weenink, 2007)
detects syllables in running speech. Peaks in intensity (dB) that are preceded and followed by dips in intensity are
considered to be potential syllable nuclei. The script subsequently discards peaks that are not voiced. Testing the
resulting syllable counts of this script on two corpora of spoken Dutch, we obtained high correlations between
speech rate calculated from human syllable counts and speech rate calculated from automatically determined
syllable counts. We conclude that a syllable count measured in this automatic fashion suffices to reliably assess
and compare speech rates between participants and tasks.

Speech rate is used as a measure of fluency in second
language acquisition and bilingualism research (e.g.,
O’Brien, Segalowitz, Freed, & Collentine, 2007; Riggenbach, 1991; Towell, Hawkins, & Bazergui, 1996); in research diagnosing speech and language disorders (e.g.,
Feyereisen, Pillon, & de Partz, 1991; Redmond, 2004;
Shenker, 2006); and in research on speech of pathological populations (e.g., Cannizzaro, Harel, Reilly, Chappell,
& Snyder, 2004; Covington et al., 2005). However, calculating speech rate by hand is a tedious task, which is
therefore often not carried out. In this article, we present a
script written in the software program Praat (Boersma &
Weenink, 2007) that automatically detects syllable nuclei
in order to calculate speech rate. The nucleus of a syllable,
also called the peak, is the central part of a syllable (most
commonly, the vowel in the syllable). Locating these syllable nuclei allows for a computation of number of syllables, which can be used to calculate speech rate.
According to Tavakoli and Skehan (2005), fluency is
multifaceted in nature. They distinguish three different
facets of fluency: breakdown fluency (number and length
of pauses), speed and density per time unit (speech rate),
and repair fluency (false starts and repetitions). In second language testing practice, fluency is usually a score
awarded by human judges who use several aspects of
fluency in their judgment. However, Cucchiarini, Strik,
and Boves (2002) have shown that of several objectively
measured aspects of fluency, speech rate (as measured
by phonemes per time unit) is the best predictor of sub-

jective fluency. Kormos and Dénes (2004) likewise have
shown that speech rate (in terms of number of syllables
per time unit) is a good predictor of subjective fluency.
We conclude that, for researchers wanting to include a
measure of fluency, speech rate is an important factor to
take into account. However, because of time constraints,
this measure is often impossible to carry out. For instance,
the script presented in this article was written in order to
be able to measure the speech rate of 250 participants in
a corpus of over 45 h of speech, a task that would take at
least 8 months of full-time work for one person to measure
by hand. In the context of a large-scale research project on
the correlates of speaking proficiency carried out at the
University of Amsterdam (What Is Speaking Proficiency:
www.hum.uva.nl/wisp), we developed two tools to measure fluency automatically. For the purpose of measuring
pauses in running speech, we wrote a script in the software
program Praat to automatically detect silence in speech
[a simplified version of which is now incorporated in the
button To TextGrid (silences) in the Praat software]. For
the purpose of estimating the speech rate of speech performances, we wrote a script in Praat that automatically
detects syllable nuclei to compute speech rate in terms of
syllables per time unit. In this article, we will present and
validate the script for detecting syllable nuclei.
For automatic speech recognition, speech rate is an important factor as well. Human listeners are able to understand
both fast and slow speech. Speech recognizers implemented
in computers, however, perform relatively poorly when

N. H. de Jong, nivja.dejong@let.uu.nl

	

385	

© 2009 The Psychonomic Society, Inc.

386     de Jong and Wempe
speech rate is very fast or very slow. In order to improve
computer performance, several researchers have proposed
that measuring speech rate prior to speech recognition will
result in higher success rates of automatic speech recognizers (Pfau, Faltlhauser, & Ruske, 2000), and several ways to
automatically measure speech rate in terms of phones and/
or syllables per time unit have been put forward.
Mermelstein (1975) developed an algorithm with which
to segment speech into syllables by finding minima in loudness that serve as possible syllable boundaries. Verhasselt
and Martens (1996) presented an automatic speech detector
that measures phone boundaries and thus calculates rate of
speech as phone rate. The phone boundaries are provided
by a multilayer perceptron that is trained on a subset of the
data that must be hand-segmented at the phone level. Pfitzinger (1999) used a combination of syllable rate and phone
rate for correlations with perceptual speech rate. Syllable
rate was calculated by counting peaks in the energy contour,
whereas phone rate was calculated by use of transcription.
The syllable, phone, and perceptual speech rates were measured over (very) short stimuli (625 msec). Hunt (1993)
used recurrent neural networks to detect syllables. Pfau and
Ruske (1998) determined syllable nuclei by detecting vowels on smoothed modified loudness, and then calculated
speech rate. Pellegrino and Andre-Obrecht (2000) used a
vowel-detection algorithm based on the spectral analysis
to detect formant–antiformant structure that is specific
for vowels (and some nasals). In their study, the voweldetection algorithm was used for automatic language identification. In the study by Pellegrino, Farinas, and Rouas
(2004), the same vowel-detection algorithm was used to
estimate speech rate. They compared hand-­ easured and
m
automatically measured speech rates, calculated for speech
performances of around 45 sec with speakers of several
languages. Finally, Wang and Narayanan (2007) also developed a method for speech-rate estimation; they made
use of spectral subband correlation including temporal correlation, prominent spectral subbands, and pitch information to estimate syllable nuclei.
All of these different automatic ways to measure syllable and/or phone rate are quite successful. It is difficult,
however, to compare the success of these automatic measurers, because they were all used on different corpora,
and their success was reported in different ways. Some researchers have reported correlations between human and
automatic speech rate; others, a percentage of syllables (or
phones) undetected and falsely detected as compared with
human-measured syllables (or phones); and still others,
a correlation between the number of manually measured
and automatically measured syllables (or phones). The
difference in corpora used should also be noted, because
some studies have used large corpora with many different
speakers and others have used quite small corpora with few
speakers; some have used corpora of speech read aloud,
whereas others have used (semi-) spontaneous speech
corpora. Finally, a noteworthy difference between studies
concerns the length of the spurt on which speech rate was
calculated. Some studies have used extremely short time
windows to calculate speech rate, and others have used
much longer windows. Perhaps the most obvious reason

why we cannot compare success of these different automatic speech-rate measurers is that the length of the time
window (or spurt) as well as the variance in spurt length
will strongly influence calculations of speech rate.
Many of the proposed speech-rate measurers need to
be trained on a subset of the data that is transcribed or
preprocessed by hand (Hunt, 1993; Pfau & Ruske, 1998;
Pfitzinger, 1999; Verhasselt & Martens, 1996). In this article, we will present an easy way to automatically measure speech rate without the use of preprocessing or the
need for transcriptions and test it on two different corpora
of spontaneously spoken Dutch. To be able to compare
the success of the script over these two different corpora,
spurt length was controlled. We wrote a script in Praat
using a combination of intensity (similar to Pfitzinger,
1999) and voicedness (similar to Pfau & Ruske, 1998) to
find syllable nuclei.
The Praat Script
In what follows, we describe the sequence of actions
that the script completes to find syllable nuclei using intensity (dB) and voicedness. We use intensity first to be
able to find peaks in the energy contour, since a vowel
within a syllable (the syllable nucleus) has higher energy
than do surrounding sounds (described in Steps 1 and 2,
below). We then use the intensity contour to make sure
that the intensity between the current peak and the preceding peak is sufficiently low. With this procedure, we delete
multiple peaks within one syllable (described in Step 3,
below). Finally, we use voicedness to exclude peaks that
are unvoiced, which is required to delete surrounding
voiceless consonants that have high intensity (described
in Step 4, below). Before the script is run, sound files that
are quite noisy should be filtered so that the frequency
range is speech-band limited.
Step 1. We extract the intensity, with the parameter
“minimum pitch” set to 50 Hz, using autocorrelation. With
this parameter setting, we extract intensity smoothed over
a time window of 64 msec, with 16-msec time steps.
Step 2. We consider all peaks above a certain threshold
in intensity to be potential syllables. We set the threshold
to 0 or 2 dB above the median intensity measured over the
total sound file (0 dB if the sound is not filtered; 2 dB if
the sound is filtered). We use the median, rather than the
mean, to calculate the threshold in order to avoid including extreme peaks in the calculation of the threshold.
Step 3. We inspect the preceding dip in intensity and consider only a peak with a preceding dip of at least 2 or 4 dB
with respect to the current peak as a potential syllable (2 dB
if the sound is not filtered; 4 dB if the sound is filtered).
Step 4. We extract the pitch contour, this time using
a window size of 100 msec and 20-msec time steps, and
exclude all peaks that are unvoiced.
Step 5. The remaining peaks are considered syllable
nuclei and are saved in a TextGrid (point tier).
Figure 1 shows a part of a sound file together with the
output as TextGrid made by the script. The speech utterance depicted here is dat uh was wel goed bevallen toen
(“that uhm was quite well liked then”), totaling nine syllables, including “uh.” The script is available at sites.google

Praat Script To Detect Syllable Nuclei     387

Figure 1. Part of a speech file in Praat with intensity (white line in spectrum)
and pitch (dark line in spectrum) shown. The points in the tier are the syllable
nuclei as detected by the script.

.com/site/speechrate under the terms of the GNU General
Public License (de Jong & Wempe, 2008).
Performance
As a part of the project “What Is Speaking Proficiency”
(WISP), conducted at the Amsterdam Center for Language
and Communication at the University of Amsterdam, we
have collected speech data from 258 ­ articipants—200
p
nonnative speakers of Dutch (with various first languages) and 58 native speakers of Dutch. Each participant performed 8 speaking tasks, resulting in a total of approximately 46 h of speech. See de Jong, Steinel, Florijn,
Schoonen, and Hulstijn (2007) for a description of the
speaking tasks and an application of the fluency measures.
In order to be able to include measures of fluency in our
research, we made two scripts written in Praat. The first
script automatically detects pauses [a modified version
is now incorporated in Praat in the TextGrid (to silences)
button], and the second script automatically detects syllables. The second script is described in this article. In
what follows, we report a validation of the computation
of syllables per time unit as generated by the script in two
different corpora. First, we randomly selected 50 out of
the total of 258 * 8 speaking tasks and measured syllables
by hand. This corpus comprised 75 min of speech. Second, we tested the script on a subset of the IFA corpus that
was comparable to the speaking tasks in the WISP study
(van Son, Binnenpoorte, van den Heuvel, & Pols, 2001).
This part of the corpus comprised 125 min of speech
summed over 8 participants.

Speech Data for the WISP Study
We counted the syllables of 50 speech files. Pauses longer than 0.4 sec were considered possible spurt boundaries. We used all spurts of 5 sec or more to calculate speech
rate, and combined consecutive shorter spurts to get to
5 sec (excluding pauses). We thus avoided calculating
speech rate over very short periods of time. We then automatically detected syllables using the Praat script. Many
sound files in this corpus were moderately noisy; therefore, we filtered all sounds prior to the syllable measuring,
using 100 Hz as the lower edge of the pass band, 5000 Hz
as the upper edge of the pass band, and 50 Hz as the width
of the smoothing region. With these settings, we attenuate
nonspeech frequency components and keep all possible
voice-related information about intensity and voicedness,
across all formants. Measuring peaks in intensity (dB),
we used 2 dB above the median intensity per sound file
as threshold, and 4 dB as minimum dip between peaks,
excluding peaks that are unvoiced.
We computed speech rate by hand and from the script
per spurt by dividing the number of syllables per spurt for
both measures by the spurt length in seconds. Figure 2
shows the scatterplot of the human and automatic speechrate calculations per spurt; the correlation was .71. For
our purposes, of comparing speakers and/or tasks, however, we needed a less refined calculation of speech rate.
Figure 3 shows the scatterplot when we calculated speech
rate over the total speech file: total number of syllables
per speech file divided by total speaking time (correlation
.88). In other words, the automatically measured speech

388     de Jong and Wempe

Syllables/sec by Human

6

5

4

3

2

1

2

3

4

5

Syllables/sec From Script
Figure 2. Scatterplot of WISP speech data of 50 participants;
441 spurts. The number of syllables per second is calculated per
spurt by hand and automatically.

rate correlates well with the human-measured speech rate.
However, with these data and these parameters, it seems
to be the case that the script tended to miss syllables that
were actually present. Upon inspection of the TextGrids
made by the script, we concluded that the script missed
mostly unstressed syllables that were detected by hand.
Speech Data for the IFA Corpus
The IFA corpus is an open source database of handsegmented Dutch speech. Eight participants (4 female,
4 male) performed several speech tasks, ranging from
reading aloud lists of syllables to informal storytelling.
To validate the script on another corpus of Dutch, we selected the three tasks that were similar to the tasks used
in the WISP study, eliciting (semi-) spontaneous speech.
The three tasks were informal storytelling, face to face, to
an “interviewer,” retelling of the story previously told, and
retelling of a story previously read (van Son et al., 2001).
For this corpus, we decided not to use a filter because
the speech data of this corpus were not as noisy as the
above-described speech data. Furthermore, filtering long
sounds takes a lot of time and uses up a lot of computer
memory (too much for the computer this script was run on
at the time). As a result, we decided to lower the threshold
and minimum preceding dip in intensity. We used the median intensity per sound file as threshold and 2 dB as the
minimum preceding dip in intensity. In this corpus, sentences are defined on the basis of pauses as well as syntax,
and the number of hand-measured syllables could therefore be counted per sentence. Because sentences were also
defined on syntax, many sentences were very short. Such
sentences comprised a single word, such as uh or en (“uh”
or “and”), mostly as beginnings of unfinished sentences.

To test the automatic measures against these existing
human-made measures, and to be able to compare success of speech-rate measures across the two corpora, we
redefined spurts in this corpus as stretches of speech (including pauses) of at least 5 sec (except when the end of
the speech file was reached, in which case the remaining
shorter spurt was selected). We then counted the number of syllables using the human transcripts and counted
the number of syllables measured automatically for the
same time period. For this corpus, we had 8 participants
for whom human-measured information was available in
speech tasks quite comparable to those in the WISP study.
In Figure 4, we show, for all 8 participants, the scatterplot
of human-measured speech rate per spurt with automatically measured speech rate for that same spurt.
Again, for the purpose of comparing tasks and speakers, we need a calculation of speech rate computed per
task. Figure 5 shows the correlation of the 8 speakers in
three different tasks (r 5 .8). As with the speech data for
the WISP study, the script missed syllables that were detected by hand. An inspection of the TextGrids produced
by the script revealed that most of the undetected syllables
were unstressed syllables. We think that many of these
unstressed syllables might be phonological syllables and
therefore can be detected when measured by hand, but
probably not all are also phonetic syllables in the sense
that they are present in the signal in any detectable way.
For example, Ernestus (2000, pp. 129–132) explains that
unstressed vowels and even adjacent consonants may be
absent—in particular, for casual Dutch. Therefore, we
may conclude that the algorithm picks up on prominent
syllables. As shown by the correlations between human

r = .88

4

Syllables/sec by Human

r = .71

3

2

1

1.0

1.5

2.0

2.5

3.0

Syllables/sec From Script
Figure 3. Scatterplot of WISP speech data of 50 participants.
The number of syllables per second is calculated per task (participant) by hand and automatically.

Praat Script To Detect Syllable Nuclei     389

7

Syllables/sec by Human

6
5
4
3
2
1
0
0

1

2

3

4

5

Syllables/sec From Script
Figure 4. Scatterplot of the IFA corpus; 8 participants, 1,171
spurts. The speech rate and number of syllables per second are
counted per spurt by hand and automatically.

measures and automatic measures, missing such unprominent syllables is not problematic for researchers who want
to compare speech rate between speakers and tasks, since
the underestimation of speech rate is consistent. The script
will miss syllable nuclei that are very soft in comparison
with most syllables in the speech signal (since they do not
exceed the set threshold of intensity) and syllables that
are not present in the speech signal in any detectable way,
as explained above. To determine how the speech rate as
detected by hand relates to the speech rate as computed
from the script, we fitted linear regression models. When
we fix the intercept of these models to zero, we can compare the slopes of the two regression models of the two
corpora (using the speech rates calculated over tasks).1
For the speech rates in the WISP corpus, we found a slope
of 1.27, and in the IFA corpus, the slope in the model was
1.29. In other words, to predict hand-measured speech
rate using the speech rate as derived from the script, you
should multiply by approximately 1.28.
Research by Kormos and Dénes (2004) suggests that
in fact it is the number of stressed syllables that correlates
best with subjective fluency. Perhaps it is the case that
number of prominent syllables better reflects speech rate in
the sense that it measures density of content per time unit.
Future research is needed to further explore this thesis.

correlations between human-measured speech rate and
automatically measured speech rate. Although the script
misses (mostly unstressed) syllables that are detected by
human judges, the correlations suggest that the algorithm
works well in predicting the actual number of syllables.
We conclude that for the purpose of measuring speech
rate as number of syllables per time unit when comparing
speakers and tasks, this script suffices.
In second language testing (see, e.g., the speaking rubrics of the TOEFL test as reported on the ETS Web site
[Educational Testing Service, 2004]) and second language
research (e.g., Kormos & Dénes, 2004), as well as in the
diagnosis of different language and speech disorders
(­ eyereisen et al., 1991; Redmond, 2004; Shenker, 2006),
F
fluency is an important factor to take into account. The
script described and validated in this article may be useful to easily and objectively measure speech rate in terms
of syllables per  second without the need to transcribe
speech beforehand. For research investigating differences
in speech rate for pathological populations, this script may
also be useful. For instance, Cannizzaro et al. (2004) investigated the relation between ratings of major depression and speech rate (measured by hand) and found strong
correlations. Depressed and schizophrenic patients may
speak monotonously (see, e.g., Covington et al., 2005, for
an overview). Even though the script uses pitch (along
with intensity) to detect syllable nuclei, monotonous
speech will not pose a problem since the pitch contour is
used only to ascertain voicedness of syllable nuclei, and
not to measure any changes in pitch. In any case, the recorded speech should not be too soft, since for soft speech
the signal/noise ratio is too low for syllable nuclei to be
reliably detected as a function of intensity (dB).
r = .8

4.5

Syllables/sec by Human

r = .77

4.0

3.5

3.0

2.5

2.0

Discussion
In this article, we have described a script written in
Praat that automatically detects syllables in sound files
of speech. No transcription of the speech data is necessary to run this script. The script takes sound files as input
and writes a TextGrid file with syllable nuclei marked
in a point tier. In two validation studies, we found high

2.0

2.5

3.0

3.5

Syllables/sec From Script
Figure 5. Scatterplot of the IFA corpus; 8 participants in three
tasks. The speech rate and number of syllables per second are
calculated per task by hand and automatically; N 5 24.

390     de Jong and Wempe
As yet, it is impossible to directly compare the amount
of success of the different syllable measurers described in
the introduction. First of all, other syllable measurers have
been developed to detect syllables in spoken English or
German, which might be different from detecting syllables
in Dutch. For instance, Pellegrino et al. (2004) compared
success of speech-rate estimation for several languages and
found that the correlations between hand-measured speech
rate and automatically measured speech rate (calculated
over approximately 45 sec) differed between languages.
For German speakers, the Pearson correlation was r 5 .73,
but for English it was r 5 .82. Furthermore, the different
corpora on which the existing syllable measurers have been
tested have been transcribed by different criteria. Finally,
most researchers report Pearson correlations for speech
rate or for number of syllables per spurt. However, comparisons are confounded if spurt length is uncontrolled.
For longer spurts, a count of one or two extra or fewer syllables will not result in a large deviation of the calculated
speech rate. For short spurts, a count of a single extra or
fewer syllable will result in an enormous difference in the
calculated speech rate. Future research should take these
mathematical issues into account when comparing different methods that automatically measure speech rate. In the
present article, we opted for choosing at least 5 sec as a
constant spurt length. In this way, we were able to compare
success in syllable detection across corpora.
AUTHOR NOTE
Research was funded by the Netherlands Organization of Scientific
Research (NWO) under Grant 254-70-030, with the project title, “Unraveling Second Language Proficiency.” Project leaders were Jan H.
Hulstijn and Rob Schoonen. We thank Renske Berns for her help in
counting syllables in speech. We also thank Jan Hulstijn, Rob Schoonen,
and two anonymous reviewers for comments on earlier versions of this
article. Correspondence relating to this article should be addressed to
N. H. de Jong, Department of Dutch Language and Culture, Utrecht
University, Trans 10, 3512 JK Utrecht, The Netherlands (e-mail: nivja
.dejong@let.uu.nl).
References
Boersma, P., & Weenink, D. (2007). Praat (Version 4.5.25) [Software].
Latest version available for download from www.praat.org.
Cannizzaro, M., Harel, B., Reilly, N., Chappell, P., & Snyder, P. J. (2004). Voice acoustical measurement of the severity of
major depression. Brain & Cognition, 56, 30-35. doi:10.1016/j
.bandc.2004.05.003
Covington, M. A., He, C., Brown, C., Naçi, L., McClain, J. T.,
Fjordbak, B. S., et al. (2005). Schizophrenia and the structure of
language: The linguist’s view. Schizophrenia Research, 77, 85-98.
doi:10.1016/j.schres.2005.01.016
Cucchiarini, C., Strik, H., & Boves, L. (2002). Quantitative assessment of second language learners’ fluency: Comparisons between
read and spontaneous speech. Journal of the Acoustical Society of
America, 111, 2862-2873. doi:10.1121/1.1471894
de Jong, N. H., Steinel, M. P., Florijn, A. F., Schoonen, R., &
Hulstijn, J. H. (2007). The effect of task complexity on fluency
and functional adequacy of speaking performance. In S. Van Daele,
A. Housen, M. Pierrard, F. Kuiken, & I. Vedder (Eds.), Complexity,
accuracy and fluency in second language use, learning and teaching (pp. 53-63). Brussels: Koninklijke Vlaamse Academie van België
voor Wetenschappen en Kunsten.
de Jong, N. H., & Wempe, T. (2008). Praat script speech rate. Retrieved
October 14, 2008, from sites.google.com/site/speechrate/.
Educational Testing Service (2004). iBT/Next Generation TOEFL

Test: Independent Speaking Rubrics. Retrieved December 10, 2007,
from www.ets.org/Media/Tests/TOEFL/pdf /Speaking_Rubrics.pdf.
Ernestus, M. T. C. (2000). Voice assimilation and segment reduction in casual Dutch: A corpus-based study of the phonology–phonetics interface.
Ph.D. dissertation, Vrije Universiteit, Amsterdam (LOT Series 36).
Feyereisen, P., Pillon, A., & de Partz, M.-P. (1991). On the measures
of fluency in the assessment of spontaneous speech production by aphasic subjects. Aphasiology, 5, 1-21. doi:10.1080/02687039108248516
Hunt, A. (1993). Recurrent neural networks for syllabification. Speech
Communication, 13, 323-332. doi:10.1016/0167-6393(93)90031-F
Kormos, J., & Dénes, M. (2004). Exploring measures and perceptions
of fluency in the speech of second language learners. System, 32, 145164. doi:10.1016/j.system.2004.01.001
Mermelstein, P. (1975). Automatic segmentation of speech into syllabic units. Journal of the Acoustical Society of America, 58, 880-883.
doi:10.1121/1.380738
O’Brien, I., Segalowitz, N., Freed, B., & Collentine, J. (2007).
Phonological memory predicts second language oral fluency gains
in adults. Studies in Second Language Acquisition, 29, 557-582.
doi:10.1017/S027226310707043X
Pellegrino, F., & Andre-Obrecht, R. (2000). Automatic language
identification: An alternative approach to phonetic modelling. Signal
Processing, 80, 1231-1244. doi:10.1016/S0165-1684(00)00032-3
Pellegrino, F., Farinas, J., & Rouas, J.-L. (2004). Automatic estimation of speaking rate in multilingual spontaneous speech. Proceedings
of Speech Prosody 2004, Nara, Japan (pp. 517-520).
Pfau, T., Faltlhauser, R., & Ruske, G. (2000). A combination of speaker
normalization and speech rate normalization for automatic speech recognition. Proceedings of ICSLP 2000, Peking, China, 4, 362-365.
Pfau, T., & Ruske, G. (1998). Estimating the speaking rate by vowel
detection. Acoustics, Speech, and Signal Processing (ICASSP 2005
Proceedings), 2, 945-948. doi:10.1109/ICASSP.1998.675422
Pfitzinger, H. R. (1999). Local speech rate perception in German
speech. Proceedings of the XIVth International Congress of Phonetic
Sciences, 2, 893-896.
Redmond, S. (2004). Conversational profiles of children with ADHD,
SLI and typical development. Clinical Linguistics & Phonetics, 18,
107-125. doi:10.1080/02699200310001611612
Riggenbach, H. (1991). Toward an understanding of fluency: A microanalysis of nonnative speaker conversations. Discourse Processes,
14, 423-441.
Shenker, R. C. (2006). Connecting stuttering management and measurement: I. Core speech measures of clinical process and outcome.
International Journal of Language & Communication Disorders, 41,
355-364. doi:10.1080/13682820600623861
Tavakoli, P., & Skehan, P. (2005). Strategic planning, task structure, and
performance testing. In R. Ellis (Ed.), Planning and task performance in
a second language (pp. 239-276). Amsterdam: John Benjamins.
Towell, R., Hawkins, R., & Bazergui, N. (1996). The development
of fluency in advanced learners of French. Applied Linguistics, 17,
84-119. doi:10.1093/applin/17.1.84
van Son, R. J. J. H., Binnenpoorte, D., van den Heuvel, H., & Pols,
L. C. W. (2001). The IFA corpus: A phonemically segmented Dutch
“open source” speech database. EUROSPEECH 2001, 2051-2054.
Verhasselt, J. P., & Martens, J. P. (1996). A fast and reliable rate of
speech detector. Spoken Language (ICSLP 96 Proceedings), 4, 22582261. doi:10.1109/ICSLP.1996.607256
Wang, D., & Narayanan, S. (2007). Robust speech rate estimation for
spontaneous speech. IEEE Transactions on Speech, Audio and Language Processing, 15, 2190-2201. doi:10.1109/TASL.2007.905178
Note
1. For the IFA corpus, this simplification does not lead to loss of fit (R2
for both models is .645). For the WISP corpus, R2 changes from .77 (intercept is 2.9) to .73 (intercept fixed to 0). However, for the present goal
of comparing slopes for the two models, we can assume the intercept to
be 0. Theoretically, this simplification is warranted because, if nothing is
said, both automatically calculated speech rate and speech rate measured
by hand should be 0.
(Manuscript received October 14, 2008;
revision accepted for publication January 6, 2009.)

Phonetica 2005;62:215–226
DOI: 10.1159/000090099

Received: June 24, 2005
Accepted: October 4, 2005

Exploring Prosody in Interaction Control
Jens Edlund

Mattias Heldner

KTH Speech, Music and Hearing, Stockholm, Sweden

Abstract
This paper investigates prosodic aspects of turn-taking in conversation with a
view to improving the efficiency of identifying relevant places at which a machine
can legitimately begin to talk to a human interlocutor. It examines the relationship
between interaction control, the communicative function of which is to regulate
the flow of information between interlocutors, and its phonetic manifestation.
Specifically, the listener’s perception of such interaction control phenomena is
modelled. Algorithms for automatic online extraction of prosodic phenomena
liable to be relevant for interaction control, such as silent pauses and intonation
patterns, are presented and evaluated in experiments using Swedish map task
data. We show that the automatically extracted prosodic features can be used to
avoid many of the places where current dialogue systems run the risk of interrupting their users, as well as to identify suitable places to take the turn.
Copyright © 2005 S. Karger AG, Basel

Introduction

Conversation is a primary means for human communication. An important function
of conversation is to exchange propositional content, and during this exchange the interlocutors must somehow regulate the flow of information to make it proceed smoothly
and efficiently. This interaction control is a collaborative effort where interlocutors
continuously monitor various aspects of each other’s behaviour, including semantics,
gestures and prosody, for example, in order to make decisions about turn-taking and
feedback. The term ‘turn-taking’ includes how to take the floor without interrupting,
how to keep the floor and how to pass the initiative on to others. Feedback is used to
indicate to the speaker that the listener is attentive, understanding and agreeing.
The aim of our research is to improve the interaction control in spoken humancomputer dialogue. The primary motivation for this is that if a spoken dialogue system
is to be perceived as a good conversational partner, it has to be able to understand the
speaker’s organization of the content, and to know how to time its own contributions to
the dialogue appropriately, in addition to recognizing and responding to verbal input
and generating verbal output.
Current spoken dialogue systems commonly detect where the user ceases speaking
in order to find out where they should take their turn. The method is based on the assumption

Fax ϩ41 61 306 12 34
E-Mail karger@karger.ch
www.karger.com

© 2005 S. Karger AG, Basel
0031–8388/05/0624–0215
$22.00/0
Accessible online at:
www.karger.com/journals/pho

Jens Edlund
KTH Speech, Music and Hearing
Lindstedtsvägen 24
SE–100 44 Stockholm (Sweden)
Tel. ϩ46 8 790 7874, Fax ϩ46 8 790 7854
E-Mail edlund@speech.kth.se

that speakers have finished what they intended to say when they become silent, and that
these points in time are also suitable places for the system to speak. Such endpoint detection triggers on a certain set amount of silence, or non-speech. The method makes sense;
given that a speaker is allowed to complete what she/he intends to say, the end of the
utterance is likely to coincide with silence at a place where an interlocutor might take the
next turn. The method segments speech into reasonably sized units, in many cases corresponding to sentences or some sentence-like units. However, spontaneous conversational
speech frequently contains silent pauses inside what we would intuitively group into
turns, complete utterances or sentence-like units, and inside what are indeed semantically
coherent units. Typical examples are hesitations such as ‘You will get to a eh [long
silence] well what shall we call it’. Silences (or hesitations) may even occur inside
prosodic words or compound words: ‘There is a eh cycleway and some horse- [long
silence] path maybe’. Thus, dialogue systems using silence-based endpoint detection run
into problems with unfinished utterances when encountering spontaneous speech, as the
silences following them are not likely to be suitable places for the system to speak, and as
unfinished utterances may be difficult to interpret [Bell et al., 2001].
There are a number of common tasks within speech technology and natural language processing where it would be useful to perform automatic segmentation into
units that better match what humans perceive as finished utterances. The following two
types of tasks summarize our primary motivation for this work.
(1) System barge-in. Computers may want to use speech to notify human interlocutors about various events, for example ‘coffee is ready’. This is perhaps analogous to a
meeting secretary, and it is important that the barge-in behaviour is perceived as polite.
(2) Interaction control, turn-taking, feedback. Segmentation of user input is
essential for the conversational components of a dialogue system, notably for identifying suitable places to speak [Heldner et al., in press]. A dialogue system is likely to be
perceived as a better conversational partner if it has a clearer idea of when human interlocutors have finished talking, and if it is able to respond rapidly.
In advanced spoken dialogue systems, spoken language understanding and interaction control are combined. The AdApt system, for example, uses a semantically
based approach in order to deal with the problems that occur as a result of silence-onlybased utterance segmentation [Bell et al., 2001]. Another semantic approach is used in
Skantze and Edlund [2004].
The study presented here represents a continuation of work presented elsewhere
[Edlund et al., 2005; Heldner et al., in press]. Here, we explore online prosodic analysis with the aim of bringing human-like interaction control capabilities to conversational computers. More specifically, we look at prosodic phenomena liable to be
relevant for interaction control, such as silent pauses and intonation patterns. It is worth
noting that although our primary goal is to create natural, human-like spoken dialogue
systems, human-computer conversation experiments provide good evidence as well as
counterevidence for the relationship between interaction control phenomena and their
prosodic manifestations.
Prosodic Phenomena and Interaction Control

Previous work suggests that a number of prosodic or phonetic cues are associated
with turn-yielding and thus potentially relevant for interaction control. These cues include

216

Phonetica 2005;62:215–226

Edlund/Heldner

phenomena such as silent pauses; various intonation patterns (rises, falls, down-steps,
up-steps); decreases in speech rate; final lengthening; intensity patterns; centralized
vowel quality, creaky voice quality, and exhalations. Note that both rises and falls have
been associated with turn-yielding. These cues are typically located somewhere
towards the end of the turn, although not necessarily on the final syllable [e.g. Ford and
Thompson, 1996; Local and Kelly, 1986; Local et al., 1985, 1986; Ogden, 2001; Wells
and MacFarlane, 1998].
Similarly, there are studies suggesting that certain prosodic or phonetic cues are
associated with turn-keeping, and these cues are of course also potentially relevant for
interaction control. They include phenomena such as glottal or vocal tract stops without
audible release; a different quality of silent pauses as a result of these glottal or vocal
tract closures; assimilation across the silent pause, and other ‘held’ articulations (e.g.
lengthened vowels, laterals, nasals or fricatives) [Local and Kelly, 1986; Ogden, 2001].
Certain intonation patterns are associated with turn-keeping. In particular, level
intonation patterns in the middle of the speakers’ fundamental frequency range have
been observed to act as turn-keeping cues in several different languages. For example,
Duncan [1972] reported that any pattern other than a level tone in the speaker’s
mid-register signals turn-yielding in English. Thus, the mid-level pattern acts as a turnkeeping signal, although Duncan did not use that term. Similarly, Selting [1996]
reported that level pitch accents before a pause are used to signal turn-holding (or turnkeeping) in German; Koiso et al. [1998] observed that flat, flat-fall and rise-fall intonation patterns tended to co-occur with speaker holds (i.e. turn-keeping) in Japanese, and
in another study on Japanese conversations, Noguchi and Den [1998] reported that flat
intonation at the end of pause-bounded phrases acts as an inhibitory cue for backchannels. Furthermore, in a study of final pitch accents and boundary tones in the turntaking system of Dutch, Caspers [2003] identified two intonation patterns that seem to
be associated with turn-keeping: an accent-lending rise followed by level high pitch
used for bridging syntactic breaks between utterances, and a filled pause with a
mid-level boundary tone for bridging hesitations within syntactic constituents. However,
Caspers could not find any intonation patterns clearly associated with turn-yielding.
This observation led her to conjecture that turn-changing is the unmarked case and that
only the wish to keep the turn needs to be marked with specific intonation patterns.
When delving into this impressive body of work, it is worth noting that the methods employed were impressionistic auditory analyses or manual acoustic analyses for
the most part. The observations rely on full access to the knowledge of trained linguists. This is perhaps reflected in the fact that several cues in the above-mentioned
studies were defined with reference to either the metrical structure or to the accentual
structure of the utterances [e.g. Local et al., 1986; Wells and MacFarlane, 1998]. In
order to capture these cues, accurate classification into metrically heavy and light syllables, as well as into (focally) accented and non-accented words is required. These
classifications in turn require access to information that is not present in the acoustic
signal alone. Consequently, it may not be possible to operationalize all of the abovementioned cues for use in online, automatic systems. Some cues however, such as intonation patterns immediately before silent pauses, are more readily available for
automatic analysis.
Silent pauses are frequently used for chunking the speech stream into manageable units for speech technology applications. The end-of-utterance detectors in current automatic speech recognition typically rely exclusively on a silence threshold
Prosody in Interaction Control

Phonetica 2005;62:215–226

217

somewhere between 500 and 2,000 ms for delimiting the units to be recognized [Ferrer
et al., 2002, and references mentioned therein]. That is to say that the output of the recognizer comes in chunks corresponding to speech bounded by ‘long enough’ silent
pauses; that recognizers using these pauses may deliver the users’ dialogue contributions to the dialogue system only when there is a ‘long enough’ pause, and furthermore
that the system response times are long.
However, as noted above, silent pauses may be indicative of turn-yielding as well
as of turn-keeping, and spontaneous speech frequently contains silent pauses also
within segments we would intuitively call utterance units, and within segments that are
indeed semantically coherent units. Presumably, the uncomfortably long silence
requirement of 2,000 ms mentioned above is used in the hope of excluding such
utterance-internal silences. Part of the goal of this study was simply to assess the extent
of this problem. Human listeners can discriminate these utterance-internal pauses from
utterance-final ones using other prosodic cues, gestural cues and knowledge of semantic completeness. However, these pauses are often well above the silence thresholds in
the end-of-utterance detectors. Moreover, these silences often occur before semantically heavy words without which the unit preceding the pause may be difficult to
interpret.

Method
The method used in the present study is in many ways similar to that used by Koiso et al. [1998]
and Caspers [2003]. That is, map task dialogues were segmented into pause-bounded units – so-called
interpausal units (IPUs). The transitions from one IPU to the next were classified in terms of speaker
changes and speaker holds. Prosodic features were extracted from the region immediately before the
boundary, under the assumption that information relevant to interaction control is localized just before
the speaker changes or holds. In addition, we supplemented the speech material with several kinds of
mark-up and offline analyses, and we evaluated the predictive power of the automatically extracted
prosodic features for making interaction control decisions.
Speech Material
The speech data used for the present study consist of Swedish map task dialogues recorded and
transcribed by Pétur Helgason at the Stockholm University [Helgason, 2002]. Map tasks were
designed to elicit natural-sounding spontaneous dialogues [Anderson et al., 1991]. There are two participants in a map task dialogue: an instruction giver and an instruction follower. They have one map
each, and these maps are similar but not identical. For example, certain landmarks on these maps may
be shared, whereas others are only present on one map, some landmarks occur on both maps but are in
different positions. The task is for the instruction giver to describe a route indicated on his or her map
to the follower.
The Swedish map task data used in this study consist of recordings of two pairs of speakers.
Within each pair, each speaker acted as giver once and as follower once. They were recorded in an anechoic room at the Phonetics lab, Stockholm University, using close-talking microphones, and facing
away from each other. They were recorded on separate channels with a good separation of the channels. In other words, the data were obtained under close to ideal conditions. The tasks elicited spontaneous dialogues with a number of disfluencies and hesitations, where almost every turn was
acknowledged by another turn or by means of verbal feedback. There were four dialogues containing
about 1,100 dialogue contributions (including feedback or backchannels) and the total duration of the
four dialogues was about 1.
These recordings were accompanied by verbatim transcriptions, tentatively segmented into dialogue contributions, but there was no indication of stretches of overlapping speech [for further details
on the Swedish map task data, see Helgason, 2002].

218

Phonetica 2005;62:215–226

Edlund/Heldner

Fig. 1. Schematic illustration

Giver channel:

[…] Speech

Long enough silent pause […]
ICI

Follower channel:

[…] Long enough silent pause

Speech […]

of a speaker change (from giver
to follower). Long enough silent
pauses are pauses that exceed
300 ms, and the minimum
amount of silence between giver
and follower contributions (ICI)
is 10 ms.

Segmentation into IPUs
As in Koiso et al. [1998] and Caspers [2003], the material was segmented into IPUs. The details
of our segmentation, however, differed from the previous ones in several respects. First, each of the
giver and the follower channels was automatically segmented into speech and silence using a basic
speech activity detector (SAD). The frame level decisions produced by the SAD were smoothed and a
minimum silence of 300 ms was required. The minimum silence decreases the likelihood of detecting
pauses in the occlusion phases in stops (that often exceed the 100-ms threshold used in previous studies). Second, each transition from speech to silence in the giver’s channel where there was no overlapping speech in the follower’s channel was marked as an IPU boundary. Transitions from follower to
giver were left out for simplicity, whereas omitting cases of overlapping speech was motivated by a
wish to avoid interruptions and contributions where there is competition for the turn, since these are
not what this study aims to model.
Speaker Change vs. Speaker Hold Classification
Each IPU selected was labelled as an instance of either speaker hold (‘hold’) or speaker change
(‘change’). The ‘hold’ label was given to those IPUs that were followed by a contribution from the
same speaker, that is the giver (recall that only the giver IPUs were investigated). The IPUs that were
followed by a contribution from the other speaker (i.e. the follower) were labelled ‘change’. This
mark-up was also made automatically, based on the acoustic signal in the giver and follower channels.
Note, however, that this mark-up makes no distinction between turns and backchannels. If a giver contribution was succeeded by a follower contribution, there was a ‘change’ irrespective of whether the
follower contribution was a backchannel. Thus, transitions that Koiso et al. [1998] and Caspers [2003]
would have classified as ‘hold’ due to backchannels on either side of the silent pause were classified as
‘change’ here. The speaker change vs. speaker hold classification was later used as a gold standard for
the predictions based on prosody.
This mark-up shows the actual turn of events in the dialogue: it is a direct reflection of the interlocutors’ behaviour ensuring that the speaker changes and holds were perceived as such by the participants as well. It does not, however, show how things must be by necessity. A speaker change may, for
example, be an unsuitable place for a polite contribution if the follower interrupts the giver in the actual
dialogue. Similarly, a speaker hold may be a suitable place for the follower to give a contribution, except
one where the follower simply refrains from saying something. In addition, it is far from obvious that
a place where the follower contributes a backchannel is good for any other contributions than backchannels. The opposite, a backchannel instead of some other turn, is probably more acceptable.
A speaker change then, in our approach, is speech in the giver channel followed by at least a
300-ms silence in the same channel, and non-overlapping speech in the follower channel (fig. 1). Thus,
we exclude a small number of potentially interesting cases where there is a speaker change with a slight
non-competitive overlap. For reasons of temporal resolution, the minimum amount of silence between
giver and follower in a speaker change transition is 10 ms. Correspondingly, a speaker hold is speech in
the giver channel followed by at least a 300-ms silence and then more speech in the same channel. The
minimum amount of silence between contributions in ‘hold’ transitions is thus 300 ms.
In addition to the segmentation of IPUs, we extracted the intercontribution intervals (ICI) – the
actual durations of silent pauses between the IPUs (i.e. interspeaker intervals in the case of speaker
change and intraspeaker intervals in the case of speaker holds). Again, this was done automatically and
with a temporal resolution of 10 ms.

Prosody in Interaction Control

Phonetica 2005;62:215–226

219

Perceptual Judgments
As the classification of speaker change vs. speaker hold only illustrates the actual events, not
how things have to be, judgments of how each IPU was perceived by human listeners were added as a
reality check. The task for the judges was to decide, based on a presentation of 2 s of speech prior to the
silence (but not what followed after the silence), whether the speaker was finished or not at the end of
the IPU. Three judges (two are the present authors) independently judged every IPU in the speech
material (824 cases) on a five-point scale where 1 represented definitely unfinished; 2 probably
finished; 3 could be unfinished or could be finished; 4 probably finished, and 5 definitely finished.
Eighteen judgments were lost for technical reasons yielding a total of 2,454 judgments.
Assuming that it is suitable to take the turn after IPUs that are perceived as finished, these judgments may be seen as a mark-up of potential places for speaker changes. Correspondingly, assuming
that it is unsuitable to take the turn after IPUs judged to be unfinished, such judgments indicate places
perceived as unsuitable for speaker changes.
Prosodic Feature Extraction with /nailon/
Primarily, this research aims at finding and describing the turn-taking information that is encoded
in the speech signal, and how this information can be extracted. One way of accomplishing that is to test
extracted results against some gold standard – a flawless record often assembled by asking human
judges or by somehow describing human behaviour. In our case, the matter is made more complicated
by our wish to extract only such information that humans have access to in real dialogue situations – we
are for example limited to left context only. Limiting ourselves to such information as is accessible to
humans is not just a method to safeguard against some possibly confounding variables – it is also necessary if the resulting analysis is to be useful in practical applications, such as spoken dialogue systems.
The prosodic analysis tool described here is on-line in the sense that uses no acoustic right context or
look-ahead. On the acoustic level, this goes well with human circumstances. Humans rarely need
acoustic right context to make decisions about speech segmentation. On the contrary, they often seem to
be able to predict turn endings and suchlike. Naturally, semantic expectations provide quite considerable ‘look-ahead’ to humans, and in an ideal system these should be used in conjunction with acoustic
analysis. Furthermore, albeit not a theoretical requirement, any implementation must run in real time in
order to be useful as far as live user studies are concerned. The present implementation is real-time in
the sense that it performs in real time, with a small and constant latency, on a standard PC.
The on-line real-time prosodic analysis is implemented within /nailon/ (a phonetic anagram for
online), a Tcl/Tk package based on the Snack Sound Toolkit [http://www.speech.kth.se/snack/].
/nailon/ uses Snack to manage sounds and to extract intensity, voicing and pitch information. In its
present state, the package also captures speech duration, voiced speech duration, silence duration, and
the relative position of intonation patterns in an on-line estimation of the speaker’s F0 range. The
analysis is in some ways similar to that used by Ward and Tsukahara [2000], and is performed in several consecutive steps, including speech activity detection, voice, pitch and intensity extraction,
pseudo-syllabification and intonation pattern classification.
In order to be on-line as well as efficient, each step is performed on a small, continuously moving window consisting of a small number of 10-ms frames. In the experiments presented here, the window size is 300 ms, or 30 10-ms frames. The latency of the system is a function of the frame size plus
whatever processing time is needed for each step and each frame. The algorithms used here are implemented incrementally, keeping both processing and memory footprint to a minimum. Each processing
step is described in detail below.
Speech Activity Detection. A basic SAD is used to discriminate speech from non-speech (or
silence). This decision is based on a noise threshold determined from the intensity distribution (simply
the local minimum following the first local maximum in the distribution). A measure of the intensity
(in dB) is computed for every 10-ms sound frame and the intensity distribution is updated continuously.
Any frame with more energy than the threshold is marked as speech. The sequence of frame level decisions is converted into durations of speech and silence segments by requiring that a minimum number
of consecutive frames (10 frames or 100 ms in the present experiments) be given the same classification in order for a change to be reported. This padding removes some of the effect of various lowenergy components of speech such as fricatives, short silences such as the occlusion part in stops, and

220

Phonetica 2005;62:215–226

Edlund/Heldner

various short high-energy segments embedded in silences. Although this SAD is simplistic, it is so far
sufficient for our needs. SAD is a vivid research topic, but here, we are interested in the extraction of
prosodic features, and from that point of view SAD is a prerequisite, not a research topic in itself. It
will not be discussed further here.
Voice, Pitch and Intensity Extraction. A pitch extractor (the ESPS get-F0 program included in
Snack) acquires information about voiced and unvoiced speech frames and the F0 values of the voiced
frames. This sequence of frame level voicing decisions is used to compute durations of voiced and
unvoiced speech, again under the requirement that a change is stable over a number of frames for it to
be reported, to allow for artefacts introduced by the pitch tracker. The F0 values in hertz are transformed into semitones relative to a fixed value and smoothed using a median filter (currently over 9
frames). The median filter is applied separately to correct for the pitch extractor’s inability to smooth
the pitch curve over such short speech segments. The semitone-transformed F0 data are then used to
estimate speaker F0 range based on the cumulative distribution of F0 data. The F0 range is bounded by
a topline and a baseline defined as the cumulative mean Ϯ2 standard deviations (also calculated cumulatively). The semitone scale is used to ensure that the ϩ1 standard deviation interval is the same musical and perceptual interval as Ϫ1 standard deviations. The F0 range is divided into three equal parts:
high, mid and low. Intensity is treated in a similar manner: the median filtered intensity in decibel in
each frame is used to incrementally build up a cumulative mean Ϯ2 standard deviations.
Pseudo-Syllabification. The pseudo-syllabification algorithm is loosely based on Mermelstein’s
[1975] technique to find intensity minima in the speech signal by using convex hulls. Convex hulls are
continuously tracked over the median filtered intensity values. Note, however, that only voiced segments are included. The convex hulls are taken to correspond to pseudo-syllables that can be retrieved
and analysed as the need arises. In the present experiments, the last convex hull that has been seen over
the intensity curve is extracted each time a sufficiently long (300 ms) silence is detected.
Intonation Pattern Classification. Whenever a sufficient silence is found, the information from
the pseudo-syllabification is used to point out a number of frames that roughly corresponds to the last
syllable nucleus (i.e. minimally the vowel) before the silent sequence. The information from the pitch
extractor pertaining to the pseudo-syllable is then used to classify the intonation patterns. These intonation patterns are classified in terms of their position in the F0 range (currently as high, mid or low
tones) and in terms of their shapes (rises, falls and level tones).
Turn-Keeping and Turn-Yielding Decisions
Finally, the speech and silent pause durations in combination with the intonation pattern classification are used to make decisions about interaction control. Any silent pause in the giver channel
exceeding the pause threshold (i.e. 300 ms), which is preceded by a mid-level intonation pattern,
belongs to the ‘turn-keeping’ category. Silent pauses followed by low intonation patterns are categorized as ‘turn-yielding’. All other places belong to the ‘don’t know’ category.
The thresholds and parameter values were manually set in the initial implementation used in the
tests reported here, but they were set before the tests were performed and have not been subsequently
altered to improve results on the current speech material. In future versions, they should be optimized
based on corpus studies.
Evaluating the Predictive Power of the Automatically Extracted Prosodic Features
To evaluate the predictive power of the prosodic features extracted with /nailon/, the interaction
control decisions made by /nailon/ were compared with what actually happened in the dialogues, that
is the automatic classification into speaker change vs. speaker hold described above.

Results and Discussion

The Extent of the Problem with Silence-Based Segmentation
By combining the results of the perceptual judgments and the automatic speaker
change vs. speaker hold classification, we can get an estimate of the extent of the problem

Prosody in Interaction Control

Phonetica 2005;62:215–226

221

Table 1. The distribution of perceptual judgments by the three judges as to whether the IPUs were

finished or not tabulated against the automatic speaker change vs. speaker hold classification
1
(definitely
unfinished)
Speaker change
Speaker hold
Total

2
(probably
unfinished)

3
(could be
either finished
or unfinished)

4
(probably
finished)

5
(definitely
finished)

Total

169
806
975

66
91
157

40
25
65

200
112
312

705
240
945

1,180
1,274
2,454

Table 2. The distribution of the
majority votes for the perceptual
judgments

Unfinished
Speaker change
Speaker hold
Total

Finished

Disagree

Total

59
303
362

328
120
448

6
8
14

393
431
824

Before determining the majority votes across the three judges, the probably and definitely unfinished cases were collapsed into the category unfinished, and the probably and definitely finished votes were collapsed into the
category finished. The column labelled ‘disagree’ represents the cases
where no majority vote could be established.

with silence-based segmentation in spontaneous dialogues. Table 1 shows the distribution of perceptual judgments of the IPUs tabulated against the speaker change vs.
speaker hold classification.
Several observations can be made. First, to state the obvious, all the speaker hold
cases represent pause-bounded units where no speaker change occurred, and these
cases correspond to 52.3% of all silent pauses in the material. Some of these speaker
hold cases were judged as probably or definitely finished utterances, and may be seen
as potential places for speaker changes only that no speaker change occurred. These
cases are not part of the problem with silence-based segmentation. However, by collapsing the votes for probably and definitely unfinished as well as those for probably
and definitely finished and taking the majority vote across the three judges, we find that
the speaker hold cases judged to be unfinished correspond to 36.7% of all pauses
(table 2). These cases pose a real problem for silence-based segmentation since they are
the cases where a dialogue system using silence-based segmentation runs the risk of
interrupting its users. These are the cases we want to avoid.
There was also a substantial number of pause-bounded units where speaker
changes actually occurred (47.7% of all pauses). By collapsing the votes, this time for
probably and definitely finished, and then taking the majority vote across the three
judges, we find that the majority of the speaker change cases (83.5%) were indeed perceived as finished utterances (table 2). So, these are the places where we want the system to speak. Finally, there were also a few cases where speaker changes occurred

222

Phonetica 2005;62:215–226

Edlund/Heldner

Table 3. Number of IPUs and the proportion of speaker holds amongst these as a function of

increased thresholds in silence-based segmentation
Silence thresholds
Ͼ300 ms
IPUs, n
Holds, %

Ͼ500 ms

Ͼ1,000 ms

Ͼ1,500 ms

Ͼ2,000 ms

634
68

441
71

168
68

69
67

22
55

although the judges felt that they were probably or definitely unfinished (15%). Some
of these were probably deliberate interruptions, and therefore not places where we
want to detect an opportunity for the system to speak.
In addition to illustrating the problem with silence-based segmentation, tables 1
and 2 give an indication of the amount of noise in the data. In the majority votes by the
three human listeners, 27.8% of the ‘hold’ cases were perceived as finished and 15% of
the ‘change’ cases were perceived as unfinished (table 2). In our view, this points to the
minimum error we can expect from an automatic categorization, unless it has access to
more information than the human judges were given.
Would Increasing the Silence Threshold Solve the Problem
with Silence-Based Segmentation?
The minimum amount of silence required for IPU segmentation in this study was
300 ms. Humans often respond even faster than that, yet longer minimum pauses are
often used in spoken dialogue systems in the hope of ensuring correct turn-taking
behaviour. The question, then, is: does an increased silence threshold solve any problems involved in silence-based segmentation? Table 3 presents the proportion of IPUs
that resulted in a speaker hold in five subsets of the material, each consisting of the
IPUs with an ICI above a certain threshold.
In order to assess the amount of errors created by using silence as the sole basis for
turn-taking decisions, and to find out how successful the strategy of lengthening the
required silence is, we analysed the map task dialogues to see how ‘hold’ and ‘change’
depended on ICI. As noted before, most current systems use silence thresholds of 500 –
2,000 ms, and our system currently requires 300 ms of silence. First of all, it is worth
noting that less than 50% of the turns in the map task dialogues had an ICI of more than
500 ms, implying that continuously waiting for 500 ms or more before responding is
not natural. Considerably more, over 70% of the turns, are captured if the minimum ICI
is lowered to 300 ms. Even more interestingly, the percentage of ‘holds’ is virtually
unchanged whether one looks at all IPUs with a minimum ICI of 300, 500, 1,000 or
1,500 ms. At ICIs of 2,000 ms or more, the percentage of ‘holds’ falls somewhat, but
the material is too small to be reliable – we only have 19 instances to go on. The numbers suggest that the only thing a spoken dialogue system would achieve by using a
silence threshold of 2,000 ms is a sluggish behaviour.
Can Prosody Help in Solving the Problem with Silence-Based Segmentation?
Requiring longer silences obviously does nothing to solve the problems involved
in silence-based segmentation, but can prosody and intonation patterns help? Recall that

Prosody in Interaction Control

Phonetica 2005;62:215–226

223

Table 4. Automatic classifica-

tion of IPUs into ‘turn-keeping’,
‘turn-yielding’ and ‘don’t know’
tabulated against the classification of speaker changes and
speaker holds

Turn-keeping
Change
Hold
Total

Don’t know

Turn-yielding

Total

23
105
128

212
255
467

158
71
229

393
431
824

level intonation patterns in the middle of the speakers’ fundamental frequency range
have been observed to act as turn-keeping cues in several different languages.
Based on the prosodic features extracted with /nailon/, the automatic classifier
classified each IPU into one out of three categories: ‘turn-keeping’, ‘don’t know’ and
‘turn-yielding’. Table 4 presents the results tabulated against the speaker ‘change’/‘hold’
classification. The material contained 824 IPUs, 28% of which were classified as ‘turnyielding’, 16% as ‘turn-keeping’ and 56% ended up in the garbage category ‘don’t
know’.
Among the IPUs classified as suitable for turn-taking (i.e. ‘turn-yielding’), the
speaker changes were in the majority (69%). Correspondingly, the speaker holds were
in the majority in the IPUs classified as ‘turn-keeping’ (82%). From a different point of
view, the classifier identified at least 41% of the possible places for turn-taking (recall
that some of the speaker holds were perceived as finished by the human listeners – these
may well be suitable places although no speaker change actually occurred; table 1).
Furthermore, if only the ‘turn-yielding’ IPUs are seen as suitable places for turn-taking
(and ‘turn-keepings’ and ‘don’t knows’ are pooled), the classifier identified 84% of the
places where interruptions are impossible.
In a spoken dialogue system, the fairly large garbage category produced by the
parameter settings in this experiment can be pooled in two ways. If used conservatively,
‘don’t know’ and ‘turn-keeping’ would be pooled and the system only takes turn at
IPUs categorized as ‘turn-yielding’. Such a system would avoid 84% of the IPUs
unsuitable for system utterances, and detect roughly every second opportunity for a
system utterance. The approach is suitable for a system that barges into human conversations, perhaps in order to give notifications. Conversely, a more aggressive system
would pool ‘don’t know’ with ‘turn-yielding’ and only avoid taking turns when the categorization gave ‘turn-keeping’. This would avoid 24% of the unsuitable places whilst
detecting 94% of the suitable places. The latter strategy would be used in a system that
engages in a dialogue directly with a user.
It is possible to improve these results if the thresholds used in the classifier are
tuned with machine learning techniques, and with further subdivisions of the ‘don’t
know’ category, for example by adding categories such as backchannels, listings, feedback on various grounding levels [Allwood et al., 1993; Clark, 1996]. We intend to
explore these possibilities in future work.
Conclusions

In this paper, we have explored on-line prosodic analysis as a means to improve
the interaction control in spoken human-computer dialogue. The experiments have
shown that dialogue systems relying on silence-based segmentation run the risk of

224

Phonetica 2005;62:215–226

Edlund/Heldner

interrupting its users in as much as 35% of all silent pauses, at least if they encounter
speech of the kind investigated here.
Furthermore, we have shown that the number of incorrect turn-taking decisions
can be reduced substantially by combining standard silence-based endpoint detection
with an automatic classification of intonation patterns. In the process, it is also possible
to decrease the length of the required silence without any loss in performance. This can
be used to make a conversational computer more responsive by allowing it to reply
faster without simultaneously making it more obtrusive.
Level intonation patterns in the middle of the speakers’ fundamental frequency
range were found to act as turn-keeping cues, and may thus be used to avoid interrupting human interlocutors with high precision. Although there are several observations of
the function of these mid-level intonation patterns, to our knowledge, they have never
been used for avoiding interrupting users in spoken dialogue systems before.
Rising intonation before a silent pause can be associated with turn-yielding as well
as with turn-keeping [e.g. Local et al., 1986]. As 51% of the rising intonation patterns
co-occurred with actual speaker changes and 49% with speaker holds, we opted to have
/nailon/ classify these as ‘don’t know’. However, it is evident that the classification
would benefit from a more thorough analysis of rising intonation preceding silent
pauses. We suspect that a fourth conditional turn-yielding category is needed (in addition to ‘turn-yielding’, ‘turn-keeping’ and ‘don’t know’) to capture places where only
certain types of contributions, such as positive feedback, objections and clarification
requests, are permitted. We intend to explore these possibilities presently, although this
probably cannot be done without taking lexical aspects into account.
In general, it is clear that prosodic analysis is not enough to create conversational
computers with interaction control skills at near-human levels. Humans use higher levels
of understanding and a variety of information. We feel, however, that on-line access to
prosodic information provides a valuable source of information that should be combined
with other sources to guide the interaction control in conversational dialogue systems.
Acknowledgments
We are grateful to the two anonymous reviewers for their comments on an earlier version of this
paper. This work was done within the Project CHIL ‘Computers in the Human Interaction Loop’ (IP
506909). CHIL is an Integrated Project under the European Commission’s Sixth Framework Program.

References
Allwood, J.; Nivre, J.; Ahlsén, E.: On the semantics and pragmatics of linguistic feedback. J. Semant. 9: 1–26
(1993).
Anderson, A.H.; Bader, M.; Bard, E.G.; Boyle, E.; Doherty, G.; Garrod, S.; Isard, S.; Kowtko, J.; McAllister, J.; Miller, J.;
Sotillo, C.; Thompson, H.; Weinert, R.: The HCRH map task Corpus. Lang. Speech 34: 83–97 (1991).
Bell, L.; Boye, J.; Gustafson, J.: Real-time handling of fragmented utterances. Proc. North Am. Chapt. Assoc.
Comput. Linguist. 2001.
Caspers, J.: Local speech melody as a limiting factor in the turn-taking system in Dutch. J. Phonet. 31: 251–276
(2003).
Clark, H.H.: Using language (Cambridge University Press, Cambridge 1996).
Duncan, S. Jr.: Some signals and rules for taking speaking turns in conversations. J. Pers. soc. Psychol. 23: 283–292
(1972).
Edlund, J.; Heldner, M.; Gustafson, J.: Utterance segmentation and turn-taking in spoken dialogue systems; in Fisseni,
Schmitz, Schröder, Wagner, Sprachtechnologie, mobile Kommunikation und linguistische Ressourcen,
pp. 576–587 (Peter Lang, Frankfurt am Main 2005).

Prosody in Interaction Control

Phonetica 2005;62:215–226

225

Ferrer, L.; Shriberg, E.; Stolcke, A.: Is the speaker done yet? Faster and more accurate end-of-utterance detection using
prosody in human-computer dialog. Proc. Int. Conf. spoken Lang. Processing, Denver 2002, pp. 2061–2064.
Ford, C.E.; Thompson, S.A.: Interactional units in conversation: syntactic, intonational, and pragmatic resources for
the management of turns; in Ochs, Schegloff, Thompson, Interaction and grammar, pp. 134–184 (Cambridge
University Press, Cambridge 1996).
Heldner, M.; Edlund, J.; Carlson, R.: Interruption impossible; in Horne, Bruce, Proc. of Nordic Prosody IX (Peter
Lang, Frankfurt am Main in press).
Helgason, P.: Preaspiration in the Nordic languages: synchronic and diachronic aspects; PhD diss. Stockholm (2002).
Koiso, H.; Horiuchi, Y.; Tutiya, S.; Ichikawa, A.; Den, Y.: An analysis of turn-taking and backchannels based on
prosodic and syntactic features in Japanese map task dialogs. Lang. Speech 41: 295–321 (1998).
Local, J.K.; Kelly, J.: Projection and ‘silences’: notes on phonetic and conversational structure. Hum. Stud. 9:
185–204 (1986).
Local, J.K.; Kelly, J.; Wells, W.H.G.: Towards a phonology of conversation: turn-taking in Tyneside English.
J. Linguist. 22: 411–437 (1986).
Local, J.K.; Wells, W.H.G.; Sebba, M.: Phonology for conversation: phonetic aspects of turn delimitation in London
Jamaican. J. Pragm. 9: 309–330 (1985).
Mermelstein, P.: Automatic segmentation of speech into syllabic units. J. acoust. Soc. Am. 58: 880–883 (1975).
Noguchi, H.; Den, Y.: Prosody-based detection of the context of backchannel responses. Proc. 5th Int. Conf. spoken
Lang. Processing, Sydney 1998, pp. 487–490.
Ogden, R.: Turn transition, creak and glottal stop in Finnish talk-in-interaction. J. Int. Phonet. Assoc. 31: 139–152
(2001).
Selting, M.: On the interplay of syntax and prosody in the constitution of turn-constructional units and turns in conversation. Pragmatics 6: 357–388 (1996).
Skantze, G.; Edlund, J.: Robust interpretation in the Higgins spoken dialogue system. Proc. Robust, Norwich 2004.
Ward, N.; Tsukahara, W.: Prosodic features which cue back-channel responses in English and Japanese. J. Pragm.
32: 1177–1207 (2000).
Wells, B.; MacFarlane, S.: Prosody as an interactional resource: turn projection and overlap. Lang. Speech 41:
265–294 (1998).

226

Phonetica 2005;62:215–226

Edlund/Heldner

Phonetica 2005;62:215–226
DOI: 10.1159/000090099

Received: June 24, 2005
Accepted: October 4, 2005

Exploring Prosody in Interaction Control
Jens Edlund

Mattias Heldner

KTH Speech, Music and Hearing, Stockholm, Sweden

Abstract
This paper investigates prosodic aspects of turn-taking in conversation with a
view to improving the efficiency of identifying relevant places at which a machine
can legitimately begin to talk to a human interlocutor. It examines the relationship
between interaction control, the communicative function of which is to regulate
the flow of information between interlocutors, and its phonetic manifestation.
Specifically, the listener’s perception of such interaction control phenomena is
modelled. Algorithms for automatic online extraction of prosodic phenomena
liable to be relevant for interaction control, such as silent pauses and intonation
patterns, are presented and evaluated in experiments using Swedish map task
data. We show that the automatically extracted prosodic features can be used to
avoid many of the places where current dialogue systems run the risk of interrupting their users, as well as to identify suitable places to take the turn.
Copyright © 2005 S. Karger AG, Basel

Introduction

Conversation is a primary means for human communication. An important function
of conversation is to exchange propositional content, and during this exchange the interlocutors must somehow regulate the flow of information to make it proceed smoothly
and efficiently. This interaction control is a collaborative effort where interlocutors
continuously monitor various aspects of each other’s behaviour, including semantics,
gestures and prosody, for example, in order to make decisions about turn-taking and
feedback. The term ‘turn-taking’ includes how to take the floor without interrupting,
how to keep the floor and how to pass the initiative on to others. Feedback is used to
indicate to the speaker that the listener is attentive, understanding and agreeing.
The aim of our research is to improve the interaction control in spoken humancomputer dialogue. The primary motivation for this is that if a spoken dialogue system
is to be perceived as a good conversational partner, it has to be able to understand the
speaker’s organization of the content, and to know how to time its own contributions to
the dialogue appropriately, in addition to recognizing and responding to verbal input
and generating verbal output.
Current spoken dialogue systems commonly detect where the user ceases speaking
in order to find out where they should take their turn. The method is based on the assumption

Fax ϩ41 61 306 12 34
E-Mail karger@karger.ch
www.karger.com

© 2005 S. Karger AG, Basel
0031–8388/05/0624–0215
$22.00/0
Accessible online at:
www.karger.com/journals/pho

Jens Edlund
KTH Speech, Music and Hearing
Lindstedtsvägen 24
SE–100 44 Stockholm (Sweden)
Tel. ϩ46 8 790 7874, Fax ϩ46 8 790 7854
E-Mail edlund@speech.kth.se

that speakers have finished what they intended to say when they become silent, and that
these points in time are also suitable places for the system to speak. Such endpoint detection triggers on a certain set amount of silence, or non-speech. The method makes sense;
given that a speaker is allowed to complete what she/he intends to say, the end of the
utterance is likely to coincide with silence at a place where an interlocutor might take the
next turn. The method segments speech into reasonably sized units, in many cases corresponding to sentences or some sentence-like units. However, spontaneous conversational
speech frequently contains silent pauses inside what we would intuitively group into
turns, complete utterances or sentence-like units, and inside what are indeed semantically
coherent units. Typical examples are hesitations such as ‘You will get to a eh [long
silence] well what shall we call it’. Silences (or hesitations) may even occur inside
prosodic words or compound words: ‘There is a eh cycleway and some horse- [long
silence] path maybe’. Thus, dialogue systems using silence-based endpoint detection run
into problems with unfinished utterances when encountering spontaneous speech, as the
silences following them are not likely to be suitable places for the system to speak, and as
unfinished utterances may be difficult to interpret [Bell et al., 2001].
There are a number of common tasks within speech technology and natural language processing where it would be useful to perform automatic segmentation into
units that better match what humans perceive as finished utterances. The following two
types of tasks summarize our primary motivation for this work.
(1) System barge-in. Computers may want to use speech to notify human interlocutors about various events, for example ‘coffee is ready’. This is perhaps analogous to a
meeting secretary, and it is important that the barge-in behaviour is perceived as polite.
(2) Interaction control, turn-taking, feedback. Segmentation of user input is
essential for the conversational components of a dialogue system, notably for identifying suitable places to speak [Heldner et al., in press]. A dialogue system is likely to be
perceived as a better conversational partner if it has a clearer idea of when human interlocutors have finished talking, and if it is able to respond rapidly.
In advanced spoken dialogue systems, spoken language understanding and interaction control are combined. The AdApt system, for example, uses a semantically
based approach in order to deal with the problems that occur as a result of silence-onlybased utterance segmentation [Bell et al., 2001]. Another semantic approach is used in
Skantze and Edlund [2004].
The study presented here represents a continuation of work presented elsewhere
[Edlund et al., 2005; Heldner et al., in press]. Here, we explore online prosodic analysis with the aim of bringing human-like interaction control capabilities to conversational computers. More specifically, we look at prosodic phenomena liable to be
relevant for interaction control, such as silent pauses and intonation patterns. It is worth
noting that although our primary goal is to create natural, human-like spoken dialogue
systems, human-computer conversation experiments provide good evidence as well as
counterevidence for the relationship between interaction control phenomena and their
prosodic manifestations.
Prosodic Phenomena and Interaction Control

Previous work suggests that a number of prosodic or phonetic cues are associated
with turn-yielding and thus potentially relevant for interaction control. These cues include

216

Phonetica 2005;62:215–226

Edlund/Heldner

phenomena such as silent pauses; various intonation patterns (rises, falls, down-steps,
up-steps); decreases in speech rate; final lengthening; intensity patterns; centralized
vowel quality, creaky voice quality, and exhalations. Note that both rises and falls have
been associated with turn-yielding. These cues are typically located somewhere
towards the end of the turn, although not necessarily on the final syllable [e.g. Ford and
Thompson, 1996; Local and Kelly, 1986; Local et al., 1985, 1986; Ogden, 2001; Wells
and MacFarlane, 1998].
Similarly, there are studies suggesting that certain prosodic or phonetic cues are
associated with turn-keeping, and these cues are of course also potentially relevant for
interaction control. They include phenomena such as glottal or vocal tract stops without
audible release; a different quality of silent pauses as a result of these glottal or vocal
tract closures; assimilation across the silent pause, and other ‘held’ articulations (e.g.
lengthened vowels, laterals, nasals or fricatives) [Local and Kelly, 1986; Ogden, 2001].
Certain intonation patterns are associated with turn-keeping. In particular, level
intonation patterns in the middle of the speakers’ fundamental frequency range have
been observed to act as turn-keeping cues in several different languages. For example,
Duncan [1972] reported that any pattern other than a level tone in the speaker’s
mid-register signals turn-yielding in English. Thus, the mid-level pattern acts as a turnkeeping signal, although Duncan did not use that term. Similarly, Selting [1996]
reported that level pitch accents before a pause are used to signal turn-holding (or turnkeeping) in German; Koiso et al. [1998] observed that flat, flat-fall and rise-fall intonation patterns tended to co-occur with speaker holds (i.e. turn-keeping) in Japanese, and
in another study on Japanese conversations, Noguchi and Den [1998] reported that flat
intonation at the end of pause-bounded phrases acts as an inhibitory cue for backchannels. Furthermore, in a study of final pitch accents and boundary tones in the turntaking system of Dutch, Caspers [2003] identified two intonation patterns that seem to
be associated with turn-keeping: an accent-lending rise followed by level high pitch
used for bridging syntactic breaks between utterances, and a filled pause with a
mid-level boundary tone for bridging hesitations within syntactic constituents. However,
Caspers could not find any intonation patterns clearly associated with turn-yielding.
This observation led her to conjecture that turn-changing is the unmarked case and that
only the wish to keep the turn needs to be marked with specific intonation patterns.
When delving into this impressive body of work, it is worth noting that the methods employed were impressionistic auditory analyses or manual acoustic analyses for
the most part. The observations rely on full access to the knowledge of trained linguists. This is perhaps reflected in the fact that several cues in the above-mentioned
studies were defined with reference to either the metrical structure or to the accentual
structure of the utterances [e.g. Local et al., 1986; Wells and MacFarlane, 1998]. In
order to capture these cues, accurate classification into metrically heavy and light syllables, as well as into (focally) accented and non-accented words is required. These
classifications in turn require access to information that is not present in the acoustic
signal alone. Consequently, it may not be possible to operationalize all of the abovementioned cues for use in online, automatic systems. Some cues however, such as intonation patterns immediately before silent pauses, are more readily available for
automatic analysis.
Silent pauses are frequently used for chunking the speech stream into manageable units for speech technology applications. The end-of-utterance detectors in current automatic speech recognition typically rely exclusively on a silence threshold
Prosody in Interaction Control

Phonetica 2005;62:215–226

217

somewhere between 500 and 2,000 ms for delimiting the units to be recognized [Ferrer
et al., 2002, and references mentioned therein]. That is to say that the output of the recognizer comes in chunks corresponding to speech bounded by ‘long enough’ silent
pauses; that recognizers using these pauses may deliver the users’ dialogue contributions to the dialogue system only when there is a ‘long enough’ pause, and furthermore
that the system response times are long.
However, as noted above, silent pauses may be indicative of turn-yielding as well
as of turn-keeping, and spontaneous speech frequently contains silent pauses also
within segments we would intuitively call utterance units, and within segments that are
indeed semantically coherent units. Presumably, the uncomfortably long silence
requirement of 2,000 ms mentioned above is used in the hope of excluding such
utterance-internal silences. Part of the goal of this study was simply to assess the extent
of this problem. Human listeners can discriminate these utterance-internal pauses from
utterance-final ones using other prosodic cues, gestural cues and knowledge of semantic completeness. However, these pauses are often well above the silence thresholds in
the end-of-utterance detectors. Moreover, these silences often occur before semantically heavy words without which the unit preceding the pause may be difficult to
interpret.

Method
The method used in the present study is in many ways similar to that used by Koiso et al. [1998]
and Caspers [2003]. That is, map task dialogues were segmented into pause-bounded units – so-called
interpausal units (IPUs). The transitions from one IPU to the next were classified in terms of speaker
changes and speaker holds. Prosodic features were extracted from the region immediately before the
boundary, under the assumption that information relevant to interaction control is localized just before
the speaker changes or holds. In addition, we supplemented the speech material with several kinds of
mark-up and offline analyses, and we evaluated the predictive power of the automatically extracted
prosodic features for making interaction control decisions.
Speech Material
The speech data used for the present study consist of Swedish map task dialogues recorded and
transcribed by Pétur Helgason at the Stockholm University [Helgason, 2002]. Map tasks were
designed to elicit natural-sounding spontaneous dialogues [Anderson et al., 1991]. There are two participants in a map task dialogue: an instruction giver and an instruction follower. They have one map
each, and these maps are similar but not identical. For example, certain landmarks on these maps may
be shared, whereas others are only present on one map, some landmarks occur on both maps but are in
different positions. The task is for the instruction giver to describe a route indicated on his or her map
to the follower.
The Swedish map task data used in this study consist of recordings of two pairs of speakers.
Within each pair, each speaker acted as giver once and as follower once. They were recorded in an anechoic room at the Phonetics lab, Stockholm University, using close-talking microphones, and facing
away from each other. They were recorded on separate channels with a good separation of the channels. In other words, the data were obtained under close to ideal conditions. The tasks elicited spontaneous dialogues with a number of disfluencies and hesitations, where almost every turn was
acknowledged by another turn or by means of verbal feedback. There were four dialogues containing
about 1,100 dialogue contributions (including feedback or backchannels) and the total duration of the
four dialogues was about 1.
These recordings were accompanied by verbatim transcriptions, tentatively segmented into dialogue contributions, but there was no indication of stretches of overlapping speech [for further details
on the Swedish map task data, see Helgason, 2002].

218

Phonetica 2005;62:215–226

Edlund/Heldner

Fig. 1. Schematic illustration

Giver channel:

[…] Speech

Long enough silent pause […]
ICI

Follower channel:

[…] Long enough silent pause

Speech […]

of a speaker change (from giver
to follower). Long enough silent
pauses are pauses that exceed
300 ms, and the minimum
amount of silence between giver
and follower contributions (ICI)
is 10 ms.

Segmentation into IPUs
As in Koiso et al. [1998] and Caspers [2003], the material was segmented into IPUs. The details
of our segmentation, however, differed from the previous ones in several respects. First, each of the
giver and the follower channels was automatically segmented into speech and silence using a basic
speech activity detector (SAD). The frame level decisions produced by the SAD were smoothed and a
minimum silence of 300 ms was required. The minimum silence decreases the likelihood of detecting
pauses in the occlusion phases in stops (that often exceed the 100-ms threshold used in previous studies). Second, each transition from speech to silence in the giver’s channel where there was no overlapping speech in the follower’s channel was marked as an IPU boundary. Transitions from follower to
giver were left out for simplicity, whereas omitting cases of overlapping speech was motivated by a
wish to avoid interruptions and contributions where there is competition for the turn, since these are
not what this study aims to model.
Speaker Change vs. Speaker Hold Classification
Each IPU selected was labelled as an instance of either speaker hold (‘hold’) or speaker change
(‘change’). The ‘hold’ label was given to those IPUs that were followed by a contribution from the
same speaker, that is the giver (recall that only the giver IPUs were investigated). The IPUs that were
followed by a contribution from the other speaker (i.e. the follower) were labelled ‘change’. This
mark-up was also made automatically, based on the acoustic signal in the giver and follower channels.
Note, however, that this mark-up makes no distinction between turns and backchannels. If a giver contribution was succeeded by a follower contribution, there was a ‘change’ irrespective of whether the
follower contribution was a backchannel. Thus, transitions that Koiso et al. [1998] and Caspers [2003]
would have classified as ‘hold’ due to backchannels on either side of the silent pause were classified as
‘change’ here. The speaker change vs. speaker hold classification was later used as a gold standard for
the predictions based on prosody.
This mark-up shows the actual turn of events in the dialogue: it is a direct reflection of the interlocutors’ behaviour ensuring that the speaker changes and holds were perceived as such by the participants as well. It does not, however, show how things must be by necessity. A speaker change may, for
example, be an unsuitable place for a polite contribution if the follower interrupts the giver in the actual
dialogue. Similarly, a speaker hold may be a suitable place for the follower to give a contribution, except
one where the follower simply refrains from saying something. In addition, it is far from obvious that
a place where the follower contributes a backchannel is good for any other contributions than backchannels. The opposite, a backchannel instead of some other turn, is probably more acceptable.
A speaker change then, in our approach, is speech in the giver channel followed by at least a
300-ms silence in the same channel, and non-overlapping speech in the follower channel (fig. 1). Thus,
we exclude a small number of potentially interesting cases where there is a speaker change with a slight
non-competitive overlap. For reasons of temporal resolution, the minimum amount of silence between
giver and follower in a speaker change transition is 10 ms. Correspondingly, a speaker hold is speech in
the giver channel followed by at least a 300-ms silence and then more speech in the same channel. The
minimum amount of silence between contributions in ‘hold’ transitions is thus 300 ms.
In addition to the segmentation of IPUs, we extracted the intercontribution intervals (ICI) – the
actual durations of silent pauses between the IPUs (i.e. interspeaker intervals in the case of speaker
change and intraspeaker intervals in the case of speaker holds). Again, this was done automatically and
with a temporal resolution of 10 ms.

Prosody in Interaction Control

Phonetica 2005;62:215–226

219

Perceptual Judgments
As the classification of speaker change vs. speaker hold only illustrates the actual events, not
how things have to be, judgments of how each IPU was perceived by human listeners were added as a
reality check. The task for the judges was to decide, based on a presentation of 2 s of speech prior to the
silence (but not what followed after the silence), whether the speaker was finished or not at the end of
the IPU. Three judges (two are the present authors) independently judged every IPU in the speech
material (824 cases) on a five-point scale where 1 represented definitely unfinished; 2 probably
finished; 3 could be unfinished or could be finished; 4 probably finished, and 5 definitely finished.
Eighteen judgments were lost for technical reasons yielding a total of 2,454 judgments.
Assuming that it is suitable to take the turn after IPUs that are perceived as finished, these judgments may be seen as a mark-up of potential places for speaker changes. Correspondingly, assuming
that it is unsuitable to take the turn after IPUs judged to be unfinished, such judgments indicate places
perceived as unsuitable for speaker changes.
Prosodic Feature Extraction with /nailon/
Primarily, this research aims at finding and describing the turn-taking information that is encoded
in the speech signal, and how this information can be extracted. One way of accomplishing that is to test
extracted results against some gold standard – a flawless record often assembled by asking human
judges or by somehow describing human behaviour. In our case, the matter is made more complicated
by our wish to extract only such information that humans have access to in real dialogue situations – we
are for example limited to left context only. Limiting ourselves to such information as is accessible to
humans is not just a method to safeguard against some possibly confounding variables – it is also necessary if the resulting analysis is to be useful in practical applications, such as spoken dialogue systems.
The prosodic analysis tool described here is on-line in the sense that uses no acoustic right context or
look-ahead. On the acoustic level, this goes well with human circumstances. Humans rarely need
acoustic right context to make decisions about speech segmentation. On the contrary, they often seem to
be able to predict turn endings and suchlike. Naturally, semantic expectations provide quite considerable ‘look-ahead’ to humans, and in an ideal system these should be used in conjunction with acoustic
analysis. Furthermore, albeit not a theoretical requirement, any implementation must run in real time in
order to be useful as far as live user studies are concerned. The present implementation is real-time in
the sense that it performs in real time, with a small and constant latency, on a standard PC.
The on-line real-time prosodic analysis is implemented within /nailon/ (a phonetic anagram for
online), a Tcl/Tk package based on the Snack Sound Toolkit [http://www.speech.kth.se/snack/].
/nailon/ uses Snack to manage sounds and to extract intensity, voicing and pitch information. In its
present state, the package also captures speech duration, voiced speech duration, silence duration, and
the relative position of intonation patterns in an on-line estimation of the speaker’s F0 range. The
analysis is in some ways similar to that used by Ward and Tsukahara [2000], and is performed in several consecutive steps, including speech activity detection, voice, pitch and intensity extraction,
pseudo-syllabification and intonation pattern classification.
In order to be on-line as well as efficient, each step is performed on a small, continuously moving window consisting of a small number of 10-ms frames. In the experiments presented here, the window size is 300 ms, or 30 10-ms frames. The latency of the system is a function of the frame size plus
whatever processing time is needed for each step and each frame. The algorithms used here are implemented incrementally, keeping both processing and memory footprint to a minimum. Each processing
step is described in detail below.
Speech Activity Detection. A basic SAD is used to discriminate speech from non-speech (or
silence). This decision is based on a noise threshold determined from the intensity distribution (simply
the local minimum following the first local maximum in the distribution). A measure of the intensity
(in dB) is computed for every 10-ms sound frame and the intensity distribution is updated continuously.
Any frame with more energy than the threshold is marked as speech. The sequence of frame level decisions is converted into durations of speech and silence segments by requiring that a minimum number
of consecutive frames (10 frames or 100 ms in the present experiments) be given the same classification in order for a change to be reported. This padding removes some of the effect of various lowenergy components of speech such as fricatives, short silences such as the occlusion part in stops, and

220

Phonetica 2005;62:215–226

Edlund/Heldner

various short high-energy segments embedded in silences. Although this SAD is simplistic, it is so far
sufficient for our needs. SAD is a vivid research topic, but here, we are interested in the extraction of
prosodic features, and from that point of view SAD is a prerequisite, not a research topic in itself. It
will not be discussed further here.
Voice, Pitch and Intensity Extraction. A pitch extractor (the ESPS get-F0 program included in
Snack) acquires information about voiced and unvoiced speech frames and the F0 values of the voiced
frames. This sequence of frame level voicing decisions is used to compute durations of voiced and
unvoiced speech, again under the requirement that a change is stable over a number of frames for it to
be reported, to allow for artefacts introduced by the pitch tracker. The F0 values in hertz are transformed into semitones relative to a fixed value and smoothed using a median filter (currently over 9
frames). The median filter is applied separately to correct for the pitch extractor’s inability to smooth
the pitch curve over such short speech segments. The semitone-transformed F0 data are then used to
estimate speaker F0 range based on the cumulative distribution of F0 data. The F0 range is bounded by
a topline and a baseline defined as the cumulative mean Ϯ2 standard deviations (also calculated cumulatively). The semitone scale is used to ensure that the ϩ1 standard deviation interval is the same musical and perceptual interval as Ϫ1 standard deviations. The F0 range is divided into three equal parts:
high, mid and low. Intensity is treated in a similar manner: the median filtered intensity in decibel in
each frame is used to incrementally build up a cumulative mean Ϯ2 standard deviations.
Pseudo-Syllabification. The pseudo-syllabification algorithm is loosely based on Mermelstein’s
[1975] technique to find intensity minima in the speech signal by using convex hulls. Convex hulls are
continuously tracked over the median filtered intensity values. Note, however, that only voiced segments are included. The convex hulls are taken to correspond to pseudo-syllables that can be retrieved
and analysed as the need arises. In the present experiments, the last convex hull that has been seen over
the intensity curve is extracted each time a sufficiently long (300 ms) silence is detected.
Intonation Pattern Classification. Whenever a sufficient silence is found, the information from
the pseudo-syllabification is used to point out a number of frames that roughly corresponds to the last
syllable nucleus (i.e. minimally the vowel) before the silent sequence. The information from the pitch
extractor pertaining to the pseudo-syllable is then used to classify the intonation patterns. These intonation patterns are classified in terms of their position in the F0 range (currently as high, mid or low
tones) and in terms of their shapes (rises, falls and level tones).
Turn-Keeping and Turn-Yielding Decisions
Finally, the speech and silent pause durations in combination with the intonation pattern classification are used to make decisions about interaction control. Any silent pause in the giver channel
exceeding the pause threshold (i.e. 300 ms), which is preceded by a mid-level intonation pattern,
belongs to the ‘turn-keeping’ category. Silent pauses followed by low intonation patterns are categorized as ‘turn-yielding’. All other places belong to the ‘don’t know’ category.
The thresholds and parameter values were manually set in the initial implementation used in the
tests reported here, but they were set before the tests were performed and have not been subsequently
altered to improve results on the current speech material. In future versions, they should be optimized
based on corpus studies.
Evaluating the Predictive Power of the Automatically Extracted Prosodic Features
To evaluate the predictive power of the prosodic features extracted with /nailon/, the interaction
control decisions made by /nailon/ were compared with what actually happened in the dialogues, that
is the automatic classification into speaker change vs. speaker hold described above.

Results and Discussion

The Extent of the Problem with Silence-Based Segmentation
By combining the results of the perceptual judgments and the automatic speaker
change vs. speaker hold classification, we can get an estimate of the extent of the problem

Prosody in Interaction Control

Phonetica 2005;62:215–226

221

Table 1. The distribution of perceptual judgments by the three judges as to whether the IPUs were

finished or not tabulated against the automatic speaker change vs. speaker hold classification
1
(definitely
unfinished)
Speaker change
Speaker hold
Total

2
(probably
unfinished)

3
(could be
either finished
or unfinished)

4
(probably
finished)

5
(definitely
finished)

Total

169
806
975

66
91
157

40
25
65

200
112
312

705
240
945

1,180
1,274
2,454

Table 2. The distribution of the
majority votes for the perceptual
judgments

Unfinished
Speaker change
Speaker hold
Total

Finished

Disagree

Total

59
303
362

328
120
448

6
8
14

393
431
824

Before determining the majority votes across the three judges, the probably and definitely unfinished cases were collapsed into the category unfinished, and the probably and definitely finished votes were collapsed into the
category finished. The column labelled ‘disagree’ represents the cases
where no majority vote could be established.

with silence-based segmentation in spontaneous dialogues. Table 1 shows the distribution of perceptual judgments of the IPUs tabulated against the speaker change vs.
speaker hold classification.
Several observations can be made. First, to state the obvious, all the speaker hold
cases represent pause-bounded units where no speaker change occurred, and these
cases correspond to 52.3% of all silent pauses in the material. Some of these speaker
hold cases were judged as probably or definitely finished utterances, and may be seen
as potential places for speaker changes only that no speaker change occurred. These
cases are not part of the problem with silence-based segmentation. However, by collapsing the votes for probably and definitely unfinished as well as those for probably
and definitely finished and taking the majority vote across the three judges, we find that
the speaker hold cases judged to be unfinished correspond to 36.7% of all pauses
(table 2). These cases pose a real problem for silence-based segmentation since they are
the cases where a dialogue system using silence-based segmentation runs the risk of
interrupting its users. These are the cases we want to avoid.
There was also a substantial number of pause-bounded units where speaker
changes actually occurred (47.7% of all pauses). By collapsing the votes, this time for
probably and definitely finished, and then taking the majority vote across the three
judges, we find that the majority of the speaker change cases (83.5%) were indeed perceived as finished utterances (table 2). So, these are the places where we want the system to speak. Finally, there were also a few cases where speaker changes occurred

222

Phonetica 2005;62:215–226

Edlund/Heldner

Table 3. Number of IPUs and the proportion of speaker holds amongst these as a function of

increased thresholds in silence-based segmentation
Silence thresholds
Ͼ300 ms
IPUs, n
Holds, %

Ͼ500 ms

Ͼ1,000 ms

Ͼ1,500 ms

Ͼ2,000 ms

634
68

441
71

168
68

69
67

22
55

although the judges felt that they were probably or definitely unfinished (15%). Some
of these were probably deliberate interruptions, and therefore not places where we
want to detect an opportunity for the system to speak.
In addition to illustrating the problem with silence-based segmentation, tables 1
and 2 give an indication of the amount of noise in the data. In the majority votes by the
three human listeners, 27.8% of the ‘hold’ cases were perceived as finished and 15% of
the ‘change’ cases were perceived as unfinished (table 2). In our view, this points to the
minimum error we can expect from an automatic categorization, unless it has access to
more information than the human judges were given.
Would Increasing the Silence Threshold Solve the Problem
with Silence-Based Segmentation?
The minimum amount of silence required for IPU segmentation in this study was
300 ms. Humans often respond even faster than that, yet longer minimum pauses are
often used in spoken dialogue systems in the hope of ensuring correct turn-taking
behaviour. The question, then, is: does an increased silence threshold solve any problems involved in silence-based segmentation? Table 3 presents the proportion of IPUs
that resulted in a speaker hold in five subsets of the material, each consisting of the
IPUs with an ICI above a certain threshold.
In order to assess the amount of errors created by using silence as the sole basis for
turn-taking decisions, and to find out how successful the strategy of lengthening the
required silence is, we analysed the map task dialogues to see how ‘hold’ and ‘change’
depended on ICI. As noted before, most current systems use silence thresholds of 500 –
2,000 ms, and our system currently requires 300 ms of silence. First of all, it is worth
noting that less than 50% of the turns in the map task dialogues had an ICI of more than
500 ms, implying that continuously waiting for 500 ms or more before responding is
not natural. Considerably more, over 70% of the turns, are captured if the minimum ICI
is lowered to 300 ms. Even more interestingly, the percentage of ‘holds’ is virtually
unchanged whether one looks at all IPUs with a minimum ICI of 300, 500, 1,000 or
1,500 ms. At ICIs of 2,000 ms or more, the percentage of ‘holds’ falls somewhat, but
the material is too small to be reliable – we only have 19 instances to go on. The numbers suggest that the only thing a spoken dialogue system would achieve by using a
silence threshold of 2,000 ms is a sluggish behaviour.
Can Prosody Help in Solving the Problem with Silence-Based Segmentation?
Requiring longer silences obviously does nothing to solve the problems involved
in silence-based segmentation, but can prosody and intonation patterns help? Recall that

Prosody in Interaction Control

Phonetica 2005;62:215–226

223

Table 4. Automatic classifica-

tion of IPUs into ‘turn-keeping’,
‘turn-yielding’ and ‘don’t know’
tabulated against the classification of speaker changes and
speaker holds

Turn-keeping
Change
Hold
Total

Don’t know

Turn-yielding

Total

23
105
128

212
255
467

158
71
229

393
431
824

level intonation patterns in the middle of the speakers’ fundamental frequency range
have been observed to act as turn-keeping cues in several different languages.
Based on the prosodic features extracted with /nailon/, the automatic classifier
classified each IPU into one out of three categories: ‘turn-keeping’, ‘don’t know’ and
‘turn-yielding’. Table 4 presents the results tabulated against the speaker ‘change’/‘hold’
classification. The material contained 824 IPUs, 28% of which were classified as ‘turnyielding’, 16% as ‘turn-keeping’ and 56% ended up in the garbage category ‘don’t
know’.
Among the IPUs classified as suitable for turn-taking (i.e. ‘turn-yielding’), the
speaker changes were in the majority (69%). Correspondingly, the speaker holds were
in the majority in the IPUs classified as ‘turn-keeping’ (82%). From a different point of
view, the classifier identified at least 41% of the possible places for turn-taking (recall
that some of the speaker holds were perceived as finished by the human listeners – these
may well be suitable places although no speaker change actually occurred; table 1).
Furthermore, if only the ‘turn-yielding’ IPUs are seen as suitable places for turn-taking
(and ‘turn-keepings’ and ‘don’t knows’ are pooled), the classifier identified 84% of the
places where interruptions are impossible.
In a spoken dialogue system, the fairly large garbage category produced by the
parameter settings in this experiment can be pooled in two ways. If used conservatively,
‘don’t know’ and ‘turn-keeping’ would be pooled and the system only takes turn at
IPUs categorized as ‘turn-yielding’. Such a system would avoid 84% of the IPUs
unsuitable for system utterances, and detect roughly every second opportunity for a
system utterance. The approach is suitable for a system that barges into human conversations, perhaps in order to give notifications. Conversely, a more aggressive system
would pool ‘don’t know’ with ‘turn-yielding’ and only avoid taking turns when the categorization gave ‘turn-keeping’. This would avoid 24% of the unsuitable places whilst
detecting 94% of the suitable places. The latter strategy would be used in a system that
engages in a dialogue directly with a user.
It is possible to improve these results if the thresholds used in the classifier are
tuned with machine learning techniques, and with further subdivisions of the ‘don’t
know’ category, for example by adding categories such as backchannels, listings, feedback on various grounding levels [Allwood et al., 1993; Clark, 1996]. We intend to
explore these possibilities in future work.
Conclusions

In this paper, we have explored on-line prosodic analysis as a means to improve
the interaction control in spoken human-computer dialogue. The experiments have
shown that dialogue systems relying on silence-based segmentation run the risk of

224

Phonetica 2005;62:215–226

Edlund/Heldner

interrupting its users in as much as 35% of all silent pauses, at least if they encounter
speech of the kind investigated here.
Furthermore, we have shown that the number of incorrect turn-taking decisions
can be reduced substantially by combining standard silence-based endpoint detection
with an automatic classification of intonation patterns. In the process, it is also possible
to decrease the length of the required silence without any loss in performance. This can
be used to make a conversational computer more responsive by allowing it to reply
faster without simultaneously making it more obtrusive.
Level intonation patterns in the middle of the speakers’ fundamental frequency
range were found to act as turn-keeping cues, and may thus be used to avoid interrupting human interlocutors with high precision. Although there are several observations of
the function of these mid-level intonation patterns, to our knowledge, they have never
been used for avoiding interrupting users in spoken dialogue systems before.
Rising intonation before a silent pause can be associated with turn-yielding as well
as with turn-keeping [e.g. Local et al., 1986]. As 51% of the rising intonation patterns
co-occurred with actual speaker changes and 49% with speaker holds, we opted to have
/nailon/ classify these as ‘don’t know’. However, it is evident that the classification
would benefit from a more thorough analysis of rising intonation preceding silent
pauses. We suspect that a fourth conditional turn-yielding category is needed (in addition to ‘turn-yielding’, ‘turn-keeping’ and ‘don’t know’) to capture places where only
certain types of contributions, such as positive feedback, objections and clarification
requests, are permitted. We intend to explore these possibilities presently, although this
probably cannot be done without taking lexical aspects into account.
In general, it is clear that prosodic analysis is not enough to create conversational
computers with interaction control skills at near-human levels. Humans use higher levels
of understanding and a variety of information. We feel, however, that on-line access to
prosodic information provides a valuable source of information that should be combined
with other sources to guide the interaction control in conversational dialogue systems.
Acknowledgments
We are grateful to the two anonymous reviewers for their comments on an earlier version of this
paper. This work was done within the Project CHIL ‘Computers in the Human Interaction Loop’ (IP
506909). CHIL is an Integrated Project under the European Commission’s Sixth Framework Program.

References
Allwood, J.; Nivre, J.; Ahlsén, E.: On the semantics and pragmatics of linguistic feedback. J. Semant. 9: 1–26
(1993).
Anderson, A.H.; Bader, M.; Bard, E.G.; Boyle, E.; Doherty, G.; Garrod, S.; Isard, S.; Kowtko, J.; McAllister, J.; Miller, J.;
Sotillo, C.; Thompson, H.; Weinert, R.: The HCRH map task Corpus. Lang. Speech 34: 83–97 (1991).
Bell, L.; Boye, J.; Gustafson, J.: Real-time handling of fragmented utterances. Proc. North Am. Chapt. Assoc.
Comput. Linguist. 2001.
Caspers, J.: Local speech melody as a limiting factor in the turn-taking system in Dutch. J. Phonet. 31: 251–276
(2003).
Clark, H.H.: Using language (Cambridge University Press, Cambridge 1996).
Duncan, S. Jr.: Some signals and rules for taking speaking turns in conversations. J. Pers. soc. Psychol. 23: 283–292
(1972).
Edlund, J.; Heldner, M.; Gustafson, J.: Utterance segmentation and turn-taking in spoken dialogue systems; in Fisseni,
Schmitz, Schröder, Wagner, Sprachtechnologie, mobile Kommunikation und linguistische Ressourcen,
pp. 576–587 (Peter Lang, Frankfurt am Main 2005).

Prosody in Interaction Control

Phonetica 2005;62:215–226

225

Ferrer, L.; Shriberg, E.; Stolcke, A.: Is the speaker done yet? Faster and more accurate end-of-utterance detection using
prosody in human-computer dialog. Proc. Int. Conf. spoken Lang. Processing, Denver 2002, pp. 2061–2064.
Ford, C.E.; Thompson, S.A.: Interactional units in conversation: syntactic, intonational, and pragmatic resources for
the management of turns; in Ochs, Schegloff, Thompson, Interaction and grammar, pp. 134–184 (Cambridge
University Press, Cambridge 1996).
Heldner, M.; Edlund, J.; Carlson, R.: Interruption impossible; in Horne, Bruce, Proc. of Nordic Prosody IX (Peter
Lang, Frankfurt am Main in press).
Helgason, P.: Preaspiration in the Nordic languages: synchronic and diachronic aspects; PhD diss. Stockholm (2002).
Koiso, H.; Horiuchi, Y.; Tutiya, S.; Ichikawa, A.; Den, Y.: An analysis of turn-taking and backchannels based on
prosodic and syntactic features in Japanese map task dialogs. Lang. Speech 41: 295–321 (1998).
Local, J.K.; Kelly, J.: Projection and ‘silences’: notes on phonetic and conversational structure. Hum. Stud. 9:
185–204 (1986).
Local, J.K.; Kelly, J.; Wells, W.H.G.: Towards a phonology of conversation: turn-taking in Tyneside English.
J. Linguist. 22: 411–437 (1986).
Local, J.K.; Wells, W.H.G.; Sebba, M.: Phonology for conversation: phonetic aspects of turn delimitation in London
Jamaican. J. Pragm. 9: 309–330 (1985).
Mermelstein, P.: Automatic segmentation of speech into syllabic units. J. acoust. Soc. Am. 58: 880–883 (1975).
Noguchi, H.; Den, Y.: Prosody-based detection of the context of backchannel responses. Proc. 5th Int. Conf. spoken
Lang. Processing, Sydney 1998, pp. 487–490.
Ogden, R.: Turn transition, creak and glottal stop in Finnish talk-in-interaction. J. Int. Phonet. Assoc. 31: 139–152
(2001).
Selting, M.: On the interplay of syntax and prosody in the constitution of turn-constructional units and turns in conversation. Pragmatics 6: 357–388 (1996).
Skantze, G.; Edlund, J.: Robust interpretation in the Higgins spoken dialogue system. Proc. Robust, Norwich 2004.
Ward, N.; Tsukahara, W.: Prosodic features which cue back-channel responses in English and Japanese. J. Pragm.
32: 1177–1207 (2000).
Wells, B.; MacFarlane, S.: Prosody as an interactional resource: turn projection and overlap. Lang. Speech 41:
265–294 (1998).

226

Phonetica 2005;62:215–226

Edlund/Heldner

PROSODIC FEATURES OF VERY SHORT
UTTERANCES IN DIALOGUE
Jens Edlund
Mattias Heldner
Antoine Pelcé

1 Introduction
A large number of vocalizations in everyday conversation are traditionally not
regarded as part of the information exchange. Examples include confirmations
such as yeah and ok as well as traditionally non-lexical items, such as uh-huh, um,
and hmm. Vocalizations like these have been grouped in different constellations
and called different names, for example backchannels (i.e. back-channel activity,
Yngve, 1970), continuers (Schegloff, 1982), feedback and grunts, and attempts at
formalizing their function and meaning have been made (e.g. Ward, 2004). We
follow (Ward & Tsukahara, 2000), who argue that the term backchannel feedback
is relatively neutral, and henceforth use the term backchannel.
The working definitions of these overlapping concepts, however, are
imprecise, and different labeling schemes treat them quite differently, often with
dubious inter-annotator agreement numbers as a result (e.g. Hardy, et al., 2003).
The schemes are also often complex. A typical complication occurs with the
distinction between an answer of some type, say “I agree”, and a backchannel
signaling understanding, say “mhm”. The distinction between the two relies
heavily on third-party judgments of the speakers’ intentions, such as “was a
response required or optional here?” or “was this acknowledgment unsolicited or
prompted by a request?”. In some coding schemes, the distinction is based on
lexical context. SWBD-DAMSL, for example, states that “yeah” belongs to the
category AA (AGREEMENT-ACCEPT) if followed by additional verbal evidence,
such as “me too”, while it is B (BACKCHANNEL, or ACKNOWLEDGE) if uttered in
isolation (Jurafsky, Shriberg, & Biasca, 1997).
In spite of difficulties involved in defining these phenomena, there is little
controversy surrounding the utility of modeling them. They behave differently
from other utterances, and so are interesting both for models of human
conversation and for spoken dialogue systems aiming at more human-like
behavior. Commonly reported characteristics include that they can be spoken in
overlap without disrupting the original speaker (hence the term backchannel), they

are relatively brief, and inspection of their constituent tokens (e.g. Ward, 2004)
reveals that they contain almost exclusively voiced sounds.
Inspired by others, for example Shriberg et al. (1998), who state that “duration
was expected to be a good cue for discriminating STATEMENTS and QUESTIONS
from dialogue acts functioning to manage the dialogue (e.g. BACKCHANNELS)”,
we propose to use an intermediate, auxiliary unit – the very short utterance (VSU)
– which is defined operationally and is automatically extractable from recorded
dialogues. VSUs are intended to capture a large proportion of interactional
dialogue phenomena such as those commonly referred to as backchannels,
feedback, continuers, et cetera, at zero manual effort.
Note that we are not describing a backchannel classifier. We tentatively define
VSUs in terms of prosodic characteristics commonly associated with
backchannels, specifically using thresholds for duration and ratio of voiced versus
unvoiced frames. VSUs are then automatically extracted from a corpus of
spontaneous two-party dialogues. The corpus is also manually labeled for
backchannel-likeness, and the manual labels are compared with the VSUs.
Finally, we present visualizations of pitch contours for VSUs and NONVSUs.

2 Methods
2.1 Materials
We used speech material from the Swedish Map Task Corpus (Helgason, 2006),
designed as a Swedish counterpart to the HCRC Map Task Corpus. The map task
is a cooperative task involving two speakers, intended to elicit natural
spontaneous dialogues. Each of two speakers has one map which the other
speaker cannot see. One of the speakers, the instruction giver, has a route marked
on his or her map. The other speaker, the instruction follower, has no such route.
The two maps are not identical and the subjects are explicitly told that the maps
differ, but not how. The task is to collaborate in order to reproduce the giver’s
route on the follower’s map. Eight speakers, five females and three males, are
represented in the corpus. The speakers formed four pairs, three female-male pairs
and one female-female pair. Each speaker acted as instruction giver and follower
at least once, and no speaker occurred in more than one pair. The corpus includes
ten such dialogues, the total duration of which is approximately 2 hours and 18
minutes. The dialogues were recorded in an anechoic room, using close-talking
microphones, with the subjects facing away from each other, and with acceptable
acoustic separation of the speaker channels.
2.2 Procedures
The following procedures were involved in the study; each step is described in
detail in the following subsections:

•
•

•
•
•

Automatic segmentation of verbal production into talkspurts; talkspurts are
subsequently treated as atomic units.
Manual labeling of the talkspurts, using a simple annotation scheme, as
either backchannel-like (LOW propositional content) or not backchannellike (NONLOW propositional content).
Automatic classification of the talkspurts into VSUs and NONVSUs, using
duration and ratio of voiced frames.
Quantitative and visual comparison of manual and automatic labels.
Extraction and visualization of F0 patterns – both in VSUs and in
interlocutor speech immediately preceding each VSU.

2.2.1 Segmentation into talkspurts
Although the concept of utterance makes for a useful unit, segmentation into
utterances typically relies on higher-level linguistic knowledge, which we attempt
to avoid here. Instead, we propose as our candidate unit a more convenient and
less ambiguously defined segment, namely the talkspurt defined by Norwine &
Murphy: “A talkspurt is speech by one party, including her pauses, which is
preceded and followed, with or without intervening pauses, by speech of the other
party perceptible to the one producing the talkspurt” (Norwine & Murphy, 1938).
We note that this definition differs from that used by Brady (1968), in which a
talkspurt is a sequence of speech activity flanked by silences in one speaker’s
channel. Brady’s definition has been used by ourselves in previous work (e.g.
Edlund & Heldner, 2005; Laskowski, Edlund, & Heldner, 2008), but Norwine &
Murphy’s concept is better suited for our current purposes, and we adopt their
definition in what follows. Note that a talkspurt, as used here, may contain pauses
and overlapping speech; a talkspurt from one participant is said to end only when
solitary speech from the other participant is encountered. Any initial or final
silence is removed from the talkspurt proper, as illustrated in Figure 1.

Figure 1. Schematic illustration of a talkspurt as used in the current work.

The VADER voice activity detector from the CMU Sphinx Project
(http://cmusphinx.sourceforge.net/) was used to perform speech activity detection
individually for each speaker. As a result, each instant for each speaker was
labeled as either SPEECH or SILENCE. The combined information from both
speakers is sufficient to extract all talkspurts.

2.2.2 Manual annotation of talkspurt backchannel-likeness
In order to compare the proposed VSU unit with existing conceptualizations of
backchannels, we manually annotated all talkspurts in the Swedish Map Task
Corpus. The annotation scheme used is based primarily on the amount of
propositional content, as backchannels are generally considered to be related to
interaction control and hence to be relatively devoid of propositional content.
Additionally, the scheme: (a) is based on non-prosodic features; (b) is independent
of linguistic theory; and (c) yields good cross-annotator agreement among
untrained annotators It consists of three mutually exclusive labels: HIGH (content),
LOW (content), and MISC(ellaneous). LOW designates the backchannel-like
talkspurts that we were out to capture. We deliberately opted not to use a more
comprehensive scheme, such as that proposed by Ward & Tsukahara (2000), since
the latter requires annotators to make judgments based on the context of the
talkspurt. Another reason for adopting a simpler scheme was the need for
annotation to be fast and accurate on the first attempt.
Our scheme asked annotators to apply ordered questions which, if answered in
the positive, lead to a classification of HIGH or MISC, respectively. If both were
answered in the negative, the talkspurt was categorized as LOW. The instructions
were phrased as follows:
1. Does the talkspurt contain high-content elements: objects (e.g. nouns
(excluding pronouns) or proper names), attributes (e.g. adjectives) or
actions (e.g. verbs, not including modal verbs and codas)? If so, label as
HIGH.
2. Does the talkspurt seem to be the result of a technical error when
extracting the talkspurts; or does it consist of a disfluency, breathing,
coughing, laughter, etc.? If so, label as MISC.
3. Otherwise label as LOW.
The annotation scheme was applied to all talkspurts by two annotators
working independently. Once annotation was begun, the annotators did not
discuss the scheme until all talkspurts were annotated.
2.2.3 VSUs
The objective of automatically defining VSUs draws on the observation that
backchannel-like phenomena are limited in duration and tend to contain mainly
voiced speech sounds. Thus, we extracted two parameters from each talkspurt: its
DURATION, from start to finish (see Figure 1); and its voicing ratio
(VOICINGRATIO) – the proportion of frames in the talkspurt marked as voiced by
YIN, an automatic pitch extractor (de Cheveigné & Kawahara, 2002). A talkspurt
was classified as a VSU if both of the following were true:

1. the talkspurt’s DURATION was shorter than a given threshold, and
2. the talkspurt’s VOICINGRATIO exceeded a given threshold.
Finding optimal thresholds for Duration and VoicingRatio is a task for future
work. For the present study, we tentatively set them by testing each combination
of observed VOICINGRATIO and DURATION, and selecting the threshold pair
resulting in the smallest sum of false positives and false negatives when compared
to the manual backchannel-likeness annotation. Note, however, that the manual
LOW annotation is a crude approximation of the set of backchannel-like activity
that we aim to capture.
2.2.4 Extracting F0 patterns
For each talkspurt, F0 estimates of the last 500ms of speech were computed using
YIN (de Cheveigné & Kawahara, 2002). These were transformed from Hertz to
semitones to make the pitch excursions of men and women more comparable. The
data was subsequently smoothed using a median filter over 9 10ms frames to
eliminate outlier errors. The resulting contours of smoothed F0 estimates were
shifted vertically such that the median of the first three voiced frames in each
contour fell on the mid-point of the y-axis; they were also right-aligned
horizontally such that the end of the talkspurt lay on the right edge of the resulting
diagram. By plotting the contours with partially transparent dots, the
visualizations give an indication of the distribution of different patterns, with
darker bands for concentrations of patterns and vice versa. This visualization,
which we refer to as bitmap clustering, has also been used in (Heldner, Edlund,
Laskowski, & Pelcé, forthcoming).
In addition, for each talkspurt, we extracted the F0 estimates of the last
1000ms in the interlocutor’s channel, immediately preceding the talkspurt in
question. We note that in some cases, this interval would consist mainly, or even
entirely, of silence. These were again semitone transformed and median filtered,
and then Z-normalized for each speaker along the vertical octave axis; the mean
and standard deviation was computed using all voiced speech available from the
interlocutor. Along the horizontal time axis, no shifting was done as the segments
were all of the same length. These contours were also visualized using bitmap
clustering.

3 Results and discussion
Segmentation yielded a total of 3054 talkspurts. In this section, we report on the
per-talkspurt distribution of manually annotated propositional content, LOW and
NONLOW, as well as on that of the automatically generated VSU/non-VSU labels;
the bivariate distribution suggests that, at the talkspurt level, degree of
propositional content is strongly correlated with our operations VSU definition.

We show how the parameters used to define VSUS, DURATION and
VOICINGRATIO, are distributed across talkspurt types, and present the distributions
of talkspurt-terminating pitch contours, both inside each talkspurt type and inside
the interlocutor’s most recent talkspurt. Together, these results show that VSUs
represent a consistent group of talkspurts with characteristics not present in
NONVSUs.
3.1 Manual annotation
Both annotators labeled all 3054 utterances in one sweep, in less than 4 hours. The
agreement between the two annotators was high, with a kappa of 0.887 for all
three categories LOW, HIGH and MISC. Agreement increases to 0.895 when HIGH
and MISC are collapsed, yielding the binary distinction (LOW/NONLOW) of
greatest interest here. Table 1 shows the inter-annotator agreement matrix.
Table 1. Manual annotation confusion matrix.

Rater 2

LOW
HIGH
MISC
Total

LOW
903
10
21
934

Rater 1
HIGH MISC
93
15
1656
12
44
300
1793
327

Total
1011
1678
365
3054

Further analysis showed that the disagreement in the LOW category – the
category of interest here – is largely due to the first annotator using a wider
concept for the category. The second annotator’s LOW category is a subset of that
of the first, and there are very few instances (31) where the second annotator has
used LOW and the first has not.
3.2 DURATION and VOICINGRATIO in relation to manual annotation
The distributions of DURATION for talkspurts labeled as LOW, HIGH and MISC by
one annotator (the first annotator is used consistently in this section; results for the
second annotator are similar) are shown in Figure 2. It can be seen that the mode
for LOW talkspurt durations lies in the 300-400ms range, and that there are very
few LOW talkspurts whose duration falls below 200ms. For HIGH talkspurt
durations, the mode is clearly above 1000ms; there are very few HIGH talkspurt
durations below 400ms. Furthermore, talkspurts can be quite long, well above 10
seconds. The mode for MISC is similar to that of LOW, around 200-300ms,
suggesting that LOW and MISC may not be discriminated using DURATION alone.

Figure 2. Distributions of DURATION of talkspurts annotated as LOW, HIGH and MISC
(from left to right) by the one annotator. The x-axis is in ms on a logarithmic scale.

Figure 3 shows the distributions of VOICINGRATIO for each of LOW, HIGH and
MISC talkspurts. As can be seen, LOW talkspurt VOICINGRATIO is severely skewed
towards unity; a large number of these talkspurts consists of at least 80% voiced
frames, and almost no LOW talkspurt is entirely unvoiced. HIGH talkspurt
VOICINGRATIO shows a clear mode at 50-60% voiced frames, with very few fully
voiced or fully unvoiced talkspurts. It also appears that MISC has one weak mode
similar to that found in HIGH, and one consisting of a very high spike in the 0-5%
VOICINGRATIO bin, possibly corresponding to breathing, throat clearing, etc.

Figure 3. Distributions of VOICINGRATIO of talkspurts annotated as LOW, HIGH and
MISC, respectively, by one annotator.

3.3 VSU classification in relation to manual annotation
The distributions in Figures 2 and 3 suggest that discrimination of LOW talkspurts
may indeed be possible using VOICINGRATIO and DURATION. Figure 4 shows
ROC (Receiver Operating Characteristic) curves describing how well DURATION
and VOICING, respectively, discriminate among talkspurt types; manual annotation
of LOW by one annotator was used as the reference. Both curves are statistically
significant deviations from no discrimination (the diagonal line), with an
asymptotic significance of <0.001. However, VOICINGRATIO alone is a weaker
predictor than DURATION alone (the areas above the line of no discrimination are
0.728 and 0.884, respectively). This is also the case with respect to the asymptotic
95% confidence interval, where the upper bound of the interval for VOICINGRATIO
is below the lower bound of the interval for DURATION.

Figure 4. ROC curves for LOW/NONLOW classification using a continuum of
VOICINGRATIO and DURATION thresholds.
We note that the ROC curve for DURATION alone shows a significant false
positive rate of approximately 0.1 before it achieves a true positive rate of 0.5;
thereafter, the curve asymptotically approaches a true positive rate of unity.
Inspection of the thresholds resulting in this curve reveals that this is because the
proportion of talkspurts judged as LOW increases with decreasing DURATION, but
that in the end there are a certain number of NONLOW with very short DURATION
– mainly breathing, laughter, coughs, and disfluencies – and these cannot be
separated from LOW talkspurts by means of duration.
In contrast, the VOICINGRATIO ROC curve incurs much fewer false positives
before reaching a true positive rate of 0.5, but does not attain a true positive rate
near unity until the false positive rate exceeds 0.95. Closer inspection suggests
that this is because almost all talkspurts with a very high VOICINGRATIO are very
short, but breathing and coughing is excluded by the VOICINGRATIO threshold, as
they consist of mostly unvoiced frames.

We note that at a true positive rate (recall rate) of 90%, we obtain as few as
22% false positives on the DURATION curve. Conversely, at a rate of zero false
positives, we still capture 10% of the true positives on the VOICINGRATIO curve.
Finally, we note that discrimination using VOICINGRATIO and DURATION appears
to be complementary in part. Most importantly, placing the constraint that a
talkspurt should contain a non-zero amount of voiced speech may have a positive
effect on limiting false positives. The confusion matrix of one annotator’s
LOW/NONLOW decisions versus automatic VSU identification using a
combination of VOICINGRATIO and DURATION is shown in Table 2.
Table 2. Confusion matrix of manual annotation (annotator 1) versus VSUs.

Rater 1

LOW
NONLOW
Total

VSU
859
152
1011

Automatic
NONVSU
205
1838
2043

Total
1064
1990
3054

3.4 Bitmap clusters of VSUs
In order to visualize the discrepancies between automatic VSU identification and
manual LOW annotation, we produce bitmap clusters for the four cells in Table 2.
These are shown in Figure 5.

Figure 5. Bitmap clusters (see 2.2.4 above) of the final 500ms of talkspurts labeled LOWVSU (upper left panel), LOW-NONVSU (upper right panel), NONLOW-VSU (lower left
panel), and NONLOW-NONVSU (lower right panel).

The bitmap clusters reveal a clear tendency for rising patterns at the end of VSUs
judged as LOW, whereas NONLOW NONVSUs fail to show such a rise and have
most of the activity in the lower half of the panel, possibly indicating a preference
for falling contours. The upper right and lower left panes, where human
judgments and automatic classification are in conflict, contain much less data (cf.
Table 2) and show no clear patterns. They account for roughly 12% of talkspurts,
and we leave their detailed analysis for future work, when more data becomes
available.
The observation of rising patterns in the LOW VSUs is in line with our recent
observation (Heldner, et al., forthcoming) of a tendency for speakers in the
follower role of the Map Task to end utterances in a rising tone, as the follower
role contains a high proportion of backchannels.
3.5 Bitmap clusters of VSUs preceding interlocutor speech
Figure 6 shows bitmap clusters of the 1000ms of the interlocutor’s speech
preceding each of the 3054 talkspurts.

Figure 6. Plots of pitch contours as described in 2.2.4 above, of the 1000ms immediately
preceding each talkspurt in the other speaker’s channel. The panels show the speech
preceding talkspurts labeled as LOW-VSU (upper left panel), as LOW-NONVSU (upper
right panel), as NONLOW-VSU (lower left panel), and as NONLOW-NONVSU (lower right
panel). The upper left panel contains a line showing the 26th percentile of the speakers’
pitch from 860 to 700ms prior to the talkspurt.

In Figure 6, we observe that interlocutor speech preceding talkspurts judged as
LOW and identified as VSUs is compatible with the finding that 700-860ms prior
to the start of a backchannel, interlocutors exhibit increased activity below the 26th
percentile of their pitch range (Ward & Tsukahara, 2000) and little activity in
above the center line, as opposed to the lower right panel. The top left panel
contains a line on the 26th percentile along the 700-860ms interval for illustration.
This qualitative agreement is an indication that VSUs capture phenomena which
Ward and others refer to as backchannels.
In the lower right panel, we observe a relatively symmetric, bimodal
distribution. There is a somewhat lighter band along the horizontal mid-line,
indicating an interlocutor dispreference for approximately mean pitch before the
onset of NONLOW NONVSUs, which is in line with previous findings (cf. Edlund
& Heldner, 2005, and references mentioned therein), namely that mid-level pitch
is a common floor-keeper.

4 Conclusions
The very short utterance (or VSU) proposed in this work is automatically
identified using easily accessible acoustic information such as the duration of and
proportion of voicing in talkspurts. Our analyses suggest that it captures a large
proportion of the dialogue phenomena commonly referred to as backchannels.
This is evidenced by: the strong overlap between VSUs and manually labeled
low-content talkspurts; by the similarity in final pitch contour between VSUs and
the backchannel-intensive follower role in the Map Task; and by the fact that
interlocutor pitch 700-860ms prior to a VSU is consistent with Ward &
Tsukahara’s (2000) findings regarding interlocutor pitch preceding backchannels.
Prosodically, VSUs are characterized by a final rising pitch, in addition to the
duration and voicing characteristics that define them.
The finding that the easily distinguishable VSUs share many characteristics
with backchannels has important implications for future research. From a basic
research perspective, it means that we can, at zero manual effort, make more
detailed descriptions of conversational behavior. We may for example separate
speaker changes involving VSUs from others. From a speech technology
perspective, VSUs may be used to afford spoken dialogue systems less rigid
interaction control by allowing VSUs to be interpreted differently from other user
utterances. For example, if users are allowed to barge in – to speak in overlap with
the system – the system may go on uninterrupted in case the user utterance is a
VSU, and relinquish the floor if it is not. We believe this to be an important step
towards more human-like behavior in spoken dialogue systems.

Acknowledgements
We thank Kornel Laskowski for insightful discussions and criticism, and Pétur
Helgason for access to the Swedish Map Task Corpus. Funding was provided by
the Swedish Research Council (VR) project 2006-2172 Vad gör tal till samtal?
References
Brady, P. T. (1968). A statistical analysis of on-off patterns in 16 conversations.
The Bell System Technical Journal, 47, 73-91.
de Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency
estimator for speech and music. Journal of the Acoustical Society of America,
111(4), 1917-1930.
Edlund, J., & Heldner, M. (2005). Exploring prosody in interaction control.
Phonetica, 62(2-4), 215-226.
Hardy, H., Baker, K., Bonneau-Maynard, H., Devillers, L., Rosset, S., &
Strzalkowski, T. (2003). Semantic and dialogic annotation for automated
multilingual customer service. In EUROSPEECH-2003 (pp. 201–204).
Geneva, Switzerland.
Heldner, M., Edlund, J., Laskowski, K., & Pelcé, A. (forthcoming). Prosodic
features in the vicinity of silences and overlaps. In Nordic Prosody X (this
volume). Helsinki, Finland.
Helgason, P. (2006). SMTC - A Swedish Map Task Corpus. In Working Papers
52: Proceedings from Fonetik 2006 (pp. 57-60). Lund, Sweden.
Jurafsky, D., Shriberg, E., & Biasca, D. (1997). Switchboard SWBD-DAMSL
Annotation Coders Manual, Draft 13 Retrieved 2008-12-06, from
http://www.stanford.edu/~jurafsky/ws97/manual.august1.html
Laskowski, K., Edlund, J., & Heldner, M. (2008). An instantaneous vector
representation of delta pitch for speaker-change prediction in conversational
dialogue systems. In Proceedings ICASSP 2008 (pp. 5041-5044). Las Vegas,
NV, USA.
Norwine, A. C., & Murphy, O. J. (1938). Characteristic time intervals in
telephonic conversation. The Bell System Technical Journal, 17, 281-291.
Schegloff, E. (1982). Discourse as an interactional achievement: Some uses of 'uh
huh' and other things that come between sentences. In D. Tannen (Ed.),
Analyzing Discourse: Text and Talk (pp. 71-93). Washington, D.C., USA:
Georgetown University Press.
Shriberg, E., Bates, R., Stolcke, A., Taylor, P., Jurafsky, D., Ries, K., et al.
(1998). Can prosody aid in the automatic classification of dialog acts in
conversational speech. Language and Speech, 41(3-4), 439-487.
Ward, N. (2004). Pragmatic functions of prosodic features in non-lexical
utterances. In Proceedings of Speech Prosody 2004 (pp. 325-328). Nara,
Japan.
Ward, N., & Tsukahara, W. (2000). Prosodic features which cue back-channel
responses in English and Japanese. Journal of Pragmatics, 32, 1177-1207.
Yngve, V. H. (1970). On getting a word in edgewise. In Papers from the Sixth
Regional Meeting Chicago Linguistic Society (pp. 567-578). Chicago, IL,
USA: Chicago Linguistic Society.

PROSODIC FEATURES OF VERY SHORT
UTTERANCES IN DIALOGUE
Jens Edlund
Mattias Heldner
Antoine Pelcé

1 Introduction
A large number of vocalizations in everyday conversation are traditionally not
regarded as part of the information exchange. Examples include confirmations
such as yeah and ok as well as traditionally non-lexical items, such as uh-huh, um,
and hmm. Vocalizations like these have been grouped in different constellations
and called different names, for example backchannels (i.e. back-channel activity,
Yngve, 1970), continuers (Schegloff, 1982), feedback and grunts, and attempts at
formalizing their function and meaning have been made (e.g. Ward, 2004). We
follow (Ward & Tsukahara, 2000), who argue that the term backchannel feedback
is relatively neutral, and henceforth use the term backchannel.
The working definitions of these overlapping concepts, however, are
imprecise, and different labeling schemes treat them quite differently, often with
dubious inter-annotator agreement numbers as a result (e.g. Hardy, et al., 2003).
The schemes are also often complex. A typical complication occurs with the
distinction between an answer of some type, say “I agree”, and a backchannel
signaling understanding, say “mhm”. The distinction between the two relies
heavily on third-party judgments of the speakers’ intentions, such as “was a
response required or optional here?” or “was this acknowledgment unsolicited or
prompted by a request?”. In some coding schemes, the distinction is based on
lexical context. SWBD-DAMSL, for example, states that “yeah” belongs to the
category AA (AGREEMENT-ACCEPT) if followed by additional verbal evidence,
such as “me too”, while it is B (BACKCHANNEL, or ACKNOWLEDGE) if uttered in
isolation (Jurafsky, Shriberg, & Biasca, 1997).
In spite of difficulties involved in defining these phenomena, there is little
controversy surrounding the utility of modeling them. They behave differently
from other utterances, and so are interesting both for models of human
conversation and for spoken dialogue systems aiming at more human-like
behavior. Commonly reported characteristics include that they can be spoken in
overlap without disrupting the original speaker (hence the term backchannel), they

are relatively brief, and inspection of their constituent tokens (e.g. Ward, 2004)
reveals that they contain almost exclusively voiced sounds.
Inspired by others, for example Shriberg et al. (1998), who state that “duration
was expected to be a good cue for discriminating STATEMENTS and QUESTIONS
from dialogue acts functioning to manage the dialogue (e.g. BACKCHANNELS)”,
we propose to use an intermediate, auxiliary unit – the very short utterance (VSU)
– which is defined operationally and is automatically extractable from recorded
dialogues. VSUs are intended to capture a large proportion of interactional
dialogue phenomena such as those commonly referred to as backchannels,
feedback, continuers, et cetera, at zero manual effort.
Note that we are not describing a backchannel classifier. We tentatively define
VSUs in terms of prosodic characteristics commonly associated with
backchannels, specifically using thresholds for duration and ratio of voiced versus
unvoiced frames. VSUs are then automatically extracted from a corpus of
spontaneous two-party dialogues. The corpus is also manually labeled for
backchannel-likeness, and the manual labels are compared with the VSUs.
Finally, we present visualizations of pitch contours for VSUs and NONVSUs.

2 Methods
2.1 Materials
We used speech material from the Swedish Map Task Corpus (Helgason, 2006),
designed as a Swedish counterpart to the HCRC Map Task Corpus. The map task
is a cooperative task involving two speakers, intended to elicit natural
spontaneous dialogues. Each of two speakers has one map which the other
speaker cannot see. One of the speakers, the instruction giver, has a route marked
on his or her map. The other speaker, the instruction follower, has no such route.
The two maps are not identical and the subjects are explicitly told that the maps
differ, but not how. The task is to collaborate in order to reproduce the giver’s
route on the follower’s map. Eight speakers, five females and three males, are
represented in the corpus. The speakers formed four pairs, three female-male pairs
and one female-female pair. Each speaker acted as instruction giver and follower
at least once, and no speaker occurred in more than one pair. The corpus includes
ten such dialogues, the total duration of which is approximately 2 hours and 18
minutes. The dialogues were recorded in an anechoic room, using close-talking
microphones, with the subjects facing away from each other, and with acceptable
acoustic separation of the speaker channels.
2.2 Procedures
The following procedures were involved in the study; each step is described in
detail in the following subsections:

•
•

•
•
•

Automatic segmentation of verbal production into talkspurts; talkspurts are
subsequently treated as atomic units.
Manual labeling of the talkspurts, using a simple annotation scheme, as
either backchannel-like (LOW propositional content) or not backchannellike (NONLOW propositional content).
Automatic classification of the talkspurts into VSUs and NONVSUs, using
duration and ratio of voiced frames.
Quantitative and visual comparison of manual and automatic labels.
Extraction and visualization of F0 patterns – both in VSUs and in
interlocutor speech immediately preceding each VSU.

2.2.1 Segmentation into talkspurts
Although the concept of utterance makes for a useful unit, segmentation into
utterances typically relies on higher-level linguistic knowledge, which we attempt
to avoid here. Instead, we propose as our candidate unit a more convenient and
less ambiguously defined segment, namely the talkspurt defined by Norwine &
Murphy: “A talkspurt is speech by one party, including her pauses, which is
preceded and followed, with or without intervening pauses, by speech of the other
party perceptible to the one producing the talkspurt” (Norwine & Murphy, 1938).
We note that this definition differs from that used by Brady (1968), in which a
talkspurt is a sequence of speech activity flanked by silences in one speaker’s
channel. Brady’s definition has been used by ourselves in previous work (e.g.
Edlund & Heldner, 2005; Laskowski, Edlund, & Heldner, 2008), but Norwine &
Murphy’s concept is better suited for our current purposes, and we adopt their
definition in what follows. Note that a talkspurt, as used here, may contain pauses
and overlapping speech; a talkspurt from one participant is said to end only when
solitary speech from the other participant is encountered. Any initial or final
silence is removed from the talkspurt proper, as illustrated in Figure 1.

Figure 1. Schematic illustration of a talkspurt as used in the current work.

The VADER voice activity detector from the CMU Sphinx Project
(http://cmusphinx.sourceforge.net/) was used to perform speech activity detection
individually for each speaker. As a result, each instant for each speaker was
labeled as either SPEECH or SILENCE. The combined information from both
speakers is sufficient to extract all talkspurts.

2.2.2 Manual annotation of talkspurt backchannel-likeness
In order to compare the proposed VSU unit with existing conceptualizations of
backchannels, we manually annotated all talkspurts in the Swedish Map Task
Corpus. The annotation scheme used is based primarily on the amount of
propositional content, as backchannels are generally considered to be related to
interaction control and hence to be relatively devoid of propositional content.
Additionally, the scheme: (a) is based on non-prosodic features; (b) is independent
of linguistic theory; and (c) yields good cross-annotator agreement among
untrained annotators It consists of three mutually exclusive labels: HIGH (content),
LOW (content), and MISC(ellaneous). LOW designates the backchannel-like
talkspurts that we were out to capture. We deliberately opted not to use a more
comprehensive scheme, such as that proposed by Ward & Tsukahara (2000), since
the latter requires annotators to make judgments based on the context of the
talkspurt. Another reason for adopting a simpler scheme was the need for
annotation to be fast and accurate on the first attempt.
Our scheme asked annotators to apply ordered questions which, if answered in
the positive, lead to a classification of HIGH or MISC, respectively. If both were
answered in the negative, the talkspurt was categorized as LOW. The instructions
were phrased as follows:
1. Does the talkspurt contain high-content elements: objects (e.g. nouns
(excluding pronouns) or proper names), attributes (e.g. adjectives) or
actions (e.g. verbs, not including modal verbs and codas)? If so, label as
HIGH.
2. Does the talkspurt seem to be the result of a technical error when
extracting the talkspurts; or does it consist of a disfluency, breathing,
coughing, laughter, etc.? If so, label as MISC.
3. Otherwise label as LOW.
The annotation scheme was applied to all talkspurts by two annotators
working independently. Once annotation was begun, the annotators did not
discuss the scheme until all talkspurts were annotated.
2.2.3 VSUs
The objective of automatically defining VSUs draws on the observation that
backchannel-like phenomena are limited in duration and tend to contain mainly
voiced speech sounds. Thus, we extracted two parameters from each talkspurt: its
DURATION, from start to finish (see Figure 1); and its voicing ratio
(VOICINGRATIO) – the proportion of frames in the talkspurt marked as voiced by
YIN, an automatic pitch extractor (de Cheveigné & Kawahara, 2002). A talkspurt
was classified as a VSU if both of the following were true:

1. the talkspurt’s DURATION was shorter than a given threshold, and
2. the talkspurt’s VOICINGRATIO exceeded a given threshold.
Finding optimal thresholds for Duration and VoicingRatio is a task for future
work. For the present study, we tentatively set them by testing each combination
of observed VOICINGRATIO and DURATION, and selecting the threshold pair
resulting in the smallest sum of false positives and false negatives when compared
to the manual backchannel-likeness annotation. Note, however, that the manual
LOW annotation is a crude approximation of the set of backchannel-like activity
that we aim to capture.
2.2.4 Extracting F0 patterns
For each talkspurt, F0 estimates of the last 500ms of speech were computed using
YIN (de Cheveigné & Kawahara, 2002). These were transformed from Hertz to
semitones to make the pitch excursions of men and women more comparable. The
data was subsequently smoothed using a median filter over 9 10ms frames to
eliminate outlier errors. The resulting contours of smoothed F0 estimates were
shifted vertically such that the median of the first three voiced frames in each
contour fell on the mid-point of the y-axis; they were also right-aligned
horizontally such that the end of the talkspurt lay on the right edge of the resulting
diagram. By plotting the contours with partially transparent dots, the
visualizations give an indication of the distribution of different patterns, with
darker bands for concentrations of patterns and vice versa. This visualization,
which we refer to as bitmap clustering, has also been used in (Heldner, Edlund,
Laskowski, & Pelcé, forthcoming).
In addition, for each talkspurt, we extracted the F0 estimates of the last
1000ms in the interlocutor’s channel, immediately preceding the talkspurt in
question. We note that in some cases, this interval would consist mainly, or even
entirely, of silence. These were again semitone transformed and median filtered,
and then Z-normalized for each speaker along the vertical octave axis; the mean
and standard deviation was computed using all voiced speech available from the
interlocutor. Along the horizontal time axis, no shifting was done as the segments
were all of the same length. These contours were also visualized using bitmap
clustering.

3 Results and discussion
Segmentation yielded a total of 3054 talkspurts. In this section, we report on the
per-talkspurt distribution of manually annotated propositional content, LOW and
NONLOW, as well as on that of the automatically generated VSU/non-VSU labels;
the bivariate distribution suggests that, at the talkspurt level, degree of
propositional content is strongly correlated with our operations VSU definition.

We show how the parameters used to define VSUS, DURATION and
VOICINGRATIO, are distributed across talkspurt types, and present the distributions
of talkspurt-terminating pitch contours, both inside each talkspurt type and inside
the interlocutor’s most recent talkspurt. Together, these results show that VSUs
represent a consistent group of talkspurts with characteristics not present in
NONVSUs.
3.1 Manual annotation
Both annotators labeled all 3054 utterances in one sweep, in less than 4 hours. The
agreement between the two annotators was high, with a kappa of 0.887 for all
three categories LOW, HIGH and MISC. Agreement increases to 0.895 when HIGH
and MISC are collapsed, yielding the binary distinction (LOW/NONLOW) of
greatest interest here. Table 1 shows the inter-annotator agreement matrix.
Table 1. Manual annotation confusion matrix.

Rater 2

LOW
HIGH
MISC
Total

LOW
903
10
21
934

Rater 1
HIGH MISC
93
15
1656
12
44
300
1793
327

Total
1011
1678
365
3054

Further analysis showed that the disagreement in the LOW category – the
category of interest here – is largely due to the first annotator using a wider
concept for the category. The second annotator’s LOW category is a subset of that
of the first, and there are very few instances (31) where the second annotator has
used LOW and the first has not.
3.2 DURATION and VOICINGRATIO in relation to manual annotation
The distributions of DURATION for talkspurts labeled as LOW, HIGH and MISC by
one annotator (the first annotator is used consistently in this section; results for the
second annotator are similar) are shown in Figure 2. It can be seen that the mode
for LOW talkspurt durations lies in the 300-400ms range, and that there are very
few LOW talkspurts whose duration falls below 200ms. For HIGH talkspurt
durations, the mode is clearly above 1000ms; there are very few HIGH talkspurt
durations below 400ms. Furthermore, talkspurts can be quite long, well above 10
seconds. The mode for MISC is similar to that of LOW, around 200-300ms,
suggesting that LOW and MISC may not be discriminated using DURATION alone.

Figure 2. Distributions of DURATION of talkspurts annotated as LOW, HIGH and MISC
(from left to right) by the one annotator. The x-axis is in ms on a logarithmic scale.

Figure 3 shows the distributions of VOICINGRATIO for each of LOW, HIGH and
MISC talkspurts. As can be seen, LOW talkspurt VOICINGRATIO is severely skewed
towards unity; a large number of these talkspurts consists of at least 80% voiced
frames, and almost no LOW talkspurt is entirely unvoiced. HIGH talkspurt
VOICINGRATIO shows a clear mode at 50-60% voiced frames, with very few fully
voiced or fully unvoiced talkspurts. It also appears that MISC has one weak mode
similar to that found in HIGH, and one consisting of a very high spike in the 0-5%
VOICINGRATIO bin, possibly corresponding to breathing, throat clearing, etc.

Figure 3. Distributions of VOICINGRATIO of talkspurts annotated as LOW, HIGH and
MISC, respectively, by one annotator.

3.3 VSU classification in relation to manual annotation
The distributions in Figures 2 and 3 suggest that discrimination of LOW talkspurts
may indeed be possible using VOICINGRATIO and DURATION. Figure 4 shows
ROC (Receiver Operating Characteristic) curves describing how well DURATION
and VOICING, respectively, discriminate among talkspurt types; manual annotation
of LOW by one annotator was used as the reference. Both curves are statistically
significant deviations from no discrimination (the diagonal line), with an
asymptotic significance of <0.001. However, VOICINGRATIO alone is a weaker
predictor than DURATION alone (the areas above the line of no discrimination are
0.728 and 0.884, respectively). This is also the case with respect to the asymptotic
95% confidence interval, where the upper bound of the interval for VOICINGRATIO
is below the lower bound of the interval for DURATION.

Figure 4. ROC curves for LOW/NONLOW classification using a continuum of
VOICINGRATIO and DURATION thresholds.
We note that the ROC curve for DURATION alone shows a significant false
positive rate of approximately 0.1 before it achieves a true positive rate of 0.5;
thereafter, the curve asymptotically approaches a true positive rate of unity.
Inspection of the thresholds resulting in this curve reveals that this is because the
proportion of talkspurts judged as LOW increases with decreasing DURATION, but
that in the end there are a certain number of NONLOW with very short DURATION
– mainly breathing, laughter, coughs, and disfluencies – and these cannot be
separated from LOW talkspurts by means of duration.
In contrast, the VOICINGRATIO ROC curve incurs much fewer false positives
before reaching a true positive rate of 0.5, but does not attain a true positive rate
near unity until the false positive rate exceeds 0.95. Closer inspection suggests
that this is because almost all talkspurts with a very high VOICINGRATIO are very
short, but breathing and coughing is excluded by the VOICINGRATIO threshold, as
they consist of mostly unvoiced frames.

We note that at a true positive rate (recall rate) of 90%, we obtain as few as
22% false positives on the DURATION curve. Conversely, at a rate of zero false
positives, we still capture 10% of the true positives on the VOICINGRATIO curve.
Finally, we note that discrimination using VOICINGRATIO and DURATION appears
to be complementary in part. Most importantly, placing the constraint that a
talkspurt should contain a non-zero amount of voiced speech may have a positive
effect on limiting false positives. The confusion matrix of one annotator’s
LOW/NONLOW decisions versus automatic VSU identification using a
combination of VOICINGRATIO and DURATION is shown in Table 2.
Table 2. Confusion matrix of manual annotation (annotator 1) versus VSUs.

Rater 1

LOW
NONLOW
Total

VSU
859
152
1011

Automatic
NONVSU
205
1838
2043

Total
1064
1990
3054

3.4 Bitmap clusters of VSUs
In order to visualize the discrepancies between automatic VSU identification and
manual LOW annotation, we produce bitmap clusters for the four cells in Table 2.
These are shown in Figure 5.

Figure 5. Bitmap clusters (see 2.2.4 above) of the final 500ms of talkspurts labeled LOWVSU (upper left panel), LOW-NONVSU (upper right panel), NONLOW-VSU (lower left
panel), and NONLOW-NONVSU (lower right panel).

The bitmap clusters reveal a clear tendency for rising patterns at the end of VSUs
judged as LOW, whereas NONLOW NONVSUs fail to show such a rise and have
most of the activity in the lower half of the panel, possibly indicating a preference
for falling contours. The upper right and lower left panes, where human
judgments and automatic classification are in conflict, contain much less data (cf.
Table 2) and show no clear patterns. They account for roughly 12% of talkspurts,
and we leave their detailed analysis for future work, when more data becomes
available.
The observation of rising patterns in the LOW VSUs is in line with our recent
observation (Heldner, et al., forthcoming) of a tendency for speakers in the
follower role of the Map Task to end utterances in a rising tone, as the follower
role contains a high proportion of backchannels.
3.5 Bitmap clusters of VSUs preceding interlocutor speech
Figure 6 shows bitmap clusters of the 1000ms of the interlocutor’s speech
preceding each of the 3054 talkspurts.

Figure 6. Plots of pitch contours as described in 2.2.4 above, of the 1000ms immediately
preceding each talkspurt in the other speaker’s channel. The panels show the speech
preceding talkspurts labeled as LOW-VSU (upper left panel), as LOW-NONVSU (upper
right panel), as NONLOW-VSU (lower left panel), and as NONLOW-NONVSU (lower right
panel). The upper left panel contains a line showing the 26th percentile of the speakers’
pitch from 860 to 700ms prior to the talkspurt.

In Figure 6, we observe that interlocutor speech preceding talkspurts judged as
LOW and identified as VSUs is compatible with the finding that 700-860ms prior
to the start of a backchannel, interlocutors exhibit increased activity below the 26th
percentile of their pitch range (Ward & Tsukahara, 2000) and little activity in
above the center line, as opposed to the lower right panel. The top left panel
contains a line on the 26th percentile along the 700-860ms interval for illustration.
This qualitative agreement is an indication that VSUs capture phenomena which
Ward and others refer to as backchannels.
In the lower right panel, we observe a relatively symmetric, bimodal
distribution. There is a somewhat lighter band along the horizontal mid-line,
indicating an interlocutor dispreference for approximately mean pitch before the
onset of NONLOW NONVSUs, which is in line with previous findings (cf. Edlund
& Heldner, 2005, and references mentioned therein), namely that mid-level pitch
is a common floor-keeper.

4 Conclusions
The very short utterance (or VSU) proposed in this work is automatically
identified using easily accessible acoustic information such as the duration of and
proportion of voicing in talkspurts. Our analyses suggest that it captures a large
proportion of the dialogue phenomena commonly referred to as backchannels.
This is evidenced by: the strong overlap between VSUs and manually labeled
low-content talkspurts; by the similarity in final pitch contour between VSUs and
the backchannel-intensive follower role in the Map Task; and by the fact that
interlocutor pitch 700-860ms prior to a VSU is consistent with Ward &
Tsukahara’s (2000) findings regarding interlocutor pitch preceding backchannels.
Prosodically, VSUs are characterized by a final rising pitch, in addition to the
duration and voicing characteristics that define them.
The finding that the easily distinguishable VSUs share many characteristics
with backchannels has important implications for future research. From a basic
research perspective, it means that we can, at zero manual effort, make more
detailed descriptions of conversational behavior. We may for example separate
speaker changes involving VSUs from others. From a speech technology
perspective, VSUs may be used to afford spoken dialogue systems less rigid
interaction control by allowing VSUs to be interpreted differently from other user
utterances. For example, if users are allowed to barge in – to speak in overlap with
the system – the system may go on uninterrupted in case the user utterance is a
VSU, and relinquish the floor if it is not. We believe this to be an important step
towards more human-like behavior in spoken dialogue systems.

Acknowledgements
We thank Kornel Laskowski for insightful discussions and criticism, and Pétur
Helgason for access to the Swedish Map Task Corpus. Funding was provided by
the Swedish Research Council (VR) project 2006-2172 Vad gör tal till samtal?
References
Brady, P. T. (1968). A statistical analysis of on-off patterns in 16 conversations.
The Bell System Technical Journal, 47, 73-91.
de Cheveigné, A., & Kawahara, H. (2002). YIN, a fundamental frequency
estimator for speech and music. Journal of the Acoustical Society of America,
111(4), 1917-1930.
Edlund, J., & Heldner, M. (2005). Exploring prosody in interaction control.
Phonetica, 62(2-4), 215-226.
Hardy, H., Baker, K., Bonneau-Maynard, H., Devillers, L., Rosset, S., &
Strzalkowski, T. (2003). Semantic and dialogic annotation for automated
multilingual customer service. In EUROSPEECH-2003 (pp. 201–204).
Geneva, Switzerland.
Heldner, M., Edlund, J., Laskowski, K., & Pelcé, A. (forthcoming). Prosodic
features in the vicinity of silences and overlaps. In Nordic Prosody X (this
volume). Helsinki, Finland.
Helgason, P. (2006). SMTC - A Swedish Map Task Corpus. In Working Papers
52: Proceedings from Fonetik 2006 (pp. 57-60). Lund, Sweden.
Jurafsky, D., Shriberg, E., & Biasca, D. (1997). Switchboard SWBD-DAMSL
Annotation Coders Manual, Draft 13 Retrieved 2008-12-06, from
http://www.stanford.edu/~jurafsky/ws97/manual.august1.html
Laskowski, K., Edlund, J., & Heldner, M. (2008). An instantaneous vector
representation of delta pitch for speaker-change prediction in conversational
dialogue systems. In Proceedings ICASSP 2008 (pp. 5041-5044). Las Vegas,
NV, USA.
Norwine, A. C., & Murphy, O. J. (1938). Characteristic time intervals in
telephonic conversation. The Bell System Technical Journal, 17, 281-291.
Schegloff, E. (1982). Discourse as an interactional achievement: Some uses of 'uh
huh' and other things that come between sentences. In D. Tannen (Ed.),
Analyzing Discourse: Text and Talk (pp. 71-93). Washington, D.C., USA:
Georgetown University Press.
Shriberg, E., Bates, R., Stolcke, A., Taylor, P., Jurafsky, D., Ries, K., et al.
(1998). Can prosody aid in the automatic classification of dialog acts in
conversational speech. Language and Speech, 41(3-4), 439-487.
Ward, N. (2004). Pragmatic functions of prosodic features in non-lexical
utterances. In Proceedings of Speech Prosody 2004 (pp. 325-328). Nara,
Japan.
Ward, N., & Tsukahara, W. (2000). Prosodic features which cue back-channel
responses in English and Japanese. Journal of Pragmatics, 32, 1177-1207.
Yngve, V. H. (1970). On getting a word in edgewise. In Papers from the Sixth
Regional Meeting Chicago Linguistic Society (pp. 567-578). Chicago, IL,
USA: Chicago Linguistic Society.

Information Structure: Notional Distinctions, Ways of Expression
Running Title: Information Structure
Prof. Dr. Caroline Féry
Institut für Linguistik
Universität Potsdam
Karl-Liebknecht-Str. 24-25
D-14476 Golm
caroline.fery@googlemail.com
Prof. Dr. Manfred Krifka
Institut für deutsche Sprache und Linguistik
Humboldt-Universität zu Berlin
Unter den Linden 6
10099 Berlin
krifka@rz.hu-berlin.de

1

Information Structure:
Notional Distinctions, Ways of Expression
Caroline Féry and Manfred Krifka

Abstract. Information Structure, the packaging of information to satisfy the
immediate communicative needs, exerts a powerful force on all structural
levels of language. We show how this concept can be defined, we argue for
focus, givenness, topic, frame setting and delimitation as important
subconcepts, and we illustrate the wide variety in which these information
structural functions are expressed in languages.

1. What is Information Structure?
The phenomena we subsume under the notion of information structure (IS,
for short) have enjoyed the attention of linguists for a long time. They have
been identified since the medieval Arab grammatical tradition by different
linguistic schools in a number of ways. To mention the perhaps most
influential one, the Prague School initiated by Mathesius has argued that the
identification of given material (the theme) and the highlighting of new
material (the rheme) exerts a powerful force on language structure. Today,
the effects of IS are recognized in every theoretic framework that strives for
a comprehensive view of linguistic structure, and they are investigated in a
wide variety of distinct languages – witness the contributions to the Parallel
Session on Information Structure at CIL 18.
But what is IS? Following Chafe (1976), we understand it to refer to the
packaging of information that meets the immediate communicative needs of
the interlocutors, i.e. the techniques that optimize the form of the message
with the goal that it be well understood by the addressee in the current
attentional state. One such feature, for example, is the highlighting of
constituents, called focus. In (1), a question creates a particular attentional
state, which is recognized by the focus in the answer, expressed by pitch
accent on tiger (cf. 1a). Pitch accent on road, as in (1b), would lead to an
infelicitous answer, even though the truth conditions of (1a) and (1b) are the
same, as it does not fit to the context question.
(1)

{What did you see on the road?}
a. We saw a TIGER on the road.
b. #We saw a tiger on the ROAD.

2

Chafe’s metaphor of packaging suggests that IS never affects truth
conditions. However, one shold be aware of the fact that the markings of IS
can have truth-conditional effects, for example with focus-sensitive particles
like only. In (2a), the speaker may have seen other animals on the road, but
the only place where a tiger was spotted was on the road. In (2b), no other
animal was seen there. Depending on the placement of the pitch accent, this
English sentence is true in different contexts.
(2)

a. We only saw a tiger on the ROAD.
b. We only saw a TIGER on the road.

One and the same linguistic device, sentence accent, can be used for
packaging as well as for constructing the truth-conditional content. There
are two ways of dealing with this: One is to assume that the two uses of the
same feature are essentially unrelated, just as the uses of accent in English
to express focus and to distinguish words such as REcord and reCORD. The
other is to assume that the feature is to be interpreted in a particular way that
makes sense for the purposes of information packaging and of building
information content. The second alternative is more attractive, as we should
not assume multiple meanings if possible. We will see that focus indeed can
be interpreted in this way.
We will first provide definitions of the notions of IS, and then examine
some of the linguistic means used for the realization of IS. The grammatical
devices for focusing, defocusing or topicalizing will turn out to be parts of a
set of reflexes existing independently in the language under consideration.
We wish to point the readers to Féry, Fanselow & Krifka (ed.) (2006) for a
more comprehensive exposition of some of the points discussed here.

2. The notions of information structure
If we want to talk about communication as transfer of information and its
optimization relative to the momentary needs of interlocutors, it is useful to
adopt a model of information exchange rooted in the notion of Common
Ground. The original notion of CG (cf. Stalnaker 1974) saw it as a way to
model the information that is mutually known to be shared, which is
continuously modified in the course of communication. This allowed for
modeling the distinction between presuppositions, as requirements for the
input CG, and assertions or the proffered content, as the proposed change in
the output CG. This distinction is relevant for information packaging, as the
CG changes continuously, and information has to be packaged
corresponding to the CG at the point at which it is uttered.
CG does not only consist of a set of propositions that is presumed to be
mutually accepted, but also of a set of entities that have been introduced into

3

the CG before, the discourse referents. They can be taken up by pronouns or
by definite NPs, which express requirements to the input CG. The choice of
anaphoric expression depends on the recency of the antecedent, again a
notion that falls squarely within Chafe’s notion of packaging.
The properties of CG mentioned so far have to do with the truth-conditional
information in the CG, so we can subsume them under the heading of CG
content. But any notion of CG that can be applied to the analysis of real
communication must also contain information about the manifest communicative interests and goals of the participants. For example, questions
typically do not add factual information to the common ground, but indicate
informational needs on the side of one participant that should be satisfied by
a conversational move of the other. We call this dimension of the common
ground CG management, as it is concerned with the way the CG content is
supposed to develop. Just as CG content, the tasks of CG management is
shared by the interlocutors, with the understanding that the responsibility for
it may be asymmetrically distributed among participants.
Turning now to definitions of the IS categories that we consider crucial, we
propose first a three-way distinction between focus, givenness and topic.
A general definition of focus, making use of a central insight of Alternative
Semantics (Rooth 1992), appears in (3).
(3)

Definition of focus:
Focus indicates the presence of alternatives that are
relevant for the interpretation of linguistic expressions.

One prominent use of focus is the identification of context questions in
answers, as in (1). The idea is that the meaning of a question identifies a set
of alternative propositions, the answer picks out one of these, and the focus
within the answer signals the alternative propositions inherent in the
question.
The alternative denotations indicated by focus have to be comparable to the
denotation of the expression in focus, i.e. they have to be of the same type,
and often also of the same ontological sort (e.g., persons or times). They
also can be more narrowly restricted by the context of utterance. The
complement of focus is ‘background.’
The second notion to be defined is givenness:
(4)

Definition of givenness:
A feature X of an expression α is a Givenness feature iff X
indicates whether the denotation of α is present in the CG or
not, and/or indicates the degree to which it is present in the
immediate CG.

Schwarzschild (1999) develops a refined theory of interaction between
givenness and focus, which checks givenness recursively and states that
constituents not in focus must be given, and that focus has to be applied

4

only when necessary, i.e. to prevent that a constituent is given. But while
focus is restricted in Schwarzschild’s theory, it cannot be eliminated totally.
In fact focus/background and given/newness cannot be reduced to just one
opposition, as these pairs of notions are only partially overlapping. For
example, given expressions, like pronouns, can be focused.
The notion of ‘topic’ comes with a complementary part called ‘comment.’
Reinhart (1982) integrates it into a theory of communication that makes use
of the notion of CG. According to her, new information is not just added to
the CG content in form of unstructured propositions, but is rather associated
with entities, just like information in a file card system is associated with
file cards that bear a particular heading. A definition based on this insight
appears in (6).
(5)

Definition of topic:
The topic constituent identifies the entity or set of entities under
which the information expressed in the comment constituent
should be stored in the CG content.

For example, while (6a,b) express the same proposition, they structure it
differently insofar as (a) should be stored as information about Aristotle
Onassis, whereas (b) should be stored as information about Jacqueline
Kennedy.
(6)

a. [Aristotle Onassis] T [married Jacqueline Kennedy]C
b. [Jacqueline Kennedy]T [married Aristotle Onassis]C

Theories of IS often introduce additional notions beyond focus, givenness
and topic. One such notion is contrast, which is distinguished from pure
information focus that shows up, e.g., in the answer to constituent questions.
With contrast, the alternatives have to be given explicitly, and usually it is
also assumed that only one of the contrasted alternatives is acceptable. For
example, answers to alternative questions would qualify as having
contrastive focus:
(7)

A: Do you want TEA or COFFEE?
B: I want TEA.

There is a plausible argument that we do not need contrastive focus as a
separate basic notion, as we already have introduced givenness; hence
contrastive focus can be defined as that subtype of focus in which the
alternatives are given. The uniqueness assumption may follow if we assume
that apparent non-uniqueness arises because alternatives can be combined
(e.g., we saw a tiger and a baboon on the road), but that the explicit
enumeration of alternatives that does not include a combination (e.g., or
both in (7.A)) suggests that such combinations should be disregarded.
Another important notion is contrastive topic, as in the following example:

5

(8)

A: What are your sisters playing?
B: My YOUNGER sister plays the VIOLIN,
and my OLDER sister, the FLUTE.

The phrase my YOUNGER sister is a contrastive topic. Rising accent indicates
that at the current position of discourse, other topics could have been chosen
(e.g, my OLDER sister). Again, we do not need contrastive topic as a basic
notion. If focus, in general, indicates the presence of alternatives, then focus
within a topic would indicate the presence of alternative topics at the current
point in discourse. In (8), the choice of my YOUNGER sister as topic indicates
that there are other possible topics that can be described by my X sister; and
indeed, the second clause, my OLDER sister identifies such an alternative
topic. Contrastive topics are to be expected whenever a question is too
complex to be answered on the basis of one single topic; they indicate a
particular answering strategy by introducing subtopics (cf. Büring 2003).
There is a phenomenon that is somewhat related to topichood, which has
been called frame setting (cf. Jacobs 2001).
(9)

A: How is John?
B: {Healthwise / As for his health}, he is FINE.

This is a statement about John, but it is restricted to those aspects that
concern John’s health (in contrast, e.g., to his financial sitation). We call
phrases like healthwise “frame setters”. Clearly, focus plays a role with
frame setters, just as with contrastive topics, as they express a certain
restriction of the ensuing predication to some perspective that is not clearly
identified by the context already – if the health perspective were already
established, there would be no need to express it explicitly. Frame setters
restrict context-sensitive expressions, like be fine, to the specified
dimension, or delimit the predications that can be made. For example, As for
his health, he had a serious flu recently is fine, but As for his financial
situation, he has a serious flu recently is not.
There is an obvious similarity between contrastive topics and frame setters
that is reflected in the way how these information-structural functions are
marked (e.g., by a B-accent in English, or by the postposition nun in
Korean). Both express that the predication is restricted in some way – e.g.,
(8b) restricts the predication plays the violin to the younger sister (where the
expected value is the sisters in general), and (9) restricts the predication to
aspects concerning John’s health. It is useful to introduce a new term for
this function: delimitation. It is a genuin phenomenon of IS, as it responds to
the current informational need of the addressee: It is indicated that the issue
at hand is broader, and that the ensuing speech act concerns only a part of
this more general issue. Hence, delimitation can be defined as follows:
(10) Definition of Delimitation
A delimiter α in within an expression […α….] always comes
with a focus within α indicating alternatives α′, α″ etc.

6

It indicates that the current informational need is not totally
satisfied by […α…] but would be satisfied by additional
expressions [---α′---], [α″---] etc.
We do not claim that the notions of focus, givenness, topic, frame setting
and delimitation exhaust what there is to say about IS. For example, in an
argumentative discourse, the current informational need might dictate the
selection and ordering of arguments to gain support for a particular
conclusion. But such effects go beyond the limit of the sentence, and relate
it to discourse structure. Here we will stay within the confines of the
sentence (in a particular context), and we will try to illustrate some of the
ways in which the IS notions specified above are expressed in languages.

2. The expression of information structure
How do languages mark the various IS distinctions? While there is
considerable variety in the strategies that we find in different languages,
they always have a relationship to prosody: focus tends to be prosodically
prominent, and givenness tends to be prosodically non-prominent, while
topic tends to form a separate prosodic phrase, and is thus also prominent
(the same holds for frame setters and delimiters in general). But this
prosodic connection is achieved by different grammatical correlates in
different languages, depending on the languages’ general properties. And
languages differ in the obligatoriness of expressing IS distinctions; for
example, it has been shown that in Northern Sotho (Bantu) and Hausa do
not express the focus of answers as rigidly as English (cf. Zerbian 2006,
Hartmann & Zimmermann 2007).
In English, focus and topic correlate with pitch accents, and givenness is
often expressed by deaccenting, see (1) and (2). But in a number of Asian
and African languages, pitch accents only play a minor role, if at all, and
morphological and syntactic means are prevalent. In tone languages,
phrasing can replace the pitch accents of intonation languages, and particles
can play the role of boundary tones. An extreme case of prosodic marking
of IS is ellipsis, where only the focused part of a sentence is pronounced,
and the given part is just deleted.
Following Jackendoff (1972), we assume that IS roles are identified at the
surface syntactic structure by features, in the way shown in (11) to (13).
(11) a. We only saw a tiger [on the ROAD]F.
b. We only saw [a TIGER]F on the road.
(12) {What did you see on the road?}
[We saw]G [a TIGER]F [on the road]G.

7

(13) [As for tigers,]T [we saw one on the road]F.
We examine syntactic, phonological and morphological reflexes of IS in the
next subsection, and show in each case how they relate to prosody.
2.1 Sentence Position
First, IS roles are often associated with sentence positions. Halliday (19678) holds that the initial position is a necessary condition for a ‘theme’ (a
topic in our terminology). This preferred place for a topic is easily
explained: since it is the address at which the infomration of the sentence is
supposed to be stored, it makes sense to introduce it right at the beginning.
However, topic are not necessarily located sentence-initially. In the
following Korean sentence, the topic dezaato-wa ‘dessert’ is placed after a
quantifier phrase and is thus not initial.1 A subscript P shows a prosodic
phrase (p-phrase), and a subscript I a larger intonation phrase (i-phrase).
(14) ((Nwukwuna-ka)P ([dessert-nun]T)P (ice cream-ul mek-ess-ta)P)I
everyone-NOM
dessert-TOP
ice cream-ACC eat-PAST-DEC
‘As for dessert, everyone ate ice cream.’
Sentence-final topics, sometimes called ‘anti-topics’ are also possible, as
illustrated in (15) for Cantonese and (16) for French.
(15) ((Go loupo)P (nei gin-gwo gaa)P, ([ni go namjan ge]T)P)I.
CLF wife
2.SG see-EXP DSP this CLF man DSP
‘The wife you have seen, of this man.’
(16) ((Pierre l’ a mangée)P, ([la pomme]T)P)I.
Peter it-ACC has eaten, the apple
‘Peter has eaten the apple.’
The common property of topics is their separation from the remainder of the
sentence. They tend to form their own i-phrase (intonation phrase), which
allows for a clear intonational separation. In languages like Japanese or
Cantonese, particles not only signal the role of the constituent as a topic, but
also add place for a boundary tone. This allows the topic in (16) to be
inserted in a non-initial position.
Focus has also been associated with special positions in certain languages.
Hungarian has been described as a language which obligatorily places an
exhaustive focus preverbally (É. Kiss 1998), and Italian as a language with
clause-initial (Rizzi 1997) or clause-final (Samek-Lodovici 2006) foci.

1

Thanks to Shin-Sook Kim for providing this sentence.

8

An alternative explanation, which accounts for the Hungarian facts without
forcing an association between focus and preverbal position, assumes that
Hungarian is phonologically a left-headed language, both for prosodic
words and prosodic phrases. Focus wants to be prominent and the preferred
stress position is at the beginning of the main i-phrase, directly after the iphrase of the topic. The initial position is occupied by the narrow focus, as
often as possible, and happens to be the verb in all other cases (see Szendrői
2003). But focus may also be located postverbally: In (17), both the VP and
the dative object are focused and the accusative object is given, but the
dative object is postverbal. In such cases, focus is indicated by pitch accent
only.
(17) ((Tegnap este)P)I ((BEMUTATTAM Pétert)P (MARINAK)P)I.
yesterday evening PRT-introduced-I Peter.ACC Mary.DAT
‘Yesterday evening, I introduced Peter to Mary.’
In Italian, given elements may be moved out of the matrix clause, and
typically, it is this movement which causes finality of focus. In (18), adapted
from Samek-Lodovici (2006), Parigi is the focus, and the following
constituents are right-dislocated as they are given. Italian is a language with
final stress, both at the level of the p-word, where it is trochaic, and at the
level of the p-phrase, and syntactic reorganization helps prosody in moving
narrow foci to the furthest possible rightward position. Thus, both in
Hungarian and in Italian the peripheral position of focus is not a special
feature of focus, but reflects the general preference for prominence.
(18) ((L’ho
incontrato [a PARIGI]F)P, (Luigi)P, (ieri)P)I.
(I) him have-met in Paris,
Luigi, yesterday
‘I met Luigi in Paris yesterday.’
2.2 Accents
There have been numerous attempts in the literature to relate specific
information roles to the form of pitch accents. Bolinger (1958) introduces a
distinction between accent A, a falling accent, and accent B, a fall-rise
accent, for English, and Jackendoff (1972) and Liberman & Pierrehumbert
(1984) relates the former to focus and the latter to topic, as in (19). Manny
has accent B, and Anna accent A.
(20) {What about Manny? Who did he come with?}
(([MANNY]T)P (came with [ANNA]F)P)I.
Büring (2003), for German, and Steedman (2000), for English, establish an
obligatory relationship between contours and roles by having pitch accent
contours participate in the definition of topics and foci. Attempts to relate
forms of accents to specific IS roles are found for other languages as well.

9

For instance, Frota (2000) claims that narrow foci in Portuguese are always
associated with a certain kind of accent.
An alternative explanation is possible which only indirectly relates IS to the
forms of accents.. The preference for associating specific contours with IS
roles can be explained by general properties of the language. As far as
topics are concerned, the preference for sentence-initiality is paired with a
preference for rising tones. The rising tone is just a reflex of the non-finality
of this accent. And the falling contour often found on focus may be related
to the late position of a focus in a sentence.
Do languages with pitch accents necessarily use them for topics and foci?
The question bears on the necessity of accents (and of deaccenting) in
general in relation to focus/topic/givenness. Jackendoff formulates a rule
which directly relates a focus with an accent. ‘If a phrase P is chosen as the
focus of a sentence S, the highest stress in S will be on the syllable of P that
is assigned highest stress by the regular stress rules’ (1972:247). Nearly all
models relating focus with phonology rely on a direct correspondence
between semantics and phonetics and require an accent signaling the
presence of a focused constituent (see for instance Cinque 1993, Rooth
1992, Selkirk 1995, Schwarzschild 1999, and many others).
But in fact, there are examples in which the association between focus and
accent seems to be cancelled. One such case is the so-called Second
Occurrence Focus (cf. Partee 1999, Beaver et al. 2007, Féry & Ishihara
2005), which combines elements of association with focus and givenness.
Only vegetables in (21b) is associated with the focus operator only, and is
thus a focus, but it is also given, because it is repeated from (21a).
(21) a. {Everyone already knew that Mary only eats [vegetables]F}.
b. If even [Paul]F knew that Mary only eats [vegetables]SOF,
then he should have suggested a different restaurant.
There are only weak correlates of accent, and no pitch excursions in the
postnuclear position, although Féry & Ishihara (2005) show that a pitch
accent is indeed present in the prenuclear position.
Other cases of absence of accent on a focus are a consequence of avoidance
of stress-clash and the consequent deaccenting. In (18a), herself is a socalled intensifier which is claimed to be obligatorily accented in the
literature. But in the presence of an adjacent narrow focus, accent on herself
disappears. The same is true of the association with focus adjacent to a
parallel focus in (18b), cf. Rooth (1992). In (18c), the answer to the question
is completely deaccented. Instead the additive particle also carries the stress.
(18d), from Reis & Rosengren (1997), shows that a contrastive topic (Peter
in Krifka’s 1999 analysis) can also be realized without excursion if another,
more prominent topic (Gauguin) is adjacent.
(22) a. Marie-Luise even grows RICE herself.
b. People who GROW rice only EAT rice.

10

c. {John said that Mark is coming, but what did Sue say?}
She ALSO said that Mark is coming.
d. {Boy, Paul possesses a Gauguin.}
Einen GAUGUIN besitzt Peter AUCH
‘Peter also owns a Gauguin’
We see that there is no strict association between focus and accent or topic
and accent. Accent is a preferred option but it is not obligatory. It is only
present if the phonological structure of the sentence allows it. To sum up,
the preference for associating some specific contours with IS roles, or just
pitch accents can be explained by general properties of the language.2
2.3 Morphological markers
Morphological markers are compatible with the general claim of this section
that the marking of focus and topic is always prosodically prominent if can
be shown that the presence of a particle change the prosody of the sentence
it appears in. Examples confirming this claim appeared in (11) for Japanese
and in (15) for Cantonese topic markers. A number of tone languages have
been studied as for their focus realizations which do not seem to have other
correlates of IS than optional presence of morphological markers. Examples
involving morphological markers for focus appear in the Gur languages Buli
in (23) and Ditammari in (24), both from Fiedler et al. (to appear). In Buli,
the focus marker kà precedes the focused constituent. But when the focused
túé is sentence-initial, the marker kà is not obligatory. As for Ditammari, the
focus marker nyā follows the focused constituent.
(23) Q: What did the woman eat?
A: ò
ŋòb kà túé.
3.SG eat FM beans
‘She ate BEANS.’
(24) Q: What did the woman eat?
A: ò dī yātũrà nyā.
3.SG eat beans FM
‘She ate BEANS.’
These markers have a delimiting function in creating a prosodic boundary.
We thus propose that the prosodic connection of the focus and topic markers
is to be found in the phrasing properties of a constituent delimited by such a
marker. Even if not enough is known about the exact behavior of particles, it

2

Many languages do not use pitch accents to highlight a focused element, but rather
raise the pitch register of a focused phrase as a whole. This happens for instance in
Mandarin Chinese (Xu 1999), in Korean, in Georgian and in Hindi.

11

seems to be a valid generalization that they always appear at the periphery
of the constituent they mark.
In languages without special markers for IS, that is in languages which do
not have pitch accent and which have only optional morphological markers,
the answer to wh-question typically involves ellipsis of the given material.
Only the constituent in focus is realized, a strategy which we propose to
analyze in prosodic terms.
3.

To conclude

This paper has proposed definitions of IS concepts in a model of
information exchange that makes use of the notion of Common Ground. We
also argued – in the limited place available – that there are universal
principles in the way how IS roles are coded. One tendency, the tonal
prominence of focused and the de-prominence of given expresseions, can be
explained by the effort code (Gussenhoven 2004): Higher or lower
expenditure in prosodic explicitness reflects the importance of
subconstituents for communication. Others, like the tendency of topics to be
prosodically separated from and to precede the rest, can be traced back to
the optimal flow of information transmission. In spite of such
commonalities, we have pointed out drastic differences in the ways how
they are realized in the grammars of individual languages.

References
Beaver, David, Brady Zack Clark, Edward Flemming, T. Florian Jäger &
Maria Wolters. 2007. When semantics meets phonetics: Acoustical
studies of second occurrence focus. Language 83. 245-276.
Bolinger, Dwight. 1958. A theory of pitch accent in English. Word 14. 109–
149.
Büring, Daniel. 2003. On D-trees, beans, and B-accents. Linguistics and
Philosophy 26, 511–545.
Chafe, Wallace L. 1976. Givenness, contrastiveness, definiteness, subjects,
topics and point of view. in Charles N. Li (ed.), Subject and Topic.
27–55. New York: Academic Press.
Cinque, Guglielmo. 1993. A null theory of phrase and compound stress.
Linguistic Inquiery 24: 239-297.
Féry, Caroline, Gisbert Fanselow & Manfred Krifka (eds.), The notions of
Information Structure. Working Papers of the SFB632,
Interdisciplinary Studies on Information Structure (ISIS) 6. Potsdam:

12

Universitätsverlag
potsdam.de/isg.html.

Potsdam.

http://www.sfb632.uni-

Féry, Caroline & Shinichiro Ishihara. 2005. Interpreting second occurrence
focus. Ms. University of Potsdam.
Fiedler, Ines, Katharina Hartmann, Brigitte Reineke, Anne Schwarz &
Malte Zimmermann. To appear. Subject focus in West African
languages. In Zimmermann, Malte & Caroline Féry, Eds. Information
Structure in Different Perspectives.
Frota, Sónia. 2000. Prosody and Focus in European Portuguese. New York:
Garland Publishing.
Gussenhoven, Carlos. 2004. The phonology of tone and intonation,
Cambridge University Press, Cambridge.
Halliday, M. A. K. 1967-68 Notes on transitivity and theme in English.
Journal of Linguistics 3 & 4.É. Kiss, Katalin (1998), “Identificational
focus versus information focus”, Language 74. 245–273.
Hartmann, Katharina & Malte Zimmermann. 2007. In place -- out of place:
Focus in Hausa.”, in Kerstin Schwabe & Susanne Winkler, On
information structure, meaning and form, Amsterdam, John
Benjamins, 365-403.
Horváth, Júlia. 1986. Focus in the Theory of Grammar and the Syntax of
Hungarian. Dordrecht: Foris.
Hyman, Larry & Maria Polinsky. To appear. Is there a focus position in
Aghem? In: Zimmermann, Malte & Caroline Féry, Eds. Information
Structure in Different Perspectives.
Jackendoff, Ray. 1972. Semantic Interpretation in Generative Grammar.
Cambridge, MA: MIT Press.
Jacobs, Joachim. 2001. The Dimensions of Topic-Comment. Linguistics 39:
641-681
Krifka, Manfred. 1999. Additive particles under stress. Proceedings of SALT
8. Cornell, CLC Publications. 111–128.
Liberman, Mark & Janet Pierrehumbert. 1984. Intonational invariance under
changes in pitch range and length. In Language Sound Structure, eds.
Mark Aronoff & Richard T. Oehrle. Cambridge, MA: MIT Press.
157–233.
Partee, Barbara H. 1999. Focus, quantification, and semantics-pragmatics
issues. In Focus: Linguistic, Cognitive, and Computational
Perspectives, eds. Peter Bosch & Rob van der Sandt. Cambridge:
Cambridge University Press. 213–231.
Reinhart, Tanya. 1982. Pragmatics and linguistics. An analysis of sentence
topics. Philosophica 27. 53–94.

13

Reis, Marga & Inger Rosengren. 1997. A modular approach to the grammar
of additive particles: The case of German auch. Journal of Semantics
14. 237–309.
Rizzi, Luigi. 1997. The fine structure of the left periphery. In Elements of
Grammar: Handbook in Generative Syntax, ed. Liliane Haegeman,
Dordrecht: Kluwer Academic Publishers. 281–337.
Rochemont, M. 1986. Focus in generative grammar. Amsterdam/
Philadelphia: John Benjamins Publishing Company.
Rooth, Mats. 1992. A theory of focus interpretation. Natural Language
Semantics 1. 75–116.
Samek-Lodovici, Vieri. 2006. When right dislocation meets the leftperiphery: A unified analysis of Italian non-final focus. Lingua 116.
836–873.
Schwarzschild, Roger. 1999. GIVENness, AvoidF and other constraints on
the placement of accent. Natural Language Semantics 7. 141–177.
Selkirk, Elisabeth O. 1995. Sentence prosody: Intonation, stress and
phrasing. In Handbook of Phonological Theory, ed. John Goldsmith.
Cambridge, MA: Blackwell. 550–569.
Stalnaker, Robert. 1974. Pragmatic presuppositions. in Milton K. Munitz &
Peter K. Unger (eds.), Semantics and Philosophy. 197–214. New
York: New York University Press.
Steedman, Mark. 2000. Information Structure and the Syntax-Phonology
Interface. Linguistic Inquiry 31. 649–689.
Szendrői, Kriszta. 2003. A stress-based approach to the syntax of Hungarian
focus. The Linguistic Review 20 (1). 37–78.
Xu, Yi. 1999. Effects of tone and focus on the formation and alignment of
F0 contours. Journal of Phonetics 27, 55–105.
Zerbian, Sabine. 2006. Expression of Information Structure in the Bantu
Language Northern Sotho, Doctoral diss., Humboldt University,
Berlin.

14

Information Structure: Notional Distinctions, Ways of Expression
Running Title: Information Structure
Prof. Dr. Caroline Féry
Institut für Linguistik
Universität Potsdam
Karl-Liebknecht-Str. 24-25
D-14476 Golm
caroline.fery@googlemail.com
Prof. Dr. Manfred Krifka
Institut für deutsche Sprache und Linguistik
Humboldt-Universität zu Berlin
Unter den Linden 6
10099 Berlin
krifka@rz.hu-berlin.de

1

Information Structure:
Notional Distinctions, Ways of Expression
Caroline Féry and Manfred Krifka

Abstract. Information Structure, the packaging of information to satisfy the
immediate communicative needs, exerts a powerful force on all structural
levels of language. We show how this concept can be defined, we argue for
focus, givenness, topic, frame setting and delimitation as important
subconcepts, and we illustrate the wide variety in which these information
structural functions are expressed in languages.

1. What is Information Structure?
The phenomena we subsume under the notion of information structure (IS,
for short) have enjoyed the attention of linguists for a long time. They have
been identified since the medieval Arab grammatical tradition by different
linguistic schools in a number of ways. To mention the perhaps most
influential one, the Prague School initiated by Mathesius has argued that the
identification of given material (the theme) and the highlighting of new
material (the rheme) exerts a powerful force on language structure. Today,
the effects of IS are recognized in every theoretic framework that strives for
a comprehensive view of linguistic structure, and they are investigated in a
wide variety of distinct languages – witness the contributions to the Parallel
Session on Information Structure at CIL 18.
But what is IS? Following Chafe (1976), we understand it to refer to the
packaging of information that meets the immediate communicative needs of
the interlocutors, i.e. the techniques that optimize the form of the message
with the goal that it be well understood by the addressee in the current
attentional state. One such feature, for example, is the highlighting of
constituents, called focus. In (1), a question creates a particular attentional
state, which is recognized by the focus in the answer, expressed by pitch
accent on tiger (cf. 1a). Pitch accent on road, as in (1b), would lead to an
infelicitous answer, even though the truth conditions of (1a) and (1b) are the
same, as it does not fit to the context question.
(1)

{What did you see on the road?}
a. We saw a TIGER on the road.
b. #We saw a tiger on the ROAD.

2

Chafe’s metaphor of packaging suggests that IS never affects truth
conditions. However, one shold be aware of the fact that the markings of IS
can have truth-conditional effects, for example with focus-sensitive particles
like only. In (2a), the speaker may have seen other animals on the road, but
the only place where a tiger was spotted was on the road. In (2b), no other
animal was seen there. Depending on the placement of the pitch accent, this
English sentence is true in different contexts.
(2)

a. We only saw a tiger on the ROAD.
b. We only saw a TIGER on the road.

One and the same linguistic device, sentence accent, can be used for
packaging as well as for constructing the truth-conditional content. There
are two ways of dealing with this: One is to assume that the two uses of the
same feature are essentially unrelated, just as the uses of accent in English
to express focus and to distinguish words such as REcord and reCORD. The
other is to assume that the feature is to be interpreted in a particular way that
makes sense for the purposes of information packaging and of building
information content. The second alternative is more attractive, as we should
not assume multiple meanings if possible. We will see that focus indeed can
be interpreted in this way.
We will first provide definitions of the notions of IS, and then examine
some of the linguistic means used for the realization of IS. The grammatical
devices for focusing, defocusing or topicalizing will turn out to be parts of a
set of reflexes existing independently in the language under consideration.
We wish to point the readers to Féry, Fanselow & Krifka (ed.) (2006) for a
more comprehensive exposition of some of the points discussed here.

2. The notions of information structure
If we want to talk about communication as transfer of information and its
optimization relative to the momentary needs of interlocutors, it is useful to
adopt a model of information exchange rooted in the notion of Common
Ground. The original notion of CG (cf. Stalnaker 1974) saw it as a way to
model the information that is mutually known to be shared, which is
continuously modified in the course of communication. This allowed for
modeling the distinction between presuppositions, as requirements for the
input CG, and assertions or the proffered content, as the proposed change in
the output CG. This distinction is relevant for information packaging, as the
CG changes continuously, and information has to be packaged
corresponding to the CG at the point at which it is uttered.
CG does not only consist of a set of propositions that is presumed to be
mutually accepted, but also of a set of entities that have been introduced into

3

the CG before, the discourse referents. They can be taken up by pronouns or
by definite NPs, which express requirements to the input CG. The choice of
anaphoric expression depends on the recency of the antecedent, again a
notion that falls squarely within Chafe’s notion of packaging.
The properties of CG mentioned so far have to do with the truth-conditional
information in the CG, so we can subsume them under the heading of CG
content. But any notion of CG that can be applied to the analysis of real
communication must also contain information about the manifest communicative interests and goals of the participants. For example, questions
typically do not add factual information to the common ground, but indicate
informational needs on the side of one participant that should be satisfied by
a conversational move of the other. We call this dimension of the common
ground CG management, as it is concerned with the way the CG content is
supposed to develop. Just as CG content, the tasks of CG management is
shared by the interlocutors, with the understanding that the responsibility for
it may be asymmetrically distributed among participants.
Turning now to definitions of the IS categories that we consider crucial, we
propose first a three-way distinction between focus, givenness and topic.
A general definition of focus, making use of a central insight of Alternative
Semantics (Rooth 1992), appears in (3).
(3)

Definition of focus:
Focus indicates the presence of alternatives that are
relevant for the interpretation of linguistic expressions.

One prominent use of focus is the identification of context questions in
answers, as in (1). The idea is that the meaning of a question identifies a set
of alternative propositions, the answer picks out one of these, and the focus
within the answer signals the alternative propositions inherent in the
question.
The alternative denotations indicated by focus have to be comparable to the
denotation of the expression in focus, i.e. they have to be of the same type,
and often also of the same ontological sort (e.g., persons or times). They
also can be more narrowly restricted by the context of utterance. The
complement of focus is ‘background.’
The second notion to be defined is givenness:
(4)

Definition of givenness:
A feature X of an expression α is a Givenness feature iff X
indicates whether the denotation of α is present in the CG or
not, and/or indicates the degree to which it is present in the
immediate CG.

Schwarzschild (1999) develops a refined theory of interaction between
givenness and focus, which checks givenness recursively and states that
constituents not in focus must be given, and that focus has to be applied

4

only when necessary, i.e. to prevent that a constituent is given. But while
focus is restricted in Schwarzschild’s theory, it cannot be eliminated totally.
In fact focus/background and given/newness cannot be reduced to just one
opposition, as these pairs of notions are only partially overlapping. For
example, given expressions, like pronouns, can be focused.
The notion of ‘topic’ comes with a complementary part called ‘comment.’
Reinhart (1982) integrates it into a theory of communication that makes use
of the notion of CG. According to her, new information is not just added to
the CG content in form of unstructured propositions, but is rather associated
with entities, just like information in a file card system is associated with
file cards that bear a particular heading. A definition based on this insight
appears in (6).
(5)

Definition of topic:
The topic constituent identifies the entity or set of entities under
which the information expressed in the comment constituent
should be stored in the CG content.

For example, while (6a,b) express the same proposition, they structure it
differently insofar as (a) should be stored as information about Aristotle
Onassis, whereas (b) should be stored as information about Jacqueline
Kennedy.
(6)

a. [Aristotle Onassis] T [married Jacqueline Kennedy]C
b. [Jacqueline Kennedy]T [married Aristotle Onassis]C

Theories of IS often introduce additional notions beyond focus, givenness
and topic. One such notion is contrast, which is distinguished from pure
information focus that shows up, e.g., in the answer to constituent questions.
With contrast, the alternatives have to be given explicitly, and usually it is
also assumed that only one of the contrasted alternatives is acceptable. For
example, answers to alternative questions would qualify as having
contrastive focus:
(7)

A: Do you want TEA or COFFEE?
B: I want TEA.

There is a plausible argument that we do not need contrastive focus as a
separate basic notion, as we already have introduced givenness; hence
contrastive focus can be defined as that subtype of focus in which the
alternatives are given. The uniqueness assumption may follow if we assume
that apparent non-uniqueness arises because alternatives can be combined
(e.g., we saw a tiger and a baboon on the road), but that the explicit
enumeration of alternatives that does not include a combination (e.g., or
both in (7.A)) suggests that such combinations should be disregarded.
Another important notion is contrastive topic, as in the following example:

5

(8)

A: What are your sisters playing?
B: My YOUNGER sister plays the VIOLIN,
and my OLDER sister, the FLUTE.

The phrase my YOUNGER sister is a contrastive topic. Rising accent indicates
that at the current position of discourse, other topics could have been chosen
(e.g, my OLDER sister). Again, we do not need contrastive topic as a basic
notion. If focus, in general, indicates the presence of alternatives, then focus
within a topic would indicate the presence of alternative topics at the current
point in discourse. In (8), the choice of my YOUNGER sister as topic indicates
that there are other possible topics that can be described by my X sister; and
indeed, the second clause, my OLDER sister identifies such an alternative
topic. Contrastive topics are to be expected whenever a question is too
complex to be answered on the basis of one single topic; they indicate a
particular answering strategy by introducing subtopics (cf. Büring 2003).
There is a phenomenon that is somewhat related to topichood, which has
been called frame setting (cf. Jacobs 2001).
(9)

A: How is John?
B: {Healthwise / As for his health}, he is FINE.

This is a statement about John, but it is restricted to those aspects that
concern John’s health (in contrast, e.g., to his financial sitation). We call
phrases like healthwise “frame setters”. Clearly, focus plays a role with
frame setters, just as with contrastive topics, as they express a certain
restriction of the ensuing predication to some perspective that is not clearly
identified by the context already – if the health perspective were already
established, there would be no need to express it explicitly. Frame setters
restrict context-sensitive expressions, like be fine, to the specified
dimension, or delimit the predications that can be made. For example, As for
his health, he had a serious flu recently is fine, but As for his financial
situation, he has a serious flu recently is not.
There is an obvious similarity between contrastive topics and frame setters
that is reflected in the way how these information-structural functions are
marked (e.g., by a B-accent in English, or by the postposition nun in
Korean). Both express that the predication is restricted in some way – e.g.,
(8b) restricts the predication plays the violin to the younger sister (where the
expected value is the sisters in general), and (9) restricts the predication to
aspects concerning John’s health. It is useful to introduce a new term for
this function: delimitation. It is a genuin phenomenon of IS, as it responds to
the current informational need of the addressee: It is indicated that the issue
at hand is broader, and that the ensuing speech act concerns only a part of
this more general issue. Hence, delimitation can be defined as follows:
(10) Definition of Delimitation
A delimiter α in within an expression […α….] always comes
with a focus within α indicating alternatives α′, α″ etc.

6

It indicates that the current informational need is not totally
satisfied by […α…] but would be satisfied by additional
expressions [---α′---], [α″---] etc.
We do not claim that the notions of focus, givenness, topic, frame setting
and delimitation exhaust what there is to say about IS. For example, in an
argumentative discourse, the current informational need might dictate the
selection and ordering of arguments to gain support for a particular
conclusion. But such effects go beyond the limit of the sentence, and relate
it to discourse structure. Here we will stay within the confines of the
sentence (in a particular context), and we will try to illustrate some of the
ways in which the IS notions specified above are expressed in languages.

2. The expression of information structure
How do languages mark the various IS distinctions? While there is
considerable variety in the strategies that we find in different languages,
they always have a relationship to prosody: focus tends to be prosodically
prominent, and givenness tends to be prosodically non-prominent, while
topic tends to form a separate prosodic phrase, and is thus also prominent
(the same holds for frame setters and delimiters in general). But this
prosodic connection is achieved by different grammatical correlates in
different languages, depending on the languages’ general properties. And
languages differ in the obligatoriness of expressing IS distinctions; for
example, it has been shown that in Northern Sotho (Bantu) and Hausa do
not express the focus of answers as rigidly as English (cf. Zerbian 2006,
Hartmann & Zimmermann 2007).
In English, focus and topic correlate with pitch accents, and givenness is
often expressed by deaccenting, see (1) and (2). But in a number of Asian
and African languages, pitch accents only play a minor role, if at all, and
morphological and syntactic means are prevalent. In tone languages,
phrasing can replace the pitch accents of intonation languages, and particles
can play the role of boundary tones. An extreme case of prosodic marking
of IS is ellipsis, where only the focused part of a sentence is pronounced,
and the given part is just deleted.
Following Jackendoff (1972), we assume that IS roles are identified at the
surface syntactic structure by features, in the way shown in (11) to (13).
(11) a. We only saw a tiger [on the ROAD]F.
b. We only saw [a TIGER]F on the road.
(12) {What did you see on the road?}
[We saw]G [a TIGER]F [on the road]G.

7

(13) [As for tigers,]T [we saw one on the road]F.
We examine syntactic, phonological and morphological reflexes of IS in the
next subsection, and show in each case how they relate to prosody.
2.1 Sentence Position
First, IS roles are often associated with sentence positions. Halliday (19678) holds that the initial position is a necessary condition for a ‘theme’ (a
topic in our terminology). This preferred place for a topic is easily
explained: since it is the address at which the infomration of the sentence is
supposed to be stored, it makes sense to introduce it right at the beginning.
However, topic are not necessarily located sentence-initially. In the
following Korean sentence, the topic dezaato-wa ‘dessert’ is placed after a
quantifier phrase and is thus not initial.1 A subscript P shows a prosodic
phrase (p-phrase), and a subscript I a larger intonation phrase (i-phrase).
(14) ((Nwukwuna-ka)P ([dessert-nun]T)P (ice cream-ul mek-ess-ta)P)I
everyone-NOM
dessert-TOP
ice cream-ACC eat-PAST-DEC
‘As for dessert, everyone ate ice cream.’
Sentence-final topics, sometimes called ‘anti-topics’ are also possible, as
illustrated in (15) for Cantonese and (16) for French.
(15) ((Go loupo)P (nei gin-gwo gaa)P, ([ni go namjan ge]T)P)I.
CLF wife
2.SG see-EXP DSP this CLF man DSP
‘The wife you have seen, of this man.’
(16) ((Pierre l’ a mangée)P, ([la pomme]T)P)I.
Peter it-ACC has eaten, the apple
‘Peter has eaten the apple.’
The common property of topics is their separation from the remainder of the
sentence. They tend to form their own i-phrase (intonation phrase), which
allows for a clear intonational separation. In languages like Japanese or
Cantonese, particles not only signal the role of the constituent as a topic, but
also add place for a boundary tone. This allows the topic in (16) to be
inserted in a non-initial position.
Focus has also been associated with special positions in certain languages.
Hungarian has been described as a language which obligatorily places an
exhaustive focus preverbally (É. Kiss 1998), and Italian as a language with
clause-initial (Rizzi 1997) or clause-final (Samek-Lodovici 2006) foci.

1

Thanks to Shin-Sook Kim for providing this sentence.

8

An alternative explanation, which accounts for the Hungarian facts without
forcing an association between focus and preverbal position, assumes that
Hungarian is phonologically a left-headed language, both for prosodic
words and prosodic phrases. Focus wants to be prominent and the preferred
stress position is at the beginning of the main i-phrase, directly after the iphrase of the topic. The initial position is occupied by the narrow focus, as
often as possible, and happens to be the verb in all other cases (see Szendrői
2003). But focus may also be located postverbally: In (17), both the VP and
the dative object are focused and the accusative object is given, but the
dative object is postverbal. In such cases, focus is indicated by pitch accent
only.
(17) ((Tegnap este)P)I ((BEMUTATTAM Pétert)P (MARINAK)P)I.
yesterday evening PRT-introduced-I Peter.ACC Mary.DAT
‘Yesterday evening, I introduced Peter to Mary.’
In Italian, given elements may be moved out of the matrix clause, and
typically, it is this movement which causes finality of focus. In (18), adapted
from Samek-Lodovici (2006), Parigi is the focus, and the following
constituents are right-dislocated as they are given. Italian is a language with
final stress, both at the level of the p-word, where it is trochaic, and at the
level of the p-phrase, and syntactic reorganization helps prosody in moving
narrow foci to the furthest possible rightward position. Thus, both in
Hungarian and in Italian the peripheral position of focus is not a special
feature of focus, but reflects the general preference for prominence.
(18) ((L’ho
incontrato [a PARIGI]F)P, (Luigi)P, (ieri)P)I.
(I) him have-met in Paris,
Luigi, yesterday
‘I met Luigi in Paris yesterday.’
2.2 Accents
There have been numerous attempts in the literature to relate specific
information roles to the form of pitch accents. Bolinger (1958) introduces a
distinction between accent A, a falling accent, and accent B, a fall-rise
accent, for English, and Jackendoff (1972) and Liberman & Pierrehumbert
(1984) relates the former to focus and the latter to topic, as in (19). Manny
has accent B, and Anna accent A.
(20) {What about Manny? Who did he come with?}
(([MANNY]T)P (came with [ANNA]F)P)I.
Büring (2003), for German, and Steedman (2000), for English, establish an
obligatory relationship between contours and roles by having pitch accent
contours participate in the definition of topics and foci. Attempts to relate
forms of accents to specific IS roles are found for other languages as well.

9

For instance, Frota (2000) claims that narrow foci in Portuguese are always
associated with a certain kind of accent.
An alternative explanation is possible which only indirectly relates IS to the
forms of accents.. The preference for associating specific contours with IS
roles can be explained by general properties of the language. As far as
topics are concerned, the preference for sentence-initiality is paired with a
preference for rising tones. The rising tone is just a reflex of the non-finality
of this accent. And the falling contour often found on focus may be related
to the late position of a focus in a sentence.
Do languages with pitch accents necessarily use them for topics and foci?
The question bears on the necessity of accents (and of deaccenting) in
general in relation to focus/topic/givenness. Jackendoff formulates a rule
which directly relates a focus with an accent. ‘If a phrase P is chosen as the
focus of a sentence S, the highest stress in S will be on the syllable of P that
is assigned highest stress by the regular stress rules’ (1972:247). Nearly all
models relating focus with phonology rely on a direct correspondence
between semantics and phonetics and require an accent signaling the
presence of a focused constituent (see for instance Cinque 1993, Rooth
1992, Selkirk 1995, Schwarzschild 1999, and many others).
But in fact, there are examples in which the association between focus and
accent seems to be cancelled. One such case is the so-called Second
Occurrence Focus (cf. Partee 1999, Beaver et al. 2007, Féry & Ishihara
2005), which combines elements of association with focus and givenness.
Only vegetables in (21b) is associated with the focus operator only, and is
thus a focus, but it is also given, because it is repeated from (21a).
(21) a. {Everyone already knew that Mary only eats [vegetables]F}.
b. If even [Paul]F knew that Mary only eats [vegetables]SOF,
then he should have suggested a different restaurant.
There are only weak correlates of accent, and no pitch excursions in the
postnuclear position, although Féry & Ishihara (2005) show that a pitch
accent is indeed present in the prenuclear position.
Other cases of absence of accent on a focus are a consequence of avoidance
of stress-clash and the consequent deaccenting. In (18a), herself is a socalled intensifier which is claimed to be obligatorily accented in the
literature. But in the presence of an adjacent narrow focus, accent on herself
disappears. The same is true of the association with focus adjacent to a
parallel focus in (18b), cf. Rooth (1992). In (18c), the answer to the question
is completely deaccented. Instead the additive particle also carries the stress.
(18d), from Reis & Rosengren (1997), shows that a contrastive topic (Peter
in Krifka’s 1999 analysis) can also be realized without excursion if another,
more prominent topic (Gauguin) is adjacent.
(22) a. Marie-Luise even grows RICE herself.
b. People who GROW rice only EAT rice.

10

c. {John said that Mark is coming, but what did Sue say?}
She ALSO said that Mark is coming.
d. {Boy, Paul possesses a Gauguin.}
Einen GAUGUIN besitzt Peter AUCH
‘Peter also owns a Gauguin’
We see that there is no strict association between focus and accent or topic
and accent. Accent is a preferred option but it is not obligatory. It is only
present if the phonological structure of the sentence allows it. To sum up,
the preference for associating some specific contours with IS roles, or just
pitch accents can be explained by general properties of the language.2
2.3 Morphological markers
Morphological markers are compatible with the general claim of this section
that the marking of focus and topic is always prosodically prominent if can
be shown that the presence of a particle change the prosody of the sentence
it appears in. Examples confirming this claim appeared in (11) for Japanese
and in (15) for Cantonese topic markers. A number of tone languages have
been studied as for their focus realizations which do not seem to have other
correlates of IS than optional presence of morphological markers. Examples
involving morphological markers for focus appear in the Gur languages Buli
in (23) and Ditammari in (24), both from Fiedler et al. (to appear). In Buli,
the focus marker kà precedes the focused constituent. But when the focused
túé is sentence-initial, the marker kà is not obligatory. As for Ditammari, the
focus marker nyā follows the focused constituent.
(23) Q: What did the woman eat?
A: ò
ŋòb kà túé.
3.SG eat FM beans
‘She ate BEANS.’
(24) Q: What did the woman eat?
A: ò dī yātũrà nyā.
3.SG eat beans FM
‘She ate BEANS.’
These markers have a delimiting function in creating a prosodic boundary.
We thus propose that the prosodic connection of the focus and topic markers
is to be found in the phrasing properties of a constituent delimited by such a
marker. Even if not enough is known about the exact behavior of particles, it

2

Many languages do not use pitch accents to highlight a focused element, but rather
raise the pitch register of a focused phrase as a whole. This happens for instance in
Mandarin Chinese (Xu 1999), in Korean, in Georgian and in Hindi.

11

seems to be a valid generalization that they always appear at the periphery
of the constituent they mark.
In languages without special markers for IS, that is in languages which do
not have pitch accent and which have only optional morphological markers,
the answer to wh-question typically involves ellipsis of the given material.
Only the constituent in focus is realized, a strategy which we propose to
analyze in prosodic terms.
3.

To conclude

This paper has proposed definitions of IS concepts in a model of
information exchange that makes use of the notion of Common Ground. We
also argued – in the limited place available – that there are universal
principles in the way how IS roles are coded. One tendency, the tonal
prominence of focused and the de-prominence of given expresseions, can be
explained by the effort code (Gussenhoven 2004): Higher or lower
expenditure in prosodic explicitness reflects the importance of
subconstituents for communication. Others, like the tendency of topics to be
prosodically separated from and to precede the rest, can be traced back to
the optimal flow of information transmission. In spite of such
commonalities, we have pointed out drastic differences in the ways how
they are realized in the grammars of individual languages.

References
Beaver, David, Brady Zack Clark, Edward Flemming, T. Florian Jäger &
Maria Wolters. 2007. When semantics meets phonetics: Acoustical
studies of second occurrence focus. Language 83. 245-276.
Bolinger, Dwight. 1958. A theory of pitch accent in English. Word 14. 109–
149.
Büring, Daniel. 2003. On D-trees, beans, and B-accents. Linguistics and
Philosophy 26, 511–545.
Chafe, Wallace L. 1976. Givenness, contrastiveness, definiteness, subjects,
topics and point of view. in Charles N. Li (ed.), Subject and Topic.
27–55. New York: Academic Press.
Cinque, Guglielmo. 1993. A null theory of phrase and compound stress.
Linguistic Inquiery 24: 239-297.
Féry, Caroline, Gisbert Fanselow & Manfred Krifka (eds.), The notions of
Information Structure. Working Papers of the SFB632,
Interdisciplinary Studies on Information Structure (ISIS) 6. Potsdam:

12

Universitätsverlag
potsdam.de/isg.html.

Potsdam.

http://www.sfb632.uni-

Féry, Caroline & Shinichiro Ishihara. 2005. Interpreting second occurrence
focus. Ms. University of Potsdam.
Fiedler, Ines, Katharina Hartmann, Brigitte Reineke, Anne Schwarz &
Malte Zimmermann. To appear. Subject focus in West African
languages. In Zimmermann, Malte & Caroline Féry, Eds. Information
Structure in Different Perspectives.
Frota, Sónia. 2000. Prosody and Focus in European Portuguese. New York:
Garland Publishing.
Gussenhoven, Carlos. 2004. The phonology of tone and intonation,
Cambridge University Press, Cambridge.
Halliday, M. A. K. 1967-68 Notes on transitivity and theme in English.
Journal of Linguistics 3 & 4.É. Kiss, Katalin (1998), “Identificational
focus versus information focus”, Language 74. 245–273.
Hartmann, Katharina & Malte Zimmermann. 2007. In place -- out of place:
Focus in Hausa.”, in Kerstin Schwabe & Susanne Winkler, On
information structure, meaning and form, Amsterdam, John
Benjamins, 365-403.
Horváth, Júlia. 1986. Focus in the Theory of Grammar and the Syntax of
Hungarian. Dordrecht: Foris.
Hyman, Larry & Maria Polinsky. To appear. Is there a focus position in
Aghem? In: Zimmermann, Malte & Caroline Féry, Eds. Information
Structure in Different Perspectives.
Jackendoff, Ray. 1972. Semantic Interpretation in Generative Grammar.
Cambridge, MA: MIT Press.
Jacobs, Joachim. 2001. The Dimensions of Topic-Comment. Linguistics 39:
641-681
Krifka, Manfred. 1999. Additive particles under stress. Proceedings of SALT
8. Cornell, CLC Publications. 111–128.
Liberman, Mark & Janet Pierrehumbert. 1984. Intonational invariance under
changes in pitch range and length. In Language Sound Structure, eds.
Mark Aronoff & Richard T. Oehrle. Cambridge, MA: MIT Press.
157–233.
Partee, Barbara H. 1999. Focus, quantification, and semantics-pragmatics
issues. In Focus: Linguistic, Cognitive, and Computational
Perspectives, eds. Peter Bosch & Rob van der Sandt. Cambridge:
Cambridge University Press. 213–231.
Reinhart, Tanya. 1982. Pragmatics and linguistics. An analysis of sentence
topics. Philosophica 27. 53–94.

13

Reis, Marga & Inger Rosengren. 1997. A modular approach to the grammar
of additive particles: The case of German auch. Journal of Semantics
14. 237–309.
Rizzi, Luigi. 1997. The fine structure of the left periphery. In Elements of
Grammar: Handbook in Generative Syntax, ed. Liliane Haegeman,
Dordrecht: Kluwer Academic Publishers. 281–337.
Rochemont, M. 1986. Focus in generative grammar. Amsterdam/
Philadelphia: John Benjamins Publishing Company.
Rooth, Mats. 1992. A theory of focus interpretation. Natural Language
Semantics 1. 75–116.
Samek-Lodovici, Vieri. 2006. When right dislocation meets the leftperiphery: A unified analysis of Italian non-final focus. Lingua 116.
836–873.
Schwarzschild, Roger. 1999. GIVENness, AvoidF and other constraints on
the placement of accent. Natural Language Semantics 7. 141–177.
Selkirk, Elisabeth O. 1995. Sentence prosody: Intonation, stress and
phrasing. In Handbook of Phonological Theory, ed. John Goldsmith.
Cambridge, MA: Blackwell. 550–569.
Stalnaker, Robert. 1974. Pragmatic presuppositions. in Milton K. Munitz &
Peter K. Unger (eds.), Semantics and Philosophy. 197–214. New
York: New York University Press.
Steedman, Mark. 2000. Information Structure and the Syntax-Phonology
Interface. Linguistic Inquiry 31. 649–689.
Szendrői, Kriszta. 2003. A stress-based approach to the syntax of Hungarian
focus. The Linguistic Review 20 (1). 37–78.
Xu, Yi. 1999. Effects of tone and focus on the formation and alignment of
F0 contours. Journal of Phonetics 27, 55–105.
Zerbian, Sabine. 2006. Expression of Information Structure in the Bantu
Language Northern Sotho, Doctoral diss., Humboldt University,
Berlin.

14

FOLKER: An Annotation Tool For Efficient
Transcription Of Natural, Multi-Party Interaction
Thomas Schmidt, Wilfried Schütte
SFB 538 ‘Multilingualism’
Max Brauer-Allee 60
D-22765 Hamburg
E-mail: thomas.schmidt@uni-hamburg.de, schuette@ids-mannheim.de

Abstract
This paper presents FOLKER, an annotation tool developed for the efficient transcription of natural, multi-party interaction in a
conversation analysis framework. FOLKER is being developed at the Institute for German Language in and for the FOLK project,
whose aim is the construction of a large corpus of spoken present-day German, to be used for research and teaching purposes.
FOLKER builds on the experience gained with multi-purpose annotation tools like ELAN and EXMARaLDA, but attempts to improve
transcription efficiency by restricting and optimizing both data model and tool functionality to a single, well-defined purpose. This
paper starts with a description of the GAT transcription conventions and the data model underlying the tool. It then gives an overview
of the tool functionality and compares this functionality to that of other widely used tools.

establishment of a best practice in our field.

1.

Introduction

This paper presents FOLKER, an annotation tool
developed for the efficient transcription of natural,
multi-party interaction in a conversation analysis
framework. FOLKER is being developed at the Institute
for German Language in and for the FOLK project, whose
aim is the construction of a large corpus of spoken
present-day German, to be used for research and teaching
purposes (see FOLK 2010). FOLKER builds on the
experience gained with multi-purpose annotation tools
like ELAN and EXMARaLDA (Rohlfing et al. 2006), but
attempts to improve transcription efficiency by restricting
and optimizing both data model and tool functionality to a
single, well-defined purpose.

2.

FOLK Corpus

FOLK (Forschungs- und Lehrkorpus) is the "Research
and Teaching Corpus of Spoken German". Recognizing
that there is, to date, no larger, systematically stratified
collection of publicly available recordings of authentic
spoken interaction, let alone a consistent set of
corresponding, computer-accessible transcriptions for
German, the Pragmatics Department of the Institute for
German Language (IDS) started to set up FOLK in 2008.
Recordings for the corpus are partly collected from other
sources (the institute's spoken language archive and other
corpora of talk in interaction collected outside the IDS),
partly done from scratch for the project. The aim is to
cover a broad spectrum both in terms of regional variation
and in terms of different interaction types. All recordings
are transcribed within the project. In order to ensure a
high level of consistency, an efficient transcription
workflow, high community acceptance and good
automatic processability of the data, a group of
conversation analysis researchers and a group of software
developers were actively involved in the planning stage of
the corpus. By coordinating corpus development, the
specification of transcription conventions (see next
section) and the development of an annotation tool (i.e.
FOLKER) in this way, we also hope to contribute to the

3.

GAT Transcription Conventions

The GAT transcription conventions (see Selting et al.
1998 and 2009) are a de-facto standard in Germany for the
transcription of natural interaction in conversation
analytic research. Since, however, they originally
disregarded the question of an adequate computer
encoding, an initiative was started to revise the
conventions and modify them such that GAT
transcriptions could be represented in a formal data model
and a corresponding XML file format (see Schmidt 2007
and Schmidt et al. 2008, also next section).
0001 PRE good evening
0002 AUD ((laughter))
0003 PRE i have with me (.) tonight (0.3)
ann elk
0004
mistress ann elk
0005 ELK (0.2) miss
0006 PRE (0.7) you
0007
°hh have a new theory the
brontosaurus
0008 ELK well ehm can i just eh say here
chris for one moment
that i have a new theory about the
0009
brontosaurus
0010 AUD ((laughter))
0011 PRE exactly
0012 AUD ((laughter))
0013 PRE what is it
Figure 1: GAT transcript
The revised version of GAT (Selting et al. 2009) now
defines three transcription levels which correspond to
different degrees of prosodic detail. FOLK and FOLKER
only make use of the first of these levels – the minimal

2091

Figure 2: Single timeline, multiple tiers (STMT) data model
transcript – which provides rules for the transcriptions of
words in a modified orthography, for the description of
pauses, breathing and non-phonological phenomena
(coughing, laughing etc.) and for the handling of
uncertain or incomprehensible passages. Figure 1 gives an
example of a GAT minimal transcription as a plain text
file.1

4.

Data Model / Data Format

FOLKER's basic data model is derived from the
single-timeline-multiple-tiers (STMT) data model
described in (Schmidt 2005), i.e. it conceptualizes a
transcription as a set of annotations which are assigned
via a start and an end point to a single, fully ordered
timeline, and which are partitioned into a number of tiers
such that no two annotations within a tier overlap.
<folker-transcription>
<speakers>
<speaker id="PRE"/>
<speaker id="ELK"/>
<speaker id="AUD"/>
</speakers>
<recording path="./MyTheory.wav"/>
<timeline>
<!-- [...] -->
<timepoint id="T5" time="18.79"/>
<timepoint id="T6" time="20.18"/>
<timepoint id="T7" time="23.08"/>
<timepoint id="T8" time="25.99"/>
<!-- [...] -->
</timeline>
<!-- [...] -->
<contribution speaker="ELK" start="T5" end="T6">
<pause duration="0.2"/>
<w>miss</w>
</contribution>
<contribution speaker="PRE" start="T6" end="T8">
<pause duration="0.7"/>
<w>you</w>
<time reference="T7"/>
<breathe type="in" length="2"/>
<w>have</w>
<w>a</w>
<w>new</w>
<w>theory</w>
<w>about</w>
<w>the</w>
<w>brontosaurus</w>
</contribution>
</folker-transcription>

Figure 3: Folker data format

1

We use an English example here for illustration purposes.
In the FOLK project, FOLKER is of course used
exclusively for transcriptions of German.

As a further specification and restriction, the FOLKER
data model requires that each tier be assigned to a speaker,
and that no two tiers can be assigned to the same speaker.
Figure 2 illustrates this for an excerpt from the above
example.
While this data model represents the temporal structure of
events, including possible overlaps between different
speakers, further structure is added by combining adjacent
annotations into contributions and re-segmenting them
into the entities defined in the transcription convention
(i.e. words, pauses etc.) with the help of a finite state
transducer (the process is described in more detail in
Schmidt 2005). The resulting data structure can be
serialized into a TEI-like XML format as illustrated in
figure 3. Since the temporal information in the data model
is structurally compatible with the data models of tools
like ELAN or EXMARaLDA, FOLKER can provide
export filters for these tools.

5.

Tool functionality

FOLKER's main interface offers three editable views of
the transcription data. Each of these views is optimized
for a specific step in the transcription workflow, and users
can freely switch between the views at any time in the
transcription process.

5.1 Segment view
The segment view, illustrated in figure 4, is most efficient
for initial transcription. It displays individual annotations
in a vertical list, thus optimally exploiting screen real
estate and giving the transcriber a more text-like feeling
of the transcription than horizontally organized display
methods (like musical scores) do. Speaker assignment,
annotation text and temporal assignment can be freely
modified in this view and individually for each
annotation.
Using a regular expression, the syntax of the annotation
text is checked during input for conformance with the
transcription conventions. Errors (as the missing closing
bracket in line 10 in figure 4) are indicated by a red X in
the column labelled 'Syntax'. Likewise, the temporal
integrity of annotations is verified. If two annotations
belonging to the same speaker overlap (which the data
model prohibits), this is indicated in the column labelled
'Zeit'.

5.2 Partitur view
The Partitur (musical score) view, illustrated in figure 5,
displays the same transcription in a horizontal layout,

2092

organised into tiers. This view, which is comparable to the
main interfaces of tools like ANVIL, ELAN,
EXMARaLDA or Praat, is best suited for editing temporal
relations, most prominently speaker overlap. Important
operations in this view include splitting and merging
annotations and shifting characters between annotations.

5.3 Contribution view
The contribution view, finally, also uses a vertically
organised layout, but, instead of individual annotations,
displays adjacent annotations of speakers as contributions.
This view is thus close to a traditional, drama-script like
representation of an interaction, complying with
established reading habits. It is therefore best suited for
final proof-reading and corrections of a transcript. As in
the segment view, additional columns give information
about the syntactic correctness and temporal integrity of
the transcribed data.

5.4 Audio alignment
Navigation in all three views is synchronized with
navigation in (a waveform visualization of) the audio
recording. The audio player provides buttons for playing
or looping the current selection, and for playing the last
second of the current selection, the latter functionality
being important for an efficient fine tuning of segment
boundaries. This fine tuning can be carried out either by
dragging segment boundaries with the mouse, by
scrolling the mouse wheel up or down in the vicinity of a
boundary, or by using keyboard shortcuts. Since segments
refer to an explicit timeline, rather than directly to times in
the recording, a modification of a segment boundary often
also affects the boundaries of neighbouring segments.
This makes it easier for the transcriber to keep the
temporal integrity of annotations intact.

tools in general. Quite to the contrary, we believe that
researchers should be encouraged to exploit the growing
interoperability between different solutions and use tools
with different specializations for different tasks in their
corpus construction workflows. In such a workflow, we
see FOLKER as a tool for the creation of base
transcriptions which can later be supplemented with
additional annotations through other tools.
We restrict our comparison to tools of which we know that
they have been used to construct conversation or
discourse corpora.

6.1 FOLKER vs. Transana and similar tools
As far as their transcription functionality is concerned,
tools like Transana2 are basically combinations of a text
editor with a media player, i.e. they offer the possibility to
type a free, possibly formatted text (the transcription) and
add some special functions for navigating in a recording
and for linking pieces of text to that recording.
While the similarity of these tools to ordinary text
processing software makes them popular among “naive”
users, who value the resulting “ease of use”, they do
virtually nothing to ensure adequate computer
processability of the data – since their data structure is that
of a free text, the formats do not contain any structural
information that could be systematically exploited by a
database or some other advanced application. FOLKER is
much more ambitious in this respect – it produces data in
an open standard format which validates against a specific
data schema in which all the relevant information is
encoded in a methodical manner. It is thus suitable for
reliable and systematic querying and for other types of
(semi-)automatic processing.

6.2 FOLKER vs. CLAN
5.5 Other functionality
Additional functionality of FOLKER includes:
 Support for multi-part transcriptions: this is
important for very long recordings (> 90 minutes)
whose transcription has to be distributed over several
documents in order to ensure a reasonable processing
performance;
 Export routines for EXMARaLDA and ELAN data
and an import routine for EXMARaLDA data;
 Visualisation functions for displaying a transcription
as a segment list, a musical score or a contribution list
in a browser or a text processor;
 Search and search&replace routines;
 Automatic procedures for filling gaps in the
transcription, for normalizing whitespace and for
measuring pauses.

6.

Comparison with other tools

In contrast to many other widely used transcription tools,
FOLKER is explicitly and consciously designed not as a
multi-purpose tool, but rather as a tool which supports one
specific (albeit widely used) annotation scenario in a
maximally efficient way. If we outline the tool’s particular
strengths in comparison with other tools in the remainder
of this section, we therefore do not want this to be
understood as a claim that FOLKER is superior to these

CLAN, the transcription and annotation software
provided by the CHILDES 3 system is comparable to
FOLKER insofar as it also closely tied to a specific
transcription convention (CHAT) and also has
functionality for checking the conformity of a transcribed
text with respect to these conventions.
FOLKER differs from CLAN, first of all, in its more
modern user interface. Second, FOLKER offers different
editable views on the data whereas CLAN is restricted to a
single, line-based view (comparable to FOLKER’s
contribution view).
Third, although CLAN is able to write an XML-based file
format, it does not use an underlying data model in the
strict sense of the word, i.e. in the sense of an
algebraically well-defined formalism. FOLKER, in
contrast, is based on the idea of annotation graphs (AGs),
or, more specifically, on the STMT subset of AGs, thus
making it much easier to validate and exploit certain
structural features of the data.

2
3

2093

http://www.transana.org/
http://childes.psy.cmu.edu/

Figure 4: Segment view

Figure 5: Partitur view

Figure 6: Contribution view

2094

On the other hand, CLAN is of course applicable to a
much wider range of annotation tasks. Most importantly,
it provides a large number of so-called dependent tiers in
which the main (transcription) tier can be supplemented
with further analytic information. FOLKER does not cater
for such multi-level annotation tasks.

6.3 FOLKER vs. Transcriber
Transcriber4 is comparable to FOLKER insofar as it was
also optimized to support a specific transcription and
annotation task (broadcast speech) in a maximally
efficient manner. Also similar to FOLKER, Transcriber
offers different views of the data – a line-based view
comparable to FOLKER’s contribution view and a
time-based view comparable to FOLKER’s partitur view.
However, unlike in FOLKER, only the first of these views
is editable in Transcriber, the second mainly serving as a
help for orientation in the whole document. Moreover,
FOLKER has a more general approach to the handling of
multiple speaker scenarios. Most importantly, Transcriber
treats overlaps as separate structural entities which are
assigned to multiple speakers. In FOLKER, on the other
hand, speaker assignment is carried out independently of
temporal speaker constellations, thus making the
representation of more complex types of speaker overlaps
(e.g. partial overlap between more than two speakers)
easier and more flexible.

6.4 FOLKER vs. ELAN and similar tools
Although the tools share a common basis in their
time-based data models, the functionality of ELAN5 (and
similar tools like, e.g., ANVIL) is generally much more
comprehensive than that of FOLKER. To start with,
ELAN supports different media types (audio and video)
and formats, while FOLKER is restricted to WAV audio.
Second (like CLAN and EXMARaLDA) ELAN is
designed for multi-level annotation whereas FOLKER is
restricted to one transcription layer per speaker. Third,
ELAN contains a multitude of functionality that is not
directly related to the transcription or annotation task, but
addresses more far-reaching needs like metadata
description or query. Fourth, unlike FOLKER, ELAN is
not fixed on a single annotation scenario, transcription
convention or coding scheme, but supports many such
scenarios.
In sum, FOLKER is thus a considerably less powerful
piece of software than ELAN, but this reduced
complexity also makes it more accessible for many users
with a lower level of technical know-how.
What furthermore distinguishes FOLKER from ELAN is
its approach to the representation of (possibly competing,
non-hierarchizable) temporal and linguistic structure of
speaker’s utterances. In ELAN, annotations in main tiers
referring to the timeline can be segmented (e.g. into words
or other tokens) on another tier using the concept of a
so-called “symbolic subdivision”. Different structural
divisions are thus represented on different layers of the
data model. In contrast, FOLKER (like EXMARaLDA)
integrates such linguistic segmentations and the temporal
structure of the discourse into one layer of the
representation (see figure 3 and Schmidt 2005).
4
5

http://trans.sourceforge.net/en/presentation.php
http://www.lat-mpi.eu/tools/tools/elan

6.5 FOLKER vs. EXMARaLDA Partitur-Editor
Since FOLKER’s data model is a subset of
EXMARaLDA’s6 data model and the tools also share a
fair proportion of their code base, there is a lot of
commonality between them. The main difference between
the tools is to be found at the user interface level. The
EXMARaLDA Partitur-Editor offers a single view on the
data which is more or less identical to FOLKER’s partitur
view. Like ELAN and in contrast to FOLKER,
EXMARaLDA supports multi-level annotation of
different media types and formats. Again, FOLKER is
thus a less powerful, but also a more easily accessible tool.
The close relation between the two tools, however, makes
it very easy to use them side-by-side.

7.

Availability / Outlook

FOLKER has now been tested for more than a year and is
confirmed to run reliably on different Windows operating
systems (XP, Vista and Windows 7). A Macintosh version
is also available, but this has received less attention in the
test phase.
For about a year, FOLKER has been used productively
not only inside the FOLK project, but also in other spoken
language corpus projects, for example at the Research
Centre on Multilingualism for the construction of a corpus
documenting language attrition in speakers of Italian.
At the current stage, no essential extension of the existing
functionality is planned, but we will continue to improve
and optimise the existing functionality.
A tool for orthographic normalization of FOLKER
transcriptions (i.e. for annotation of modified
orthographic forms in a GAT transcription with their
standard lemmas) is under construction.
FOLKER is available freely for use in academic research
and teaching. It can be downloaded after registration from
the website of the IDS Pragmatics Department at
http://agd.ids-mannheim.de/html/folker.shtml.

8.

Acknowledgements

The work described in this paper has been financed by the
Pragmatics Department of the Institute for the German
Language (IDS) in Mannheim. FOLKER is built in parts
on EXMARaLDA technology, developed at the Research
Centre on Multilingualism (University of Hamburg) with
a grant by the German Science Foundation (DFG).

9.

References

FOLK (2010). Website of the FOLK corpus at the IDS
Mannheim.
http://agd.ids-mannheim.de/html/folk.shtml
Rohlfing, K.; Loehr, D.; Duncan, S.; Brown, A.; Franklin,
A.; Kimbara, I.; Milde, J.; Parrill, F.; Rose, T.; Schmidt,
T.; Sloetjes, H.; Thies, A. & Wellinghoff, S. (2006)
Comparison of multimodal annotation tools —
workshop report. In: Gesprächsforschung (7), pp.
99-123.
Schmidt, T. (2007): Transkriptionskonventionen für die
computergestützte gesprächsanalytische Transkription.
In:
Gesprächsforschung
(8),
pp.
229-241.
http://www.gespraechsforschung-ozs.de.
6

2095

http://www.exmaralda.org

Schmidt, T.; Deppermann, A.; Hartung, M.; Schütte, W.
(2008) GAT: Aspekte der computertechnischen
Umsetzbarkeit. Technical report Universität Hamburg /
IDS Mannheim. http://www.exmaralda.org.
Schmidt, T. (2005): Time-based data models and the Text
Encoding Initiative's guidelines for transcription of
speech. In: Working papers in multilingualism, Series B
(62).
Selting, M.; Auer, P.; Barden, B.; Bergmann, J.;
Couper-Kuhlen, E.; Günthner, S.; Meier, C.; Quasthoff,
U.; Schlobinski, P. & Uhmann, S. (1998)
Gesprächsanalytisches Transkriptionssystem (GAT). In:
Linguistische Berichte 173, pp. 91-122.
Selting, M., Auer, P., Barth-Weingarten, D., Bergmann, J.,
Bergmann, P., Birkner, K., Couper-Kuhlen, E.,
Deppermann, A., Gilles, P., Günthner, S., Hartung, M.,
Kern, F., Mertzlufft, C., Meyer, C., Morek, M.,
Oberzaucher, F., Peters, J., Quasthoff, U., Schütte, W.,
Stukenbrock,
A.,
Uhmann,
S.
(2009).
Gesprächsanalytisches Transkriptionssystem 2 (GAT
2). In: Gesprächsforschung (10), pp. 353-402,
http://www.gespraechsforschung-ozs.de.

2096

FOLKER: An Annotation Tool For Efficient
Transcription Of Natural, Multi-Party Interaction
Thomas Schmidt, Wilfried Schütte
SFB 538 ‘Multilingualism’
Max Brauer-Allee 60
D-22765 Hamburg
E-mail: thomas.schmidt@uni-hamburg.de, schuette@ids-mannheim.de

Abstract
This paper presents FOLKER, an annotation tool developed for the efficient transcription of natural, multi-party interaction in a
conversation analysis framework. FOLKER is being developed at the Institute for German Language in and for the FOLK project,
whose aim is the construction of a large corpus of spoken present-day German, to be used for research and teaching purposes.
FOLKER builds on the experience gained with multi-purpose annotation tools like ELAN and EXMARaLDA, but attempts to improve
transcription efficiency by restricting and optimizing both data model and tool functionality to a single, well-defined purpose. This
paper starts with a description of the GAT transcription conventions and the data model underlying the tool. It then gives an overview
of the tool functionality and compares this functionality to that of other widely used tools.

establishment of a best practice in our field.

1.

Introduction

This paper presents FOLKER, an annotation tool
developed for the efficient transcription of natural,
multi-party interaction in a conversation analysis
framework. FOLKER is being developed at the Institute
for German Language in and for the FOLK project, whose
aim is the construction of a large corpus of spoken
present-day German, to be used for research and teaching
purposes (see FOLK 2010). FOLKER builds on the
experience gained with multi-purpose annotation tools
like ELAN and EXMARaLDA (Rohlfing et al. 2006), but
attempts to improve transcription efficiency by restricting
and optimizing both data model and tool functionality to a
single, well-defined purpose.

2.

FOLK Corpus

FOLK (Forschungs- und Lehrkorpus) is the "Research
and Teaching Corpus of Spoken German". Recognizing
that there is, to date, no larger, systematically stratified
collection of publicly available recordings of authentic
spoken interaction, let alone a consistent set of
corresponding, computer-accessible transcriptions for
German, the Pragmatics Department of the Institute for
German Language (IDS) started to set up FOLK in 2008.
Recordings for the corpus are partly collected from other
sources (the institute's spoken language archive and other
corpora of talk in interaction collected outside the IDS),
partly done from scratch for the project. The aim is to
cover a broad spectrum both in terms of regional variation
and in terms of different interaction types. All recordings
are transcribed within the project. In order to ensure a
high level of consistency, an efficient transcription
workflow, high community acceptance and good
automatic processability of the data, a group of
conversation analysis researchers and a group of software
developers were actively involved in the planning stage of
the corpus. By coordinating corpus development, the
specification of transcription conventions (see next
section) and the development of an annotation tool (i.e.
FOLKER) in this way, we also hope to contribute to the

3.

GAT Transcription Conventions

The GAT transcription conventions (see Selting et al.
1998 and 2009) are a de-facto standard in Germany for the
transcription of natural interaction in conversation
analytic research. Since, however, they originally
disregarded the question of an adequate computer
encoding, an initiative was started to revise the
conventions and modify them such that GAT
transcriptions could be represented in a formal data model
and a corresponding XML file format (see Schmidt 2007
and Schmidt et al. 2008, also next section).
0001 PRE good evening
0002 AUD ((laughter))
0003 PRE i have with me (.) tonight (0.3)
ann elk
0004
mistress ann elk
0005 ELK (0.2) miss
0006 PRE (0.7) you
0007
°hh have a new theory the
brontosaurus
0008 ELK well ehm can i just eh say here
chris for one moment
that i have a new theory about the
0009
brontosaurus
0010 AUD ((laughter))
0011 PRE exactly
0012 AUD ((laughter))
0013 PRE what is it
Figure 1: GAT transcript
The revised version of GAT (Selting et al. 2009) now
defines three transcription levels which correspond to
different degrees of prosodic detail. FOLK and FOLKER
only make use of the first of these levels – the minimal

2091

Figure 2: Single timeline, multiple tiers (STMT) data model
transcript – which provides rules for the transcriptions of
words in a modified orthography, for the description of
pauses, breathing and non-phonological phenomena
(coughing, laughing etc.) and for the handling of
uncertain or incomprehensible passages. Figure 1 gives an
example of a GAT minimal transcription as a plain text
file.1

4.

Data Model / Data Format

FOLKER's basic data model is derived from the
single-timeline-multiple-tiers (STMT) data model
described in (Schmidt 2005), i.e. it conceptualizes a
transcription as a set of annotations which are assigned
via a start and an end point to a single, fully ordered
timeline, and which are partitioned into a number of tiers
such that no two annotations within a tier overlap.
<folker-transcription>
<speakers>
<speaker id="PRE"/>
<speaker id="ELK"/>
<speaker id="AUD"/>
</speakers>
<recording path="./MyTheory.wav"/>
<timeline>
<!-- [...] -->
<timepoint id="T5" time="18.79"/>
<timepoint id="T6" time="20.18"/>
<timepoint id="T7" time="23.08"/>
<timepoint id="T8" time="25.99"/>
<!-- [...] -->
</timeline>
<!-- [...] -->
<contribution speaker="ELK" start="T5" end="T6">
<pause duration="0.2"/>
<w>miss</w>
</contribution>
<contribution speaker="PRE" start="T6" end="T8">
<pause duration="0.7"/>
<w>you</w>
<time reference="T7"/>
<breathe type="in" length="2"/>
<w>have</w>
<w>a</w>
<w>new</w>
<w>theory</w>
<w>about</w>
<w>the</w>
<w>brontosaurus</w>
</contribution>
</folker-transcription>

Figure 3: Folker data format

1

We use an English example here for illustration purposes.
In the FOLK project, FOLKER is of course used
exclusively for transcriptions of German.

As a further specification and restriction, the FOLKER
data model requires that each tier be assigned to a speaker,
and that no two tiers can be assigned to the same speaker.
Figure 2 illustrates this for an excerpt from the above
example.
While this data model represents the temporal structure of
events, including possible overlaps between different
speakers, further structure is added by combining adjacent
annotations into contributions and re-segmenting them
into the entities defined in the transcription convention
(i.e. words, pauses etc.) with the help of a finite state
transducer (the process is described in more detail in
Schmidt 2005). The resulting data structure can be
serialized into a TEI-like XML format as illustrated in
figure 3. Since the temporal information in the data model
is structurally compatible with the data models of tools
like ELAN or EXMARaLDA, FOLKER can provide
export filters for these tools.

5.

Tool functionality

FOLKER's main interface offers three editable views of
the transcription data. Each of these views is optimized
for a specific step in the transcription workflow, and users
can freely switch between the views at any time in the
transcription process.

5.1 Segment view
The segment view, illustrated in figure 4, is most efficient
for initial transcription. It displays individual annotations
in a vertical list, thus optimally exploiting screen real
estate and giving the transcriber a more text-like feeling
of the transcription than horizontally organized display
methods (like musical scores) do. Speaker assignment,
annotation text and temporal assignment can be freely
modified in this view and individually for each
annotation.
Using a regular expression, the syntax of the annotation
text is checked during input for conformance with the
transcription conventions. Errors (as the missing closing
bracket in line 10 in figure 4) are indicated by a red X in
the column labelled 'Syntax'. Likewise, the temporal
integrity of annotations is verified. If two annotations
belonging to the same speaker overlap (which the data
model prohibits), this is indicated in the column labelled
'Zeit'.

5.2 Partitur view
The Partitur (musical score) view, illustrated in figure 5,
displays the same transcription in a horizontal layout,

2092

organised into tiers. This view, which is comparable to the
main interfaces of tools like ANVIL, ELAN,
EXMARaLDA or Praat, is best suited for editing temporal
relations, most prominently speaker overlap. Important
operations in this view include splitting and merging
annotations and shifting characters between annotations.

5.3 Contribution view
The contribution view, finally, also uses a vertically
organised layout, but, instead of individual annotations,
displays adjacent annotations of speakers as contributions.
This view is thus close to a traditional, drama-script like
representation of an interaction, complying with
established reading habits. It is therefore best suited for
final proof-reading and corrections of a transcript. As in
the segment view, additional columns give information
about the syntactic correctness and temporal integrity of
the transcribed data.

5.4 Audio alignment
Navigation in all three views is synchronized with
navigation in (a waveform visualization of) the audio
recording. The audio player provides buttons for playing
or looping the current selection, and for playing the last
second of the current selection, the latter functionality
being important for an efficient fine tuning of segment
boundaries. This fine tuning can be carried out either by
dragging segment boundaries with the mouse, by
scrolling the mouse wheel up or down in the vicinity of a
boundary, or by using keyboard shortcuts. Since segments
refer to an explicit timeline, rather than directly to times in
the recording, a modification of a segment boundary often
also affects the boundaries of neighbouring segments.
This makes it easier for the transcriber to keep the
temporal integrity of annotations intact.

tools in general. Quite to the contrary, we believe that
researchers should be encouraged to exploit the growing
interoperability between different solutions and use tools
with different specializations for different tasks in their
corpus construction workflows. In such a workflow, we
see FOLKER as a tool for the creation of base
transcriptions which can later be supplemented with
additional annotations through other tools.
We restrict our comparison to tools of which we know that
they have been used to construct conversation or
discourse corpora.

6.1 FOLKER vs. Transana and similar tools
As far as their transcription functionality is concerned,
tools like Transana2 are basically combinations of a text
editor with a media player, i.e. they offer the possibility to
type a free, possibly formatted text (the transcription) and
add some special functions for navigating in a recording
and for linking pieces of text to that recording.
While the similarity of these tools to ordinary text
processing software makes them popular among “naive”
users, who value the resulting “ease of use”, they do
virtually nothing to ensure adequate computer
processability of the data – since their data structure is that
of a free text, the formats do not contain any structural
information that could be systematically exploited by a
database or some other advanced application. FOLKER is
much more ambitious in this respect – it produces data in
an open standard format which validates against a specific
data schema in which all the relevant information is
encoded in a methodical manner. It is thus suitable for
reliable and systematic querying and for other types of
(semi-)automatic processing.

6.2 FOLKER vs. CLAN
5.5 Other functionality
Additional functionality of FOLKER includes:
 Support for multi-part transcriptions: this is
important for very long recordings (> 90 minutes)
whose transcription has to be distributed over several
documents in order to ensure a reasonable processing
performance;
 Export routines for EXMARaLDA and ELAN data
and an import routine for EXMARaLDA data;
 Visualisation functions for displaying a transcription
as a segment list, a musical score or a contribution list
in a browser or a text processor;
 Search and search&replace routines;
 Automatic procedures for filling gaps in the
transcription, for normalizing whitespace and for
measuring pauses.

6.

Comparison with other tools

In contrast to many other widely used transcription tools,
FOLKER is explicitly and consciously designed not as a
multi-purpose tool, but rather as a tool which supports one
specific (albeit widely used) annotation scenario in a
maximally efficient way. If we outline the tool’s particular
strengths in comparison with other tools in the remainder
of this section, we therefore do not want this to be
understood as a claim that FOLKER is superior to these

CLAN, the transcription and annotation software
provided by the CHILDES 3 system is comparable to
FOLKER insofar as it also closely tied to a specific
transcription convention (CHAT) and also has
functionality for checking the conformity of a transcribed
text with respect to these conventions.
FOLKER differs from CLAN, first of all, in its more
modern user interface. Second, FOLKER offers different
editable views on the data whereas CLAN is restricted to a
single, line-based view (comparable to FOLKER’s
contribution view).
Third, although CLAN is able to write an XML-based file
format, it does not use an underlying data model in the
strict sense of the word, i.e. in the sense of an
algebraically well-defined formalism. FOLKER, in
contrast, is based on the idea of annotation graphs (AGs),
or, more specifically, on the STMT subset of AGs, thus
making it much easier to validate and exploit certain
structural features of the data.

2
3

2093

http://www.transana.org/
http://childes.psy.cmu.edu/

Figure 4: Segment view

Figure 5: Partitur view

Figure 6: Contribution view

2094

On the other hand, CLAN is of course applicable to a
much wider range of annotation tasks. Most importantly,
it provides a large number of so-called dependent tiers in
which the main (transcription) tier can be supplemented
with further analytic information. FOLKER does not cater
for such multi-level annotation tasks.

6.3 FOLKER vs. Transcriber
Transcriber4 is comparable to FOLKER insofar as it was
also optimized to support a specific transcription and
annotation task (broadcast speech) in a maximally
efficient manner. Also similar to FOLKER, Transcriber
offers different views of the data – a line-based view
comparable to FOLKER’s contribution view and a
time-based view comparable to FOLKER’s partitur view.
However, unlike in FOLKER, only the first of these views
is editable in Transcriber, the second mainly serving as a
help for orientation in the whole document. Moreover,
FOLKER has a more general approach to the handling of
multiple speaker scenarios. Most importantly, Transcriber
treats overlaps as separate structural entities which are
assigned to multiple speakers. In FOLKER, on the other
hand, speaker assignment is carried out independently of
temporal speaker constellations, thus making the
representation of more complex types of speaker overlaps
(e.g. partial overlap between more than two speakers)
easier and more flexible.

6.4 FOLKER vs. ELAN and similar tools
Although the tools share a common basis in their
time-based data models, the functionality of ELAN5 (and
similar tools like, e.g., ANVIL) is generally much more
comprehensive than that of FOLKER. To start with,
ELAN supports different media types (audio and video)
and formats, while FOLKER is restricted to WAV audio.
Second (like CLAN and EXMARaLDA) ELAN is
designed for multi-level annotation whereas FOLKER is
restricted to one transcription layer per speaker. Third,
ELAN contains a multitude of functionality that is not
directly related to the transcription or annotation task, but
addresses more far-reaching needs like metadata
description or query. Fourth, unlike FOLKER, ELAN is
not fixed on a single annotation scenario, transcription
convention or coding scheme, but supports many such
scenarios.
In sum, FOLKER is thus a considerably less powerful
piece of software than ELAN, but this reduced
complexity also makes it more accessible for many users
with a lower level of technical know-how.
What furthermore distinguishes FOLKER from ELAN is
its approach to the representation of (possibly competing,
non-hierarchizable) temporal and linguistic structure of
speaker’s utterances. In ELAN, annotations in main tiers
referring to the timeline can be segmented (e.g. into words
or other tokens) on another tier using the concept of a
so-called “symbolic subdivision”. Different structural
divisions are thus represented on different layers of the
data model. In contrast, FOLKER (like EXMARaLDA)
integrates such linguistic segmentations and the temporal
structure of the discourse into one layer of the
representation (see figure 3 and Schmidt 2005).
4
5

http://trans.sourceforge.net/en/presentation.php
http://www.lat-mpi.eu/tools/tools/elan

6.5 FOLKER vs. EXMARaLDA Partitur-Editor
Since FOLKER’s data model is a subset of
EXMARaLDA’s6 data model and the tools also share a
fair proportion of their code base, there is a lot of
commonality between them. The main difference between
the tools is to be found at the user interface level. The
EXMARaLDA Partitur-Editor offers a single view on the
data which is more or less identical to FOLKER’s partitur
view. Like ELAN and in contrast to FOLKER,
EXMARaLDA supports multi-level annotation of
different media types and formats. Again, FOLKER is
thus a less powerful, but also a more easily accessible tool.
The close relation between the two tools, however, makes
it very easy to use them side-by-side.

7.

Availability / Outlook

FOLKER has now been tested for more than a year and is
confirmed to run reliably on different Windows operating
systems (XP, Vista and Windows 7). A Macintosh version
is also available, but this has received less attention in the
test phase.
For about a year, FOLKER has been used productively
not only inside the FOLK project, but also in other spoken
language corpus projects, for example at the Research
Centre on Multilingualism for the construction of a corpus
documenting language attrition in speakers of Italian.
At the current stage, no essential extension of the existing
functionality is planned, but we will continue to improve
and optimise the existing functionality.
A tool for orthographic normalization of FOLKER
transcriptions (i.e. for annotation of modified
orthographic forms in a GAT transcription with their
standard lemmas) is under construction.
FOLKER is available freely for use in academic research
and teaching. It can be downloaded after registration from
the website of the IDS Pragmatics Department at
http://agd.ids-mannheim.de/html/folker.shtml.

8.

Acknowledgements

The work described in this paper has been financed by the
Pragmatics Department of the Institute for the German
Language (IDS) in Mannheim. FOLKER is built in parts
on EXMARaLDA technology, developed at the Research
Centre on Multilingualism (University of Hamburg) with
a grant by the German Science Foundation (DFG).

9.

References

FOLK (2010). Website of the FOLK corpus at the IDS
Mannheim.
http://agd.ids-mannheim.de/html/folk.shtml
Rohlfing, K.; Loehr, D.; Duncan, S.; Brown, A.; Franklin,
A.; Kimbara, I.; Milde, J.; Parrill, F.; Rose, T.; Schmidt,
T.; Sloetjes, H.; Thies, A. & Wellinghoff, S. (2006)
Comparison of multimodal annotation tools —
workshop report. In: Gesprächsforschung (7), pp.
99-123.
Schmidt, T. (2007): Transkriptionskonventionen für die
computergestützte gesprächsanalytische Transkription.
In:
Gesprächsforschung
(8),
pp.
229-241.
http://www.gespraechsforschung-ozs.de.
6

2095

http://www.exmaralda.org

Schmidt, T.; Deppermann, A.; Hartung, M.; Schütte, W.
(2008) GAT: Aspekte der computertechnischen
Umsetzbarkeit. Technical report Universität Hamburg /
IDS Mannheim. http://www.exmaralda.org.
Schmidt, T. (2005): Time-based data models and the Text
Encoding Initiative's guidelines for transcription of
speech. In: Working papers in multilingualism, Series B
(62).
Selting, M.; Auer, P.; Barden, B.; Bergmann, J.;
Couper-Kuhlen, E.; Günthner, S.; Meier, C.; Quasthoff,
U.; Schlobinski, P. & Uhmann, S. (1998)
Gesprächsanalytisches Transkriptionssystem (GAT). In:
Linguistische Berichte 173, pp. 91-122.
Selting, M., Auer, P., Barth-Weingarten, D., Bergmann, J.,
Bergmann, P., Birkner, K., Couper-Kuhlen, E.,
Deppermann, A., Gilles, P., Günthner, S., Hartung, M.,
Kern, F., Mertzlufft, C., Meyer, C., Morek, M.,
Oberzaucher, F., Peters, J., Quasthoff, U., Schütte, W.,
Stukenbrock,
A.,
Uhmann,
S.
(2009).
Gesprächsanalytisches Transkriptionssystem 2 (GAT
2). In: Gesprächsforschung (10), pp. 353-402,
http://www.gespraechsforschung-ozs.de.

2096

I S

K  F

I H

MIT S  E

A note about the lecture notes:
The notes for this course have been evolving for years now, starting with some
old notes from the early s by Angelika Kratzer, Irene Heim, and myself,
which have since been modiﬁed and expanded every year by Irene or myself.
Because this version of the notes has not been seen by my co-author, I alone am
responsible for any defects.
– Kai von Fintel, Spring 
This is a work in progress. We may eventually publish these materials as a followup volume to Heim & Kratzer’s Semantics in Generative Grammar, Blackwell
. In the meantime, we encourage the use of these notes in courses at other
institutions. Of course, you need to give full credit to the authors and you may
not use the notes for any commercial purposes. If you use the notes, we would
like to be notiﬁed and we would very much appreciate any comments, criticism,
and advice on these materials. We have already proﬁted from feedback sent in
by several people who have used the notes and the more the better.
Direct your communication to:
Kai von Fintel
Department of Linguistics & Philosophy
Room ·
Massachusetts Institute of Technology
 Massachusetts Avenue
Cambridge,  -
U S  A
ﬁntel@mit.edu
http://kaivonﬁntel.org
Here is the homepage for the course that these notes are designed for:
http://stellar.mit.edu/S/course//sp/.

Advice about using these notes
. These notes presuppose familiarity with the material, concepts, and notation of the Heim & Kratzer textbook.
. There are numerous exercises throughout the notes. It is highly recommended to do all of them and it is certainly necessary to do so if you at all
anticipate doing semantics-related work in the future.
. At the moment, the notes are designed to go along with explanatory lectures.
You should ask questions and make comments as you work through the
notes.
. Students with semantic ambitions should also at an early point start reading
supplementary material (as for example listed at the end of each chapter of
these notes).
. Lastly, prospective semanticists may start thinking about how they would
teach this material.

— T     —

C


Beginnings 
. Displacement 
. An Intensional Semantics in  Easy Steps
. Comments and Complications 
. Supplemental Readings 



 Propositional Attitudes 
. Hintikka’s Idea 
. Accessibility Relations 
. Suplemental Readings 


Modality 
. The Quantiﬁcational Theory of Modality 
. Flavors of Modality 
. *Kratzer’s Conversational Backgrounds 
. Supplementary Readings 

 Conditionals 
. The Material Implication Analysis 
. The Strict Implication Analysis 
. If -Clauses as Restrictors 
Supplemental Readings 


Ordering 
. The Driveway 
. Kratzer’s Solution: Doubly Relative Modality 
. The Paradox of the Good Samaritan 
. Non-Monotonicity of Conditionals 
Supplemental Readings 

 Basics of Tense and Aspect 
. A First Proposal for Tense 
. Are Tenses Referential? 
. The Need for Intervals 
. Aktionsarten 
. The Progressive 

. Tense in Embedded Clauses 
Supplemental Readings 
 DPs and Scope in Modal Contexts 
. De re vs. De dicto as a Scope Ambiguity 
. Raised subjects 
 Beyond de re — de dicto : The Third Reading 
. A Problem: Additional Readings and Scope Paradoxes 
. The Standard Solution: Overt World Variables 
. Alternatives to Overt World Variables 
. Scope, Restrictors, and the Syntax of Movement 
. A Recurring Theme: Historical Overview 
Bibliography 

C O
B
Language is the main instrument
of man’s refusal to accept the world
as it is.
George Steiner, After Babel, p. 

We introduce the idea of extension vs. intension and its main use: taking
us from the actual here and now to past, future, possible, counterfactual situations. We develop a compositional framework for intensional
semantics.

.

Displacement

.

An Intensional Semantics in  Easy Steps



..
..
.

Laying the Foundations
Intensional Operators




Comments and Complications 
..

Why Talk about Other Worlds? 

..

.

Intensions All the Way? 

..
.



The Worlds of Sherlock Holmes 

Supplemental Readings 

Displacement

Hockett () in a famous article (and a follow-up, Hockett & Altmann ())
presented a list of     . This list continues to
play a role in current discussions of animal communication. One of the design

Hockett, Charles F. .
The origin of speech. Scientiﬁc American . –

B



C 

features is . Human language is not restricted to discourse about
the actual here and now.
How does natural language untie us from the actual here and now? One
degree of freedom is given by the ability to name entities and refer to them even
if they are not where we are when we speak:
()

Thomas is in Hamburg.

This kind of displacement is not something we will explore here. We’ll take it
for granted.
Consider a sentence with no names of absent entities in it:
()

*The terms  and  descend from the Latin
modus, “way”, and are ancient
terms pertaining to the way a
proposition holds, necessarily,
contingently, etc.

It is snowing (in Cambridge).

On its own, () makes a claim about what is happening right now here in
Cambridge. But there are devices at our disposal that can be added to (),
resulting in claims about snow in displaced situations. Displacement can occur in
the  dimension and/or in what might be called the * dimension.
Here’s an example of temporal displacement:
()

At noon yesterday, it was snowing in Cambridge.

This sentence makes a claim not about snow now but about snow at noon
yesterday, a diﬀerent time from now.
Here’s an example of modal displacement:
()

See Kratzer (, ) for
more examples of modal
constructions.

If the storm system hadn’t been deﬂected by the jet stream, it would
have been snowing in Cambridge.

This sentence makes a claim not about snow in the actual world but about snow
in the world as it would have been if the storm system hadn’t been deﬂected by
the jet stream, a world distinct from the actual one (where the system did not hit
us), a merely  .
Natural language abounds in modal constructions. () is a so-called  . Here are some other examples:
()

M A
It may be snowing in Cambridge.

()

M A
Possibly, it will snow in Cambridge tomorrow.

()

P A
Jens believes that it is snowing in Cambridge.

 Steiner (: ) writes: “Hypotheticals, ‘imaginaries’, conditionals, the syntax of counterfactuality and contingency may well be the generative centres of human speech”.

§.

A I S   E S

()

H
Jane smokes.

()

G
Bears like honey.



The plan for this course is as follows. In Part , we explore modality and
associated topics. In Part , we explore temporal matters.
In this chapter, we will put in place the basic framework of 
, the kind of semantics that models displacement of the point of
evaluation in temporal and modal dimensions. To do this, we will start with one
rather special example of modal displacement:
()

In the world of Sherlock Holmes, a detective lives at B Baker Street.

() doesn’t claim that a detective lives at B Baker Street in the actual world
(presumably a false claim), but that in the world as it is described in the Sherlock
Holmes stories of Sir Arthur Conan Doyle, a detective lives at B Baker Street
(a true claim, of course). We choose this example rather than one of the more runof-the-mill displacement constructions because we want to focus on conceptual
and technical matters before we do serious empirical work.
The questions we want to answer are: How does natural language achieve
this feat of modal displacement? How do we manage to make claims about other
possible worlds? And why would we want to?
The basic idea of the account we’ll develop is this:
• expressions are assigned their semantic values relative to a possible world;
• in particular, sentences have truth-values in possible worlds;
• in the absence of modal displacement, we evaluate sentences with respect
to the “actual” world, the world in which we are speaking;
• modal displacement changes the world of evaluation;
• displacement is eﬀected by special operators, whose semantics is our primary concern here.
A terminological note: we will call the sister of the intensional operator its
, a useful term introduced by our medieval colleagues.

. An Intensional Semantics in  Easy Steps
..

Laying the Foundations

S : P W. Our ﬁrst step is to introduce possible worlds. This is
not the place to discuss the metaphysics of possible worlds in any depth. Instead,
we will just start working with them and see what they can do for us. Basically, a
possible world is a way that things might have been. In the actual world, there

Check out http://
bakerstreet.org/.



B

C 

are two coﬀee mugs on my desk, but there could have been more or less. So,
there is a possible world — albeit a rather bizarre one — where there are  coﬀee
mugs on my desk. We join Heim & Kratzer in adducing this quote from Lewis
(: f.):
The world we live in is a very inclusive thing. Every stick and every
stone you have ever seen is part of it. And so are you and I. And
so are the planet Earth, the solar system, the entire Milky Way, the
remote galaxies we see through telescopes, and (if there are such
things) all the bits of empty space between the stars and galaxies.
There is nothing so far away from us as not to be part of our world.
Anything at any distance at all is to be included. Likewise the world
is inclusive in time. No long-gone ancient Romans, no long-gone
pterodactyls, no long-gone primordial clouds of plasma are too far
in the past, nor are the dead dark stars too far in the future, to be
part of the same world. . . .
The way things are, at its most inclusive, means the way the entire world is. But things might have been diﬀerent, in ever so many
ways. This book of mine might have been ﬁnished on schedule. Or,
had I not been such a commonsensical chap, I might be defending
not only a plurality of possible worlds, but also a plurality of impossible worlds, whereof you speak truly by contradicting yourself. Or I
might not have existed at all — neither myself, nor any counterparts
of me. Or there might never have been any people. Or the physical
constants might have had somewhat diﬀerent values, incompatible
with the emergence of life. Or there might have been altogether
diﬀerent laws of nature; and instead of electrons and quarks, there
might have been alien particles, without charge or mass or spin
but with alien physical properties that nothing in this world shares.
There are ever so many ways that a world might be: and one of these
many ways is the way that this world is.
Previously, our “metaphysical inventory” included a domain of entities and a
set of two truth-values and increasingly complex functions between entities,
truth-values, and functions thereof. Now, we will add possible worlds to the
inventory. Let’s assume we are given a set W , the set of all possible worlds, which
is a vast space since there are so many ways that things might have been diﬀerent
from the way they are. Each world has as among its parts entities like you and me
and these coﬀee mugs. Some of them may not exist in other possible worlds. So,
strictly speaking each possible worlds has its own, possibly distinctive, domain
of entities. What we will use in our system, however, will be the grand union of
all these world-speciﬁc domains of entities. We will use D to stand for the set of
all possible individuals.

§.

A I S   E S



Among the many possible worlds that there are — according to Lewis, there
is a veritable plenitude of them — is the world as it is described in the Sherlock
Holmes stories by Sir Arthur Conan Doyle. In that world, there is a famous
detective Sherlock Holmes, who lives at B Baker Street in London and has a
trusted sidekick named Dr. Watson. Our sentence In the world of Sherlock Holmes,
a detective lives at B Baker Street displaces the claim that a famous detective
lives at B Baker Street from the actual world to the world as described in the
Sherlock Holmes stories. In other words, the following holds:
()

The sentence In the world of Sherlock Holmes, a detective lives at B
Baker Street is true in a world w iﬀ the sentence a detective lives at B
Baker Street is true in the world as it is described in the Sherlock Holmes
stories.

What this suggests is that we need to make space in our system for having devices
that control in what world a claim is evaluated. This is what we will do now.
S : T E W P. Recall from H& K that we
were working with a semantic interpretation function that was relativized to
an assignment function g, which was needed to take care of pronouns, traces,
variables, etc. From now on, we will relativize the semantic values in our
system to possible worlds as well. What this means is that from now on, our
interpretation function will have two superscripts: a world w and an assignment
g: · w,g .
So, the prejacent embedded in () will have its truth-conditions described
as follows:
()

a famous detective lives at B Baker Street w,g = 
iﬀ a famous detective lives at B Baker Street in world w.

It is customary to refer to the world for which we are calculating the extension
of a given expression as the  . In the absence of any shifting
devices, we would normally evaluate a sentence in the actual world. But then
there are shifting devices such as our in the world of Sherlock Holmes. We will
soon see how they work. But ﬁrst some more pedestrian steps: adding lexical
entries and composition principles that are formulated relative to a possible
world. This will allow us to derive the truth-conditions as stated in () in a
compositional manner.
 We will see in Section .. that this is not quite right. It’ll do for now.
 Recall from H& K, pp.f, that what’s inside the interpretation brackets is a mention of an
object language expression. They make this clear by bold-facing all object language expressions
inside interpretation brackets. In these notes, we will follow common practice in the ﬁeld and
not use a special typographic distinction, but let it be understood that what is interpreted are
object language expressions.

B



C 

S : L E. Among our lexical items, we can distinguish between
items which have a - semantic value and those that are worldindependent. Predicates are typically world-dependent. Here are some sample
entries.
()

Note the ruthless condensation
of the notation in (c) and (d).

For any w ∈ W and any assignment function g:
a.
famous w,g = λx ∈ D. x is famous in w.,
b.
detective w,g = λx ∈ D. x is a detective in w.
c.
lives-at w,g = λx ∈ D. λy ∈ D. y lives-at x in w.

The set of detectives will obviously diﬀer from world to world, and so will the
set of famous individuals and the set of pairs where the ﬁrst element lives at the
second element.
Other items have semantic values which do not diﬀer from world to world.
The most important such items are certain “logical” expressions, such as truthfunctional connectives and determiners:
()

a.
b.
c.
d.

and w,g = λu ∈ Dt . λv ∈ Dt . u = v = .
the w,g = λf ∈ D e,t : ∃!x. f(x) = . the y such that f(y) = .
every w,g = λf e,t . λg e,t . ∀xe : f(x) =  → g(x) = .
a/some w,g = λf e,t . λg e,t . ∃xe : f(x) =  & g(x) = .

Note that there is no occurrence of w on the right-hand side of the entries in
(). That’s the tell-tale sign of the world-independence of the semantics of these
items.
We will also assume that proper names have world-independent semantic
values, that is, they refer to the same individual in any possible world.
()

a.
b.
c.

Noam Chomsky w,g = Noam Chomsky.
Sherlock Holmes w,g = Sherlock Holmes.
B Baker Street w,g = B Baker Street.

S : C P. The old rules of Functional Application,
Predicate Modiﬁcation, and λ-Abstraction can be retained almost intact. We
just need to modify them by adding world-superscripts to the interpretation
function. For example:
 Of course, “λx ∈ D. . . . ” is short for “λx : x ∈ D. . . . ”. Get used to semanticists condensing
their notation whenever convenient!
 Always make sure that you actually understand what the notation means. Here, for example,
we are saying that the semantic value of the word famous with respect to a given possible world
w and a variable assignment g is that function that is deﬁned for an argument x only if x is a
member of the domain of individuals and that, if it is deﬁned, yields the truth-value  if and
only if x is famous in w.

§.

A I S   E S

()

F A (FA)
If α is a branching node and {β, γ} the set of its daughters, then, for
any world w and assignment g: if β w,g is a function whose domain
contains γ w,g , then α w,g = β w,g ( γ w,g ).



The rule simply passes the world parameter down.
S : T. Lastly, we will want to connect our semantic system to the
notion of the    . We ﬁrst adopt the “Appropriateness
Condition” from Heim & Kratzer (p.):
()

A C
A context c is appropriate for an LF φ only if c determines a variable
assignment gc whose domain includes every index which has a free
occurrence in φ.

We then intensionalize Heim & Kratzer’s deﬁnition of truth and falsity of
utterances:
()

T  F C  U
An utterance of a sentence φ in a context c in a possible world w is true
iﬀ φ w,gc =  and false if φ w,gc = .

E .: Compute under what conditions an utterance in possible world
w (which may or may not be the one we are all living in) of the sentence a
famous detective lives at B Baker Street is true. [Since this is the ﬁrst exercise of
the semester, please do this in excrutiating detail, not skipping any steps.]

..

Intensional Operators

So far we have merely “redecorated” our old system inherited from last semester.
We have introduced possible worlds into our inventory, our lexical entries and
our old composition principles. But with the tools we have now, all we can do
so far is to keep track of the world in which we evaluate the semantic value of
an expression, complex or lexical. We will get real mileage once we introduce
  which are capable of shifting the world parameter.
We mentioned that there are a number of devices for modal displacement. As
advertised, for now, we will just focus on a very particular one: the expression
in the world of Sherlock Holmes. We will assume, as seems reasonable, that this
expression is a sentence-modiﬁer both syntactically and semantically.
S : A S E. We begin with a heuristic step. We want
to derive something like the following truth-conditions for our sentence:
()

in the world of Sherlock Holmes,
a famous detective lives at B Baker Street

w,g

=



B

C 

iﬀ the world w as it is described in the Sherlock Holmes stories is such
that there exists a famous detective in w who lives at B Baker Street
in w .
We would get this if in general we had this rule for in the world of Sherlock
Holmes:
()

The diamond ♦ symbol for
possibility is due to C.I. Lewis,
ﬁrst introduced in Lewis &
Langford (), but he made
no use of a symbol for the dual
combination ¬♦¬. The dual
symbol was later devised by
F.B. Fitch and ﬁrst appeared in
print in  in a paper by his
doctoral student Barcan ().
See footnote  of Hughes
& Cresswell (). Another
notation one ﬁnds is L for
necessity and M for possibility,
the latter from the German
möglich ‘possible’.

For any sentence φ, any world w, and any assignment g:
in the world of Sherlock Holmes φ w,g = 
iﬀ the world w as it is described in the Sherlock Holmes stories is such
that φ w ,g = .

This is a so-called  treatment of the meaning of this expression. Instead of giving an explicit semantic value to the expression, we specify
what eﬀect it has on the meaning of a complex expression that contains it. In (),
we do not compute the meaning for in the world of Sherlock Holmes, φ from the
combination of the meanings of its parts, since in the world of Sherlock Holmes
is not given a separate meaning, but in eﬀect triggers a special composition
principle. This format is very common in modal logic systems, which usually
give a syncategorematic semantics for the two modal operators (the necessity
operator and the possibility operator ♦). When one only has a few closed
class expressions to deal with that may shift the world parameter, employing
syncategorematic entries is a reasonable strategy. But we are facing a multitude
of displacement devices. So, we will need to make our system more modular.
So, we want to give in the world of Sherlock Holmes its own meaning and combine that meaning with that of its prejacent by a general composition principle.
The Fregean slogan we adopted says that all composition is function application
(modulo the need for λ-abstraction and the possible need for predicate modiﬁcation). So, what we will want to do is to make () be the result of functional
application. But we can immediately see that it cannot be the result of our usual
rule of functional application, since that would feed to in the world of Sherlock
Holmes the semantic value of a famous detective lives in B Baker Street in w,
which would be a particular truth-value,  if a famous detective lives at B
Baker Street in w and  if there doesn’t. And whatever the semantics of in the
world of Sherlock Holmes is, it is certainly not a truth-functional operator.
So, we need to feed something else to in the world of Sherlock Holmes. At the
same time, we want the operator to be able to shift the evaluation world of its
prejacent. Can we do this?
S : I. We will deﬁne a richer notion of semantic value, the
 of an expression. This will be a function from possible worlds to
 See Heim & Kratzer, Section ., pp. – for a reminder about the status of predicate
modiﬁcation.

§.

A I S   E S



the extension of the expression in that world. The intension of a sentence can
be applied to any world and give the truth-value of the sentence in that world.
Intensional operators take the intension of their prejacent as their argument, that
is we will feed the intension of the embedded sentence to the shifting operator.
The operator will use that intension and apply it to the world it wants the
evaluation to happen in. Voilà.
Now let’s spell that account out. Our system actually provides us with two
kinds of meanings. For any expression α, we have α w,g , the semantic value
of α in w, also known as the  of α in w. But we can also calculate
λw. α w,g , the function that assigns to any world w the extension of α in that
world. This is usually called the  of α. We will sometimes use an
abbreviatory notation for the intension of α:
()

α

g

¢

:= λw. α

w,g

.

It should be immediately obvious that since the deﬁnition of intension abstracts
over the evaluation world, intensions are not world-dependent.,
Note that strictly speaking, it now makes no sense anymore to speak of
“the semantic value” of an expression α. What we have is a semantic system
that allows us to calculate extensions (for a given possible world w) as well as
intensions for all (interpretable) expressions. We will see that when α occurs in a
particular bigger tree, it will always be determinate which of the two “semantic
values” of α is the one that enters into the compositional semantics. So, that
one — whichever one it is, the extension or the intension of α — might then be
called “the semantic value of α in the tree β”.
It should be noted that the terminology of  vs.  is
time-honored but that the possible worlds interpretation thereof is more recent.
The technical notion we are using is certainly less rich a notion of meaning than
tradition assumed.

 The notation with the subscripted cent-sign comes from Montague Grammar. See e.g. Dowty
et al. (: ).
 Since intensions are by deﬁnition not dependent on the choice of a particular world, it makes
no sense to put a world-superscript on the intension-brackets. So don’t ever write “ . . . w,g ”;
¢
we’ll treat that as undeﬁned nonsense.
 The deﬁnition here is simpliﬁed, in that it glosses over the fact that some expressions, in
particular those that contain  , may fail to have an extension in certain
worlds. In such a case, the intension has no extension to map such a world to. Therefore, the
intension will have to be a partial function. So, the oﬃcial, more “pedantic”, deﬁnition will have
to be as follows: α g := λw : α ∈ dom( w,g ). α w,g .
¢
 For example, Frege’s “modes of presentation” are not obviously captured by this possible
worlds implementation of extension/intension.

The Port-Royal logicians distinguished  from
. Leibniz
preferred the term 
rather than .
The notion probably goes
back even further. See Spencer
() for some notes on this.
The possible worlds interpretation is due to Carnap ().

B



C 

S : S T  S D. If we want to be able to
feed the intensions to lexical items like in the world of Sherlock Holmes, we need
to have the appropriate types in our system.
Recall that W is the set of all possible worlds. And recall that D is the set of
all   and thus contains all individuals existing in the actual
world plus all individuals existing in any of the merely possible worlds.
We now expand the set of semantic types, to add intensions. Intensions are
functions from possible worlds to all kinds of extensions. So, basically we want
to add for any kind of extension we have in our system, a corresponding kind of
intension, a function from possible worlds to that kind of extension.
We add a new clause, (c), to the deﬁnition of semantic types:
()

S T
a. e and t are semantic types.
b. If σ and τ are semantic types, then σ, τ is a semantic type.
c. If σ is a semantic type, then s, σ is a semantic type.
d. Nothing else is a semantic type.

We also add a fourth clause to the previous deﬁnition of semantic domains:
()

S D
a. De = D, the set of all possible individuals
b. Dt = {, }, the set of truth-vales
c. If σ and τ are semantic types, then D σ,τ is the set of all functions
from Dσ to Dτ .
d. I: If σ is a type, then D s,σ is the set of all functions
from W to Dσ .

Clause (d) is the addition to our previous system of types. The functions of the
schematic type s, . . . are intensions. Here are some examples of intensions:
• The intensions of sentences are of type s, t , functions from possible
worlds to truth values. These are usually called . Note that
if the function is total, then we can see the sentence as picking out a
set of possible worlds, those in which the sentence is true. More often
than not, however, propositions will be  functions from worlds
to truth-values, that is functions that fail to map certain possible worlds
into either truth-value. This will be the case when the sentence contains
a presupposition trigger, such as the. The famous sentence The King of
 Note a curious feature of this set-up: there is no type s and no associated domain. This
corresponds to the assumption that there are no expressions of English that take as their extension
a possible world, that is, there are no pronouns or names referring to possible worlds. We will
actually question this assumption in a later chapter. For now, we will stay with this more
conventional set-up.

§.

A I S   E S



France is bald has an intension that (at least in the analysis sketched in
Heim & Kratzer) is undeﬁned for any world where there fails to be a
unique King of France.
• The intensions of one-place predicates are of type s, e, t , functions
from worlds to set of individuals. These are usually called .
• The intensions of expressions of type e are of type s, e , functions from
worlds to individuals. These are usually called  .
S : A L E   S. We are ready to formulate the semantic
entry for in the world of Sherlock Holmes:
()

in the world of Sherlock Holmes w,g =
λp s,t . the world w as it is described in the Sherlock Holmes stories
is such that p(w ) = .

That is, in the world of Sherlock Holmes expects as its argument a function of type
s, t , a proposition. It yields the truth-value  iﬀ the proposition is true in the
world as it is described in the Sherlock Holmes stories.
All that’s left to do now is to provide in the world of Sherlock Holmes with a
proposition as its argument. This is the job of a new composition principle.
S : I F A. We add the new rule of
Intensional Functional Application.
()

I F A (IFA)
If α is a branching node and {β, γ} the set of its daughters, then, for
any world w and assignment g: if β w,g is a function whose domain
contains γ g , then α w,g = β w,g ( γ g ).
¢
¢

This is the crucial move. It makes space for expressions that want to take the
intension of their sister as their argument and do stuﬀ to it. Now, everything is
in place. Given (), the semantic argument of in the world of Sherlock Holmes
will not be a truth-value but a proposition. And thus, in the world of Sherlock
Holmes will be able to check the truth-value of its prejacent in various possible
worlds. To see in practice that we have all we need, please do the following
exercise.
E .: Calculate the conditions under which an utterance in a given
possible world w of the sentence in the world of the Sherlock Holmes stories, a
famous detective lives at B Baker Street is true.
 This is not yet the ﬁnal semantics, see Section . for complications. Also, note again the
condensed notation: “λp s,t . . . . ” stands for the fully oﬃcial “λp : p ∈ D s,t . . . . ”.

B



C 

. Comments and Complications
..

Intensions All the Way?

We have seen that to adequately deal with expressions like in the world of
Sherlock Holmes, we need an intensional semantics, one that gives us access to the
extensions of expressions across the multitude of possible worlds. At the same
time, we have kept the semantics for items like and, every, and a unchanged and
extensional. This is not the only way one can set up an intensional semantics.
The following exercise demonstrates this.
E .: Consider the following “intensional” meaning for and:
()

and

w,g

= λp

s,t

. λq s,t . p(w) = q(w) = .

With this semantics, and would operate on the intensions of the two conjoined
sentences. In any possible world w, the complex sentence will be true iﬀ the
component propositions are both true of that world.
Compute the truth-conditions of the sentence In the world of Sherlock Holmes,
Holmes is quick and Watson is slow both with the extensional meaning for and
given earlier and the intensional meaning given here. Is there any diﬀerence in
the results?
There are then at least two ways one could develop an intensional system.
(i) We could “generalize to the worst case” and make the semantics deliver
intensions as the semantic value of an expression. Such systems are common
in the literature (see Cresswell ; Lewis b).
(ii) We could maintain much of the extensional semantics we have developed
so far and extend it conservatively so as to account for non-extensional
contexts.
We have chosen to pursue (ii) over (i), because it allows us to keep the semantics
of extensional expressions simpler. The philosophy we follow is that we will only
move to the intensional sub-machinery when triggered by an expression that
creates a non-extensional context. As the exercise just showed, this is more a
matter of taste than a deep scientiﬁc decision.

..

Why Talk about Other Worlds?

Why would natural language bother having such elaborate mechanisms to talk
about other possible worlds? While having devices for spatial and temporal
displacement (talking about Hamburg or what happened yesterday) seems eminently reasonable, talking about worlds other than the actual world seems only
suitable for poets and the like. So, why?
The solution to this puzzle lies in a fact that our current semantics of the
shifter in the world of Sherlock Holmes does not yet accurately capture: modal

§.

C  C



sentences have empirical content, they make  claims, claims that
are true or false depending on the circumstances in the actual world.
Our example sentence In the world of Sherlock Holmes, a famous detective
lives at  Baker Street is true in this world but it could easily have been false.
There is no reason why Sir Arthur Conan Doyle could not have decided to locate
Holmes’ abode on Abbey Road.
To see that our semantics does not yet capture this fact, notice that in the
semantics we gave for in the world of Sherlock Holmes:
()

in the world of Sherlock Holmes w,g =
λp s,t . the world w as it is described in the Sherlock Holmes stories
is such that p(w ) = .

there is no occurrence of w on the right hand side. This means that the truthconditions for sentences with this shifter are world-independent. In other words,
they are predicted to make non-contingent claims that are either true no-matterwhat or false no-matter-what. This needs to be ﬁxed.
The ﬁx is obvious: what matters to the truth of our sentence is the content
of the Sherlock Holmes stories as they are in the evaluation world. So, we need
the following semantics for our shifter:
()

in the world of Sherlock Holmes w,g =
λp s,t . the world w as it is described in the Sherlock Holmes stories
in w is such that p(w ) = .

We see now that sentences with this shifter do make a claim about the evaluation
world: namely, that the Sherlock Holmes stories as they are in the evaluation
world describe a world in which such-and-such is true. So, what is happening
is that although it appears at ﬁrst as if modal statements concern other possible
worlds and thus couldn’t really be very informative, they actually only talk about
certain possible worlds, those that stand in some relation to what is going on at
the ground level in the actual world. As a crude analogy, consider:
()

My grandmother is sick.

At one level this is a claim about my grandmother. But it is also a claim about me:
namely that I have a grandmother who is sick. Thus it is with modal statements.
They talk about possible worlds that stand in a certain relation to the actual
world and thus they make claims about the actual world, albeit slightly indirectly.

.. The Worlds of Sherlock Holmes
So far, we have played along with colloquial usage in talking of the world of
Sherlock Holmes. But it is important to realize that this is sloppy talk. Lewis
() writes:



B

C 

[I]t will not do to follow ordinary language to the extent of supposing
that we can somehow single out a single one of the worlds [as the
one described by the stories]. Is the world of Sherlock Holmes a
world where Holmes has an even or an odd number of hairs on
his head at the moment when he ﬁrst meets Watson? What is
Inspector Lestrade’s blood type? It is absurd to suppose that these
questions about the world of Sherlock Holmes have answers. The
best explanation of that is that the worlds of Sherlock Holmes are
plural, and the questions have diﬀerent answers at diﬀerent ones.
The usual move at this point is to talk about the set of worlds “ 
the (content of ) Sherlock Holmes stories in w”. We imagine that we ask of each
possible world whether what is going on in it is compatible with the stories as
they were written in our world. Worlds where Holmes lives on Abbey Road are
not compatible. Some worlds where he lives at B Baker Street are compatible
(again not all, because in some such worlds he is not a famous detective but
an obscure violinist). Among the worlds compatible with the stories are ones
where he has an even number of hairs on his head at the moment when he ﬁrst
meets Watson and there are others where he has an odd number of hairs at that
moment.
What the operator in the world of Sherlock Holmes expresses is that its complement is true throughout the worlds compatible with the stories. In other words,
the operator universally quantiﬁes over the compatible worlds. Our next iteration
of the semantics for the operator is therefore this:
()

in the world of Sherlock Holmes w,g =
λp s,t . ∀w compatible with the Sherlock Holmes stories in w :
p(w ) = .

At a very abstract level, the way we parse sentences of the form in the world of
Sherlock Holmes, φ is that both components, the in-phrase and the prejacent,
determine sets of possible worlds and that the set of possible worlds representing
the content of the ﬁction mentioned in the in-phrase is a subset of the set
of possible worlds determined by the prejacent. We will see the same rough
structure of relating sets of possible worlds in other intensional constructions.
This is where we will leave things. There is more to be said about ﬁction
operators like in the world of Sherlock Holmes, but we will just refer to you to the
relevant literature. In particular, one might want to make sense of Lewis’ idea
that a special treatment is needed for cases where the sentence makes a claim
about things that are left open by the ﬁction (no truth-value, perhaps?). One
also needs to ﬁgure out how to deal with cases where the ﬁction is internally
inconsistent. In any case, for our purposes we’re done with this kind of operator.

§.

S R

.

Supplemental Readings



There is considerable overlap between this chapter and Chapter  of Heim &
Kratzer’s textbook:
Heim, Irene & Angelika Kratzer. . Semantics in generative grammar. Blackwell.
Here, we approach intensional semantics from a diﬀerent angle. It would
probably be beneﬁcial if you read H& K’s Chapter  in addition to this chapter
and if you did the exercises in there.
Come to think of it, some other ancillary reading is also recommended. You may
want to look at relevant chapters in other textbooks:
Dowty, David, Robert Wall & Stanley Peters. . Introduction to Montague
semantics. Kluwer. [Chapters & ].
Gamut, L. T. F. . Logic, language, and meaning. Chicago University Press.
[Volume II: Intensional Logic and Logical Grammar].
Chierchia, Gennaro & Sally McConnell-Ginet. . Meaning and grammar: An
introduction to semantics (nd edition). MIT Press. [Chapter : Intensionality].
An encyclopedia article by Perry on possible worlds semantics:
Perry, John R. . Semantics, possible worlds. In E. Craig (ed.), Routledge encyclopedia of philosophy, London: Routledge. URL http://www.rep.
routledge.com/article/U. Preprint http://www-csli.stanford.edu/~john/
PHILPAPERS/posswld.pdf.
A couple of inﬂuential philosophical works on the metaphysics and uses of
possible worlds:
Kripke, Saul. . Naming and necessity. Oxford: Blackwell.
Lewis, David. . On the plurality of worlds. Oxford: Blackwell.
An interesting paper on the origins of the modern possible worlds semantics for
modal logic:
Copeland, B. Jack. . The genesis of possible worlds semantics. Journal of
Philosophical Logic (). –. doi:./A:.
A personal history of formal semantics:
Partee, Barbara H. . Reﬂections of a formal semanticist as of Feb .
Ms. (longer version of introductory essay in  book). URL http://people.
umass.edu/partee/docs/BHP_Essay_Feb.pdf.



B

C 

A must read for students who plan to go on to becoming specialists in semantics,
together with a handbook article putting it in perspective:
Montague, Richard. . The proper treatment of quantiﬁcation in ordinary
English. In Jaako Hintikka, Julius Moravcsik & Patrick Suppes (eds.), Approaches to natural language, –. Dordrecht: Reidel. URL http://www.
blackwellpublishing.com/content/BPL_Images/Content_store/Sample_chapter/
/Portner.pdf. Reprinted in Portner & Partee (), pp. –.
Partee, Barbara H. & Herman L.W. Hendriks. . Montague grammar. In
Johan van Benthem & Alice ter Meulen (eds.), Handbook of logic and language,
–. Amsterdam: Elsevier.
To learn more about discourse about ﬁction, read Lewis:
Lewis, David. . Truth in ﬁction. American Philosophical Quarterly (). –
. URL http://www.jstor.org/stable/. Reprinted with postscripts in
Lewis (), pp. –.
Recent reconsiderations:
Bonomi, Andrea & Sandro Zucchi. . A pragmatic framework for truth
in ﬁction. Dialectica (). –. doi:./j.-..tb.x.
Preprint http://ﬁlosoﬁa.dipaﬁlo.unimi.it/~bonomi/Pragmatic.pdf.
Hanley, Richard. . As good as it gets: Lewis on truth in ﬁction. Australasian
Journal of Philosophy (). –. doi:./.
A while back, there was an entry on Kai’s blog with comments from readers
about indeterminacies in ﬁction:
http://kaivonﬁntel.org/q-the-quantiﬁcational-force-of-ﬁction-operators.
Inconsistencies in ﬁctions and elsewhere are discussed in:
Varzi, Achille. . Inconsistency without contradiction. Notre Dame Journal of
Formal Logic (). –. doi:./ndjﬂ/.
Lewis, David. . Logic for equivocators. Noûs (). –. doi:./.
Reprinted in Lewis (: pp. –).
Some other interesting work on stories and pictures and their content:
Ross, Jeﬀ. . The semantics of media (Studies in Linguistics and Philosophy
(SLAP) ). Dordrecht: Kluwer.
Zucchi, Sandro. . Tense in ﬁction. In Carlo Cecchetto, Gennaro Chierchia
& Maria Teresa Guasti (eds.), Semantic interfaces: Reference, anaphora and
aspect, –. CSLI Publications. URL http://ﬁlosoﬁa.dipaﬁlo.unimi.it/
~bonomi/Zucchi.

§.

S R



Blumson, Ben. . Pictures, perspective and possibility. Philosophical Studies
doi:./s---.
If you’re interested in whether displacement really is an exclusive feature of
human language and cognition, you might want to check out this fairly recent
literature:
Suddendorf, Thomas & Michael C. Corballis. . Mental time travel and the
evolution of the human mind. Genetic, Social, and General Psychology Monographs (). –. URL http://cogprints.org///MentalTimeTravel.
txt.
Suddendorf, Thomas. . Foresight and evolution of the human mind. Science
(). –. doi:./science..
Osvath, Mathias & Peter Gärdenfors. . Oldowan culture and the evolution
of anticipatory cognition. Tech. Rep.  Lund University Cognitive Studies
LUCS, Lund. URL http://www.lucs.lu.se/ftp/pub/LUCS_Studies/LUCS.
pdf.
Astonishingly, Lewis’ doctrine of the reality of plurality of possible worlds is
being taken up here and there by researchers trying to understand quantum
mechanics via the so-called “many worlds” interpretation. See for a start, Kai’s
blog entry on a popular book on the issue, http://kaivonﬁntel.org/many-worlds,
and also MIT physics professor Max Tegmark’s FAQ on the topic, http://space.
mit.edu/home/tegmark/multiverse.html.

— T     —

C T
P A
With the basic framework in place, we now proceed to analyze a number
of intensional constructions. We start with the basic possible worlds
semantics for propositional attitude ascriptions. We talk brieﬂy about
the formal properties of accessibility relations.

.

Hintikka’s Idea 

.

Accessibility Relations 
..
..

.

*Transitivity 

..
.

Reﬂexivity 
*Symmetry 

Suplemental Readings 

Hintikka’s Idea

Expressions like believe, know, doubt, expect, regret, and so on are usually said
to describe  , expressing relations between individuals
(the attitude holder) and propositions (intensions of sentences).
The simple idea is that George believes that Henry is a spy claims that George
believes of the proposition that Henry is a spy that it is true. Note that for the
attitude ascription to be true it does not have to hold that Henry is actually a
spy. But where — in which world(s) — does Henry have to be a spy for it be true
that George believes that Henry is a spy? We might want to be inspired by the
colloquial phrase “in the world according to George” and say that George believes
that Henry is a spy is true iﬀ in the world according to George’s beliefs, Henry is
a spy. We immediately recall from the previous chapter that we need to ﬁx this
idea up by making space for multiple worlds compatible with George’s beliefs
and by tying the truth-conditions to contingent facts about the evaluation world.
That is, what George believes is diﬀerent in diﬀerent possible worlds.
The following lexical entry thus oﬀers itself:

According to Hintikka
(), the term   goes
back to Russell ().
Of course, the possible worlds
semantics for propositional
attitudes was in place long
before the extension to ﬁction contexts was proposed.
Our discussion here has inverted the historical sequence
for pedagogical purposes.

P A


()
It is important to realize the
modesty of this semantics: we
are not trying to ﬁgure out
what belief systems are and
particularly not what their
internal workings are like.
That is the job of psychologists
(and philosophers of mind,
perhaps). For our semantics,
we treat the belief system as a
black box that determines for
each possible world whether it
considers it possible that it is
the world it is located in.

Jaakko Hintikka

believe

C 

w,g

=
λp s,t . λx. ∀w compatible with x s beliefs in w : p(w ) = .

What is going on in this semantics? We conceive of George’s beliefs as a state of
his mind about whose internal structure we will remain agnostic, a matter left
to other cognitive scientists. What we require of it is that it embody opinions
about what the world he is located in looks like. In other words, if his beliefs
are confronted with a particular possible world w , they will determine whether
that world may or may not be the world as they think it is. What we are asking
of George’s mental state is whether any state of aﬀairs, any event, anything in
w is in contradiction with anything that George believes. If not, then w is
compatible with George’s beliefs. For all George believes, w may well be the
world where he lives. Many worlds will pass this criterion, just consider as one
factor that George is unlikely to have any precise opinions about the number of
leaves on the tree in front of my house. George’s belief system determines a set
of worlds compatible with his beliefs: those worlds that are viable candidates for
being the actual world, as far as his belief system is concerned.
Now, George believes a proposition iﬀ that proposition is true in all of the
worlds compatible with his beliefs. If there is just one world compatible with his
beliefs where the proposition is not true, that means that he considers it possible
that the proposition is not true. In such a case, we can’t say that he believes the
proposition. Here is the same story in the words of Hintikka (), the source
for this semantics for propositional attitudes:
My basic assumption (slightly simpliﬁed) is that an attribution of any
propositional attitude to the person in question involves a division
of all the possible worlds (. . . ) into two classes: into those possible
worlds which are in accordance with the attitude in question and
into those which are incompatible with it. The meaning of the
division in the case of such attitudes as knowledge, belief, memory,
perception, hope, wish, striving, desire, etc. is clear enough. For
instance, if what we are speaking of are (say) a’s memories, then
these possible worlds are all the possible worlds compatible with
everything he remembers. [. . . ]
How are these informal observations to be incorporated into
a more explicit semantical theory? According to what I have said,
understanding attributions of the propositional attitude in question
(. . . ) means being able to make a distinction between two kinds
of possible worlds, according to whether they are compatible with
the relevant attitudes of the person in question. The semantical
counterpart to this is of course a function which to a given individual
person assigns a set of possible worlds.
However, a minor complication is in order here. Of course,

§.

H’ I



the person in question may himself have diﬀerent attitudes in the
diﬀerent worlds we are considering. Hence this function in eﬀect
becomes a relation which to a given individual and to a given possible
world µ associates a number of possible worlds which we shall call
the  to µ. The relation will be called the alternativeness
relation. (For diﬀerent propositional attitudes, we have to consider
diﬀerent alternativeness relations.)
E .: Let’s adopt Hintikka’s idea that we can use a function that maps x
and w into the set of worlds w compatible with what x believes in w. Call this
function B. That is,
()

B = λx. λw. {w : w is compatible with what x believes in w}.

Using this notation, our lexical entry for believe would look as follows:
()

believe

w,g

= λp

s,t

. λx. B(x)(w) ⊆ p.

We are here indulging in the usual sloppiness in treating p both as a function
from worlds to truth-values and as the set characterized by that function.
Here now are two “alternatives” for the semantics of believe:
()

A  ( )
believe w,g = λp ∈ D s,t . λx ∈ D. p = B(x)(w) .

()

A  (  )
believe w,g = λp ∈ D s,t . λx ∈ D. p ∩ B(x)(w) = ∅ .

Explain why these do not adequately capture the meaning of believe.
E .: Follow-up: The semantics in () would have made believe into
an existential quantiﬁer of sorts: it would say that some of the worlds compatible
with what the subject believes are such-and-such. You have argued (successfully,
of course) that such an analysis is wrong for believe. But are there attitude
predicates with such an “existential” meaning? Discuss some candidates. If you
can’t ﬁnd any candidates that survive scrutiny, can you speculate why there might
be no existential attitude predicates? [Warning: this is unexplored territory!]
We can also think of belief states as being represented by a function BS, which
maps an individual and a world into a set of propositions: those that the
individual believes. From there, we could calculate the set of worlds compatible
with an individual x’s beliefs in world w by retrieving the set of those possible
worlds in which all of the propositions in BS(x)(w) are true: {w : ∀p ∈
BS(x)(w) : p(w ) = }, which in set talk is simply the big intersection of all the
propositions in the set: ∩BS(x)(w). Our lexical entry then would be:

BS is meant to stand for
‘belief state’, not for what
you might have thought!

P A


()

believe

w,g

= λp

s,t

C 

. λx. ∩ BS(x)(w) ⊆ p.

E .: Imagine that our individual x forms a new opinion. Imagine
that we model this by adding a new proposition p to the pool of opinions. So,
BS(x)(w) now contains one further element. There are now more opinions.
What happens to the set of worlds compatible with x’s beliefs? Does it get bigger
or smaller? Is the new set a subset or superset of the previous set of compatible
worlds?

. Accessibility Relations
Another way of reformulating Hintikka’s semantics for propositional attitudes
is via the notion of an  . We talk of a world w being
accessible from w. Each attitude can be associated with such an accessibility
B
relation. For example, we can introduce the relation wRa w which holds iﬀ w
is compatible with a’s belief state in w. We have then yet another equivalent way
of specifying the lexical entry for believe:
()
Kirill Shklovsky (in class) asked
why we call reﬂexivity, transitivity, and symmetry “formal”
properties of relations. The
idea is that certain properties
are “formal” or “logical”, while
others are more substantial. So,
the fact that the relation “have
the same birthday as” is symmetric seems a more formal
fact about it than the fact that
the relation holds between my
daughter and my brother-inlaw. Nevertheless, one of the
most common ways of characterizing formal/logical notions
(permutation-invariance, if
you’re curious) does not in
fact make symmetry etc. a
formal/logical notion. So,
while intuitively these do seem
to be formal/logical properties, we do not know how to
substantiate that intuition.
See MacFarlane () for
discussion.
We talk here about knowledge
entailing (or even presupposing) truth but we do not
mean to say that knowledge
simply equals true belief. Professors Socrates and Gettier
and their exegetes have further
considerations.

believe

w,g

= λp

s,t

. λx. ∀w : wRB w → p(w ) = .
x

It is proﬁtable to think of diﬀerent attitudes (belief, knowledge, hope, regret,
memory, . . . ) as corresponding to diﬀerent accessibility relations. Recall now that
the linguistic study of determiners beneﬁtted quite a bit from an investigation of
the formal properties of the relations between sets of individuals that determiners
express. We can do the same thing here and ask about the formal properties
of the accessibility relation associated with belief versus the one associated with
knowledge, etc. The obvious properties to think about are reﬂexivity, transitivity,
and symmetry.

..

Reﬂexivity

A relation is reﬂexive iﬀ for any object in the domain of the relation we know that
the relation holds between that object and itself. Which accessibility relations
are reﬂexive? Take knowledge:
()

wRK w iﬀ w is compatible with what x knows in w.
x

We are asking whether for any given possible world w, we know that RK holds
x
between w and w itself. It will hold if w is a world that is compatible with what
we know in w. And clearly that must be so. Take our body of knowledge in w.
The concept of knowledge crucially contains the concept of truth: what we know
must be true. So if in w we know that something is the case then it must be the
case in w. So, w must be compatible with all we know in w. RK is reﬂexive.
x

§.

A R



Now, if an attitude X corresponds to a reﬂexive accessibility relation, then we
can conclude from a Xs that p being true in w that p is true in w. This property
of an attitude predicate is often called . It is to be distinguished
from , which is a property of attitudes which presuppose – rather than
(merely) entail – the truth of their complement.
If we consider the relation RB pairing with a world w those worlds w which
x
are compatible with what x believes in w, we no longer have reﬂexivity: belief is
not a veridical attitude. It is easy to have false beliefs, which means that the actual
world is not in fact compatible with one’s beliefs, which contradicts reﬂexivity.
And many other attitudes as well do not involve veridicality/reﬂexivity: what we
hope may not come true, what we remember may not be what actually happened,
etc.
In modal logic, the correspondence between formal properties of the accessibility relation and the validity of inference patterns is well-studied. What we
have just seen is that reﬂexivity of the accessibility relation corresponds to the
validity of p → p. Other properties correspond to other characteristic patterns.
Let’s see this for transitivity and symmetry.

..

In modal logic notation:
p → p. This pattern
is sometimes called T or
M, as is the corresponding system of modal logic.

The diﬀerence between believe
and know in natural discourse
is quite delicate, especially
when one considers ﬁrst person
uses (I believe the earth is ﬂat
vs. I know the earth is ﬂat).

*Transitivity

Transitivity of the accessibility relation corresponds to the inference p →
p.
The pattern seems not obviously wrong for knowledge: if one knows that p,
doesn’t one thereby know that one knows that p? But before we comment
on that, let’s establish the formal correspondence between transitivity and that
inference pattern. This needs to go in both directions.
not p
p

w3

w2

w1

Figure .: Transitivity
What does it take for the pattern to be valid? Assume that p holds for an
arbitrary world w, i.e. that p is true in all worlds w accessible from w. Now,
the inference is to the fact that p again holds in any world w accessible from
any of those worlds w accessible from w. But what would prevent p from being
false in some w accessible from some w accessible from w? That could only
be prevented from happening if we knew that w itself is accessible from w as

Starred sections are optional.
In the literature on epistemic
modal logic, the pattern is
known as the KK T or
P I.
In general modal logic, it is
the characteristic axiom 
of the modal logic system
S, which is a system that
adds  to the previous axiom
M/T. Thus, S is the logic of
accessibility relations that are
both reﬂexive and transitive.

P A



C 

well, because then we would know from the premiss that p is true in it (since p
is true in all worlds accessible from w). Ah, but w (some world accessible from
a world w accessible from w) is only guaranteed to be accessible from w if the
accessibility relation is transitive (if w is accessible from w and w is accessible
from w , then transitivity ensures that w is accessible from w). This reasoning
has shown that validity of the pattern requires transitivity. The other half of
proving the correspondence is to show that transitivity entails that the pattern is
valid.
The proof proceeds by reductio. Assume that the accessibility relation is
transitive. Assume that (i) p holds for some world w but that (ii)
p doesn’t
hold in w. We will show that this situation cannot obtain. By (i), p is true in all
worlds w accessible from w. By (ii), there is some non-p world w accessible
from some world w accessible from w. But by transitivity of the accessibility
relation, that non-p world w must be accessible from w. And since all worlds
accessible from w are p worlds, w must be a p world, in contradiction to (ii).
So, as soon as we assume transitivity, there is no way for the inference not to go
through.
Now, do any of the attitudes have the transitivity property? It seems rather
obvious that as soon as you believe something, you thereby believe that you
believe it (and so it seems that belief involves a transitive accessibility relation).
And in fact, as soon as you believe something, you believe that you know it. But
one might shy away from saying that knowing something automatically amounts
to knowing that you know it. For example, many are attracted to the idea that to
know something requires that (i) that it is true, (ii) that you believe it, and (iii)
that you are justiﬁed in believing it: the justiﬁed true belief analysis of knowledge.
So, now couldn’t it be that you know something, and thus (?) that you believe
you know it, and thus that you believe that you are justiﬁed in believing it,
but that you are not justiﬁed in believing that you are justiﬁed in believing it?
After all, one’s source of knowledge, one’s reliable means of acquiring knowledge,
might be a mechanism that one has no insight into. So, while one can implicitly
trust (believe) in its reliability, and while it is in fact reliable, one might not have
any means to have trustworthy beliefs about it. [Further worries about the KK
Thesis are discussed by Williamson ().]

..

*Symmetry

What would the consequences be if the accessibility relation were symmetric?
Symmetry of the accessibility relation R corresponds to the validity of the
following principle:

§.

A R

()

Brouwer’s Axiom:



∀p∀w : w ∈ p → ∀w wRw → ∃w [w Rw & w ∈ p]

not p
w2

w3

In modal logic notation:
♦p, known simply as
B in modal logic. The system
that combines T/M with B is
often called Brouwer’s System
(B), after the mathematician
L.E.J. Brouwer, not because
he proposed it but because it
was thought that it had some
connections to his doctrines.

p→

p
w1

Figure .: Symmetry
Here’s the reasoning: Assume that R is in fact symmetric. Pick a world w in
which p is true. Now, could it be that the right hand side of the inference fails
to hold in w? Assume that it does fail. Then, there must be some world w
accessible from w in which ♦p is false. In other words, from that world w
there is no accessible world w in which p is true. But since R is assumed to be
symmetric, one of the worlds accessible from w is w and in w, p is true, which
contradicts the assumption that the inference doesn’t go through. So, symmetry
ensures the validity of the inference.
The other way (validity of the inference requires symmetry): the inference
says that from any p world we only have worlds accessible from which there is
at least one accessible p world. But imagine that p is true in w but not true in
any other world. So, the only way for the conclusion of the inference to hold
automatically is to have a guarantee that w (the only p world) is accessible from
any world accessible from it. That is, we need to have symmetry. QED.
To see whether a particular kind of attitude is based on a symmetric accessibility relation, we can ask whether Brouwer’s Axiom is intuitively valid with respect
to this attitude. If it is not valid, this shows that the accessibility relation can’t
be symmetric. In the case of a knowledge-based accessibility relation (epistemic
accessibility), one can argue that symmetry does not hold:
The symmetry condition would imply that if something happens
to be true in the actual world, then you know that it is compatible
with your knowledge (Brouwer’s Axiom). This will be violated by
any case in which your beliefs are consistent, but mistaken. Suppose
that while p is in fact true, you feel certain that it is false, and so
think that you know that it is false. Since you think you know this,
it is compatible with your knowledge that you know it. (Since we
 Thanks to Bob Stalnaker (pc to Kai von Fintel) for help with the following reasoning.

L.E.J. Brouwer



All one really needs to make
NI valid is to have a E accessibility relation:
any two worlds accessible from
the same world are accessible
from each other. It is a nice
little exercise to prove this, if
you have become interested in
this sort of thing. Note that
all reﬂexive and Euclidean
accessibility relations are
transitive and symmetric as
well — another nice little thing
to prove.

P A

C 

are assuming you are consistent, you can’t both believe that you
know it, and know that you do not). So it is compatible with your
knowledge that you know that not p. Equivalently : you don’t know
that you don’t know that not p. Equivalently: you don’t know that
it’s compatible with your knowledge that p. But by Brouwer’s Axiom,
since p is true, you would have to know that it’s compatible with
your knowledge that p. So if Brouwer’s Axiom held, there would
be a contradiction. So Brouwer’s Axiom doesn’t hold here, which
shows that epistemic accessibility is not symmetric.
Game theorists and theoretical computer scientists who traﬃc in logics of knowledge often assume that the accessibility relation for knowledge is an equivalence
relation (reﬂexive, symmetric, and transitive). But this is appropriate only if
one abstracts away from any error, in eﬀect assuming that belief and knowledge
coincide. One striking consequence of working with an equivalence relation
as the accessibility relation for knowledge is that one predicts the principle of
N I to hold:
()

N I ()
If one doesn’t know that p, then one knows that one doesn’t know that
p. (¬ p → ¬ p).

This surely seems rather dubious: imagine that one strongly believes that p but
that nevertheless p is false, then one doesn’t know that p, but one doesn’t seem
to believe that one doesn’t know that p, in fact one believes that one does know
that p.

. Suplemental Readings
We will come back to propositional attitudes and especially the scope of noun
phrases with respect to them, including the infamous  -  distinction.
Further connections between mathematical properties of accessibility relations
and logical properties of various notions of necessity and possibility are studied
extensively in modal logic:
Hughes, G.E. & M.J. Cresswell. . A new introduction to modal logic. London:
Routledge.
Garson, James. . Modal logic. In Edward N. Zalta (ed.), The Stanford
encyclopedia of philosophy, URL http://plato.stanford.edu/entries/logic-modal/,
 This and the following step rely on the duality of necessity and possibility: q is compatible
with your knowledge iﬀ you don’t know that not q.

§.

S R



especially section  and , “Modal Axioms and Conditions on Frames”, “Map
of the Relationships between Modal Logics”.
A thorough discussion of the possible worlds theory of attitudes, and some of its
potential shortcomings, can be found in Bob Stalnaker’s work:
Stalnaker, Robert. a. Inquiry. MIT Press.
Stalnaker, Robert. . Context and content. Oxford: Oxford University Press.
A quick and informative surveys about the notion of knowledge:
Steup, Matthias. . The analysis of knowledge. In Edward N. Zalta (ed.), The
Stanford encyclopedia of philosophy, Fall  edn. URL http://plato.stanford.
edu/archives/fall/entries/knowledge-analysis/.
Linguistic work on attitudes has often been concerned with various co-occurrence
patterns, particularly which moods (indicative or subjunctive or inﬁnitive) occur
in the complement and whether negative polarity items are licensed in the
complement.
Mood licensing:
Portner, Paul. . The semantics of mood, complementation, and conversational force. Natural Language Semantics (). –. doi:./A:.
NPI-Licensing:
Kadmon, Nirit & Fred Landman. . Any. Linguistics and Philosophy ().
–. doi:./BF.
von Fintel, Kai. . NPI licensing, Strawson entailment, and context dependency. Journal of Semantics (). –. doi:./jos/...
Giannakidou, Anastasia. . Aﬀective dependencies. Linguistics and Philosophy
(). –. doi:./A:.
There is some interesting work out of Amherst rethinking the way attitude
predicates take their complements:
Kratzer, Angelika. . Decomposing attitude verbs. Handout from a talk
honoring Anita Mittwoch on her th birthday at the Hebrew University of Jerusalem July , . URL http://semanticsarchive.net/Archive/
DcwYJkM/attitude-verbs.pdf.
Moulton, Keir. . Clausal complementation and the Wager-class. Proceedings of the North East Linguistics Society . URL http://sites.google.com/
site/keirmoulton/Moultonnelswager.pdf. http://people.umass.edu/keir/
Wager.pdf.
Moulton, Keir. . Natural selection and the syntax of clausal complementation:



P A

C 

University of Massachusetts at Amherst dissertation. URL http://scholarworks.
umass.edu/open_access_dissertations//.
Tamina Stephenson in her MIT dissertation and related work explores the way
attitude predicates interact with epistemic modals and taste predicates in their
complements:
Stephenson, Tamina. a. Judge dependence, epistemic modals, and predicates
of personal taste. Linguistics and Philosophy (). –. doi:./s--.
Stephenson, Tamina. b. Towards a theory of subjective meaning: Massachusetts Institute of Technology dissertation. URL http://semanticsarchive.
net/Archive/QxMjkO/Stephenson--thesis.pdf.
Jon Gajewski in his MIT dissertation and subsequent work explores the distribution of the - property among attitude predicates and traces it back to
presuppositional components of the meaning of the predicates:
Gajewski, Jon. . Neg-raising: Polarity and presupposition: Massachusetts
Institute of Technology dissertation. doi:./.
Gajewski, Jon. . Neg-raising and polarity. Linguistics and Philosophy
doi:./s---z.
Interesting work has also been done on presupposition projection in attitude
contexts:
Asher, Nicholas. . A typology for attitude verbs and their anaphoric properties. Linguistics and Philosophy (). –. doi:./BF.
Heim, Irene. . Presupposition projection and the semantics of attitude verbs.
Journal of Semantics (). –. doi:./jos/...
Geurts, Bart. . Presuppositions and anaphors in attitude contexts. Linguistics
& Philosophy (). –. doi:./A:.

C T
M
We turn to modal auxiliaries and related constructions. The main
diﬀerence from attitude constructions is that their semantics is more
context-dependent. Otherwise, we are still quantifying over possible
worlds.

.

The Quantiﬁcational Theory of Modality 
..
..

.

Syntactic Assumptions 
Quantiﬁcation over Possible Worlds 

Flavors of Modality 
..

Contingency 

..

Epistemic vs. Circumstantial Modality 

..

Contingency Again 

..

Iteration 

..

A technical variant of the analysis 

.
.

.

*Kratzer’s Conversational Backgrounds 
Supplementary Readings 

The Quantiﬁcational Theory of Modality

We will now be looking at modal auxiliaries like may, must, can, have to, etc.
Most of what we say here should carry over straightforwardly to modal adverbs
like maybe, possibly, certainly, etc. We will make certain syntactic assumptions,
which make our work easier but which leave aside many questions that at some
point deserve to be addressed.

M



..

The issue of raising vs. control
will be taken up later. If you
are eager to get started on it
and other questions of the
morphosyntax of modals, read
the handout from an LSA class
Sabine and Kai taught last
summer: http://web.mit.edu/
ﬁntel/lsa-class--handout.
pdf.

We will talk about reconstruction in more detail later.

C 

Syntactic Assumptions

We will assume, at least for the time being, that a modal like may is a 
predicate (rather than a  predicate), i.e., its subject is not its own argument, but has been moved from the subject-position of its inﬁnitival complement.
So, we are dealing with the following kind of structure:
()

a.
b.

Ann may be smart.
[ Ann [ λ [ may [ t be smart ]]]]

Actually, we will be working here with the even simpler structure below, in
which the subject has been reconstructed to its lowest trace position. (E.g.,
these could be generated by deleting all but the lowest copy in the movement
chain.) We will be able to prove that movement of a name or pronoun never
aﬀects truth-conditions, so at any rate the interpretation of the structure in (b)
would be the same as of (). As a matter of convenience, then, we will take
the reconstructed structures, which allow us to abstract away from the (here
irrelevant) mechanics of variable binding.
()

may [ Ann be smart ]

So, for now at least, we are assuming that modals are expressions that take a full
sentence as their semantic argument. Now then, what do modals mean?

..
This idea goes back a long
time. It was famously held
by Leibniz, but there are
precedents in the medieval
literature, see Knuuttila ().
See Copeland () for the
modern history of the possible
worlds analysis of modal
expressions.

Quantiﬁcation over Possible Worlds

The basic idea of the possible worlds semantics for modal expressions is that they
are quantiﬁers over possible worlds. Toy lexical entries for must and may, for
example, would look like this:
()

must

()

may

w,g
w,g

= λp

= λp

s,t

s,t

. ∀w : p(w ) = .
. ∃w : p(w ) = .

This analysis is too crude (in particular, notice that it would make modal sentences non-contingent — there is no occurrence of the evaluation world on the
right hand side!). But it does already have some desirable consequences that we
will seek to preserve through all subsequent reﬁnements. It correctly predicts a
number of intuitive judgments about the logical relations between must and may
and among various combinations of these items and negations. To start with
some elementary facts, we feel that must φ entails may φ, but not vice versa:
 We will assume that even though Ann be smart is a non-ﬁnite sentence, this will not have any
eﬀect on its semantic type, which is that of a sentence, which in turn means that its semantic
value is a truth-value. This is hopefully independent of the (interesting) fact that Ann be smart
on its own cannot be used to make a truth-evaluable assertion.

§.

T Q T  M

()

You must stay.
Therefore, you may stay.



You may stay.
Therefore, you must stay.



()
()

a.
b.



You may stay, but it is not the case that you must stay.
You may stay, but you don’t have to stay.


We judge must φ incompatible with its “inner negation” must [not φ ], but ﬁnd
may φ and may [not φ ] entirely compatible:
()

You must stay, and/but also, you must leave. (leave = not stay).


()

You may stay, but also, you may leave.


We also judge that in each pair below, the (a)-sentence and the (b)-sentences say
the same thing.
()

a.
b.

You must stay.
It is not the case that you may leave.
You aren’t allowed to leave.
(You may not leave.)
(You can’t leave.)

()

a.
b.

You may stay.
It is not the case that you must leave.
You don’t have to leave.
You don’t need to leave.
(You needn’t leave.)

Given that stay and leave are each other’s negations (i.e. leave w,g = not stay w,g ,
and stay w,g = not leave w,g ), the LF-structures of these equivalent pairs of
 The somewhat stilted it is not the case-construction is used in to make certain that negation
takes scope over must. When modal auxiliaries and negation are together in the auxiliary complex
of the same clause, their relative scope seems not to be transparently encoded in the surface
order; speciﬁcally, the scope order is not reliably negation modal. (Think about examples
with mustn’t, can’t, shouldn’t, may not etc. What’s going on here? This is an interesting topic
which we must set aside for now. See the references at the end of the chapter for relevant work.)
With modal main verbs (such as have to), this complication doesn’t arise; they are consistently
inside the scope of clause-mate auxiliary negation. Therefore we can use (b) to (unambiguously)
express the same scope order as (a), without having to resort to a biclausal structure.
 The parenthesized variants of the (b)-sentences are pertinent here only to the extent that we
can be certain that negation scopes over the modal. In these examples, apparently it does, but as
we remarked above, this cannot be taken for granted in all structures of this form.

M



C 

sentences can be seen to instantiate the following schemata:
()

must φ ≡ not [may [not φ]]
must [not ψ] ≡ not [may ψ]

()

More linguistic data regarding
the “parallel logic” of modals
and quantiﬁers can be found
in Larry Horn’s dissertation
(Horn ).

a.
b.
a.
b.

may φ ≡ not [must [not φ]]
may [not ψ] ≡ not [must ψ]

Our present analysis of must, have-to, . . . as universal quantiﬁers and of may, can,
. . . as existential quantiﬁers straightforwardly predicts all of the above judgments,
as you can easily prove.
()

a.
b.

∀xφ ≡ ¬∃¬φ
∀x¬φ ≡ ¬∃xφ

()

a.
b.

∃xφ ≡ ¬∀x¬φ
∃x¬φ ≡ ¬∀xφ

. Flavors of Modality
..

Contingency

We already said that the semantics we started with is too simple-minded. In
particular, we have no dependency on the evaluation world, which would make
modal statements non-contingent. This is not correct.
If one says It may be snowing in Cambridge, that may well be part of useful,
practical advice about what to wear on your upcoming trip to Cambridge. It may
be true or it may be false. The sentence seems true if said in the dead of winter
when we have already heard about a Nor’Easter that is sweeping across New
England. The sentence seems false if said by a clueless Australian acquaintance
of ours in July.
The contingency of modal claims is not captured by our current semantics.
All the may-sentence would claim under that semantics is that there is some
possible world where it is snowing in Cambridge. And surely, once you have
read Lewis’ quote in Chapter , where he asserts the existence of possible worlds
with diﬀerent physical constants than we enjoy here, you must admit that there
have to be such worlds even if it is July. The problem is that in our semantics,
repeated here
()

may

w,g

= λp

s,t

. ∃w : p(w ) = .

there is no occurrence of w on the right hand side. This means that the truthconditions for may-sentences are world-independent. In other words, they make
 In logicians’ jargon, must and may behave as  of each other. For deﬁnitions of “dual”,
see Barwise & Cooper (: ) or Gamut (: vol.,).

§.

F  M



non-contingent claims that are either true whatever or false whatever, and because
of the plenitude of possible worlds they are more likely to be true than false.
This needs to be ﬁxed. But how?
Well, what makes it may be snowing in Cambridge seem true when we know
about a Nor’Easter over New England? What makes it seem false when we
know that it is summer in New England? The idea is that we only consider
possible worlds       . And since
what evidence is available to us diﬀers from world to world, so will the truth of a
may-statement.
()

may

()

must

w,g

= λp. ∃w compatible with the evidence in w : p(w ) = .

w,g

= λp. ∀w compatible with the evidence in w : p(w ) = .

Let us consider a diﬀerent example:
()

You have to be quiet.

Imagine this sentence being said based on the house rules of the particular
dormitory you live in. Again, this is a sentence that could be true or could
be false. Why do we feel that this is a contingent assertion? Well, the house
rules can be diﬀerent from one world to the next, and so we might be unsure
or mistaken about what they are. In one possible world, they say that all noise
must stop at pm, in another world they say that all noise must stop at pm.
Suppose we know that it is : now, and that the dorm we are in has either
one or the other of these two rules, but we have forgotten which. Then, for all
we know, you have to be quiet may be true or it may be false. This suggests a
lexical entry along these lines:
()

have-to

w,g

= λp. ∀w compatible with the rules in w : p(w ) = .

Again, we are tying the modal statement about other worlds down to certain
worlds that stand in a certain relation to actual world: those worlds where the
rules as they are here are obeyed.
A note of caution: it is very important to realize that the worlds compatible
with the rules as they are in w are those worlds where nothing happens that
violates any of the w-rules. This is not at all the same as saying that the worlds
compatible with the rules in w are those worlds where the same rules are in
force. Usually, the rules do not care what the rules are, unless the rules contain
some kind of meta-statement to the eﬀect that the rules have to be the way they
are, i.e. that the rules cannot be changed. So, in fact, a world w in which
nothing happens that violates the rules as they are in w but where the rules are
quite diﬀerent and in fact what happens violates the rules as they are in w is
 From now on, we will leave oﬀ type-speciﬁcations such as that p has to be of type s, t ,
whenever it is obvious what they should be and when saving space is aesthetically called for.

M



C 

nevertheless a world compatible with the rules in w. For example, imagine that
the only relevant rule in w is that students go to bed before midnight. Take a
world w where a particular student goes to bed at : pm but where the rules
are diﬀerent and say that students have to go to bed before  pm. Such a world
w is compatible with the rules in w (but of course not with the rules in w ).
Apparently, there are diﬀerent ﬂavors of modality, varying in what kind of
facts in the evaluation world they are sensitive to. The semantics we gave for
must and may above makes them talk about evidence, while the semantics we
gave for have-to made it talk about rules. But that was just because the examples
were hand-picked. In fact, in the dorm scenario we could just as well have said
You must be quiet. And, vice versa, there is nothing wrong with using it has to
be snowing in Cambridge based on the evidence we have. In fact, many modal
expressions seem to be multiply ambiguous.
Traditional descriptions of modals often distinguish a number of “readings”:
, , , , , . . . . (Beyond “epistemic” and “deontic,” there is a great deal of terminological variety. Sometimes
all non-epistemic readings are grouped together under the term  .)
Here are some initial illustrations.
()

E M
A: Where is John?
B: I don’t know. He may be at home.

()

D M
A: Am I allowed to stay over at Janet’s house?
B: No, but you may bring her here for dinner.

()

C/D M
A: I will plant the rhododendron here.
B: That’s not a good idea. It can grow very tall.

How are may and can interpreted in each of these examples? What do the
interpretations have in common, and where do they diﬀer?
In all three examples, the modal makes an existentially quantiﬁed claim about
possible worlds. This is usually called the   of the claim. What
diﬀers is what worlds are quantiﬁed over. In  modal sentences, we
quantify over worlds compatible with the available evidence. In  modal
sentences, we quantify over worlds compatible with the rules and/or regulations.
And in the  modal sentence, we quantify over the set of worlds
which conform to the laws of nature (in particular, plant biology). What speaker
B in () is saying, then, is that there are some worlds conforming to the laws of
nature in which this rhododendron grows very tall. (Or is this another instance
of an epistemic reading? See below for discussion of the distinction between

§.

F  M



circumstantial readings and epistemic ones.)
How can we account for this variety of readings? One way would be to
write a host of lexical entries, basically treating this as a kind of (more or less
principled) ambiguity. Another way, which is preferred by many people, is to
treat this as a case of context-dependency, as argued in seminal work by Kratzer
(, , , ).
According to Kratzer, what a modal brings with it intrinsically is just a
modal force, that is, whether it is an existential (possibility) modal or a universal
(necessity) modal. What worlds it quantiﬁes over is determined by context. In
essence, the context has to supply a restriction to the quantiﬁer. How can we
implement this idea?
We encountered context-dependency before when we talked about pronouns
and their referential (and E-Type) readings (H& K, chapters –). We treated
referential pronouns as free variables, appealing to a general principle that free
variables in an LF need to be supplied with values from the utterance context. If
we want to describe the context-dependency of modals in a technically analogous
fashion, we can think of their LF-representations as incorporating or subcategorizing for a kind of invisible pronoun, a free variable that stands for a set of
possible worlds. So we posit LF-structures like this:
()

Angelika Kratzer
It is well-known that natural
language quantiﬁcation is in
general subject to contextual restriction. See Stanley & Szabó
() for a recent discussion.

[ I [ I must p s,t ] [ VP you quiet]]

s,t here is a variable over (characteristic functions of ) sets of worlds, which — like
all free variables — needs to receive a value from the utterance context. Possible
values include: the set of worlds compatible with the speaker’s current knowledge; the set of worlds in which everyone obeys all the house rules of a certain
dormitory; and many others. The denotation of the modal itself now has to be
of type st, st, t rather than st, t , thus it will be more like a quantiﬁcational
determiner rather than a complete generalized quantiﬁer. Only after the modal
has been combined with its covert restrictor do we obtain a value of type st, t .

p

()

a.
b.

= have-to w,g = need-to w,g = . . . =
λp ∈ D s,t . λq ∈ D s,t . ∀w ∈ W [p(w) =  → q(w) = ]
may w,g = can w,g = be-allowed-to w,g = . . . =
λp ∈ D s,t . λq ∈ D s,t . ∃w ∈ W [p(w) =  & q(w) = ]
must

w,g

On this approach, the epistemic, deontic, etc. “readings” of individual occurrences of modal verbs come about by a combination of two separate things.
The lexical semantics of the modal itself encodes just a quantiﬁcational force,
a relation between sets of worlds. This is either the subset-relation (universal
quantiﬁcation; necessity) or the relation of non-disjointness (existential quantiﬁcation; possibility). The covert variable next to the modal picks up a contextually
salient set of worlds, and this functions as the quantiﬁer’s restrictor. The labels
“epistemic”, “deontic”, “circumstantial” etc. group together certain conceptually

in set talk: p ⊆ q
in set talk: p ∩ q = ∅

M



C 

natural classes of possible values for this covert restrictor.
Notice that, strictly speaking, there is not just one deontic reading (for
example), but many. A speaker who utters
()

You have to be quiet.

might mean: ‘I want you to be quiet,’ (i.e., you are quiet in all those worlds
that conform to my preferences). Or she might mean: ‘unless you are quiet, you
won’t succeed in what you are trying to do,’ (i.e., you are quiet in all those worlds
in which you succeed at your current task). Or she might mean: ‘the house rules
of this dormitory here demand that you be quiet,’ (i.e., you are quiet in all those
worlds in which the house rules aren’t violated). And so on. So the label “deontic”
appears to cover a whole open-ended set of imaginable “readings”, and which
one is intended and understood on a particular utterance occasion may depend
on all sorts of things in the interlocutors’ previous conversation and tacit shared
assumptions. (And the same goes for the other traditional labels.)

..

Epistemic vs. Circumstantial Modality

Is it all context-dependency? Or do ﬂavors of modality correspond to some sorts
of signals in the structure of sentences? Read the following famous passage from
Kratzer and think about how the two sentences with their very diﬀerent modal
meanings diﬀer in structure:
Quoted from Kratzer (). In
Kratzer (), the hydrangeas
were Zwetschgenbäume ‘plum
trees’. The German word
Zwetschge, by the way, is
etymologically derived from
the name of the city Damascus
(Syria), the center of the
ancient plum trade.

Consider sentences () and ():
()
()

Hydrangeas can grow here.
There might be hydrangeas growing here.

The two sentences diﬀer in meaning in a way which is illustrated by
the following scenario.
“Hydrangeas”
Suppose I acquire a piece of land in a far away country and
discover that soil and climate are very much like at home, where
hydrangeas prosper everywhere. Since hydrangeas are my favorite
plants, I wonder whether they would grow in this place and inquire
about it. The answer is (). In such a situation, the proposition
expressed by () is true. It is true regardless of whether it is or
isn’t likely that there are already hydrangeas in the country we are
considering. All that matters is climate, soil, the special properties
of hydrangeas, and the like. Suppose now that the country we are in
has never had any contacts whatsoever with Asia or America, and
the vegetation is altogether diﬀerent from ours. Given this evidence,
my utterance of () would express a false proposition. What counts

§.

F  M



here is the complete evidence available. And this evidence is not
compatible with the existence of hydrangeas.
() together with our scenario illustrates the pure  reading of the modal can. [. . . ]. () together with our scenario
illustrates the epistemic reading of modals. [. . . ] circumstantial
and epistemic conversational backgrounds involve diﬀerent kinds
of facts. In using an epistemic modal, we are interested in what else
may or must be the case in our world given all the evidence available.
Using a circumstantial modal, we are interested in the necessities
implied by or the possibilities opened up by certain sorts of facts.
Epistemic modality is the modality of curious people like historians,
detectives, and futurologists. Circumstantial modality is the modality of rational agents like gardeners, architects, and engineers. A
historian asks what might have been the case, given all the available
facts. An engineer asks what can be done given certain relevant facts.
Consider also the very diﬀerent prominent meanings of the following two
sentences, taken from Kratzer as well:
()

a.
b.

Cathy can make a pound of cheese out of this can of milk.
Cathy might make a pound of cheese out of this can of milk.

E .: Come up with examples of epistemic, deontic, and circumstantial
uses of the necessity verb have to. Describe the set of worlds that constitutes the
understood restrictor in each of your examples.

.. Contingency Again
We messed up. If you inspect the context-dependent meanings we have on the
table now for our modals, you will see that the right hand sides again do not
mention the evaluation world w. Therefore, we will again have the problem of
not making contingent claims, indirectly about the actual world. This needs to
be ﬁxed. We need a semantics that is both context-dependent and contingent.
The problem, it turns out, is with the idea that the utterance context supplies
a determinate set of worlds as the restrictor. When I understand that you meant
your use of must, in you must be quiet, to quantify over the set of worlds in which
the house rules of our dorm are obeyed, this does not imply that you and I have
to know or agree on which set exactly this is. That depends on what the house
rules in our world actually happen to say, and this may be an open question at
the current stage of our conversation. What we do agree on, if I have understood
your use of must in the way that you intended it, is just that it quantiﬁes over
whatever set of worlds it may be that the house rules pick out.

M


You will of course recognize
that functions of type s, st
are simply a schönﬁnkeled
version of the 
 we introduced in
the previous chapter.

C 

The technical implementation of this insight requires that we think of the
context’s contribution not as a set of worlds, but rather as a function which for
each world it applies to picks out such a set. For example, it may be the function
which, for any world w, yields the set {w : the house rules that are in force in w
are obeyed in w }. If we apply this function to a world w , in which the house
rules read “no noise after  pm”, it will yield a set of worlds in which nobody
makes noise after  pm. If we apply the same function to a world w , in which
the house rules read “no noise after  pm”, it will yield a set of worlds in which
nobody makes noise after  pm.
Suppose, then, that the covert restrictor of a modal predicate denotes such a
function, i.e., its value is of type s, st .
()

[ I’ [ I must R s,st ] [ VP you quiet]]

And the new lexical entries for must and may that will ﬁt this new structure are
these:
()

a.

in set talk: (R(w) ⊆ q

b.
in set talk: (R(w) ∩ q = ∅

= have-to w,g = need-to w,g = . . . =
λR ∈ D s,st . λq ∈ D s,t . ∀w ∈ W [R(w)(w ) =  → q(w ) = ]
may w,g = can w,g = be-allowed-to w,g = . . . =
λR ∈ D s,st . λq ∈ D s,t . ∃w ∈ W [R(w)(w ) =  & q(w ) = ]
must

w,g

Let us see now how this solves the contingency problem.
()

Let w be a world, and assume that the context supplies an assignment g
such that g(R) = λw. λw . the house rules in force in w are obeyed in
w

must R you quiet w,g =
(IFA)
w,g
w
must R
(λw you quiet ) =
(FA)
w,g
w,g
w
must
(R
) (λw you quiet ) =
(lex. entries you, quiet)
must w,g ( R w,g ) (λw . you are quiet in w ) =
(lex. entry must)
w,g
(w)(w ) =  → you are quiet in w =
∀w ∈ W : R
(pronoun rule)
∀w ∈ W : g(R)(w)(w ) =  → you are quiet in w =
(def. of g)
∀w ∈ W [the house rules in force in w are obeyed in w
→ you are quiet in w ]
As we see in the last line of (), the truth-value of () depends on the evaluation
world w.
E .: Describe two worlds w and w so that
must R you quiet w ,g =  and must R you quiet w ,g = .
E .: In analogy to the deontic relation g(R) deﬁned in (), deﬁne an
appropriate relation that yields an epistemic reading for a sentence like You may
be quiet.

§.

F  M

..

Iteration



Consider the following example:
()

You might have to leave.

What does this mean? Under one natural interpretation, we learn that the speaker
considers it possible that the addressee is under the obligation to leave. This
seems to involve one modal embedded under a higher modal. It appears that
this sentence should be true in a world w iﬀ some world w compatible with
what the speaker knows in w is such that every world w in which the rules as
they are in w are followed is such that you leave in w .
Assume the following LF:
()

[ I [ might R ] [ VP [ have-to R ] [ IP you leave]]]

Suppose w is the world for which we calculate the truth-value of the whole
sentence, and the context maps R to the function which maps w to the set of all
those worlds compatible with what is known in w. might says that some of those
worlds are worlds w that make the tree below might true. Now assume further
that the context maps R to the function which assigns to any such world w the
set of all those worlds in which the rules as they are in w are followed. have to
says that all of those worlds are worlds w in which you leave.
In other words, while it is not known to be the case that you have to leave,
for all the speaker knows it might be the case.
E .: Describe values for the covert s, st -variable that are intuitively
suitable for the interpretation of the modals in the following sentences:
()

As far as John’s preferences are concerned, you may stay with us.

()

According to the guidelines of the graduate school, every PhD candidate
must take  credit hours outside his/her department.

()

John can run a mile in  minutes.

()

This has to be the White House.

()

This elevator can carry up to  pounds.

For some of the sentences, diﬀerent interpretations are conceivable depending
on the circumstances in which they are uttered. You may therefore have to
sketch the utterance context you have in mind before describing the accessibility
relation.
E .: Collect two naturally occurring examples of modalized sentences
(e.g., sentences that you overhear in conversation, or read in a newspaper or novel
– not ones that are being used as examples in a linguistics or philosophy paper!),
and give deﬁnitions of values for the covert s, st -variable which account for the

There is more to be said about
which modals can embed
under which other modals.
See for some discussion the
handout mentioned earlier:
http://web.mit.edu/ﬁntel/
lsa-class--handout.pdf.

M



C 

way in which you actually understood these sentences when you encountered
them. (If the appropriate interpretation is not salient for the sentence out of
context, include information about the relevant preceding text or non-linguistic
background.)

..

A technical variant of the analysis

In our account of the contingency of modalized sentences, we adopted lexical entries for the modals that gave them world-dependent extensions of type
s, st , st, t :
()

(repeated from earlier):
For any w ∈ W : must w,g
λR ∈ D s,st . λq ∈ D s,t . ∀w ∈ W [R(w)(w ) =  → q(w ) = ]
(in set talk: λR s,st . λq s,t . (R(w) ⊆ q)).

Unfortunately, this treatment somewhat obscures the parallel between the modals
and the quantiﬁcational determiners, which have world-independent extensions
of type et, et, t .
Let’s explore an alternative solution to the contingency problem, which will
allow us to stick with the world-independent type- st, st, t -extensions that
we assumed for the modals at ﬁrst:
()

(repeated from even earlier):
must w,g = λp ∈ D s,t . λq ∈ D s,t . ∀w ∈ W [p(w) =  → q(w) = ]
(in set talk: λp ∈ D s,t . λq ∈ D s,t . p ⊆ q).

We posit the following LF-representation:
()

[ I [ I must [ R , s,st w*]] [ VP you quiet]]

What is new here is that the covert restrictor is complex. The ﬁrst part, R , s,st ,
is (as before) a free variable of type s, st , which gets assigned an accessibility
relation by the context of utterance. The second part is a special terminal symbol
which is interpreted as picking out the evaluation world:
()

For any w ∈ W : w∗

w,g

= w.

When R , s,st and w* combine (by Functional Application), we obtain a
constituent whose extension is of type s, t (a proposition or set of worlds). This
is the same type as the extension of the free variable p in the previous proposal,
hence suitable to combine with the old entry for must (by FA). However, while
the extension of p was completely ﬁxed by the variable assignment, and did not
 Dowty () introduced an analogous symbol to pick out the evaluation time. We have
chosen the star-notation to allude to this precedent.

§.

*K’ C B



vary with the evaluation world, the new complex constituent’s extension depends
on both the assignment and the world:
()

For any w ∈ W and any assignment g:
R , s,st (w*) w,g = g( , s, st )(w).

As a consequence of this, the extensions of the higher nodes I and I will also
vary with the evaluation world, and this is how we capture the fact that () is
contingent.
Maybe this variant is more appealing. But for the rest of this chapter, we
continue to assume the original analysis as presented earlier. In the next chapter
on conditionals, we will however make crucial use of this way of formulating the
semantics for modals. So, make sure you understand what we just proposed.

. *Kratzer’s Conversational Backgrounds
Angelika Kratzer has some interesting ideas on how accessibility relations are
supplied by the context. She argues that what is really ﬂoating around in a
discourse is a  . Accessibility relations can be
computed from conversational backgrounds (as we shall do here), or one can
state the semantics of modals directly in terms of conversational backgrounds (as
Kratzer does).
A conversational background is the sort of thing that is identiﬁed by phrases
like what the law provides, what we know, etc. Take the phrase what the law
provides. What the law provides is diﬀerent from one possible world to another.
And what the law provides in a particular world is a set of propositions. Likewise,
what we know diﬀers from world to world. And what we know in a particular
world is a set of propositions. The intension of what the law provides is then that
function which assigns to every possible world the set of propositions p such that
the law provides in that world that p. Of course, that doesn’t mean that p holds in
that world itself: the law can be broken. And the intension of what we know will
be that function which assigns to every possible world the set of propositions we
know in that world. Quite generally, conversational backgrounds are functions
of type s, st, t , functions from worlds to (characteristic functions of ) sets of
propositions.
Now, consider:
()

(In view of what we know,) Brown must have murdered Smith.

The in view of -phrase may explicitly signal the intended conversational background. Or, if the phrase is omitted, we can just infer from other clues in the
discourse that such an epistemic conversational background is intended. We will
focus on the case of pure context-dependency.



M

C 

How do we get from a conversational background to an accessibility relation?
Take the conversational background at work in (). It will be the following:
()

λw. λp. p is one of the propositions that we know in w.

This conversational background will assign to any world w the set of propositions
p that in w are known by us. So we have a set of propositions. From that we can
get the set of worlds in which all of the propositions in this set are true. These
are the worlds that are compatible with everything we know. So, this is how we
get an accessibility relation:
()

For any conversational background f of type s, st, t , we deﬁne the
corresponding accessibility relation Rf of type s, st as follows:
Rf := λw. λw . ∀p [f(w)(p) =  → p(w ) = ].

In words, w is f-accessible from w iﬀ all propositions p that are assigned by f to
w are true in w .
Kratzer uses the term   for the conversational background that
determines the set of accessible worlds. We can be sloppy and use this term for a
number of interrelated concepts:
(i) the conversational background (type s, st, t ),
(ii) the set of propositions assigned by the conversational background to a
particular world (type st, t ),
(iii) the accessibility relation (type s, st ) determined by (i),
(iv) the set of worlds accessible from a particular world (type s, t ).
Kratzer calls a conversational background (modal base)  iﬀ it assigns to
any world a set of propositions that are all true in that world. The modal base
what we know is realistic, the modal bases what we believe and what we want are
not.
What follows are some (increasingly technical exercises) on conversational backgrounds.
E .: Show that a conversational background f is realistic iﬀ the corresponding accessibility relation Rf (deﬁned as in ()) is reﬂexive.
E .: Let us call an accessibility relation  if it makes every world
accessible from every world. R is  iﬀ ∀w∀w : w ∈ R(w). What would
the conversational background f have to be like for the accessibility relation Rf
to be trivial in this sense?
E .: The deﬁnition in () speciﬁes, in eﬀect, a function from D s, st,t
to D s,st . It maps each function f of type s, st, t to a unique function Rf of

§.

*K’ C B



type s, st . This mapping is not one-to-one, however. Diﬀerent elements of
D s, st,t may be mapped to the same value in D s,st .
• Prove this claim. I.e., give an example of two functions f and f ’ in D s, st,t
for which () determines Rf = Rf .
• As you have just proved, if every function of type s, st, t qualiﬁes as
a ‘conversational background’, then two diﬀerent conversational backgrounds can collapse into the same accessibility relation. Conceivably,
however, if we imposed further restrictions on conversational backgrounds
(i.e., conditions by which only a proper subset of the functions in D s, st,t
would qualify as conversational backgrounds), then the mapping between
conversational backgrounds and accessibility relations might become oneto-one after all. In this light, consider the following potential restriction:
()

Every conversational background f must be “closed under entailment”; i.e., it must meet this condition:
∀w.∀p [∩f(w) ⊆ p → p ∈ f(w)].

(In words: if the propositions in f(w) taken together entail p, then p
must itself be in f(w).) Show that this restriction would ensure that the
mapping deﬁned in () will be one-to-one.

 In this exercise, we systematically substitute sets for their characteristic functions. I.e., we
pretend that D s,t is the power set of W (i.e., elements of D s,t are sets of worlds), and D st,t is
the power set of D s,t (i.e., elements of D st,t are sets of sets of worlds). On these assumptions,
the deﬁnition in () can take the following form:
(i)

For any conversational background f of type s, st, t ,
we deﬁne the corresponding accessibility relation Rf of type s, st as follows:
Rf := λw. {w : ∀p [p ∈ f(w) → w ∈ p]}.

The last line of this can be further abbreviated to:
(ii)

Rf := λw. ∩ f(w)

This formulation exploits a set-theoretic notation which we have also used in condition () of
the second part of the exercise. It is deﬁned as follows:
(iii)

If S is a set of sets, then ∩S := {x : ∀Y [Y ∈ S → x ∈ Y]}.

M



.

C 

Supplementary Readings

The most important background readings for this chapter are the following two
papers by Kratzer:
Kratzer, Angelika. . The notional category of modality. In Hans-Jürgen
Eikmeyer & Hannes Rieser (eds.), Words, worlds, and contexts: New approaches
in word semantics (Research in Text Theory ), –. Berlin: de Gruyter.
Kratzer, Angelika. . Modality. In Arnim von Stechow & Dieter Wunderlich
(eds.), Semantics: An international handbook of contemporary research, –.
Berlin: de Gruyter.
Kratzer has been updating her classic papers for a volume of her collected
work on modality and conditionals. These are very much worth studying:
http://semanticsarchive.net/Archive/TcNjAM/.
A major new resource on modality is Paul Portner’s book:
Portner, Paul. . Modality. Oxford University Press.
You might also proﬁt from other survey-ish type papers:
von Fintel, Kai. . Modality and language. In Donald M. Borchert (ed.),
Encyclopedia of philosophy – second edition, MacMillan. URL http://mit.edu/
ﬁntel/ﬁntel--modality.pdf.
von Fintel, Kai & Anthony S. Gillies. . An opinionated guide to epistemic
modality. In Tamar Szabó Gendler & John Hawthorne (eds.), Oxford studies
in epistemology: Volume , –. Oxford University Press. URL http://mit.
edu/ﬁntel/ﬁntel-gillies--ose.pdf.
Swanson, Eric. . Modality in language. Philosophy Compass (). –.
doi:./j.-...x.
Hacquard, Valentine. . Modality. Ms, prepared for Semantics: An international handbook of meaning, edited by Klaus von Heusinger, Claudia
Maienbon, and Paul Portner. URL http://ling.umd.edu/~hacquard/papers/
HoS_Modality_Hacquard.pdf.
On the syntax of modals, there are only a few papers of uneven quality. Some
of the more recent work is listed here. Follow up on older references from the
bibliographies in these papers.
Bhatt, Rajesh. . Obligation and possession. In Heidi Harley (ed.), Papers
from the upenn/mit roundtable on argument structure and aspect, vol.  MIT
Working Papers in Linguistics, –. URL http://people.umass.edu/bhatt/
papers/bhatt-haveto.pdf.

§.

S R



Wurmbrand, Susi. . Modal verbs must be raising verbs. West Coast Conference
on Formal Linguistics . –. URL http://wurmbrand.uconn.edu/Susi/
Papers/WCCFL.pdf.
Cormack, Annabel & Neil Smith. . Modals and negation in English. In
Sjef Barbiers, Frits Beukema & Wim van der Wurﬀ (eds.), Modality and its
interaction with the verbal system, –. Benjamins.
Butler, Jonny. . A minimalist treatment of modality. Lingua (). –.
doi:./S-()-.
The following paper explores some issues in the LF-syntax of epistemic modals:
von Fintel, Kai & Sabine Iatridou. . Epistemic containment. Linguistic
Inquiry (). –. doi:./.
Valentine Hacquard’s MIT dissertation is a rich source of cross-linguistic issues
in modality, as is Fabrice Nauze’s Amsterdam dissertation:
Hacquard, Valentine. . Aspects of modality: Massachusetts Institute of
Technology dissertation. URL http://people.umass.edu/hacquard/hacquard_
thesis.pdf.
Nauze, Fabrice. . Modality in typological perspective: Universiteit van Amsterdam dissertation. URL http://www.illc.uva.nl/Publications/Dissertations/
DS--.text.pdf.
The semantics of epistemic modals has become a hot topic recently. Here are the
main references:
DeRose, Keith. . Epistemic possibilities. The Philosophical Review ().
–. doi:./.
Egan, Andy, John Hawthorne & Brian Weatherson. . Epistemic modals in
context. In Gerhard Preyer & Georg Peter (eds.), Contextualism in philosophy:
Knowledge, meaning, and truth, –. Oxford: Oxford University Press.
Egan, Andy. . Epistemic modals, relativism, and assertion. Philosophical
Studies (). –. doi:./s---x.
MacFarlane, John. . Epistemic modals are assessment-sensitive. Ms, University of California, Berkeley, forthcoming in an OUP volume on epistemic modals, edited by Brian Weatherson and Andy Egan. URL http:
//sophos.berkeley.edu/macfarlane/epistmod.pdf.
von Fintel, Kai & Anthony S. Gillies. a. CIA leaks. The Philosophical Review
(). –. doi:./--.
von Fintel, Kai & Anthony S. Gillies. b. Might made right. To appear in a
volume on epistemic modality, edited by Andy Egan and Brian Weatherson,



M

C 

Oxford University Press. URL http://mit.edu/ﬁntel/ﬁntel-gillies--mmr.
pdf.
Stephenson, Tamina. a. Judge dependence, epistemic modals, and predicates
of personal taste. Linguistics and Philosophy (). –. doi:./s--.
A recent SALT paper by Pranav Anand and Valentine Hacquard tackles what
happens to epistemic modals under attitude predicates:
Anand, Pranav & Valentine Hacquard. . Epistemics with attitude. Proceedings of Semantics and Linguistic Theory . doi:/.
Evidentiality is a topic closely related to epistemic modality. Some references:
Willett, Thomas. . A cross-linguistic survey of the grammaticalization of
evidentiality. Studies in Language (). –.
Aikhenvald, Alexandra Y. . Evidentiality. Oxford: Oxford University Press.
Drubig, Hans Bernhard. . On the syntactic form of epistemic modality. Ms,
Universität Tübingen. URL http://www.sfb.uni-tuebingen.de/b/papers/
DrubigModality.pdf.
Blain, Eleanor M. & Rose-Marie Déchaine. . Evidential types: Evidence
from Cree dialects. International Journal of American Linguistics (). –.
doi:./.
McCready, Eric & Norry Ogata. . Evidentiality, modality and probability.
Linguistics and Philosophy (). –. doi:./s---.
Speas, Peggy. . On the syntax and semantics of evidentials. Language and
Linguistics Compass (). –. doi:./j.-X...x.
von Fintel, Kai & Anthony S. Gillies. . Must . . . stay . . . strong! Final
pre-print, to appear in Natural Language Semantics. URL http://mit.edu/
ﬁntel/ﬁntel-gillies--mss.pdf.

C F
C
We integrate conditionals into the semantics of modal expressions that
we are developing. We show that the material implication analysis and
the strict implication analysis are inferior to the restrictor analysis. Our
discussion will remain focussed on some simple questions and we refer
you to the rich literature on conditionals for further topics.

.

The Material Implication Analysis 

.

The Strict Implication Analysis 

.

If -Clauses as Restrictors 

Supplemental Readings 

.

The Material Implication Analysis

Consider the following example:
()

If I am healthy, I will come to class.

The simplest analysis of such conditional constructions is the so-called 
 analysis, which treats if as contributing a truth-function operating
on the truth-values of the two component sentences (which are called the  and  — from Latin — or  and  — from
Greek). The lexical entry for if would look as follows:
()

if = λu ∈ Dt . λv ∈ Dt . u =  or v = .

Applied to example in (), this semantics would predict that the example is
false just in case the antecedent is true, I am healthy, but the consequent false, I
do not come to class. Otherwise, the sentence is true. We will see that there is
 Quoth the Stoic philosopher Philo of Megara: “a true conditional is one which does not have
a true antecedent and a false consequent” (according to Sextus Empiricus (c. : II, –)).

Note that as a truth-functional
connective, this if does not
vary its denotation depending
on the evaluation world.
It’s its arguments that vary
with the evaluation world.

C



C 

much to complain about here. But one should realize that under the assumption
that if denotes a truth-function, this one is the most plausible candidate.
Suber () does a good job of persuading (or at least trying to persuade)
recalcitrant logic students:
After saying all this, it is important to note that material implication
does conform to some of our ordinary intuitions about implication.
For example, take the conditional statement, If I am healthy, I will
come to class. We can symbolize it: H ⊃ C.
The question is: when is this statement false? When will I have
broken my promise? There are only four possibilities:
H C
T T
T F
F T
F F

H⊃ C
?
?
?
?

• In case #, I am healthy and I come to class. I have clearly kept
my promise; the conditional is true.
• In case #, I am healthy, but I have decided to stay home and
read magazines. I have broken my promise; the conditional is
false.
• In case #, I am not healthy, but I have come to class anyway. I
am sneezing all over you, and you’re not happy about it, but I
did not violate my promise; the conditional is true.
• In case #, I am not healthy, and I did not come to class. I did
not violate my promise; the conditional is true.
But this is exactly the outcome required by the material implication.
The compound is only false when the antecedent is true and the
consequence is false (case #); it is true every other time.
Despite the initial plausibility of the analysis, it cannot be maintained. Consider
this example:
()

If there is a major earthquake in Cambridge tomorrow, my house will
collapse.

 The symbol ⊃ which Suber uses here is called the “horseshoe”. We have been using the right
arrow → as the symbol for implication. We think that this is much preferable to the confusing
horseshoe symbol. There is an intimate connection between universal quantiﬁcation, material
implication, and the subset relation, usually symbolized as ⊂, which is the other way round from
the horseshoe. The horseshoe can be traced back to the notation introduced by Peano (), a
capital C standing for ‘conseguenza’ facing backwards. The C facing in the other (more “logical”)
direction was actually introduced ﬁrst by Gergonne (), but didn’t catch on.

§.

T M I A



If we adopt the material implication analysis, we predict that () will be false
just in case there is indeed a major earthquake in Cambridge tomorrow but my
house fails to collapse. This makes a direct prediction about when the negation
of () should be true. A false prediction, if ever there was one:
()

a.
b.

It’s not true that if there is a major earthquake in Cambridge
tomorrow, my house will collapse.
≡ There will be a major earthquake in Cambridge tomorrow, and
my house will fail to collapse.

Clearly, one might think that (a) is true without at all being committed to
what the material implication analysis predicts to be the equivalent statement in
(b). This is one of the inadequacies of the material implication analysis.
These inadequacies are sometimes referred to as the “paradoxes of material
implication”. But that is misleading. As far as logic is concerned, there is nothing
wrong with the truth-function of material implication. It is well-behaved and
quite useful in logical systems. What is arguable is that it is not to be used as a
reconstruction of what conditionals mean in natural language.
E .: Under the assumption that if has the meaning in (), calculate
the truth-conditions predicted for ():
()

a.
b.

No student will succeed if he goofs oﬀ.
No student λx (if x goofs oﬀ, x will succeed)

State the predicted truth-conditions in words and evaluate whether they correspond to the actual meaning of ().

S

S

Conditional
if

S

S

Modal

A problem that is not often raised for the material implication analysis is
how badly it interacts with the analysis of modal expressions, once we look at
Figure .: LF A for
sentences involving both a conditional clause and a modal. Consider:
()
()
If we are on Route , we might be in Lockhart now.
we are on Route 

might

R

we be in Lockhart

S

()

If you keep this fern dry, it cannot grow.

Modal
might

S
R
Conditional

S

We need to consider two possible LFs for these sentences, depending on whether
wider scope is given to the modal or to the conditional clause. For example, in
the margin you see LFs A and B for ().
Figure .: LF B for
The reading for () we have in mind is an epistemic one; imagine for
()
instance that () is uttered in a car by Mary to Susan, while Susan is driving
and Mary is looking at a map. The information provided by the map, together
with other background knowledge, constitutes the relevant context for the modal
might here. The accessibility relation is roughly this:
if

S

we are on Route 

we be in Lockhart


()

C

C 

λw. λw . w is compatible with what the map says in w and what Mary
knows about the geography of the relevant area in w.

Let’s suppose () is uttered in the actual world w and we are interested in its
truth-value at this world. We now proceed to show that neither of the LFs A
and B represent the intuitively natural meaning of () if we assume the material
implication analysis of if.
Consider ﬁrst LF A. There are two respects in which the predicted truthconditions for this LF deviate from intuitive judgment. First, suppose that Susan
and Mary are not on Route  in w . Then () is predicted to be true in w ,
regardless of the geographical facts, e.g. even if Lockhart is nowhere near Route
. This is counterintuitive. Imagine the following quite sensible dialogue:
()

Mary: If we are on Route , we might be in Lockhart now.
Susan (stops the car and looks at the map): You are wrong. Look here,
Route  doesn’t run anywhere near Lockhart.

If Mary concedes Susan’s claim that Route  doesn’t go through Lockhart, she
has to also concede that her original assertion was false. It wouldn’t do for her to
respond: “I know that  runs about  miles east of Lockhart, but maybe we
are not on Route , so I may still be right.” Yet we predict that this should be a
reasonable way for her to defend ().
A second inadequacy is this: we predict that the truth of the consequent of
() is a suﬃcient condition for the truth of () as a whole. If this were right,
it would take very little for () to be true. As long as the map and the rest of
Mary’s knowledge in w don’t rule out the possibility that they are in Lockhart,
we might be in Lockhart will be true in w — regardless, once again, of whether
Lockhart is anywhere near . It should therefore be reasonable for Mary to
continue the dialogue in () with the rejoinder: “But how can you be so sure we
are not in Lockhart?” According to intuitive judgment, however, this would not
be a pertinent remark and certainly would not help Mary defend () against
Susan’s objection.
Now let’s look at LF B, where the modal has widest scope. Given the material
implication analysis of if, this is predicted to mean, in eﬀect: “It might be the
case that we are either in Lockhart or not on Route ”. This truth-condition
is also far too easy to satisfy: All it takes is that the map and the rest of Mary’s
knowledge in w are compatible with Mary and Susan not being on Route ,
or that they are compatible with their being in Lockhart. So as long as it isn’t
certain that they are on Route , Mary should be justiﬁed in asserting (),
regardless, once again, of her information about the relative location of Lockhart
and Route .
E .: Show that similar diﬃculties arise for the analysis of ().

§.

T S I A

.

The Strict Implication Analysis



Some of the problems we encountered would go away if we treated if as introducing a modal meaning. The simplest way to do that would be to treat it as
a universal quantiﬁer over possible worlds. If p, q would simply mean that the
set of p-worlds is a subset of the q-worlds. This kind of analysis is usually called
 . The diﬀerence between if and must would be that if takes
an overt restrictive argument. Here is what the lexical entry for if might look
like:
()

if

w,g

= λp ∈ D

s,t

. λq ∈ D s,t . ∀w : p(w ) =  → q(w ) = .

(in set talk: p ⊆ q)

Applied to (), we would derive the truth-conditions that () is true iﬀ all of
the worlds where there is a major earthquake in Cambridge tomorrow are worlds
where my house collapses.
We immediately note that this analysis has the same problem of noncontingency that we faced with one of our early attempts at a quantiﬁcational
semantics for modals like must and may. The obvious way to ﬁx this here is
to assume that if takes a covert accessibility function as one of its arguments.
The antecedent clause then serves as an additional restrictive device. Here is the
proposal:
()

if

w,g

= λR ∈ D

. λp ∈ D s,t . λq ∈ D s,t .
∀w : (R(w)(w ) =  & p(w ) = ) → q(w ) = .

s, s,t

(in set talk: R(w) ∩ p ⊆ q)

If we understand () as involving an epistemic accessibility relation, it would
claim that among the worlds epistemically accessible from the actual world
(i.e. the worlds compatible with what we know), those where there is a major
earthquake in Cambridge tomorrow are worlds where my house collapses. This
would appear to be quite adequate — although potentially traumatic to me.
E .: Can you come up with examples where a conditional is interpreted
relative to a non-epistemic accessibility relation?
E .: What prediction does the strict implication analysis make about
the negated conditional in (a)?
What happens when we let this analysis loose on ()? We again need to
assess two LFs depending on the relative scope of if and might. Both LFs would
have two covert variables over accessibility relations, one for if and one for might.
Before we can assess the adequacy of the two candidate analyses, we need to

C



C 

decide what the contextually salient values for the accessibility relations might be.
One would think that the epistemic accessibility relation that we have already
encountered is the most likely value, and in fact for both variables.
Next, we need to consider the particular epistemic state that Mary is in.
By assumption, Mary does not know where they are. Nothing in her visual
Figure .: LF A for
environment helps her ﬁgure out where they are. She does see from the map that
()
if they are on Route , one of the towns they might be in is Lockhart. But she
doesn’t know whether they are on Route . Even if they are on , she doesn’t
know that they are and her epistemic state would still be what it is: one of being
lost.
Consider then LF A , with the modal in the scope of the conditional. Here,
we derive the claim that all worlds w compatible with what Mary knows in w
Figure .: LF B for and where they are on  are such that some world w compatible with what
Mary knows in w is such that they are in Lockhart. Is that adequate? Not really.
()
We have just convinced ourselves that whether they are on  or not has no
relevant inﬂuence on Mary’s epistemic state, since she wouldn’t know it either
way. But that means that our analysis would predict that () is true as long as it
is possible as far as Mary knows that they are in Lockhart. Whether they are on
 or not doesn’t change that. So, we would expect () to not be distinct in
truth-value from something like:
S

S

Conditional

S

if

S

Modal

R

we are on Route 

might

R

we be in Lockhart

S

Modal

might

S

R

Conditional

S

S

if

we be in Lockhart

R

we are on Route 

()

If we are on the turnpike, we might be in Lockhart.

But that is not right — Mary knows quite well that if they are on the turnpike,
they cannot be in Lockhart.
Turning to LF B , with the modal having widest scope, doesn’t help us either.
Here, we would derive the claim that it is compatible with what Mary knows
that from being on  it follows (according to what she knows) that they are in
Lockhart. Clearly, that is not what () means. Mary doesn’t consider it possible
that if they are on , she knows that they are in Lockhart. After all, she’s well
aware that she doesn’t know where they are.

. If -Clauses as Restrictors
The problem we have encountered here with the interaction of an if -clause
and the modal operator might is similar to others that have been noted in the
literature. Most inﬂuentially, David Lewis in his paper “Adverbs of Quantiﬁcation” showed how hard it is to ﬁnd an adequate analysis of the interaction of
if -clauses and    like never, rarely, sometimes, often,
usually, always. Lewis proposed that in the cases he was considering, the adverb
is the only operator at work and that the if -clause serves to restrict the adverb.
Thus, it has much the same function that a common noun phrase has in a
determiner-quantiﬁcation.
David Lewis

§.

If -C  R



The if of our restrictive if -clauses should not be regarded as a
sentential connective. It has no meaning apart from the adverb
it restricts. The if in always if . . . , . . . , sometimes if . . . , . . . , and
the rest is on a par with the non-connective and in between . . . and
. . . , with the non-connective or in whether . . . or . . . , or with the
non-connective if in the probability that . . . if . . . . It serves merely to
mark an argument-place in a polyadic construction. (Lewis : )
Building on Lewis’ insight, Kratzer argued for a uniform treatment of if -clauses
as restrictors. She claimed that
the history of the conditional is the story of a syntactic mistake.
There is no two-place if . . . then connective in the logical forms of
natural languages. If -clauses are devices for restricting the domains
of various operators. (Kratzer )
Let us repeat this:
()

K’ T
If -clauses are devices for restricting the domains of various operators.

Kratzer’s Thesis gives a uniﬁed picture of the semantics of conditional clauses.
Note that it is not meant to supplant previous accounts of the meaning of
conditionals. It just says that what those accounts are analyzing is not the
meaning of if itself but the meaning of the operators that if -clauses restrict.
Let us see how this idea helps us with our Lockhart-sentence. The idea is to
deny that there are two quantiﬁers over worlds in (). Instead, the if -clause
merely contributes a further restriction to the modal might. In eﬀect, the modal
is not quantifying over all the worlds compatible with Mary’s knowledge but
only over those where they are on Route . It then claims that at least some of
those worlds are worlds where they are in Lockhart. We cannot anymore derive
the problematic conclusion that it should also be true that if they are on the
turnpike, they might be in Lockhart. In all, we have a good analysis of what ()
means.
What we don’t yet have is a compositional calculation. What does it mean in
structural terms for the if -clause to be restricting the domain of the modal? We
Figure .: LF C for
will assume a structure as in LF C. Here, the if -clause is the sister to what used
()
to be the covert set-of-worlds argument of the modal. As you can see, we have
chosen the variant of the semantics for modals that was discussed in Section ...
The idea now is that the two restrictive devices work together: we just feed to
the modal the intersection of (i) the set of worlds that are R-accessible from the
actual world, and (ii) the set of worlds where they are on Route .
S

S

Modal

we be in Lockhart

might

R w* (if )

S

we are on Route 

E .: To make the composition work, we need to be able to intersect
the set of accessible worlds with the antecedent proposition. This could be



C

C 

done in two ways: (i) a new composition principle, which would be a slight
modiﬁcation of the P M rule, (ii) give if a functional
meaning that accomplishes the intersection. Formulate such a meaning for if.
Alternatively, we could do without the w∗ device and instead give if a
meaning that takes a proposition p and then modiﬁes an accessibility relation to
give a new accessibility relation, which is restricted to p-worlds. Formulate such
a meaning for if.
What about cases like (), now? Here there is no modal operator for the
if -clause to restrict. Should we revert to treating if as an operator on its own?
Kratzer proposes that we should not and that such cases simply involve covert
modal operators.

Supplementary Readings
A short handbook article on conditionals:
von Fintel, Kai. . Conditionals. Ms, prepared for Semantics: An international
handbook of meaning, edited by Klaus von Heusinger, Claudia Maienborn,
and Paul Portner. URL http://mit.edu/ﬁntel/ﬁntel--hsk-conditionals.pdf
Overviews of the philosophical work on conditionals:
Edgington, Dorothy. . On conditionals. Mind (). –. URL
./mind/...
Bennett, Jonathan. . A philosophical guide to conditionals. Oxford University
Press.
A handbook article on the logic of conditionals:
Nute, Donald. . Conditional logic. In Dov Gabbay & Franz Guenthner
(eds.), Handbook of philosophical logic. volume ii, –. Dordrecht: Reidel.
Three indispensable classics:
Lewis, David. . Counterfactuals. Oxford: Blackwell.
Stalnaker, Robert. . A theory of conditionals. In Nicholas Rescher (ed.), Studies in logical theory (American Philosophical Quarterly Monograph Series ),
–. Oxford: Blackwell.
Stalnaker, Robert. . Indicative conditionals. Philosophia (). –.
doi:./BF.

§.

If -C  R



The Restrictor Analysis:
Lewis, David. . Adverbs of quantiﬁcation. In Edward Keenan (ed.), Formal
semantics of natural language, –. Cambridge University Press.
Kratzer, Angelika. . Conditionals. Chicago Linguistics Society (). –.
The application of the restrictor analysis to the interaction of nominal quantiﬁers
and conditionals:
von Fintel, Kai. . Quantiﬁers and ‘if ’-clauses. The Philosophical Quarterly
(). –. doi:./-.. URL http://mit.edu/ﬁntel/
www/qandif.pdf.
von Fintel, Kai & Sabine Iatridou. . If and when If -clauses can restrict
quantiﬁers. Ms, MIT. URL http://mit.edu/ﬁntel/ﬁntel-iatridou--ifwhen.
pdf.
Higginbotham, James. . Conditionals and compositionality. Philosophical
Perspectives (). –. doi:./j.-...x.
Leslie, Sarah-Jane. . If, unless, and quantiﬁcation. In Robert J. Stainton &
Christopher Viger (eds.), Compositionality, context and semantic values: Essays
in honour of Ernie Lepore, –. Springer. doi:./----_.
Huitink, Janneke. b. Quantiﬁed conditionals and compositionality. Ms, to
appear in Language and Linguistics Compass. URL http://user.uni-frankfurt.
de/~huitink/compass-conditionals-ﬁnal.pdf.
Syntax of conditionals:
von Fintel, Kai. . Restrictions on quantiﬁer domains: University of Massachusetts at Amherst dissertation. URL http://semanticsarchive.net/Archive/
jANIwN/ﬁntel--thesis.pdf, Chapter : “Conditional Restrictors”
Iatridou, Sabine. . On the contribution of conditional Then. Natural
Language Semantics (). –. doi:./BF.
Bhatt, Rajesh & Roumyana Pancheva. . Conditionals. In The Blackwell
companion to syntax, vol. , –. Blackwell. URL http://www-rcf.usc.edu/
~pancheva/bhatt-pancheva_syncom.pdf.
A shifty alternative to the restrictor analysis:
Gillies, Anthony S. . On truth-conditions for if (but not quite only if ).
The Philosophical Review (). –. doi:./--.
Gillies, Anthony S. . Iﬃness. Semantics and Pragmatics (). –. doi:./sp...



C

C 

The Belnap alternative:
Belnap, Jr., Nuel D. . Conditional assertion and restricted quantiﬁcation.
Noûs (). –. doi:./.
Belnap, Jr., Nuel D. . Restricted quantiﬁcation and conditional assertion.
In Hugues Leblanc (ed.), Truth, syntax and modality: Proceedings of the Temple
University conference on alternative semantics, vol.  Studies in Logic and the
Foundations of Mathematics, –. Amsterdam: North-Holland.
von Fintel, Kai. . If : The biggest little word. Slides from a plenary
address given at the Georgetown University Roundtable, March , . URL
http://mit.edu/ﬁntel/gurt-slides.pdf.
Huitink, Janneke. . Modals, conditionals and compositionality: Radboud Universiteit Nijmegen dissertation. URL http://user.uni-frankfurt.de/~huitink/
Huitink-dissertation.pdf, Chapters  and  give a nice summary of what we’re
covering in this class, while Chapter  is about the Belnap-method.
Huitink, Janneke. a. Domain restriction by conditional connectives. Ms,
Goethe-University Frankfurt. URL http://semanticsarchive.net/Archive/zgMDMM/
Huitink-domainrestriction.pdf.

C F
O
We have stressed throughout the previous two chapters that there are
numerous parallels between quantiﬁcation over ordinary individuals via
determiner quantiﬁers and quantiﬁcation over possible worlds via modal
operators (including conditionals). Now, we turn to a phenomenon that
(at least at ﬁrst glance) appears to show that there are non-parallels
as well: a sensitivity to an  of the elements in the domain
of quantiﬁcation. We ﬁrst look at this in the context of simple modal
sentences and then we look at conditionals.

.

The Driveway 

.

Kratzer’s Solution: Doubly Relative Modality 

.

The Paradox of the Good Samaritan 

.

Non-Monotonicity of Conditionals 

Supplemental Readings 

.

The Driveway

Consider a typical use of a sentence like ().
()

John must pay a ﬁne.

This is naturally understood in such a way that its truth depends both on facts
about the law and facts about what John has done. For instance, it will be
judged true if (i) the law states that driveway obstructors are ﬁned, and (ii) John
has obstructed a driveway. It may be false either because the law is diﬀerent or
because John’s behavior was diﬀerent.
What accessibility relation provides the implicit restriction of the quantiﬁer
must on this reading of ()? A naïve attempt might go like this:

O


()

C 

λw. λw . [what happened in w up to now is the same as what happened
in w and w conforms to what the law in w demands].

The problem with () is that, unless there were no infractions of the law at
all in w up to now, no world w will be accessible from w. Therefore, () is
predicted to follow logically from the premise that John broke some law. This
does not represent our intuition about its truth conditions.
A better deﬁnition of the appropriate accessibility relation has to be more
complicated:
()

λw. λw . [what happened in w up to now is the same as what happened
in w and w conforms at least as well to what the law in w demands as

does any other world in which what happened up to now is the same as
in w].
() makes explicit that there is an important diﬀerence between the ways in
which facts about John’s behavior on the one hand, and facts about the law on
the other, enter into the truth conditions of sentences like (). Worlds in
which John didn’t do what he did are simply excluded from the domain of must
here. Worlds in which the law isn’t obeyed are not absolutely excluded. Rather,
we restrict the domain to those worlds in which the law is obeyed as well as it
can be, considering what has happened. We exclude only those worlds in which
there are infractions above and beyond those that are shared by all the worlds
in which John has done what he has done. The analysis of () thus crucially
involves the notion of an ordering of worlds: here they are ordered according to
how well they conform to what the law in w demands.

. Kratzer’s Solution: Doubly Relative Modality
Kratzer proposes that modal operators are sensitive to two context-dependent
parameters: a set of accessible worlds (provided by an accessibility function computed from a conversational background, the  ), and a partial ordering
of the accessible worlds (computed from another conversational background,
called the  ).
Let’s see how the analysis applies to the previous example.
• The modal base will be a function that assigns to any evaluation world
a set of propositions describing the relevant circumstances, for example,
what John did. Since in our stipulated evaluation world John obstructed a
driveway, the modal base will assign the proposition that John obstructed
a driveway to this world. The set of worlds accessible from the evaluation
world will thus only contain worlds where John obstructed a driveway.
• The ordering source will be a function that assigns to any evaluation world
a set of propositions P whose truth is demanded by the law. Imagine that

§.

•

•
•
•

K’ S: D R M



for our evaluation world this set of propositions contains (among others)
the following two propositions: (i) nobody obstructs any driveways, (ii)
anybody who obstructs a driveway pays a ﬁne.
The idea is now that such a set P of propositions can be used to order the
worlds in the modal base. For any pair of worlds w and w , we say that
w comes closer than w to the ideal set up by P (in symbols: w <P w ),
iﬀ the set of propositions from P that are true in w is a proper subset of
the set of propositions from P that are true in w .
For our simple example then, any world in modal base where John pays a
ﬁne will count as better than an otherwise similar world where he doesn’t.
Modals then make quantiﬁcational claims about the best worlds in the
modal base (those for which there isn’t a world that is better than them).
In our case, () claims that in the best worlds (among those where John
obstructed a driveway), he pays a ﬁne.

More technically:
()

Given a set of worlds X and a set of propositions P, deﬁne the 
  <P as follows:
∀w , w ∈ X : w <P w iﬀ {p ∈ P : p(w ) = } ⊂ {p ∈ P : p(w ) = }.

()

For a given strict partial order <P on worlds, deﬁne the selection function
maxP that selects the set of <P -best worlds from any set X of worlds:
∀X ⊆ W : maxP (X) = {w ∈ X : ¬∃w ∈ X : w <P w}.

()

must

w,g

= λf

s,st

. λg s,st . λq s,t .

∀w ∈ maxg(w) (∩f(w)) : q(w ) = .

T N: This only works if we can in general assume that the <P
relation has minimal elements, that there always are accessible worlds that
come closest to the P-ideal, worlds that are better than any world they can
be compared with via <P . It is possible, with some imagination, to cook up
scenarios where this assumption fails. This problem has been discussed primarily
in the area of the semantics of conditionals. There, Lewis presents relevant
scenarios and argues that one shouldn’t make this assumption, which he calls the
Limit Assumption. Stalnaker, on the one other hand, defends the assumption
against Lewis’ arguments by saying that in actual practice, in actual natural
language semantics and in actual modal/conditional reasoning, the assumption
is eminently reasonable. Kratzer is persuaded by Lewis’ evidence and does not
make the Limit Assumption; hence her semantics for modals is more convoluted
than what we have in () and (). I will side with Stalnaker, not the least
because it makes life easier. For further discussion, see Lewis (: –) and
Stalnaker (b: Chapter , esp. pp. –); Pollock (), Herzberger
(), and Warmbrod () argue for the Limit Assumption as well.



O

C 

E .: In her handbook article Kratzer (), Kratzer presents a number
of examples of modal statements and sketches an analyses in terms of doubly
relative modality. You should study her examples carefully.

. The Paradox of the Good Samaritan
Prior () introduced the following “Paradox of the Good Samaritan”. Imagine
that someone has been robbed and John is walking by. It is easy to conceive of a
code of ethics that would make the following sentence true:
()

John ought to help the person who was robbed.

In our previous one-factor semantics for modals, we would have said that ()
says that in all of the deontically accessible worlds (those compatible with the
code of ethics) John helps the person who was robbed. Prior’s point was that
under such a semantics, something rather unfortunate holds. Notice that in all
of the worlds where John helps the person who was robbed, someone was robbed
in the ﬁrst place. Therefore, it will be true that in all of the deontically accessible
worlds, someone was robbed. Thus, () will entail:
()

It ought to be the case that someone was robbed.

It clearly would be good not make such a prediction.
The doubly-relative analysis of modality can successfully avoid this unfortunate prediction. We conceive of () as being uttered with respect to a circumstantial modal base that includes the fact that someone was robbed. Among
those already somewhat ethically deﬁcient worlds, the relatively best ones are all
worlds where John helps the victim.
Note that we still have the problematic fact that among the worlds in the
modal base, all are worlds where someone was robbed, and we would thus appear
to still make the unfortunate prediction that () should be true. But this can
now be ﬁxed. For example, we could say that ought p is semantically defective if p
is true throughout the worlds in the modal base. This could be a presupposition
or some other ingredient of meaning. So, with respect to a modal base which
pre-determines that someone was robbed, one couldn’t felicitously say ().
Consequently, saying () would only be felicitous if a diﬀerent modal base
is intended, one that contains both p and non-p worlds. And given a choice
between worlds where someone was robbed and worlds where nobody was
robbed, most deontic ordering sources would probably choose the no-robbery
worlds, which would make () false, as desired.
K’    S P Kratzer () argues that
the restrictor approach to deontic conditionals is the crucial ingredient in the
solution to a conditional version of the Samaritan Paradox:

§.

N-M  C

()

If a murder occurs, the jurors must convene.



Kratzer points out that if one tried to analyze () as a material implication
embedded under deontic necessity, then one quickly runs into a problem. Surely,
one wants the following to be a true statement about the law:
()

There must be no murder.

But this means that in the deontically accessible worlds, all of them have no
murders occurring. Now, this means that in all of the deontically accessible
worlds, any material implication of the form “if a murder occurs, q” will be true
no matter what the consequent is since the antecedent will be false. Since that
is an absurd prediction, () cannot be analyzed as material implication under
deontic necessity. The combination of the restrictor approach to if -clauses and
the doubly-relative theory of modals can rescue us from this problem. () is
analyzed as the deontic necessity modal being restricted by the if -clause. The set
of accessible worlds is narrowed down by the if -clause to only include worlds
in which a murder occurs. The deontic ordering then identiﬁes the best among
those worlds and those are plausibly all worlds where the jurors convene.

.

Non-Monotonicity of Conditionals

The last case discussed takes us straight to the crucial role of the ordering of
worlds in the semantics of conditionals, as we would of course expect under
the analysis of if -clauses as restrictors of modal operators. In this arena, the
discussion usually revolves around the failure of certain inference patterns, which
one would expect a universal quantiﬁer to validate. Here are the most important
ones:
()

L D M (“D E”)
Every A is a B. → Every A & C is a B.

()

T
Every A is a B. Every B is a C. → Every A is a C.

()

C
Every A is a B. → Every non-B is a non-A.

Conditionals were once thought to obey these patterns as well, known in conditional logic as S  A, H S,
and C. But then spectacular counterexamples became known
through the work of Stalnaker and Lewis.



O

C 

()

F  S  A
a. If I strike this match, it will light.
If I dip this match into water and strike it, it will light.
b. If John stole the earrings, he must go to jail.
If John stole the earrings and then shot himself, he must go to jail.
c. If kangaroos had no tails, they would topple over. If kangaroos had
no tails but used crutches, they would topple over.

()

F   H S (T)
a. If Brown wins the election, Smith will retire to private life.
If Smith dies before the election, Brown will win the election.
If Smith dies before the election,Smith will retire to private life.
b. If Hoover had been a Communist, he would have been a traitor.
If Hoover had been born in Russia, he would have been a Communist.
If Hoover had been born in Russia, he would have been a traitor.

()

F  C
a. If it rained, it didn’t rain hard.
If it rained hard, it didn’t rain.
b. (Even) if Goethe hadn’t died in , he would still be dead now.
If Goethe were alive now, he would have died in .

The Goethe example is due to
Kratzer.

Note that these cases involve examples of both “indicative” (epistemic) conditionals and counterfactual conditionals. It is sometimes thought that indicative
conditionals are immune from these kinds of counterexamples, but it is clear that
they are not. Also note that in (b) we have a case of Failure of Strengthening
the Antecedent with a deontic conditional. Deontic counterexamples to the
other patterns seem harder to ﬁnd.
The failure of these inference patterns again indicates that the semantics of
modal operators (restricted by if -clauses) is more complicated than the simple
universal quantiﬁcation we had previously been assuming. The basic idea of
most approaches to this problem is this: the semantics of conditionals is more
complicated than simple universal quantiﬁcation. The conditional does not make
a claim about simply every antecedent world, nor even about every contextually
relevant antecedent world. Instead, in each of the conditional statements, only a
particular subset of the antecedent worlds is quantiﬁed over. Informally, we can
call those the “most highly ranked antecedent worlds”. Consider:
()

If I had struck this match, it would have lit.
If I had dipped this match into water and struck it, it would have lit.

According to the Stalnaker-Lewis account, this inference is semantically invalid.
The premise merely claims that the most highly ranked worlds in which I strike

§.

N-M  C



this match are such that it lights. No claim is made about the most highly ranked
worlds in which I ﬁrst dip this match into water and then strike it. Strengthening
the Antecedent will only be safe if it is additionally known that the strengthened
antecedent is instantiated among the worlds that verify the original antecedent.
The other fallacies receive similar treatments. Transitivity (Hypothetical
Syllogism) fails for the new non-monotonic quantiﬁer because even if all the
most highly rated p-worlds are q-worlds and all the most highly rated q-worlds
are r-worlds, we are not necessarily speaking about the same q-worlds (the qworlds that p takes us to may be rather remote ones). So in the Hoover-example,
we get the following picture: The most highly ranked p-worlds in which Hoover
was born in Russia (but where he retains his level of civic involvement), are all
q-worlds in which he becomes a Communist. On the other hand, the most
highly ranked q-worlds in which he is a Communist (but retaining his having
been born in the United States and being a high level administrator) are all
r-worlds in which he is a traitor. However, the most highly ranked p-worlds do
not get us to the most highly ranked q-worlds, so the Transitive inference does
not go through.
Contraposition fails because the fact that the most highly rated p-worlds are
q-worlds does not preclude a situation where the most highly rated non q-worlds
are also p-worlds. The most highly rated p-worlds in which Goethe didn’t die in
 are all q-worlds where he dies nevertheless (well) before the present. But of
course, the most highly rated (in fact, all) non-q-worlds (where he is alive today)
are also p-worlds where he didn’t die in .
In the conditionals literature, the ordering of worlds is usually given directly
as an evaluation parameter. The typical gloss is that the ordering ranks possible
worlds based on how similar they are to the evaluation world. Kratzer developed
an alternative where the ordering is computed from a set of propositions true in
the evaluation world. Lewis () showed that ordering semantics and premise
semantics are largely notational variants.

Supplementary Readings
The central readings for this chapter are two papers by Kratzer:
Kratzer, Angelika. . Modality. In Arnim von Stechow & Dieter Wunderlich
(eds.), Semantics: An international handbook of contemporary research, –.
Berlin: de Gruyter.
Kratzer, Angelika. . The notional category of modality. In Hans-Jürgen
Eikmeyer & Hannes Rieser (eds.), Words, worlds, and contexts: New approaches
in word semantics (Research in Text Theory ), –. Berlin: de Gruyter.



O

C 

Some work that discusses and uses Kratzer’s two factor semantics for modals:
Frank, Anette. . Context dependence in modal constructions: Universität
Stuttgart dissertation. URL http://www.dfki.de/~frank/papers/header.ps.gz.
von Fintel, Kai & Sabine Iatridou. . What to do if you want to go to
Harlem: Anankastic conditionals and related matters. Ms, MIT. URL http:
//mit.edu/ﬁntel/ﬁntel-iatridou--harlem.pdf.
von Fintel, Kai & Sabine Iatridou. . How to say ought in Foreign: The
composition of weak necessity modals. In Jacqueline Guéron & Jacqueline
Lecarme (eds.), Time and modality (Studies in Natural Language and Linguistic
Theory ), –. Springer. doi:./----.
Some work that discusses whether non-monotonicity could be or might have to
be relegated to a dynamic pragmatic component of meaning:
von Fintel, Kai. . Counterfactuals in a dynamic context. In Michael
Kenstowicz (ed.), Ken Hale: A life in language, –. MIT Press.
von Fintel, Kai. . NPI licensing, Strawson entailment, and context dependency. Journal of Semantics (). –. doi:./jos/...
Gillies, Anthony S. . Counterfactual scorekeeping. Linguistics and Philosophy
(). –. doi:./s---.
Schlenker explored whether the apparent non-monotonicity in conditional is
paralleled in quantiﬁcation over individuals:
Schlenker, Philippe. . Conditionals as deﬁnite descriptions (A referential
analysis). Research on Language and Computation (). –. doi:./s--.

C S
B  T  A
We explore an analysis of tense that treats tenses as intensional operators
manipulating a time parameter of evaluation. The treatment is formally
quite parallel to the treatment of modals in Chapter . We touch on
many basic questions about tense and aspect, without exploring them
fully.

.

A First Proposal for Tense 
..

former 

..

Some Time Adverbials 

..

A Word of Caution 

.

Are Tenses Referential? 

.

The Need for Intervals 

.

Aktionsarten 

.

The Progressive 

.

Tense in Embedded Clauses 

Supplemental Readings 

.

A First Proposal for Tense

Tense logic, or temporal logic, is a branch of logic ﬁrst developed by the aptly
named Arthur Prior in a series of works, in which he proposed treating tense
in a way that is formally quite parallel to the treatment of modality discussed
in Chapter . Since tense logic (and modal logic) typically is formulated at a
high level of abstraction regarding the structure of sentences, it doesn’t concern
itself with the internal make-up of “atomic” sentences and thus treats tenses as
sentential operators (again, in parallel to the way modal operators are typically

See the Stanford Encyclopedia
of Philosophy entry on temporal logic: http://plato.stanford.
edu/entries/logic-temporal/
and the website for Prior
studies: http://www.
prior.aau.dk/index.htm.

B  T  A



C 

treated in modal logic). We will implement a version of Prior’s tense logic in our
framework.
The ﬁrst step is to switch to a version of our intensional semantic system where
instead of a world parameter, the evaluation function is sensitive to a time
parameter (and a variable assignment). Eventually, we will want to deal with
the full complexity and relativize the evaluation function to both worlds and
times, but for now, we will just relativize to times. The composition principles
developed in Chapter  will be adopted mutatis mutandis. Predicates will now
have lexical entries that incorporate their sensitivity to time:
()

tired

t,g

= λx ∈ D. x is tired at t.

It is customary in the literature to introduce a new basic type for times; for
now, we will recycle the designation s as the type for times. Then, for example,
the intension of sentence will again be of type s, t , but now that would be a
temporal proposition, a function from times to truth-values.
In this framework, we can now formulate a very simple-minded ﬁrst analysis
of the present and past tenses and the future auxiliary will. As for (LF) syntax
let’s assume that (complete matrix) sentences are TPs, headed by T (for “tense”).
There are two morphemes of the functional category T, namely PAST (past
tense) and PRES (present tense). The complement of T is an MP or a VP.
MP is headed by M (for “modal”). Morphemes of the category M include the
modal auxiliaries must, can, etc., which we talked about in previous chapters, the
semantically vacuous do (in so-called “do-support” structures), and the future
auxiliary will. Evidently, this is a semantically heterogeneous category, grouped
together solely because of their common syntax (they are all in complementary
distribution with each other). The complement of M is a VP. When the sentence
contains none of the items in the category M, we assume that MP isn’t projected
at all; the complement of T is just a VP in this case. We thus have LF-structures
like the following. (The corresponding surface sentences are given below, and
we won’t be explicit about the derivational relation between these and the LFs.
Assume your favorite theories of syntax and morphology here.)
()

[TP Mary [ T’ PRES [ VP t [ V’ be tired ]]]]
= Mary is tired.

()

[TP Mary [ T’ PAST [ VP t [ V’ be tired ]]]]
= Mary was tired.

 We remain vague for now about what we mean by “times” (points in time? time intervals?).
This will need clariﬁcation, We will also see the need to clarify what we mean by “at” in the
metalanguage in this entry and others.

§.

A F P  T

()

[ TP Mary [ T’ PRES [ MP t [ M’ woll [ VP t [ V’ be tired ]]]]]]
= Mary will be tired.



When we have proper name subjects, we will pretend for simplicity that they are
reconstructed somehow into their VP-internal base position. (We will talk more
about reconstruction later on.)
What are the meanings of PRES, PAST, and will? For PRES, the simplest
assumption is actually that it is semantically vacuous. This means that the
interpretation of the LF in () is identical to the interpretation of the bare VP
Mary be tired:
()

For any time t:
PRES (Mary be tired) t = Mary be tired t =  iﬀ Mary is tired at t.

Does this adequately capture the intuitive truth-conditions of the sentence Mary
is tired ? It does if we make the following general assumption:
()

An utterance of a sentence (= LF) φ that is made at a time t counts as
true iﬀ φ t =  (and as false if φ t = ).

This assumption ensures that (unembedded) sentences are, in eﬀect, interpreted
as claims about the time at which they are uttered (“utterance time” or “speech
time”). If we make this assumption and we stick to the lexical entries we have
adopted, then we are driven to conclude that the present tense has no semantic
job to do. A tenseless VP Mary be tired would in principle be just as good as
() to express the assertion that Mary is tired at the utterance time. Apparently
it is just not well-formed as an unembedded structure, but this fact must be
attributed to principles of syntax rather than semantics.
What about PAST ? When a sentence like () Mary was tired is uttered at a
time t, then what are the conditions under which this utterance is judged to be
true? A quick (and perhaps ultimately wrong) answer is: an utterance of () at
t is true iﬀ there is some time before t at which Mary is tired. This suggests the
following entry:
()

For any time t:
PAST t = λp ∈ D s,t .∃t before t : p(t ) = 

So, the past tense seems to be an existential quantiﬁer over times, restricted to
times before the utterance time.
For will, we can say something completely analogous:

B  T  A


()

C 

For any time t:
will t = λp ∈ D s,t .∃t after t : p(t ) = 

Apparently, PAST and will are semantically alike, even mirror images of each
other, though they are of diﬀerent syntactic categories. The fact that PAST
is the topmost head in its sentence, while will appears below PRES, is due to
the fact that syntax happens to require a T-node in every complete sentence.
Semantically, this has no eﬀect, since PRES is vacuous.
Both () and () presuppose that the set T comes with an intrinsic order.
For concreteness, assume that the relation ‘precedes’ (in symbols: <) is a strict
linear order on T . The relation ‘follows’, of course, can be deﬁned in terms of
‘precedes’ (t follows t iﬀ t precedes t).
There are many things wrong with this simple analysis. We will not have time
here to diagnose most of the problems, much less correct them. But let’s see a
couple of things that work out OK and let’s keep problems and remedies for
later.

..

former

There is a brief discussion on p.  of H& K about the inadequacy of an
extensional semantics for the adjective former as in
()

John is a former teacher.

We can now write a semantics for former. While there are a bunch of people
who are currently teachers, there are others that wre not now teachers but were
at some previous time. The latter are the ones that the predicate former teacher
should be true of, In other words, former teacher is a predicate that is true of
individuals just in case the predicate teacher was true of them at some previous
time (and is not true of them now). So, former needs to be an intensional
operator that “displaces” the evaluation of time of its complement from “now”
to some previous time. To be able to do that, it needs to take the intension of its
complement as its argument. This suggests the following lexical entry:
 Deﬁnition: A relation R is a strict linear order on a set S iﬀ it has the following four properties:
(i) ∀x∀y∀z((Rxy&Ryz) → Rxz) “Transitivity"
(ii) ∀x(¬Rxx) “Irreﬂexivity"
(iii) ∀x∀y(Rxy → ¬Ryx) “Asymmetry", and
(iv) ∀x∀y(x = y → (Rxy ∨ Ryx)) “Connectedness"

§.

A F P  T

()

former = λf ∈ D s, e,t .λx.[f(t)(x) =  & ∃t before t : f(t )(x) = ].



E .: H& K on p. mention the adjective alleged in one breath with
former. Formulate a lexical entry for alleged as used in John is an alleged murderer.
[This will use our original intensional system with a world parameter]

..

Some Time Adverbials

At least to a certain extent, we can also provide a treatment of temporal adverbials
such as:
()

Mary was tired on February , .

The basic idea would be that phrases like on February ,  are propositional
modiﬁers. Propositions are the intensions of sentences. At this point, propositions are functions from times to truth-values. Propositional modiﬁers take a
proposition and return a proposition with the addition of a further condition on
the time argument.
()

on February ,  t
= λp ∈ D s,t . [ p(t) =  & t is part of Feb ,  ]

()

[T’ PAST [ VP [ VP Mary [ V’ be tired]] [ PP on February , ]]]

An alternative would be to treat on February ,  as a “sentence” by itself,
whose intension then would be a proposition.
()

on February ,  t =  iﬀ t is part of February , 

()

on t = λx. t is part of x

To make this work, we would then have to devise a way of combining two
tenseless sentences (Mary be tired and on February , ) into one. We could
do this by positing a silent and or by introducing a new composition rule
(“Propositional Modiﬁcation”?).
Let’s not spend time on such a project.
E .: Imagine that Mary was tired on February ,  is not given the
LF in () but this one:
()

[ T’ [ T’ PAST [ VP Mary [ V’ be tired]]] [ PP on February , ]]

B  T  A



C 

What would the truth-conditions of this LF be? Does this result correspond at
all to a possible reading of this sentence (or any other analogous sentence)? If
not, how could we prevent such an LF from being produced?
E .: When a quantiﬁer appears in a tensed sentence, we might expect
two scope construals. Consider a sentence like this:
()

Every professor (in the department) was a teenager in the Sixties.

We can imagine two LFs:
()

PAST [ [every professor be a teenager] [in the sixties] ]

()

[every professor] λ [ PAST [ [t be a teenager] [in the sixties] ]

Describe the diﬀerent truth-conditions which our system assigns to the two LFs.
Is the sentence ambiguous in this way?
If not this sentence, are there analogous sentences that do have the ambiguity?
E .: The following entry for every makes it a time-insensitive item:
()

every t = λf ∈ D e,t .λg ∈ D e,t .∀x[f(x) =  → g(x) = ]

Consider now two possible variants (we have underlined the portion where they
diﬀer):
()

every t = λf ∈ D e,t .λg ∈ D e,t .∀x at t [f(x) =  → g(x) = ]

()

every t = λf ∈ D e,t .λg ∈ D e,t .∀x[f(x) =  at t → g(x) =  at t]

Does either of these alternative entries make sense? If so, what does it say? Is it
equivalent to our oﬃcial entry? Could it lead to diﬀerent predictions about the
truth-conditions of English sentences?

..

A Word of Caution

Compare the semantics given for former and the one for PAST :
()

former = λf ∈ D s, e,t .λx.[f(t)(x) =  & ∃t before t : f(t )(x) = ].

()

PAST t = λp ∈ D s,t . ∃t before t : p(t ) = 

§.

A T R?



Notice that these entries have an interesting consequence:
()

a.
b.

John is a former teacher.
John was a teacher.

The two sentences in () diﬀer in their truth-conditions. The sentence in
(a) can only be true if John is not a teacher anymore while this is not part of
the truth-conditions of the sentence in (b). To see that this analysis is in fact
correct, consider this:
()

Last night, John was reading a book about tense.
a. !! The authors are former Italians.
b. The authors were Italian.

Consider the past tense in (b). It is not (necessarily) interpreted as claiming
that the authors are not Italian anymore. But this is in fact required by (a).
There are some cases where it seems that the past tense does trigger inferences
that one would not expect from the lexical entry that we gave. Surely, if I tell you
My cousin John was a teacher you will infer that he isn’t a teacher anymore. In fact,
you may even infer that he is not alive anymore. One promising approach that
tries to reconcile a semantics like ours with the possibility of stronger inferences
in some contexts is based on pragmatic considerations, see Musan ().
Examples like the one in () are problematic for widely held naive conceptions
of what the past tense means. One often hears that PAST expresses the fact that
“the time of the reported situation precedes the speech time”. If this were to
mean that the time of the book’s authors being Italian precedes the speech time,
this would presumably wrongly predict that they would have to be not Italian
anymore for the sentence to be true (or usable).

. Are Tenses Referential?
Our semantics for the past tense treats it essentially as an existential quantiﬁer
over times (albeit in the meta-language), the same way we treated possibility
modals as existential quantiﬁers over (accessible) worlds. This seems quite
adequate for examples like (), which seem to display the expected quantiﬁed
meaning:
()

John went to a private school.

B  T  A



C 

All we learn from () is that at some point in the past, whenever it was that
John went to school, he went to a private school.
Partee in her famous paper “Some structural analogies between tenses and
pronouns in English” (Partee ) presented an example where tense appears to
act more “referentially”:
()

I didn’t turn oﬀ the stove.

“When uttered, for instance, halfway down the turnpike, such a sentence clearly
does not mean either that there exists some time in the past at which I did not
turn oﬀ the stove or that there exists no time in the past at which I turned oﬀ
the stove. The sentence clearly refers to a particular time — not a particular
instant, most likely, but a deﬁnite interval whose identity is generally clear from
the extralinguistic context, just as the identity of the he in [He shouldn’t be in
here] is clear from the context.”
Partee here is arguing that neither of the two plausible LFs derivable in our
current system correctly captures the meaning of (). Given that the sentence
contains a past tense (which we have treated as an existential quantiﬁer over past
times) and a negation, we need to consider two possible scopings of the two
operators:
()

a.
b.

PAST NEG I turn oﬀ the stove.
NEG PAST I turn oﬀ the stove.

E .: Show that neither LF in () captures the meaning of ()
correctly.
At this point, we will not develop Partee’s analysis in formally explicit detail. If
tenses refer to times, it would be easiest to give up on the treatment of times as
evaluation parameters and move to a system where times are object language
arguments of time-sensitive expressions. We will see a system of that nature later
on.
In a commentary on Partee’s paper at the same conference it was presented at,
Stalnaker pointed out that the Priorean theory can in fact deal with (), if
one allows the existential quantiﬁer over times to be contextually restricted to
times in the salient interval of Partee leaving her house — since natural language
quantiﬁers are typically subject to contextual restrictions, this is not a problematic
assumption. (Note that Partee formulated her observation in quite a circumspect
way: “The sentence refers to a particular time”; Stalnaker’s suggestion is that the
reference to a particular time is part of the restriction to the quantiﬁer over times
expressed by tense, rather than tense itself being a referring item (of type s).)

§.

T N  I



E .: Assuming a restricted existential quantiﬁcation à la Stalnaker,
which of the LFs in () captures the meaning of () correctly?
Ogihara () argues that the restricted existential quantiﬁcation view is in fact
superior to Partee’s analysis, since Partee’s analysis needs an existential quantiﬁer
anyway. Note that it is clear that the time being referred to is a protracted interval
(the time during which Partee was leaving her house). But the sentence is not
interpreted as saying that this interval is not a time at which she turned oﬀ her
stove, which would have to be a fairly absurd turning-oﬀ-of-the-stove (turning
oﬀ the stove only takes a moment and doesn’t take up a signiﬁcant interval).
Instead, the sentence says that in that salient interval there is no time at which
she turned oﬀ the stove. Clearly, we do need an existential quantiﬁer in there
somewhere and the Priorean theory provides one. Ogihara makes the point
with the following example:
()

John:Did you see Mary?
Bill: Yes, I saw her, but I don’t remember exactly when.

The question and answer in this dialogue concern the issue of whether Bill saw
Mary at some time in a contextually salient interval.

. The Need for Intervals
We have just seen a reason to recognize that natural language can talk not just
about moments of time but also about intervals (connected sets of moments),
which is a fairly trivial fact; after all, what does the year  refer to if not an
interval of time? We have to go even farther, though. It can be shown that we
need the time parameter of the evaluation function to be able to be an interval.
Consider the tenseless clause John build a house and consider a situation where
John starts building a house (the only house he has ever built) on April , 
and ﬁnishes building it on April , . Now, which times do we want to be
times at which “John build a house” is true? If we allow the clause to be true
at moments during the building, we would make it true at other times during
the building (the ones after the ﬁrst times) that John built a house, but that is
wrong. So, the time(s) at which “John build a house” cannot be before April ,
. And clearly, times after April ,  cannot be times at which “John build
a house” is true. So, perhaps, the only time at which “John build a house” is true
 Clearly, the alternative is to say that the existential quantiﬁer is not expressed by tense but
comes from somewhere else, perhaps aspect, perhaps in the lexical meaning of turn oﬀ. We will
not pursue those options here.

Partee  adopts an existential quantiﬁer analysis.

B  T  A



C 

is the moment on April ,  when he ﬁnishes building the house? But then
we would incorrectly predict that on the day before, when he has already been
building the house for almost a year, we can truthfully say that John will build
a house. So, no moment of time can be the time at which “John build a house”
is true. The solution is that the time at which “John build a house” is true is
exactly the interval that starts with the ﬁrst moment of the building project and
ends with the last nail hammered into the wall. Then, we can say before April ,
 that John will build a house and after April , , that John built a house.
What can we say during the building of the house, though? The English present
tense is not correctly used in this circumstance:
()

!!John builds a house.

Our analysis may be read as predicting this fact. Assume that for an unembedded
clause, the time parameter is set to be the speech time. But what is the speech
time? Perhaps, it is the exact interval it takes to utter the particular clause being
evaluated. If so, an example like () can only possibly be true if the speech
interval exactly coincides with the reported event, here the building of the house.
That is, the speaker of () would have to ensure that she starts speaking at
the very ﬁrst moment of John’s building the house, continues speaking rather
slowly, and then ﬁnishes speaking with the very last nail. It is intriguing to note
that sentences like () become acceptable in situations where a sentence is
conceived of as exactly coinciding the event being reported, namely play-by-play
sports commentary (“He passes the ball to Messi”).
What English needs to do instead is to use the progressive:
()

John is building a house.

() expresses that the speech time is included in an interval of John building
a house. Elements that connect the evaluation time to the time at which a
predicate holds are usually called aspectual operators or simply aspects. The
English progressive then is an aspectual expression. We will look closer at its
meaning in a little while.

 We cannot go into this fascinating topic further here, but there is much more to explore about
the peculiar nature of (). Bennett & Partee () assume that the speech time is a moment
and use that assumption to derive the nature of (). Ejerhed () calls the typical use of
(), the “voyeur present”; see also Cooper .

§.

A

.

Aktionsarten



We can distinguish predicates with respect to their temporal proﬁle. The traditional classiﬁcation has four categories:
•
•
•
•

accomplishment predicates
achievement predicates
activity predicates
stative predicates

Accomplishment predicates (build a house, cross the street) describe an event that
has a deﬁned beginning and end (telos, ‘goal’) and takes some amount of time to
ﬁnish. Achievement predicates (reach the summit, notice the problem) also have a
telos but are conceived of as describing an instantaneous event. Accomplishment
predicates and achievement predicates constitute the class of telic predicates.
Activity predicates (run, dance) describe events that are not conceived of as
having a deﬁned goal. Stative predicates (be in New York, know French) describe
states that are true of intervals. The diﬀerence between activity predicates and
stative predicates is often said to turn on whether there is an agent being active
in the described event.
— Read Rothstein : Chapter , pp. – —

. The Progressive
— Read Portner  —

.

Tense in Embedded Clauses

What happens to the time-sensitivity of the verb in a tenseless clause? Consider
ECM complements to verbs of believing:
()

John believed it to be raining.

Evidently, there is some kind of dependency of the time reference in the lower
clause and the higher clause. The simplest approach in our framework would be
to have believe pass down its evaluation time to the lower clause and to assume
that the lower clause doesn’t have a tense operator. Then, whatever time believe
is being interpreted at would be the same time that the lower verb would be
evaluated at.

B  T  A


()

C 

believe w,t = λp s,t .λx. p(w , t), for all worlds w compatible with
what x believes in w at t.

Together with the rest of the system, we predict that () will be true iﬀ there
is a past time t such that it is raining at t in all worlds which conform to what
John believes at t, which seems adequate. Unfortunately, it only seems adequate.
Consider these four worlds:
w rain at am, John awake at am
w rain at am, John awake at am
w rain at am, John awake at am
w rain at am, John awake at am

Assume that in all four worlds, John wakes up, has no idea what time it is, hears a
dripping noise, and says to himself “it is raining (now)”. Which worlds conform
to what John believes at am in w ? In which worlds is it raining at am? Are
the former a subset of the latter? No!
Consider a variant of the story. Everything is the same as above, except that
John wakes up, thinks it is am and says to himself: “It was raining at am.”
Fact: Sentence () is not a true report of John’s beliefs in w in this story. Why
not? There is a description, viz. am, which in fact picks out the time of John’s
thinking, and under which he ascribes rain to that time.
Conclusion: Sentence () unambiguously means that there is a past time t such
that John at t ascribes rain to t under the description “now”. We need to capture
this but the proposal encapsulated in () doesn’t achieve this.
The solution: believe (and other attitude verbs, or perhaps the complementizer
they select) controls not just the world parameter of its prejacent but also the
time parameter.
()

believe w,t = λp s,t .λx. p(w , t ), for all worlds w and t such that
for all that x can tell in w at t, x might be located in w at t .

On this analysis, () means essentially that John located himself at a raining
time. This is intuitively correct.

— More on tense in tensed complement clauses —

§.

T  E C



Supplementary Readings
A nice and gentle introduction to some of the issues discussed in this chapter
comes from Ogihara:
Ogihara, Toshiyuki. . Tense and aspect in truth-conditional semantics.
Lingua (). –. doi:./j.lingua....
Partee’s seminal paper is a must read:
Partee, Barbara H. . Some structural analogies between tenses and pronouns
in English. The Journal of Philosophy (). –. doi:./.
Musan’s work on the pragmatic eﬀects of tense:
Musan, Renate. . Tense, predicates, and lifetime eﬀects. Natural Language
Semantics (). –. doi:./A:.
The three essential works on the progressive:
Dowty, David R. . Toward a semantic analysis of verb aspect and the
english ‘imperfective’ progressive. Linguistics and Philosophy (). –.
doi:./BF.
Landman, Fred. . The progressive. Natural Language Semantics (). –.
doi:./BF.
Portner, Paul. . The progressive in modal semantics. Language ().
–. doi:./.
The ﬁrst chapter of Susan Rothstein’s book on lexical aspect gives a nice overview
of Aktionsarten/aspectual classes:
Rothstein, Susan. . Structuring events: A study in the semantics of lexical aspect Explorations in Semantics. Blackwell. URL http://tinyurl.com/
rothstein-aktionsarten, Chapter : “Verb Classes and Aspectual Classiﬁcation”,
pp. –, available online at http://tinyurl.com/rothstein-aktionsarten.
Concise statements of some of the issues surrounding dependent tenses:
von Stechow, Arnim. . On the proper treatment of tense. Proceedings of
Semantics and Linguistic Theory . URL http://www.sfs.uni-tuebingen.de/
~arnim/Aufsaetze/SALT.pdf.
von Stechow, Arnim. . Tenses in compositional semantics. To be published
in Wolfgang Klein (ed) The Expression of Time in Language. URL http:
//www.sfs.uni-tuebingen.de/~arnim/Aufsaetze/Approaches.pdf.

— T     —

C S
DP  S  M C
We discuss ambiguities that arise when DPs occur in modal contexts.

.

De re vs. De dicto as a Scope Ambiguity 

.

Raised subjects 
..
..

Syntactic “Reconstruction” 

..

.

Examples of de dicto readings for raised subjects 
Some Alternatives to Syntactic Reconstruction 

De re vs. De dicto as a Scope Ambiguity

When a DP appears inside the clausal or VP complement of a modal predicate ,
there is often a so-called de re-de dicto ambiguity. A classic example is (),
which contains the DP a plumber inside the inﬁnitive complement of want.
()

John wants to marry a plumber.

According to the de dicto reading, every possible world in which John gets what
he wants is a world in which there is a plumber whom he marries. According
to the de re reading, there is a plumber in the actual world whom John marries
in every world in which he gets what he wants. We can imagine situations in
which one of the readings is true and the other one false.
For example, suppose John thinks that plumbers make ideal spouses, because
they can ﬁx things around the house. He has never met one so far, but he
 We will be using the terms “modal operator” and “modal predicate” in their widest sense here,
to include modal auxiliaries (“modals”), modal main verbs and adjectives, attitude predicates,
and also modalizing sentence-adverbs like possibly.



DP  S  M C

C 

deﬁnitely wants to marry one. In this scenario, the de dicto reading is true, but
the de re reading is false. What all of John’s desire-worlds have in common is
that they have a plumber getting married to John in them. But it’s not the same
plumber in all those worlds. In fact, there is no particular individual (actual
plumber or other) whom he marries in every one of those worlds.
For a diﬀerent scenario, suppose that John has fallen in love with Robin and
wants to marry Robin. Robin happens to be a plumber, but John doesn’t know
this; in fact, he wouldn’t like it and might even call oﬀ the engagement if he
found out. Here the de re reading is true, because there is an actual plumber,
viz. Robin, who gets married to John in every world in which he gets what he
wants. The de dicto reading is false, however, because the worlds which conform
to John’s wishes actually do not have him marrying a plumber in them. In his
favorite worlds, he marries Robin, who is not a plumber in those worlds.
When confronted with this second scenario, you might, with equal justiﬁcation,
say ‘John wants to marry a plumber’, or ‘John doesn’t want to marry a plumber’.
Each can be taken in a way that makes it a true description of the facts – although,
of course, you cannot assert both in the same breath. This intuition ﬁts well
with the idea that we are dealing with a genuine ambiguity.
Let’s look at another example:
()

John believes that your abstract will be accepted.

Here the relevant DP in the complement clause of the verb believe is your abstract.
Again, we detect an ambiguity, which is brought to light by constructing diﬀerent
scenarios.
 What is behind the Latin terminology “de re” (lit.: ‘of the thing’) and “de dicto” (lit.: ‘of what
is said’)? Apparently, the term “de dicto” is to indicate that on this reading, the words which
I, the speaker, am using to describe the attitude’s content, are the same (at least as far as the
relevant DP is concerned) as the words that the subject herself would use to express her attitude.
Indeed, if we asked the John in our example what he wants, then in the ﬁrst scenario he’d say
“marry a plumber”, but in the second scenario he would not use these words. The term “de re”,
by contrast, indicates that there is a common object (here: Robin) whom I (the speaker) am
talking about when I say “a plumber” in my report and whom the attitude holder would be
referring to if he were to express his attitude in his own words. E.g., in our second scenario, John
might say that he wanted to marry “Robin”, or “this person here” (pointing at Robin). He’d
thus be referring to the same person that I am calling “a plumber”, but wouldn’t use that same
description.
Don’t take this “deﬁnition” of the terms too seriously, though! The terminology is much older
than any precise truth-conditional analysis of the two readings, and it does not, in hindsight,
make complete sense. We will also see below that there are cases where nobody is sure how to
apply the terms in the ﬁrst place, even as purely descriptive labels. So in case of doubt, it is
always wiser to give a longer, more detailed, and less terminology-dependent description of the
relevant truth-conditional judgments.

§.

De re . De dicto   S A



(i) John’s belief may be about an abstract that he reviewed, but since the
abstract is anonymous, he doesn’t know who wrote it. He told me that
there was a wonderful abstract about subjacency in Hindi that is sure to
be accepted. I know that it was your abstract and inform you of John’s
opinion by saying (). This is the de re reading. In the same situation, the
de dicto reading is false: Among John’s belief worlds, there are many worlds
in which your abstract will be accepted is not true or even false. For all he
knows, you might have written, for instance, that terrible abstract about
Antecedent-Contained Deletion, which he also reviewed and is positive
will be rejected.
(ii) For the other scenario, imagine that you are a famous linguist, and John
doesn’t have a very high opinion about the fairness of the abstract selection
process. He thinks that famous people never get rejected, however the
anonymous reviewers judge their submissions. He believes (correctly or
incorrectly – this doesn’t matter here) that you submitted a (unique) abstract.
He has no speciﬁc information or opinion about the abstract’s content and
quality, but given his general beliefs and his knowledge that you are famous,
he nevertheless believes that your abstract will be accepted. This is the de
dicto reading. Here it is true in all of John’s belief worlds that you submitted
a (unique) abstract and it will be accepted. The de re reading of (),
though, may well be false in this scenario. Suppose – to ﬂesh it out further
– the abstract you actually submitted is that terrible one about ACD. That
one surely doesn’t get accepted in every one of John’s belief worlds. There
may be some where it gets in (unless John is certain it can’t be by anyone
famous, he has to allow at least the possibility that it will get in despite its
low quality). But there are deﬁnitely also belief-worlds of his in which it
doesn’t get accepted.
We have taken care here to construct scenarios that make one of the
readings true and the other false. This establishes the existence of two
distinct readings. We should note, however, that there are also many
possible and natural scenarios that simultaneously support the truth of both
readings. Consider, for instance, the following third scenario for sentence
().
(iii) John is your adviser and is fully convinced that your abstract will be
accepted, since he knows it and in fact helped you when you were writing
it. This is the sort of situation in which both the de dicto and the de re
reading are true. It is true, on the one hand, that the sentence your abstract
will be accepted is true in every one of John’s belief worlds (de dicto reading).
And on the other hand, if we ask whether the abstract which you actually
wrote will get accepted in each of John’s belief worlds, that is likewise true
(de re reading).
In fact, this kind of “doubly verifying” scenario is very common when
we look at actual uses of attitude sentences in ordinary conversation. There



DP  S  M C

C 

may even be many cases where communication proceeds smoothly without
either the speaker or the hearer making up their minds as to which of
the two readings they intend or understand. It doesn’t matter, since the
possible circumstances in which their truth-values would diﬀer are unlikely
and ignorable anyway. Still, we can conjure up scenarios in which the two
readings come apart, and our intuitions about those scenarios do support
the existence of a semantic ambiguity.
In the paraphrases by which we have elucidated the two readings of our examples,
we have already given away the essential idea of the analysis that we will adopt:
We will treat de dicto-de re ambiguities as ambiguities of scope. The de dicto
readings, it turns out, are the ones which we predict without further ado if
we assume that the position of the DP at LF is within the modal predicate’s
complement. (That is, it is either in situ or QRed within the complement clause.)
For example:
()

John wants [ [ a plumber] [  to marry t ]]

()

John believes [ the abstract-by-you will-be-accepted]

To obtain the de re readings, we apparently have to QR the DP to a position
above the modal predicate, minimally the VP headed by want or believe.
()

[ a plumber] [ John wants [  to marry t ]]

()

[ the abstract-by-you] [ John believes will-be-accepted]]

E .: Calculate the interpretations of the four structures in ()–(),
and determine their predicted truth-values in each of the (types of ) possible
worlds that we described above in our introduction to the ambiguity.
Some assumptions to make the job easier: (i) Assume that () and () are
evaluated with respect to a variable assignment that assigns John to the number
. This assumption takes the place of a worked out theory of how controlled
PRO is interpreted. (ii) Assume that abstract-by-you is an unanalyzed one-place
predicate. This takes the place of a worked out theory of how genitives with a
non-possessive meaning are to be analyzed.

. Raised subjects
In the examples of de re-de dicto ambiguities that we have looked at so far, the
surface position of the DP in question was inside the modal predicate’s clausal or
VP-complement. We saw that if it stays there at LF, a de dicto reading results,

§.

R 



and if it covertly moves up above the modal operator, we get a de re reading. In
the present section, we will look at cases in which a DP that is superﬁcially higher
than a modal operator can still be read de dicto. In these cases, it is the de re
reading which we obtain if the LF looks essentially like the surface structure, and
the de dicto reading for which we apparently have to posit a non-trivial covert
derivation.

..

Examples of de dicto readings for raised subjects

Suppose I come to my oﬃce one morning and ﬁnd the papers and books on my
desk in diﬀerent locations than I remember leaving them the night before. I say:
()

Somebody must have been here (since last night).

On the assumptions we have been making, somebody is base-generated as the
subject of the VP be here and then moved to its surface position above the modal.
So () has the following S-structure, which is also an interpretable LF.
()

somebody [ λ [ [ must R] [ t have-been-here]]]

What does () mean? The appropriate reading for must here is epistemic, so
suppose the variable R is mapped to the relation λw.λw . w is compatible with
what I believe in w . Let w be the utterance world. Then the truth-condtion
calculated by our rules is as follows.
()

∃x[x is a person in w &
∀w [w is compatible with what I believe in w → x was here in w ]]

But this is not the intended meaning. For () to be true, there has to be a
person who in every world compatible with what I believe was in my oﬃce. In
other words, all my belief-worlds have to have one and the same person coming
to my oﬃce. But this is not what you intuitively understood me to be saying
about my belief-state when I said (). The context we described suggests that I
do not know (or have any opinion about) which person it was that was in my
oﬃce. For all I know, it might have been John, or it might have been Mary, or it
have been this stranger here, or that stranger there. In each of my belief-worlds,
somebody or other was in my oﬃce, but no one person was there in all of them.
I do not believe of anyone in particular that he or she was there, and you did not
understand me to be saying so when I uttered (). What you did understand
me to be claiming, apparently, was not () but ().


()

DP  S  M C

C 

∀w [w is compatible with what I believe in w
→ ∃x [x is a person in w & x was here in w ]]

In other words – to use the terminology we introduced in the last section – the
DP somebody in () appears to have a de dicto reading.
How can sentence () have the meaning in ()? The LF in (), as we saw,
means something else; it expresses a de re reading, which typically is false when
() is uttered sincerely. So there must be another LF. What does it look like
and how is it derived? One way to capture the intended reading, it seems, would
be to generate an LF that’s essentially the same as the underlying structure we
posited for (), i.e., the structure before the subject has raised:
()

[IP e [I [ must R] [ somebody have-been-here]]]

() means precisely () (assuming that the unﬁlled Spec-of-IP position is
semantically vacuous), as you can verify by calculating its interpretation by our
rules. So is () (one of ) the LF(s) for (), and what assumption about syntax
allow it to be generated? Or are there other – perhaps less obvious, but easier to
generate – candidates for the de dicto LF-structure of ()?
Before we get into these question, let’s look at a few more examples. Each of
the following sentences, we claim, has a de dicto reading for the subject, as given
in the accompanying formula. The modal operators in the examples are of a
variety of syntactic types, including modal auxiliaries, main verbs, adjectives, and
adverbs.
()

Everyone in the class may have received an A.
∃w [w conforms to what I believe in w &
∀x[x is in this class in w → x received an A in w ]].

()

At least two semanticists have to be invited.
∀w [w conforms to what is desirable in w
→ ∃ x [x is a semanticist in w & x is invited in w ]].

()

Somebody from New York is expected to win the lottery.
∀w [w conforms to what is expected in w
→ ∃x[x is a person from NY in w & x wins the lottery in w ]]

()

Somebody from New York is likely to win the lottery.
∀w [w is as likely as any other world, given I know in w
→ ∃x[x is a person from NY in w & x wins the lottery in w ]]

 Hopefully the exact analysis of the modal operators likely and probably is not too crucial for

§.
()

R 



One of these two people is probably infected.
∀w [w is as likely as any other world, given what I know in w
→ ∃x[x is one of these two people & x is in infected in w ]]

To bring out the intended de dicto reading of the last example (to pick just one)
imagine this scenario: We are tracking a dangerous virus infection and have
sampled blood from two particular patients. Unfortunately, we were sloppy and
the blood samples ended up all mixed up in one container. The virus count is
high enough to make it quite probable that one of the patients is infected but
because of the mix-up we have no evidence about which one of them it may
be. In this scenario, () appears to be true. It would not be true under a de re
reading, because neither one of the two people is infected in every one of the
likely worlds.
A word of clariﬁcation about our empirical claim: We have been concentrating
on the observation that de dicto readings are available, but have not addressed
the question whether they are the only available readings or coexist with equally
possible de re readings. Indeed, some of the sentences in our list appear to be
ambiguous: For example, it seems that () could also be understood to claim
the present discussion, but you may still be wondering about it. As you see in our formula,
we are thinking of likely (probably) as a kind of epistemic necessity operator, i.e., a universal
quantiﬁer over a set of worlds that is somehow determined by the speaker’s knowledge. (We are
focussing on the “subjective probability” sense of these words. Perhaps there is a also an “objective
probability” reading that is circumstantial rather than epistemic.) What is the diﬀerence then
between likely and e.g. epistemic must (or necessary or I believe that)? Intuitively, ‘it is likely that
p’ makes a weaker claim than ‘it must be the case that p’. If both are universal quantiﬁers, then,
it appears that likely is quantifying over a smaller set than must, i.e., over only a proper subset of
the worlds that are compatible with what I believe. The diﬀerence concerns those worlds that
I cannot strictly rule out but regard as remote possibilities. These worlds are included in the
domain for must, but not in the one for likely. For example, if there was a race between John and
Mary, and I am willing to bet that Mary won but am not completely sure she did, then those
worlds where John won are remote possibilities for me. They are included in the domain of must,
and so I will not say that Mary must have won, but they are not in the domain quantiﬁed over
by likely, so I do say that Mary is likely to have won.
This is only a very crude approximation, of course. For one thing, probability is a gradable
notion. Some things are more probable than others, and where we draw the line between
what’s probable and what isn’t is a vague or context-dependent matter. Even must, necessary etc.
arguably don’t really express complete certainty (because in practice there is hardly anything we
are completely certain of ), but rather just a very high degree of probability. For more discussion
of likely, necessary, and other graded modal concepts in a possible worlds semantics, see e.g.
Kratzer , .
A diﬀerent approach may be that likely quantiﬁes over the same set of worlds as must, but
with a weaker, less than universal, quantiﬁcational force. I.e., ‘it is likely that p’ means something
like p is true in most of the worlds conforming to what I know. A prima facie problem with this
idea is that presumably every proposition is true in inﬁnitely many possible worlds, so how can
we make sense of cardinal notions like ‘more’ and ‘most’ here? But perhaps this can be worked
out somehow.



DP  S  M C

C 

that there is a particular New Yorker who is likely to win (e.g., because he has
bribed everybody). Others arguably are not ambiguous and can only be read de
dicto. This is what von Fintel & Iatridou () claim about sentences like ().
They note that if () also allowed a de re reading, it should be possible to make
coherent sense of ().
()

Everyone in the class may have received an A. But not everybody did.

In fact, () sounds contradictory, which they show is explained if only the
de dicto reading is permitted by the grammar. They conjecture that this is a
systematic property of epistemic modal operators (as opposed to deontic and
other types of modalities). Epistemic operators always have widest scope in their
sentence.
So there are really two challenges here for our current theory. We need to account
for the existence of de dicto readings, and also for the absence, in at least some of
our examples, of de re readings. We will be concerned here exclusively with the
ﬁrst challenge and will set the second aside. We will aim, in eﬀect, to set up the
system so that all sentences of this type are in principle ambiguous, hoping that
additional constraints that we are not investigating here will kick in to exclude
the de re readings where they are missing.
To complicate the empirical picture further, there are also examples where raised
subjects are unambiguously de re. Such cases have been around in the syntactic
literature for a while, and they have recently received renewed attention in the
work of Lasnik and others. To illustrate just one of the systematic restrictions,
negative quantiﬁers like nobody seem to permit only surface scope (i.e., wide
scope) with respect to a modal verb or adjective they have raised over.
()

Nobody from New York is likely to win the lottery.

() does not have a de dicto reading parallel to the one for () above, i.e., it
cannot mean that it is likely that nobody from NY will win. It can only mean
that there is nobody from NY who is likely to win. This too is an issue that we
set aside.
In the next couple of sections, all that we are trying to do is ﬁnd and justify
a mechanism by which the grammar is capable to generate both de re and de
dicto readings for subjects that have raised over modal operators. It is quite
conceivable, of course, that the nature of the additional constraints which often
exclude one reading or the other is ultimately relevant to this discussion and
that a better understanding of them may undermine our conclusions. But this is
something we must leave for further research.

§.

R 

..

Syntactic “Reconstruction”



Given that the de dicto reading of () we are aiming to generate is equivalent to
the formula in (), an obvious idea is that there is an LF which is essentially the
pre-movement structure of this sentence, i.e., the structure prior to the raising of
the subject above the operator. There are a number of ways to make such an LF
available.
One option, most recently defended in Elbourne & Sauerland (), is to
assume that the raising of the subject can happen in a part of the derivation which
only feeds PF, not LF. In that case, the subject simply stays in its underlying
VP-internal position throughout the derivation from DS to LF. (Recall that
quantiﬁers are interpretable there, as they generally are in subject positions.)
Another option is a version of the so-called Copy Theory of movement introduced in Chomsky (). This assumes that movement generally proceeds in
two separate steps, rather than as a single complex operation as we have assumed
so far. Recall that in H& K, it was stipulated that every movement eﬀects the
following four changes:
(i) a phrase α is deleted,
(ii) an index i is attached to the resulting empty node (making it a so-called
trace, which the semantic rule for “Pronouns and Traces” recognizes as a
variable),
(iii) a new copy of α is created somewhere else in the tree (at the “landing site”),
and
(iv) the sister-constituent of this new copy gets another instance of the index i
adjoined to it (which the semantic rule of Predicate Abstraction recognizes
as a binder index).
If we adopt the Copy Theory, we assume instead that there are three distinct
operations:
“Copy”: Create a new copy of α somewhere in the tree, attach an index i to the
original α , and adjoin another instance of i to the sister of the new copy
of α . (= steps (ii), (iii), and (iv) above)
“Delete Lower Copy”: Delete the original α . (= step (i) above)
“Delete Upper Copy”: Delete the new copy of α and both instances of i.
The Copy operation is part of every movement operation, and can happen
anywhere in the syntactic derivation. The Delete operations happen at the end
of the LF derivation and at the end of the PF deletion. We have a choice of
applying either Delete Lower Copy or Delete Upper Copy to each pair of copies,
and we can make this choice independently at LF and at PF. (E.g., we can do
Copy in the common part of the derivation and than Delete Lower Copy at
LF and Delete Upper Copy at PF.) If we always choose Delete Lower Copy at
LF, this system generates exactly the same structures and interpretations as the



DP  S  M C

C 

one from H& K. But if we exercise the Delete Upper Copy option at LF, we are
eﬀectively undoing previous movements, and this gives us LFs with potentially
new interpretations. In the application we are interested in here, we would apply
the Copy step of subject raising before the derivation branches, and then choose
Delete Lower Copy at PF but Delete Upper Copy at LF. The LF will thus look
as if the raising never happened, and it will straightforwardly get the desired de
dicto reading.
If the choice between the two Delete operations is generally optional, we in
principle predict ambiguity wherever there has been movement. Notice, however,
ﬁrst, that the two structures will often be truth-conditionally equivalent (e.g.
when the moved phrase is a name), and second, that they will not always be both
interpretable. (E.g., if we chose Delete Upper Copy after QRing a quantiﬁer
from object position, we’d get an uninterpretable structure, and so this option
is automatically ruled out.) Even so, we predict lots of ambiguity. Speciﬁcally,
since raised subjects are always interpretable in both their underlying and raised
locations, we predict all raising structures where a quantiﬁcational DP has raised
over a modal operator (or over negation or a temporal operator) to be ambiguous.
As we have already mentioned, this is not factually correct, and so there must be
various further constraints that somehow restrict the choices. (Similar comments
apply, of course, to the option we mentioned ﬁrst, of applying raising only on
the PF-branch.)
Yet another solution was ﬁrst proposed by May (): May assumed that QR
could in principle apply in a “downward” fashion, i.e., it could adjoin the moved
phrase to a node that doesn’t contain its trace. Exercising this option with a
raised subject would let us produce the following structure, where the subject
has ﬁrst raised over the modal and then QRed below it.
()

tj λi [ must-R [ someone λj [ ti have been here]]]

As it stands, this structure contains at least one free variable (the trace t j ) and
can therefore not possibly represent any actual reading of this sentence. May
further assumes that traces can in principle be deleted, when their presence is
not required for interpretability. This is not yet quite enough, though to make
() interpretable, at least not within our framework of assumptions, for () is
still not a candidate for an actual reading of ().
()

λi [ must-R [ someone λj [ ti have been here]]]

We would need to assume further that the topmost binder index could be deleted
along with the unbound trace, and also that the indices i and j can be the same,
so that the raising trace t j is bound by the binding-index created by QR. If these

§.

R 



things can be properly worked out somehow, then this is another way to generate
the de dicto reading. Notice that the LF is not exactly the same as on the previous
two approaches, since the subject ends up in an adjoined position rather than in
its original argument position, but this diﬀerence is obviously without semantic
import.
What all of these approaches have in common is that they place the burden of
generating the de dicto reading for raised subjects on the syntactic derivation.
Somehow or other, they all wind up with structures in which the subject is
lower than it is on the surface and thereby falls within the scope of the modal
operator. They also have in common that they take the modal operator (here the
auxiliary, in other cases a main predicate or an adverb) to be staying put. I.e.,
they assume that the de dicto readings are not due to the modal operator being
covertly higher than it seems to be, but to the subject being lower. Approaches
with these features will be said to appeal to “syntactic reconstruction” of the
subject.

.. Some Alternatives to Syntactic Reconstruction
Besides (some version of ) syntactic reconstruction, there are many other ways in
which one try to generate de dicto readings for raised subjects. Here are some
other possibilities that have been suggested and or readily come to mind. We
will see that some of them yield exactly the de dicto reading as we have been
describing it so far, whereas others yield a reading that is very similar but not
quite the same. We will conﬁne ourselves to analyses which involve no or only
minor changes to our system of syntactic and semantic assumptions. Obviously,
if departed from these further, there would be even more diﬀerent options, but
even so, there seem to be quite a few.
. R   ,  :   Conceivably, an LF for
the de dicto reading of () might be derived from the S-structure (=()) by
covertly moving must (and its covert R-argument) up above the subject. This
would have to be a movement which leaves no (semantically non-vacuous) trace.
Given our inventory of composition rules, the only type that the trace could
have to make the structure containing it interpretable would be the type of the
moved operator itself (i.e. st, t ). If it had that type, however, the movement
would be semantically inconsequential, i.e., the structure would mean exactly
 This is a very broad notion of “reconstruction”, where basically any mechanism which puts a
phrase at LF in a location nearer to its underlying site than its surface site is called “reconstruction”.
In some of the literature, the term is used more narrowly. For example, May’s downward QR
is sometimes explicitly contrasted with genuine reconstruction, since it places the quantiﬁer
somewhere else than exactly where it has moved from.



DP  S  M C

C 

the same as (). So this would not be a way to provide an LF for the de dicto
reading. If there was no trace left however (and also no binder index introduced),
we indeed would obtain the de dicto reading.
E .: Prove the claims we just made in the previous paragraph. Why
is no type for the trace other than st, t possible? Why is the movement
semantically inert when this type is chosen? How does the correct intended
meaning arise if there is no trace and binder index?
. R   ,  :     [Requires slightly
modiﬁed inventory of composition rules. Derives an interpretation that is not
quite the same as the de dicto reading we have assumed so far. Rather, it is a
“narrow-Q, R-de-re” interpretation in the sense of Section ?? below.]
. H     ,  :  et, t
this section, read and do the exercise on p./ in H& K]

[Before reading

So far in our discussion, we have taken for granted that the LF which corresponds
to the surface structure, viz. (), gives us the de re reading. This, however, is
correct only on the tacit assumption that the trace of raising is a variable of type
e. If it is part of our general theory that all variables, or at least all interpretable
binder indices (hence all bound variables), in our LFs are of type e, then there
is nothing more here to say. But it is not prima facie obvious that we must or
should make this general assumption, and if we don’t, then the tree in () is not
really one single LF, but the common structure for many diﬀerent ones, which
diﬀer in the type chosen for the trace. Most of the inﬁnitely many semantic
types we might assign to this trace will lead to uninterpretable structures, but
there turns out to be one other choice besides e that works, namely et, t :
()

somebody λ, et,t [ [ must R] [ t, et,t have-been-here]]

() is interpretable in our system, but again, as above, the predicted interpretation is not exactly the de dicto reading as we have been describing it so far, but a
“narrow-Q, R-de-re” reading.
E .: Using higher-type traces to “reverse” syntactic scope-relation is a
trick which can be used quite generally. It is useful to look at a non-intensional
example as a ﬁrst illustration. () contains a universal quantiﬁer and a negation,
and it is scopally ambiguous between the readings in (a) and (b).
()

Everything that glitters is not gold.
a. ∀x[x glitters → ¬x is gold]
b. ¬∀x[x glitters → x is gold]

“surface scope”
“inverse scope”

§.

R 



We could derive the inverse scope reading for () by generating an LF (e.g. by
some version of syntactic reconstruction") in which the every-DP is below not.
Interestingly, however, we can also derive this reading if the every-DP is in its
raised position above not but its trace has the type e, t , t .
Spell out this analysis. (I.e., draw the LF and show how the inverse-scope
interpretation is calculated by our semantic rules.)
E .: Convince yourself that there are no other types for the raising
trace besides e and et, t that would make the structure in () interpretable.
(At least not if we stick exactly to our current composition rules.)
. H     ,  :  s, et, t If we want
to get exactly the de dicto reading that results from syntactic reconstruction out
of a surface-like LF of the form (), we must use an even higher type for the
raising trace, namely s, e, t , t , the type of the intension of a quantiﬁer. As
you just proved in the exercise, this is not possible if we stick to exactly the
composition rules that we have currently available. The problem is in the VP:
the trace in subject position is of type s, e, t , t and its sister is of type e, t .
These two connot combine by either FA or IFA, but it works if we employ
another variant of functional application.
()

Extensionalizing Functional Application (EFA)
If α is a branching node and {β, γ} the set of its daughters, then, for any
world w and assignment g:
if β w,g (w) is a function whose domain contains γ w,g ,
then α w,g = β w,g (w)( γ w,g ).

E .: Calculate the truth-conditions of () under the assumption that
the trace of the subject quantiﬁer is of type s, e, t , t .
C      ? Two of the methods we tried
derived readings in which the raised subject’s quantiﬁcational determiner took
 Notice that the problem here is kind of the mirror image of the problem that led to the
introduction of “Intensional Functional Application” in H& K, ch. . There, we had a function
looking for an argument of type s, t , but the sister node had an extension of type t. IFA allowed
us to, in eﬀect, construct an argument with an added “s” in its type. This time around, we have
to get rid of an “s” rather than adding one; and this is what EFA accomplishes.
So we now have three diﬀerent “functional application”-type rules altogether in our system:
ordinary FA simply applies β w to γ w ; IFA applies β w to λw . γ w ; and EFA applies
β w (w) to γ w . At most one of them will be applicable to each given branching node,
depending on the type of γ w .
Think about the situation. Might there be other variant functional application rules?



DP  S  M C

C 

scope below the world-quantiﬁer in the modal operator, but the raised subject’s
restricting NP still was evaluated in the utterance world (or the evaluation world
for the larger sentence, whichever that may be). It is diﬃcult to assess whether
these readings are actually available for the sentences under consideration, and
we will postpone this question to a later section. We would like to argue here,
however, that even if these readings are available, they cannot be the only readings
that are available for raised subjects besides their wide-scope readings. In other
words, even if we allowed one of the mechanisms that generated these sort of
hybrid readings, we would still need another mechanism that gives us, for at
least some examples, the “real” de dicto readings that we obtain e.g. by syntactic
reconstruction. The relevant examples that show this most clearly involve DPs
with more descriptive content than somebody and whose NPs express clearly
contingent properties.
()

A neat-freak must have been here.

If I say this instead of our original () when I come to my oﬃce in the morning
and interpret the clues on my desk, I am saying that every world compatible
with my beliefs is such that someone who is a neat-freak in that world was here
in that world. Suppose there is a guy, Bill, whom I know slightly but not well
enough to have an opinion on whether or not he is neat. He may or not be, for
all I know. So there are worlds among my belief worlds where he is a neat-freak
and worlds where he is not. I also don’t have an opinion on whether he was or
wasn’t the one who came into my oﬃce last night. He did in some of my belief
worlds and he didn’t in others. I am implying with (), however, that if Bill
isn’t a neat-freak, then it wasn’t him in my oﬃce. I.e., () is telling you that,
even if I have belief-worlds in which Bill is a slob and I have belief-worlds in
which (only) he was in my oﬃce, I do not have any belief-worlds in which Bill is
a slob and the only person who was in my oﬃce. This is correctly predicted if
() expresses the “genuine” de dicto reading in (), but not if it expresses the
“hybrid” reading in ().
()

∀w [w is compatible with what I believe in w →
∃x[x is a neatfreak in w and x was here in w ]]

()

∀w [w is compatible with what I believe in w →
∃x[x is a neatfreak in w and x was here in w ]]

We therefore conclude the mechanisms  and  considered above (whatever
there merits otherwise) cannot supplant syntactic reconstruction or some other
mechanism that yields readings like ().
This leaves only the ﬁrst and fourth options that we looked at as potential competitors to syntactic reconstruction, and we will focus the rest of the discussion
on how we might be able to tease apart the predictions that these mechanisms
imply from the ones of a syntactic reconstruction approach.

§.

R 



As for moving the modal operator, there are no direct bad predictions that we are
aware of with this. But it leads us to expect that we might ﬁnd not only scope
ambiguities involving a modal operator and a DP, but also scope ambiguities
between two modal operators, since one of them might covertly move over the
other. It seems that this never happens. Sentences with stacked modal verbs
seem to be unambiguous and show only those readings where the scopes of the
operators reﬂect their surface hierarchy.
()

a. I have to be allowed to graduate.
b. #I am allowed to have to graduate.

Of course, this might be explained by appropriate constraints on the movement
of modal operators, and such constraints may even come for free in a the right
synatctic theory. Also, we should have a much more comprehensive investigation
of the empirical facts before we reach any verdict. If it is true, however, that
modal operators only engage in scope interaction with DPs and never with each
other, then a theory which does not allow any movement of modals at all could
claim the advantage of having a simple and principled explanation for this fact.
What about the “semantic reconstruction” option, where raised subjects can leave
traces of type s, et, t and thus get narrow scope semantically without ending
up low syntactically? This type of approach has been explored quite thoroughly
and defended with great sophistication. We can only sketch the main objections
to it here and must leave it to the reader to consult the literature for an informed
opinion.

S   C C An example from Fox () (building on Lebeaux  and Heycock ):
()

a.
b.

A student of his seems to David to be at the party.
OK
de re, OK de dicto
A student of David’s seems to him to be at the party.
OK
de re, *de dicto

Sketch of argument: If Cond. C is formulated in terms of c-command relations
and applies at LF, it will distinguish between de re and de dicto readings only if
those involve LFs with diﬀerent hierarchical relations.

R   
()
()

The cat seems to be out of the bag.
?Advantage might have been taken of them.



DP  S  M C

C 

Sketch of argument: If idioms must be constituents at LF in order to receive
their idiomatic interpretations, these cases call for syntactic reconstruction. An
additional mechanism of semantic reconstruction via high-type traces is then at
best redundant.
Tentative conclusion: Syntactic reconstruction (some version of it) provides the
best account of de dicto readings for raised subjects.

C E
B de re — de dicto : T T R
In this chapter, we will see that quantiﬁcational noun phrases in the
scope of a modal operator can receive a reading where their restrictive
predicate is not interpreted in the worlds introduced by the modal
operator (which is what happens in de re readings as well) while at
the same time their quantiﬁcational force takes scope below the modal
operator (which is what happens in de dicto readings as well). This
seemingly paradoxical situation might force whole-sale revisions to our
architecture. We discuss the standard solution (which involves supplying
predicates with world-arguments) and some alternatives.

.

A Problem: Additional Readings and Scope Paradoxes 

.

The Standard Solution: Overt World Variables 
..
..

Lexical entries 

..

Composition Rules 

..

Syntax 

..

The Need for a Binding Theory for World Variables 

..

Two Kinds of World Pronouns 

..
.

Semantic Values 

Excursus: Semantic reconstruction for de dicto
raised subjects? 

Alternatives to Overt World Variables 
..

Indexed Operators 

..

Scoping After All? 

.

Scope, Restrictors, and the Syntax of Movement 

.

A Recurring Theme: Historical Overview 



.

B de re — de dicto : T T R

C 

A Problem: Additional Readings and Scope
Paradoxes

Janet Dean Fodor discussed examples like () in her dissertation ().
()

Mary wanted to buy a hat just like mine.

Fodor observes that () has three readings, which she labels “speciﬁc de re,”
“non-speciﬁc de re,” and “non-speciﬁc de dicto.”
(i) On the “speciﬁc de re” reading, the sentence says that there is a particular
hat which is just like mine such that Mary has a desire to buy it. Say, I
am walking along Newbury Street with Mary. Mary sees a hat in a display
window and wants to buy it. She tells me so. I don’t reveal that I have one
just like it. But later I tell you by uttering ().
(ii) On the “non-speciﬁc de dicto” reading, the sentence says that Mary’s desire
was to buy some hat or other which fulﬁlls the description that it is just
like mine. She is a copycat.
(iii) On the “non-speciﬁc de re” reading, ﬁnally, the sentence will be true, e.g.,
in the following situation: Mary’s desire is to buy some hat or other, and
the only important thing is that it be a Red Sox cap. Unbeknownst to her,
my hat is one of those as well.
The existence of three diﬀerent readings appears to be problematic for the scopal
account of de re-de dicto ambiguities that we have been assuming. It seems that
our analysis allows just two semantically distinct types of LFs: Either the DP
a hat just like mine takes scope below want, as in (), or it takes scope above
want, as in ().
()

Mary wanted [ [a hat-just-like-mine] [  to buy t ]]

()

[a hat-just-like-mine] [ Mary wanted [  to buy t ]]

In the system we have developed so far, () says that in every world w in which
Mary gets what she wants, there is something that she buys in w that’s a hat in
w and like my hat in w . This is Fodor’s “non-speciﬁc de dicto” reading. (),
on the other hand, says that there is some thing x which is a hat in the actual
world and like my hat in the actual world, and Mary buys x in every one of her
desire worlds. That is Fodor’s “speciﬁc de re.” But what about the “non-speciﬁc
de re”? To obtain this reading, it seems that we would have to evaluate the
predicate hat just like mine in the actual world, so as to obtain its actual extension
(in the scenario we have sketched, the set of all Red Sox caps). But the existential

§.

A P: A R  S P



quantiﬁer expressed by the indeﬁnite article in the hat-DP should not take scope
over the modal operator want, but below it, so that we can account for the fact
that in diﬀerent desire-worlds of Mary’s, she buys possibly diﬀerent hats.
There is a tension here: one aspect of the truth-conditions of this reading
suggests that the DP a hat just like mine should be outside of the scope of want,
but another aspect of these truth-conditions compels us to place it inside the
scope of want. We can’t have it both ways, it would seem, which is why this has
been called a “scope paradox”
Another example of this sort, due to Bäuerle (), is ():
()

Georg believes that a woman from Stuttgart loves every member of the
VfB team.

Bäuerle describes the following scenario: Georg has seen a group of men on
the bus. This group happens to be the VfB team (Stuttgart’s soccer team), but
Georg does not know this. Georg also believes (Bäuerle doesn’t spell out on
what grounds) that there is some woman from Stuttgart who loves every one
of these men. There is no particular woman of whom he believes that, so there
are diﬀerent such women in his diﬀerent belief-worlds. Bäuerle notes that ()
can be understood as true in this scenario. But there is a problem in ﬁnding
an appropriate LF that will predict its truth here. First, since there are diﬀerent
women in diﬀerent belief-worlds of Georg’s, the existential quantiﬁer a woman
from Stuttgart must be inside the scope of believe. Second, since (in each belief
world) there aren’t diﬀerent women that love each of the men, but one that loves
them all, the a-DP should take scope over the every-DP. If the every-DP is in
the scope of the a-DP, and the a-DP is in the scope of believe, then it follows
that the every-DP is in the scope of believe. But on the other hand, if we want to
capture the fact that the men in question need not be VfB-members in Georg’s
belief-worlds, the predicate member of the VfB team needs to be outside of the
scope of believe. Again, we have a “scope paradox”.
Before we turn to possible solutions for this problem, let’s have one more
example:
()

Mary hopes that a friend of mine will win the race.

This again seems to have three readings. In Fodor’s terminology, the DP a friend
of mine can be “non-speciﬁc de dicto,” in which case () is true iﬀ in every
world where Mary’s hopes come true, there is somebody who is my friend and
wins. It can also have a “speciﬁc de re” reading: Mary wants John to win, she
doesn’t know John is my friend, but I can still report her hope as in (). But
there is a third option, the “non-speciﬁc de re” reading. To bring out this rather



B de re — de dicto : T T R

C 

exotic reading, imagine this: Mary looks at the ten contestants and says I hope
one of the three on the right wins - they are so shaggy - I like shaggy people. She
doesn’t know that those are my friends. But I could still report her hope as in
().

.

The Standard Solution: Overt World Variables

The scope paradoxes we have encountered can be traced back to a basic design
feature of our system of intensional semantics: the relevant “evaluation world”
for each predicate in a sentence is strictly determined by its LF-position. All
predicates that occur in the (immediate) scope of the same modal operator must
be evaluated in the same possible worlds. E.g. if the scope of want consists of
the clause a friend of mine (to) win, then every desire-world w will be required
to contain an individual that wins in w and is also my friend in w . If we want
to quantify over individuals that are my friends in the actual world (and not
necessarily in all the subject’s desire worlds), we have no choice but to place
friend of mine outside of the scope of want. And if we want to accomplish this
by means of QR, we must move the entire DP a friend of mine.
Not every kind of intensional semantics constrains our options in this way. One
way to visualize what we might want is to write down an LF that looks promising:
()

Mary wantedw [λw [ a hat-just-like-mine w ]λx [  to buyw x ]]

We have annotated each predicate with the world in which we wish to evaluate
it. w is the evaluation world for the entire sentence and it is the world in
which we evaluate the predicates want and hat-just-like-mine. The embedded
sentence contributes a function from worlds to truth-values and we insert an
explicit λ-operator binding the world where the predicate buy is evaluated. The
crucial aspect of () is that the world in which hat-just-like-mine is evaluated is
the matrix evaluation world and not the same world in which its clause-mate
predicate buy is evaluated. This LF thus looks like it might faithfully capture
Fodor’s third reading.
Logical forms with overt world variables such as () are in fact the standard
solution to the problem presented by the third reading. Let us spell out some of
the technicalities. Later, we will consider a couple of alternatives.

§.

T S S: O W V

..

Semantic Values



In this new system, we do not relativize the interpretation function to a possible
world. As in the old extensional system, the basic notion is just “ α ,” i.e., “the
semantic value of α”. (Or “ α g ,” “the semantic value of α under assignment
g”, if α contains free variables.) However, semantic values are no longer always
extensions; some of them still are, but others are intensions. Here are some
representative examples of the types of semantic values for various kinds of words.

..

Lexical entries

()

a.
b.
c.
d.

smart = λw ∈ Ds . λx ∈ De . x is smart in w
likes = λw ∈ Ds . λx ∈ De . λy ∈ De . y likes x in w
teacher = λw ∈ Ds . λx ∈ De . x is a teacher in w
friend = λw ∈ Ds . λx ∈ De . λy ∈ De . y is x’s friend in w

()

a.

believe = λw ∈ Ds . λp ∈ D s,t . λx ∈ D.
∀w [w conforms to what x believes in w → p(w ) = ]
must = λw ∈ Ds . λR ∈ D s,st . λp ∈ D s,t .
∀w [R(w)(w ) =  → p(w ) = ]

b.
()

a.
b.
c.
d.

Ann =Ann
and = λu ∈ Dt . [λv ∈ Dt . u = v = ]
the = λf ∈ D e,t : ∃!x. f(x) = . the y such that f(y) = .
every = λf ∈ D e,t . λg ∈ D e,t . ∀x[f(x) =  → g(x) = ]

The entries in () (for words whose extensions are constant across worlds)
have stayed the same; their semantic values are still extensions. But the ones for
predicates (ordinary ones and modal ones) in () and () have changed; these
items now have as their semantic values what used to be their intensions.

..

Composition Rules

We abolish the special rule of Intensional Functional Application (IFA) and
go back to our old inventory of Functional Application, λ-Abstraction, and
Predicate Modiﬁcation .
 We also abolish the Extensional Functional Application rule (EFA), if we had that one (see
section .. “Semantic Reconstruction”).
 Actually, PM requires a slightly revised formulation: α β g = λw ∈ Ds . λx ∈
De . α g (w)(x) = β g (w)(x) = . But we will not be concerned with the compositional
interpretation of modiﬁer-structures here, so you won’t be needing this rule.



..

B de re — de dicto : T T R

C 

Syntax

What we have at this point does not allow us to interpret even the simplest
syntactic structures. For instance, we can’t interpret the tree in ().
()

[ VP John leave]

The verb’s type is s, et , so it’s looking for a sister node which denotes a world.
John, which denotes an individual, is not a suitable argument.
We get out of this problem by positing more abstract syntactic structures (at the
LF level). Speciﬁcally, we assume that there is a set of covert “world pronouns”
which are generated as sisters to all lexical predicates in LF-structures. Oﬃcially,
the variable would be a pair of an index and the type s. Inoﬃcially, we will use
“w” with a subscripted index, with the understanding that the “w” indicates we
are dealing with a variable of type s. So, the syntax would generate something
like ():
()

[ John [ leave w ]]

The sentence would then obviously have an assignment-dependent extension (a
truth-value), depending on what world the variable assignment assigns to the
world variable with index . In our intensional system of Chapter  — , we
were assuming the following principle:
()

An utterance of a sentence (=LF) φ in world w is true iﬀ φ

w

= .

To achieve the same in our new system, we would have to ensure that the variable
assignment assign the utterance world to the free world variable(s) in the sentence.
Another possibility, which we will adopt here, is to introduce a variable binder
on top of the sentence. We will assume the following kind of syntactic structure
at LF:
()

[λw [ John [ leave w ]]]

The sentence now has as its extension what used to be its intension, a proposition.
The principle of utterance truth now is this:
()

An utterance of a sentence (=LF) φ in world w is true iﬀ φ (w) = .

Now, we have to look at more complex sentences. First, a simple case of
embedding. The sentence is John wants to leave, which now as an LF like this:

§.

T S S: O W V

()

[ λw [ John [[ wants w [[ λw [  [ leave w ]]]]]]]]



E .: Calculate the semantic value of ().
Next, look at an example involving a complex subject, such as the teacher left.
()

[ λw [[ the [ teacher w ]][ left w ]]]

The verb will need a world argument as before. The noun teacher will likewise
need one, so that the can get the required argument of type e, t (not s, et !).
If we co-index the two world variables, we derive as the semantic value for ()
what its intension would have been in old system. But nothing we have said
forces us to co-index the two world variables, which is what will allow us to
derive the third reading for relevant examples.
Consider what happens when the sentence contains both a modal operator and
a complex DP in its complement.
()

Mary wants a friend of mine to win.

There are now three predicates that need world arguments. Furthermore, there
will be two λ-operators binding world variables. We can now represent the
three readings (to make the structures more readable, we’ll leave oﬀ most of the
bracketing and start writing the world arguments as subscripts to the predicates):
()

a.
b.
c.

non-speciﬁc de dicto:
λw Mary wantsw [λw a friend-of-minew leavew ]
speciﬁc de re:
λw [a friend-of-minew ]λ x Mary wantsw [λw x leavew ]]
non-speciﬁc de re:
λw Mary wantsw [λw a friend-of-minew leavew ]

In this new framework, then, we have a way of resolving the apparent “scope
paradoxes” and of acknowledging Fodor’s point that there are two separate
distinctions to be made when DPs interact with modal operators. First, there is
the scopal relation between the DP and the operator; the DP may take wider
scope (Fodor’s “speciﬁc” reading) or narrower scope (“non-speciﬁc” reading) than
the operator. Second, there is the choice of binder for the world-argument of
the DP’s restricting predicate; this may be cobound with the world-argument of
the embedded predicate (Fodor’s “de dicto”) or with the modal operator’s own
world-argument (“de re”). So the de re-de dicto distinction in the sense of Fodor
is not per se a distinction of scope; but it has a principled connection with scope
in one direction: Unless the DP is within the modal operator’s scope, the de



B de re — de dicto : T T R

C 

dicto option (= co-binding the world-pronoun with the embedded predicate’s
world-argument) is in principle unavailable. (Hence “speciﬁc” implies “de re”,
and “de dicto” implies “non-speciﬁc”.) But there is no implication in the other
direction: if the DP has narrow scope w.r.t. to the modal operator, either the
local or the long-distance binding option for its world-pronoun is in principle
available. Hence “non-speciﬁc” readings may be either “de re” or “de dicto”.
For the sake of clarity, we should introduce a diﬀerent terminology than Fodor’s.
The labels “speciﬁc” and “non-speciﬁc” especially have been used in so many
diﬀerent senses by so many diﬀerent people that it is best to avoid them altogether.
So we will refer to Fodor’s “speciﬁc readings” and “non-speciﬁc readings” as
“wide-quantiﬁcation readings” and “narrow-quantiﬁcation readings”, or “narrowQ/wide-Q readings” for short. For the distinction pertaining to the interpretation
of the restricting NP, we will keep the terms “de re” and “de dicto”, but will
amplify them to “restrictor-de re” and “restrictor-de dicto” (“R-de re”/”R-de
dicto”).
E .: For DPs with extensions of type e (speciﬁcally, DPs headed by
the deﬁnite article), there is a truth-conditionally manifest R-de re/R-de dicto
distinction, but no truth-conditionally detectable wide-Q/narrow-Q distinction.
In other words, if we construct LFs analogous to (a-c) above for an example
with a deﬁnite DP, we can always prove that the ﬁrst option (wide scope DP)
and the third option (narrow scope DP with distantly bound world-pronoun)
denote identical propositions. In this exercise, you are asked to show this for the
example in ().
()

John believes that your abstract will be accepted.

.. The Need for a Binding Theory for World Variables
One could in principle imagine some indexings of our LFs that we have not
considered so far. The following LF indexes the predicate of the complement
clause to the matrix λ-operator rather than to the one on top of its own clause.
()

λw John wantsw [λw  leavew ]

Of course, the resulting semantics would be pathological: what John would be
claimed to stand in the wanting relation to is a set of worlds that is either the
entire set W of possible worlds (if the evaluation world is one in which John
leaves) or the empty set (if the evaluation world is one in which John doesn’t
leave). Clearly, the sentence has no such meaning. Do we need to restrict our
system to not generate such an LF? Perhaps not, if the meaning is so absurd that

§.

T S S: O W V



the LF would be ﬁltered out by some overarching rules distinguishing sense from
nonsense.
But the problem becomes real when we look at more complex examples. Here is
one discussed by Percus in important work (Percus ):
()

Mary thinks that my brother is Canadian.

Since the subject of the lower clause is a type e expression, we expect at least two
readings: de dicto and de re, cf. Exercise .. The two LFs are as follows:
()

a.
b.

de dicto
λw Mary thinksw [ (that) λw my brotherw (is) Canadianw ]
de re
λw Mary thinksw [ (that) λw my brotherw (is) Canadianw ]

But as Percus points out, there is another indexing that might be generated:
()

λw Mary thinksw [ (that) w my brotherw (is) Canadianw ]

In (), we have co-indexed the main predicate of the lower clause with the
matrix λ-operator and co-indexed the nominal predicate brother with the embedded λ-operator. That is, in comparison with the de re reading in (b), we
have just switched around the indices on the two predicates in the lower clause.
Note that this LF will not lead to a pathological reading. So, is the predicted
reading one that the sentence actually has? No. For the de re reading, we can
easily convince ourselves that the sentence does have that reading. Here is Percus’
scenario: “My brother’s name is Allon. Suppose Mary thinks Allon is not my
brother but she also thinks that Allon is Canadian.” In such a scenario, our
sentence can be judged as true, as predicted if it can have the LF in (b). But
when we try to ﬁnd evidence that () is a possible LF for our sentence, we fail.
Here is Percus:
If the sentence permitted a structure with this indexing, we would
take the sentence to be true whenever there is some actual Canadian
who Mary thinks is my brother — even when this person is not my
brother in actuality, and even when Mary mistakenly thinks that he
is not Canadian. For instance, we would take the sentence to be
true when Mary thinks that Pierre (the Canadian) is my brother and
naturally concludes — since she knows that I am American — that
Pierre too is American. But in fact we judge the sentence to be false
on this scenario, and so there must be something that makes the



B de re — de dicto : T T R

C 

indexing in () impossible.
Percus then proposes the following descriptive generalization:
()

G X: The situation pronoun that a verb selects for must
be coindexed with the nearest λ above it.

We expect that there will need to be a lot of work done to understand the deeper
sources of this generalization. For fun, we oﬀer the following implementation
(devised by Irene Heim).

.. Two Kinds of World Pronouns
We distinguish two syntactic types of world-pronouns. One type, w-, behaves
like relative pronouns and  in the analysis of H& K, ch. . (pp. ﬀ.): it
is semantically vacuous itself, but can move and leave a trace that is a variable.
The only diﬀerence between w- and  is that the latter leaves a variable
of type e when it moves, whereas the former leaves a variable of type s. The
other type of world-pronoun, w-pro, is analogous to bound-variable personal
pronouns, i.e., it is itself a variable (here of type s). Like a personal pronoun, it
can be coindexed with the trace of an existing movement chain.
With this inventory of world-pronouns, we can capture the essence of Generalization X by stipulating that w-pro is only generated in the immediate scope
of a determiner (i.e., as sister to the determiner’s argument). Everywhere else
where a world-pronoun is needed for interpretability, we must generate a w-
and move it. This (with some tacit assumptions left to the reader to puzzle over)
derives the result that the predicates inside nominals can be freely indexed but
that the ones inside predicates are captured by the closest λ-operator.
As we said, there is plenty more to be explored in the Binding Theory for world
pronouns. The reader is referred to the paper by Percus and the references he
cites.

.. Excursus: Semantic reconstruction for de dicto raised
subjects?
Let us look back at the account of de dicto readings of raised subjects that we
sketched earlier in Section ... We showed that you can derive such readings
 Percus works with situation pronouns rather than world pronouns, an immaterial diﬀerence
for our purposes here.

§.

T S S: O W V



by positing a high type trace for the subject raising, a trace of type s, et, t .
Before the lower predicate can combine with the trace, the semantic value of the
trace has to be extensionalized by being applied to the lower evaluation world
(done via the EFA composition principle). Upstairs the raised subject has to
be combined with the λ-abstract (which will be of type s, et, t , t ) via its
intension.
We then saw recently discovered data suggesting that syntactic reconstruction is
actually what is going on. This, of course, raises the question of why semantic
reconstruction is unavailable (otherwise we wouldn’t expect the data that we
observed).
Fox (: p. , fn. ) mentions two possible explanations:
(i) “traces, like pronouns, are always interpreted as variables that range over
individuals (type e)”,
(ii) “the semantic type of a trace is determined to be the lowest type compatible
with the syntactic environment (as suggested in Beck ())”.
In this excursus, we will brieﬂy consider whether our new framework has something to say about this issue. Let’s ﬁgure out what we would have to do in the new
framework to replicate the account in the section on semantics reconstruction.
Downstairs, we would have a trace of type s, et, t . To calculate its extension,
we do not need recourse to a special composition principle, but can simply give it
a world-argument (co-indexed with the abstractor resulting from the movement
of the w- in the argument position of the lower verb).
Now, what has to happen upstairs? Well, there we need the subject to be of type
s, et, t , the same type as the trace, to make sure that its semantics will enter
the truth-conditions downstairs. But how can we do this?
We need the DP somebody from New York to have as its semantic value an intension, the function from any world to the existential quantiﬁer over individuals
who are people from New York in that world. This is actually hard to do in our
system. It would be possible if (i) the predicate(s) inside the DP received w-
as their argument, and if (ii) that w- were allowed to moved to adjoin to the
DP. If we manage to rule out at least one of the two preconditions on principled
grounds, we would have derived the impossibility of semantic reconstruction as
a way of getting de dicto readings of raised subjects.
(i) may be ruled out by the Binding Theory for world pronominals, when it
gets developed.
(ii) may be ruled out by principled considerations as well. Perhaps, worldabstractors are only allowed at sentential boundaries. See Larson () for
some discussion of recalcitrant cases, one of which is the object position of
so-called intensional transitive verbs, the topic of another section.



B de re — de dicto : T T R

C 

. Alternatives to Overt World Variables
We presented (a variant of ) what is currently the most widely accepted solution to
the scope paradoxes, which required the use of non-locally bound world-variables.
There are some alternatives, one of which is to some extent a “notational variant”,
the others involved syntactic scoping after all.

..

Indexed Operators

It is possible to devise systems where predicates maintain the semantics we originally gave them, according to which they are sensitive to a world of evaluation
parameter. The freedom needed to account for the third reading and further
facts would be created by assuming more sophisticated operators that shift the
evaluation world. Here is a toy example:
()

Mary wants [ a [  friend-of-mine ] leave ]

The idea is that the  “temporarily” shifts the evaluation world back to
what it was “before” the abstraction over worlds triggered by want happened.
This kind of system can be spelled out in as much detail as the world-variable
analysis. Cresswell () proves that the two systems are equivalent in their
expressive power. The decision is therefore a syntactic one. Does natural
language have a multitude of indexed world-shifters or a multitude of indexed
world-variables? Cresswell suspects the former, as did Kamp () who wrote:
I of course exclude the possibility of symbolizing the sentence by
means of explicit quantiﬁcation over moments. Such a symbolization
would certainly be possible; and it would even make the operators P
and F superﬂuous. Such symbolizations, however, are a considerable
departure from the actual form of the original sentences which they
represent — which is unsatisfactory if we want to gain insight into
the semantics of English. Moreover, one can object to symbolizations
involving quantiﬁcation over such abstract objects as moments, if
these objects are not explicitly mentioned in the sentences that are
to be symbolized.
There is some resistance to world-time variables because they are not phonetically
realized. But in an operator-based system, we’ll have non-overt operators all over
the place. So, there is no a priori advantage for either system. We will stick with
the more transparent LFs with world variables.

§.

A  O W V

..

Scoping After All?



Suppose we didn’t give up our previous framework, in which the evaluationworld for any predicate was strictly determined by its LF-position. It turns out
that there is a way (actually, two ways) to derive Fodor’s non-speciﬁc de re reading
in that framework after all.
Recall again what we need. We need a way to evaluate the restrictive predicate
of a DP with respect to the higher evaluation world while at the same time
interpreting the quantiﬁcational force of the DP downstairs in its local clause.
We saw that if we move the DP upstairs, we get the restriction evaluated upstairs
but we also have removed the quantiﬁer from where it should exert its force.
And if we leave the DP downstairs where its quantiﬁcational forces is felt, its
restriction is automatically evaluated down there as well. That is why Fodor’s
reading is paradoxical for the old framework. In fact, though there is no paradox.
Way  Raise the DP upstairs but leave a e, t , t trace. This way the restriction
is evaluated upstairs, then a quantiﬁer extension is calculated, and that
quantiﬁer extension is transmitted to trace position. This is just what we
needed.
Way  Move the NP-complement of a quantiﬁcational D independently of the
containing DP. Then we could generate three distinct LFs for a sentence
like Mary wants a friend of mine to win: two familiar ones, in which the
whole DP a friend of mine is respectively inside and outside the scope
of want, plus a third one, in which the NP friend of mine is outside the
scope of want but the remnant DP a [NP t] has been left behind inside
it:
()

[ [ NP f-o-m] λ [ Mary [ want [ [ DP a t e,t , ] win]]]]

E .: Convince yourself that this third LF represents the narrowquantiﬁcation, restrictor-de re reading (Fodor’s “non-speciﬁc de re").
We have found, then, that it is in principle possible after all to account for narrowQ R-de re readings within our original framework of intensional semantics.
E .: In (), we chose to annotate the trace of the movement of
the NP with the type-label e, t , thus treating it as a variable whose values are
predicate-extensions (characteristic functions of sets of individuals). As we just
saw, this choice led to an interpretable structure. But was it our only possible
choice? Suppose the LF-structure were exactly as in (), except that the
 Something like this was proposed by Groenendijk & Stokhof () in their treatment of
questions with which-DPs.



B de re — de dicto : T T R

C 

trace had been assigned type s, et instead of e, t . Would the tree still be
interpretable? If yes, what reading of the sentence would it express?
E .: We noted in the previous section about the world-pronouns framework that there was a principled reason why restrictor-de dicto readings necessarily
are narrow-quantiﬁcation readings. (Or, in Fodor’s terms, why there is no such
thing as a “speciﬁc de dicto” reading.) In that framework, this was simply a consequence of the fact that bound variables must be in the scope of their binders.
What about the alternative account that we have sketched in the present section?
Does this account also imply that R-de dicto readings are necessarily narrow-Q?

.

Scope, Restrictors, and the Syntax of
Movement

To conclude our discussion of the ambiguities of DPs in the complements of
modal operators, let us consider some implications for the study of LF-syntax.
This will be very inconclusive.
Accepting the empirical evidence for the existence of narrow-Q R-de re readings
which are truth-conditionally distinct from both the wide-Q R-de re and the
narrow-Q R-de dicto readings, we are facing a choice between two types of
theories. One theory, which we have referred to as the “standard” one, uses a
combination of DP-movement and world-pronoun binding; it maintains that
wide-quantiﬁcation readings really do depend on (covert) syntactic movement,
but de re interpretations of the restrictor do not. The other theory, which we may
dub the “scopal” account, removes the restrictor from the scope of the modal
operator, either by QR (combined with an et, t type trace) or by movement of
the NP-restrictor by itself.
In order to adjudicate between these two competing theories, we may want to
inquire whether the R-de re — de dicto distinction exhibits any of the properties
that current syntactic theory would take to be diagnostic of movement. This is a
very complex enterprise, and the few results to have emerged so far appear to be
pointing in diﬀerent directions.
We have already mentioned that it is questionable whether NPs that are complements to D can be moved out of their DPs. Even if it is possible, we might
expect this movement to be similar to the movement of other predicates, such
as APs, VPs, and predicative NPs. Such movements exist, but — as discussed
by Heycock, Fox, and the sources they cite — they typically have no eﬀect on
semantic interpretation and appear to be obligatorily reconstructed at LF. The

§.

S, R,   S  M



type of NP-movement required by the purely scopal theory of R-de re readings
would be exceptional in this respect.
Considerations based on the locality of uncontroversial instances of QR provide
another reason to doubt the plausibility of the scopal theory. May () argued,
on the basis of examples like (), that quantiﬁers do not take scope out of
embedded tensed clauses.
()

a.
b.

Some politician will address every rally in John’s district.
Some politician thinks that he will address every rally in John’s
district.

While in (a) the universal quantiﬁer can take scope over the existential
quantiﬁer in subject position, this seems impossible in (b), where the universal
quantiﬁer would have to scope out of its ﬁnite clause. Therefore, May suggested,
we should not attribute the de re reading in an example like our () to the
operation of QR.
()

John believes that your abstract will be accepted.

As we saw above, the standard theory which appeals to non-locally bound worldpronouns does have a way of capturing the de re reading of () without any
movement, so it is consistent with May’s suggestion. The purely scopal theory
would have to say something more complicated in order to reconcile the facts
about () and (). Namely, it might have to posit that DP-movement is
ﬁnite-clause bound, but NP-movement is not. Or, in the other version, it would
have to say that QR can escape ﬁnite clauses but only if it leaves a et, t type
trace.
Both theories, by the way, have a problem with the fact that May’s ﬁnite-clauseboundedness does not appear to hold for all quantiﬁcational DPs alike. If we
look at the behavior of every, no, and most, we indeed can maintain that there is
no DP-movement out of tensed complements. For example, () could mean
that Mary hopes that there won’t be any friends of mine that win. Or it could
mean (with suitable help from the context) that she hopes that there is nobody
who will win among those shaggy people over there (whom I describe as my
friends). But it cannot mean merely that there isn’t any friend of mine who she
hopes will win.
()

Mary hopes that no friend of mine will win.

So () has R-de dicto and R-de re readings for no friend of mine, but no
wide-quantiﬁcation reading where the negative existential determiner no takes

B de re — de dicto : T T R



C 

matrix scope. Compare this with the minimally diﬀerent inﬁnitival complement
structure, which does permit all three kinds of readings.
()

Mary expects no friend of mine to win.

However, indeﬁnite DPs like a friend of mine, two friends of mine are notoriously
much freeer in the scope options for the existential quantiﬁaction they express.
For instance, even the ﬁnite clause in () seems to be no impediment to a
reading that is not only R-de re but also wide-quantiﬁcational (i.e., it has the
existential quantiﬁer over individuals outscoping the universal world-quantiﬁer).
()

Mary hopes that a friend of mine will win.

The peculiar scope-taking behavior of indeﬁnites (as opposed to universal, proportional, and negative quantiﬁers) has recently been addressed by a number of
authors (Abusch ; Kratzer ; Matthewson ; Reinhart ; Winter
), and there are good prospects for a successful theory that generates even
the wide-Q R-de re readings of indeﬁnites without any recourse to non-local
DP-movement. You are encouraged to read these works, but for our current
purposes here, all we want to point out is that, with respect to the behavior of
indeﬁnites, neither of the two theories we are trying to compare seems to have a
special advantage over the other. This is because wide-Q readings result from
DP-movement according to both theories.
As we mentioned in the previous chapter, a number of recent papers have been
probing the connection between de dicto readings and the eﬀects of Binding
Condition C applying at LF. These authors have converged on the conclusion
that DPs which are read as de dicto behave w.r.t. Binding Theory as if they are
located below the relevant modal predicate at LF, and DPs that are read as de
re (i.e., wide-Q, R-de re) behave as if they are located above. It is natural to
inquire whether the same kind of evidence could also be exploited to determine
the LF-location of the NP-part of a DP which is read as narrow-quantiﬁcational
but restrictor-de re. If this acted for Condition C purposes as if it were below
the attitude verb, it would conﬁrm the standard theory (non-locally bound
world-pronouns), whereas if it acted as if it was scoped out, we’d have evidence
for the scopal account. Sharvit () constructs some of the relevant examples
and reports judgments that actually favor the scopal theory. For example, she
observes that (a) does allow the narrow-Q, R-de re-reading indicated in (b).
()

a.

How many students who like John does he think every professor
talked to?

 Sharvit’s own conclusion, however, is not that her data supports the purely scopal theory.

§.

S, R,   S  M
b.



For which n does John think that every professor talked to n people
in the set of students who actually like John?

More research is required to corroborate this ﬁnding.
As a ﬁnal piece of potentially relevant data, consider a contrast in Marathi
recently discussed by Bhatt ().
()

[ji bai
kican madhe ahe]i Ram-la watte ki [[ti [ti
 woman kitchen in
is
Ram thinks that that woman
bai]i ] kican madhe nahi]
kitchen in not is
‘Ram thinks that the woman who is in the kitchen is not in the
kitchen’

()

Ram-la watte ki [ [ji bai kican madhe ahe]i [[ti [ti bai]i ] kican madhe
nahi] ]
Ram thinks that  woman kitchen in is that woman kitchen in not is
‘Ram thinks that the woman who is in the kitchen is not in the kitchen’

The English translation of both examples has two readings: a (plausible) de re
reading, on which Ram thinks of the woman who is actually in the kitchen
that she isn’t, and an (implausible) de dicto reading, on which Ram has the
contradictory belief that he would express by saying: “the woman in the kitchen
is not in the kitchen”. The Marathi sentence () also allows these two readings,
but () unambiguously expresses the implausible de dicto reading. Bhatt’s
explanation invokes the assumption that covert movement in Hindi cannot
cross a ﬁnite clause boundary. In (), where the correlative clause has moved
overtly, it can stay high or else reconstruct at LF, thus yielding either reading.
But in (), where it has failed to move up overtly, it must also stay low at LF,
and therefore can only be de dicto. What is interesting about this account is
that it crucially relies on a scopal account of the R-de re-R-de dicto distinction.
(Recall that with type-e DPs like deﬁnite descriptions, there is no additional
wide/narrow-Q ambiguity.) If the standard theory with its non-locally bindable
world-pronouns were correct, we would not expect the constraint that blocks
covert movement in () to aﬀect the possibility of a de re reading.
In sum, then, the evidence appears to be mixed. Some observations appear
to favor the currently standard account, whereas others look like they might
conﬁrm the purely scopal account after all. Much more work is needed.



B de re — de dicto : T T R

C 

.

A Recurring Theme: Historical Overview

To recap, the main shape of the phenomenon discussed in this chapter is that the
intensional parameter (time, world) with respect to which the predicate restricting a quantiﬁer is interpreted can be distinct from the one that is introduced
by the intensional operator that immediately scopes over the quantiﬁer. The
crucial cases have the character of a “scope paradox”. This discovery is one that
has been made repeatedly in the history of semantics. It has been made both in
the domain of temporal dependencies and in the domain of modality. Here are
some of the highlights of that history. .
. The now-operator
Prior () noticed a semantic problem with the adverb now. The main
early researchers that addressed the problem were Kamp () and Vlach
(). A good survey was prepared by van Benthem (). Another early
reference is Saarinen (). The simplest scope paradox examples looked
like this:
()

One day all persons now alive will be dead.

While for this example one could say that now is special in always having
access to the utterance time, other examples show that an unbounded
number of times need to be tracked. It became clear in this work that
whether one uses a multitude of indexed now and then-operators or allows
variables over times is a syntactic and not a deep semantic question.
. The actually-operator
The modal equivalent of the Prior-Kamp scope paradox sentence is:
()

It might have been that everyone actually rich was poor.

Crossley & Humberstone () discuss such examples. Double-indexed
systems of modal logic were studied by Segerberg () and Åqvist ().
See also work by Lewis (a), van Inwagen (), and Hazen ().
Indexed actually-operators are discussed by Prior & Fine (), Peacocke
(), and Forbes (, , ).
. The time of nominal predicates
There is quite a bit of work that argues that freedom in the time-dependency
of nominals even occurs when there is no apparent space for temporal
operators. Early work includes Enç (, ). But see also Ejerhed
(). More recently Musan’s dissertation (Musan ) is relevant.
 Some of this history can be found in comments throughout Cresswell’s book (Cresswell ),
which also contains additional references

§.

A R T: H O
()



Every fugitive is back in custody.

. Tense in Nominals
There is some syntactic work on tense in nominals, see for example
Wiltschko ().
. The Fodor-Reading
Examples similar to the ones from Fodor and Bäuerle that we used at
the beginning of this chapter are discussed in many places (Abusch ;
Bonomi ; Farkas ; Hellan ; Ioup ). The point that all
these authors have made is that the NP-predicate restricting a quantiﬁer
may be evaluated in the actual world, even when that quantiﬁer clearly
takes scope below a modal predicate.
Heim (?) gives an example like this:
()

Every time it could have been the case that the player on the left
was on the right instead.

Here, the player on the left must be evaluated with respect to the actual
world. But it is inside a tensed clause, which — as we saw earlier — is
usually considered a scope island for quantiﬁers.
. Explicit World Variables
Systems with explicit world/time variables were introduced by Tichy ()
and Gallin (). A system (Ty) with overt world-variables is used by
Groenendijk & Stokhof in their dissertation on the semantics of questions.
See also Zimmermann () on the expressive power of that system.
. Movement
The idea of getting the third reading via some kind of syntactic scoping
has not been pursued much. But there is an intriguing idea in a paper
by Bricker (), cited by Cresswell (: p. ). Bricker formalizes a
sentence like Everyone actually rich might have been poor as follows:
()

∃X(∀y(Xy ≡ rich y)& ∀y(Xy → poor y))

This is apparently meant to be interpreted as ‘there is a plurality X all of
whose members are rich and it might have been the case that all of the
members of X are poor’. This certainly looks like somehow a syntactic
scoping of the restrictive material inside the universal quantiﬁer out of the
scope of the modal operator has occurred.

— T     —

B
Abusch, Dorit. . The scope of indeﬁnites. Natural Language Semantics ().
–. doi:./BF.
Aikhenvald, Alexandra Y. . Evidentiality. Oxford: Oxford University Press.
Anand, Pranav & Valentine Hacquard. . Epistemics with attitude. Proceedings of Semantics and Linguistic Theory . doi:/.
Åqvist, Lennart. . Modal logic with subjunctive conditionals and
dispositional predicates.
Journal of Philosophical Logic (). –.
doi:./BF.
Asher, Nicholas. . A typology for attitude verbs and their anaphoric properties. Linguistics and Philosophy (). –. doi:./BF.
Barcan, Ruth C. . A functional calculus of ﬁrst order based on strict
implication. Journal of Symbolic Logic (). –. doi:./.
Barwise, John & Robin Cooper. . Generalized quantiﬁers and natural
language. Linguistics and Philosophy (). –. doi:./BF.
Bäuerle, Rainer. . Pragmatisch-semantische aspekte der NP-interpretation.
In M. Faust, R. Harweg, W. Lehfeldt & G. Wienold (eds.), Allgemeine sprachwissenschaft, sprachtypologie und textlinguistik: Festschrift für peter hartmann,
–. Narr Tübingen.
Beck, Sigrid. . Wh–constructions and transparent logical form: Universität
Tübingen dissertation.
Belnap, Jr., Nuel D. . Conditional assertion and restricted quantiﬁcation.
Noûs (). –. doi:./.
Belnap, Jr., Nuel D. . Restricted quantiﬁcation and conditional assertion.
In Hugues Leblanc (ed.), Truth, syntax and modality: Proceedings of the Temple
University conference on alternative semantics, vol.  Studies in Logic and the
Foundations of Mathematics, –. Amsterdam: North-Holland.
Bennett, Jonathan. . A philosophical guide to conditionals. Oxford University
Press.



B

Bennett, Michael & Barbara Partee. . Toward the logic of tense and aspect in
English. Indiana University Linguistics Club.
van Benthem, Johan. . Tense logic and standard logic. Logique et Analyse .
–.
Bhatt, Rajesh. . Obligation and possession. In Heidi Harley (ed.), Papers
from the upenn/mit roundtable on argument structure and aspect, vol.  MIT
Working Papers in Linguistics, –. URL http://people.umass.edu/bhatt/
papers/bhatt-haveto.pdf.
Bhatt, Rajesh. . Locality in apparently non-local relativization: Correlatives
in the modern indo-aryan languages. Handout for Talk Presented at UT
Austin and MIT.
Bhatt, Rajesh & Roumyana Pancheva. . Conditionals. In The Blackwell
companion to syntax, vol. , –. Blackwell. URL http://www-rcf.usc.edu/
~pancheva/bhatt-pancheva_syncom.pdf.
Blain, Eleanor M. & Rose-Marie Déchaine. . Evidential types: Evidence
from Cree dialects. International Journal of American Linguistics (). –.
doi:./.
Blumson, Ben. . Pictures, perspective and possibility. Philosophical Studies
doi:./s---.
Bonomi, Andrea. . Transparency and speciﬁcity in intensional contexts.
In P. Leonardi & M. Santambrogio (eds.), On quine, –. Cambridge
University Press.
Bonomi, Andrea & Sandro Zucchi. . A pragmatic framework for truth
in ﬁction. Dialectica (). –. doi:./j.-..tb.x.
Preprint http://ﬁlosoﬁa.dipaﬁlo.unimi.it/~bonomi/Pragmatic.pdf.
Bricker, Phillip. . Quantiﬁed modal logic and the plural de re. In French,
Uehling & Wettstein (eds.), Contemporary perspectives in the philosophy of
language ii Midwest Studies in Philosophy, Vol. , –. University of
Notre Dame Press.
Butler, Jonny. . A minimalist treatment of modality. Lingua (). –.
doi:./S-()-.
Carnap, Rudolf. . Meaning and necessity: A study in semantics and modal
logic. Chicago: University of Chicago Press.
Chierchia, Gennaro & Sally McConnell-Ginet. . Meaning and grammar:
An introduction to semantics (nd edition). MIT Press.


Chomsky, Noam. . A minimalist program for linguistic theory. In Kenneth
Hale & Samuel Jay Keyser (eds.), The view from building : Essays in linguistics
in honor of sylvain bromberger, –. MIT Press Cambridge, MA.
Cooper, Robin. . Tense and discourse location in situation semantics.
Linguistics and Philosophy (). –. doi:./BF.
Copeland, B. Jack. . The genesis of possible worlds semantics. Journal of
Philosophical Logic (). –. doi:./A:.
Cormack, Annabel & Neil Smith. . Modals and negation in English. In
Sjef Barbiers, Frits Beukema & Wim van der Wurﬀ (eds.), Modality and its
interaction with the verbal system, –. Benjamins.
Cresswell, Max. . Logics and languages. London: Methuen.
Cresswell, Max. . Entities and indices. Kluwer Dordrecht.
Crossley, J. N. & I. L. Humberstone. . The logic of actually. Reports on
Mathematical Logic . –.
DeRose, Keith. . Epistemic possibilities. The Philosophical Review ().
–. doi:./.
Dowty, David. . Tenses, time adverbs, and compositional semantic theory.
Linguistics and Philosophy . –.
Dowty, David, Robert Wall & Stanley Peters. . Introduction to Montague
semantics. Kluwer.
Dowty, David R. . Toward a semantic analysis of verb aspect and the
english ‘imperfective’ progressive. Linguistics and Philosophy (). –.
doi:./BF.
Drubig, Hans Bernhard. . On the syntactic form of epistemic modality. Ms,
Universität Tübingen. URL http://www.sfb.uni-tuebingen.de/b/papers/
DrubigModality.pdf.
Edgington, Dorothy. . On conditionals. Mind (). –. URL
./mind/...
Egan, Andy. . Epistemic modals, relativism, and assertion. Philosophical
Studies (). –. doi:./s---x.
Egan, Andy, John Hawthorne & Brian Weatherson. . Epistemic modals in
context. In Gerhard Preyer & Georg Peter (eds.), Contextualism in philosophy:
Knowledge, meaning, and truth, –. Oxford: Oxford University Press.



B

Ejerhed, Eva. . The syntax and semantics of English tense markers. Monographs from the Institute of Linguistics, University of Stockholm, No. .
Ejerhed, Eva. . Tense as a source of intensional ambiguity. In Frank Heny
(ed.), Ambiguities in intensional contexts, –. Reidel Dordrecht.
Elbourne, Paul & Uli Sauerland. . Total reconstruction, PF movement, and
derivational order. Linguistic Inquiry (). –.
Enç, Mürvet. . Tense without scope: An analysis of nouns as indexicals: University of Wisconsin, Madison dissertation.
Enç, Mürvet. . Towards a referential analysis of temporal expressions.
Linguistics and Philosophy . –.
Farkas, Donka. . Evaluation indices and scope. In Anna Szabolcsi (ed.),
Ways of scope taking, –. Dordrecht: Kluwer.
von Fintel, Kai. . Restrictions on quantiﬁer domains: University of Massachusetts at Amherst dissertation. URL http://semanticsarchive.net/Archive/
jANIwN/ﬁntel--thesis.pdf.
von Fintel, Kai. . Quantiﬁers and ‘if ’-clauses. The Philosophical Quarterly
(). –. doi:./-.. URL http://mit.edu/ﬁntel/
www/qandif.pdf.
von Fintel, Kai. . NPI licensing, Strawson entailment, and context dependency. Journal of Semantics (). –. doi:./jos/...
von Fintel, Kai. . Counterfactuals in a dynamic context. In Michael
Kenstowicz (ed.), Ken Hale: A life in language, –. MIT Press.
von Fintel, Kai. . Modality and language. In Donald M. Borchert (ed.),
Encyclopedia of philosophy – second edition, MacMillan. URL http://mit.edu/
ﬁntel/ﬁntel--modality.pdf.
von Fintel, Kai. . If : The biggest little word. Slides from a plenary
address given at the Georgetown University Roundtable, March , . URL
http://mit.edu/ﬁntel/gurt-slides.pdf.
von Fintel, Kai. . Conditionals. Ms, prepared for Semantics: An international
handbook of meaning, edited by Klaus von Heusinger, Claudia Maienborn, and
Paul Portner. URL http://mit.edu/ﬁntel/ﬁntel--hsk-conditionals.pdf.
von Fintel, Kai & Anthony S. Gillies. . An opinionated guide to epistemic
modality. In Tamar Szabó Gendler & John Hawthorne (eds.), Oxford studies
in epistemology: Volume , –. Oxford University Press. URL http://mit.
edu/ﬁntel/ﬁntel-gillies--ose.pdf.


von Fintel, Kai & Anthony S. Gillies. a. CIA leaks. The Philosophical Review
(). –. doi:./--.
von Fintel, Kai & Anthony S. Gillies. b. Might made right. To appear in a
volume on epistemic modality, edited by Andy Egan and Brian Weatherson,
Oxford University Press. URL http://mit.edu/ﬁntel/ﬁntel-gillies--mmr.
pdf.
von Fintel, Kai & Anthony S. Gillies. . Must . . . stay . . . strong! Final
pre-print, to appear in Natural Language Semantics. URL http://mit.edu/
ﬁntel/ﬁntel-gillies--mss.pdf.
von Fintel, Kai & Sabine Iatridou. . If and when If -clauses can restrict
quantiﬁers. Ms, MIT. URL http://mit.edu/ﬁntel/ﬁntel-iatridou--ifwhen.
pdf.
von Fintel, Kai & Sabine Iatridou. . Epistemic containment. Linguistic
Inquiry (). –. doi:./.
von Fintel, Kai & Sabine Iatridou. . What to do if you want to go to
Harlem: Anankastic conditionals and related matters. Ms, MIT. URL
http://mit.edu/ﬁntel/ﬁntel-iatridou--harlem.pdf.
von Fintel, Kai & Sabine Iatridou. . How to say ought in Foreign: The
composition of weak necessity modals. In Jacqueline Guéron & Jacqueline
Lecarme (eds.), Time and modality (Studies in Natural Language and Linguistic
Theory ), –. Springer. doi:./----.
Fodor, Janet Dean. . The linguistic description of opaque contexts: Massachusetts Institute of Technology dissertation. Published in  by Indiana
University Linguistics Club and in  in the Series “Outstanding Dissertations in Linguistics” by Garland.
Forbes, Graeme. . Physicalism, instrumentalism, and the semantics of modal
logic. Journal of Philosophical Logic . –.
Forbes, Graeme. . The metaphysics of modality. Oxford: Clarendon Press.
Forbes, Graeme. . Languages of possibility. Oxford: Blackwell.
Fox, Danny. . Economy and semantic interpretation. MIT Press.
Frank, Anette. . Context dependence in modal constructions: Universität
Stuttgart dissertation. URL http://www.dfki.de/~frank/papers/header.ps.gz.
Gajewski, Jon. . Neg-raising: Polarity and presupposition: Massachusetts
Institute of Technology dissertation. doi:./.

B



Gajewski, Jon. . Neg-raising and polarity. Linguistics and Philosophy
doi:./s---z.
Gallin, D. . Intensional and higher-order modal logic. North-Holland Amsterdam.
Gamut, L. T. F. . Logic, language, and meaning. Chicago University Press.
Garson, James. . Modal logic. In Edward N. Zalta (ed.), The Stanford
encyclopedia of philosophy, URL http://plato.stanford.edu/entries/logic-modal/.
Gergonne, Joseph Diaz. . Essai de dialectique rationnelle. Annales de
Mathématiques Pures et Appliquées . –. URL http://archive.numdam.
org/article/AMPA_-_____.pdf.
Geurts, Bart. . Presuppositions and anaphors in attitude contexts. Linguistics
& Philosophy (). –. doi:./A:.
Giannakidou, Anastasia. . Aﬀective dependencies. Linguistics and Philosophy
(). –. doi:./A:.
Gillies, Anthony S. . Counterfactual scorekeeping. Linguistics and Philosophy
(). –. doi:./s---.
Gillies, Anthony S. . On truth-conditions for if (but not quite only if ).
The Philosophical Review (). –. doi:./--.
Gillies, Anthony S. .
doi:./sp...

Iﬃness.

Semantics and Pragmatics (). –.

Groenendijk, Jeroen & Martin Stokhof. . Semantic analysis of Whcomplements. Linguistics and Philosophy . –.
Hacquard, Valentine. . Aspects of modality: Massachusetts Institute of
Technology dissertation. URL http://people.umass.edu/hacquard/hacquard_
thesis.pdf.
Hacquard, Valentine. . Modality. Ms, prepared for Semantics: An international handbook of meaning, edited by Klaus von Heusinger, Claudia
Maienbon, and Paul Portner. URL http://ling.umd.edu/~hacquard/papers/
HoS_Modality_Hacquard.pdf.
Hanley, Richard. . As good as it gets: Lewis on truth in ﬁction. Australasian
Journal of Philosophy (). –. doi:./.
Hazen, Allen. . One of the truths about actuality. Analysis . –.


Heim, Irene. . Presupposition projection and the semantics of attitude verbs.
Journal of Semantics (). –. doi:./jos/...
Heim, Irene & Angelika Kratzer. . Semantics in generative grammar. Blackwell.
Hellan, Lars. . On semantic scope. In Ambiguities in intensional contexts,
Dordrecht: Reidel.
Herzberger, Hans. . Counterfactuals and consistency. The Journal of
Philosophy (). –. doi:./.
Higginbotham, James. . Conditionals and compositionality. Philosophical
Perspectives (). –. doi:./j.-...x.
Hintikka, Jaako. . Semantics for propositional attitudes. In J.W. Davis, D.J.
Hockney & W.K. Wilson (eds.), Philosophical logic, –. Dordrecht: Reidel.
Hockett, Charles F. . The origin of speech. Scientiﬁc American . –.
Hockett, Charles F. & Stuart A. Altmann. . A note on design features.
In Thomas A. Sebeok (ed.), Animal communication: Techniques of study and
results of research, –. Indiana University Press.
Horn, Laurence R. . On the semantic properties of the logical operators in
english: UCLA dissertation.
Hughes, G.E. & M.J. Cresswell. . An introduction to modal logic. London:
Methuen.
Hughes, G.E. & M.J. Cresswell. . A new introduction to modal logic. London:
Routledge.
Huitink, Janneke. . Modals, conditionals and compositionality: Radboud Universiteit Nijmegen dissertation. URL http://user.uni-frankfurt.de/~huitink/
Huitink-dissertation.pdf.
Huitink, Janneke. a. Domain restriction by conditional connectives.
Ms, Goethe-University Frankfurt. URL http://semanticsarchive.net/Archive/
zgMDMM/Huitink-domainrestriction.pdf.
Huitink, Janneke. b. Quantiﬁed conditionals and compositionality. Ms, to
appear in Language and Linguistics Compass. URL http://user.uni-frankfurt.
de/~huitink/compass-conditionals-ﬁnal.pdf.
Iatridou, Sabine. . On the contribution of conditional Then. Natural
Language Semantics (). –. doi:./BF.



B

van Inwagen, Peter. . Indexicality and actuality. Philosophical Review .
–.
Ioup, Georgette. . Speciﬁcity and the interpretation of quantiﬁers. Linguistics
and Philosophy (). –.
Kadmon, Nirit & Fred Landman. . Any. Linguistics and Philosophy ().
–. doi:./BF.
Kamp, Hans. . Formal properties of now. Theoria . –.
Knuuttila, Simo. . Medieval theories of modality. In Edward N. Zalta
(ed.), The stanford encyclopedia of philosophy, Center for the Study of Language
and Information. URL http://plato.stanford.edu/archives/fall/entries/
modality-medieval/.
Kratzer, Angelika. . What must and can must and can mean. Linguistics and
Philosophy (). –. doi:./BF.
Kratzer, Angelika. . Semantik der Rede: Kontexttheorie – Modalwörter –
Konditionalsätze. Königstein/Taunus: Scriptor.
Kratzer, Angelika. . The notional category of modality. In Hans-Jürgen
Eikmeyer & Hannes Rieser (eds.), Words, worlds, and contexts: New approaches
in word semantics (Research in Text Theory ), –. Berlin: de Gruyter.
Kratzer, Angelika. . Conditionals. Chicago Linguistics Society (). –.
Kratzer, Angelika. . Modality. In Arnim von Stechow & Dieter Wunderlich
(eds.), Semantics: An international handbook of contemporary research, –.
Berlin: de Gruyter.
Kratzer, Angelika. . Scope or pseudoscope? are there wide-scope indeﬁnites?
In Susan Rothstein (ed.), Events and grammar, –. Kluwer Dordrecht.
Kratzer, Angelika. . Decomposing attitude verbs. Handout from a talk
honoring Anita Mittwoch on her th birthday at the Hebrew University of Jerusalem July , . URL http://semanticsarchive.net/Archive/
DcwYJkM/attitude-verbs.pdf.
Kripke, Saul. . Naming and necessity. Oxford: Blackwell.
Landman, Fred. . The progressive. Natural Language Semantics (). –.
doi:./BF.
Larson, Richard. . The grammar of intensionality. In G. Preyer (ed.), On
logical form, Oxford University Press.


Leslie, Sarah-Jane. . If, unless, and quantiﬁcation. In Robert J. Stainton &
Christopher Viger (eds.), Compositionality, context and semantic values: Essays
in honour of Ernie Lepore, –. Springer. doi:./----_.
Lewis, Clarence Irving & Cooper Harold Langford. . Symbolic logic. New
York: Century.
Lewis, David. a. Anselm and actuality. Noûs . –. Reprinted (with a
postscript) in Lewis (Lewis : pp. –).
Lewis, David. b.
General semantics.
doi:./BF.

Synthese (-). –.

Lewis, David. . Counterfactuals. Oxford: Blackwell.
Lewis, David. . Adverbs of quantiﬁcation. In Edward Keenan (ed.), Formal
semantics of natural language, –. Cambridge University Press.
Lewis, David. . Truth in ﬁction. American Philosophical Quarterly (). –
. URL http://www.jstor.org/stable/. Reprinted with postscripts in
Lewis (), pp. –.
Lewis, David. . Ordering semantics and premise semantics for counterfactuals. Journal of Philosophical Logic (). –. doi:./BF.
Reprinted in David Lewis, Papers in Philosophical Logic, Cambridge: Cambridge University Press, , pp. -.
Lewis, David. .
Logic for equivocators.
Noûs (). –.
doi:./. Reprinted in Lewis (: pp. –).
Lewis, David. . Philosophical papers: Volume . Oxford: Oxford University
Press.
Lewis, David. . On the plurality of worlds. Oxford: Blackwell.
Lewis, David. . Papers in philosophical logic. Cambridge: Cambridge
University Press.
MacFarlane, John. . Logical constants. In Edward N. Zalta (ed.), The
stanford encyclopedia of philosophy, URL http://plato.stanford.edu/archives/
win/entries/logical-constants/.
MacFarlane, John. . Epistemic modals are assessment-sensitive. Ms,
University of California, Berkeley, forthcoming in an OUP volume on
epistemic modals, edited by Brian Weatherson and Andy Egan. URL
http://sophos.berkeley.edu/macfarlane/epistmod.pdf.



B

Matthewson, Lisa. . On the interpretation of wide-scope indeﬁnites. Natural
Language Semantics ().
May, Robert. . The grammar of quantiﬁcation: Massachusetts Institute of
Technology dissertation.
McCready, Eric & Norry Ogata. . Evidentiality, modality and probability.
Linguistics and Philosophy (). –. doi:./s---.
Montague, Richard. . The proper treatment of quantiﬁcation in ordinary English. In Jaako Hintikka, Julius Moravcsik & Patrick Suppes (eds.), Approaches to natural language, –. Dordrecht: Reidel.
URL http://www.blackwellpublishing.com/content/BPL_Images/Content_
store/Sample_chapter//Portner.pdf. Reprinted in Portner &
Partee (), pp. –.
Moulton, Keir. . Clausal complementation and the Wager-class. Proceedings
of the North East Linguistics Society . URL http://sites.google.com/site/
keirmoulton/Moultonnelswager.pdf.
Moulton, Keir. . Natural selection and the syntax of clausal complementation:
University of Massachusetts at Amherst dissertation. URL http://scholarworks.
umass.edu/open_access_dissertations//.
Musan, Renate. . On the temporal interpretation of noun phrases: Massachusetts Institute of Technology dissertation. Published in  in the Series
“Outstanding Dissertations in Linguistics” by Garland.
Musan, Renate. . Tense, predicates, and lifetime eﬀects. Natural Language
Semantics (). –. doi:./A:.
Nauze, Fabrice. . Modality in typological perspective: Universiteit van Amsterdam dissertation. URL http://www.illc.uva.nl/Publications/Dissertations/
DS--.text.pdf.
Nute, Donald. . Conditional logic. In Dov Gabbay & Franz Guenthner
(eds.), Handbook of philosophical logic. volume ii, –. Dordrecht: Reidel.
Ogihara, Toshiyuki. . The semantics of tense in embedded clauses. Linguistic
Inquiry (). –. URL http://www.jstor.org/stable/.
Ogihara, Toshiyuki. . Tense and aspect in truth-conditional semantics.
Lingua (). –. doi:./j.lingua....
Osvath, Mathias & Peter Gärdenfors. . Oldowan culture and the evolution
of anticipatory cognition. Tech. Rep.  Lund University Cognitive Studies
LUCS, Lund. URL http://www.lucs.lu.se/ftp/pub/LUCS_Studies/LUCS.
pdf.


Partee, Barbara H. . Some structural analogies between tenses and pronouns
in English. The Journal of Philosophy (). –. doi:./.
Partee, Barbara H. . Nominal and temporal anaphora. Linguistics and
Philosophy (). –. doi:./BF.
Partee, Barbara H. . Reﬂections of a formal semanticist as of Feb .
Ms. (longer version of introductory essay in  book). URL http://people.
umass.edu/partee/docs/BHP_Essay_Feb.pdf.
Partee, Barbara H. & Herman L.W. Hendriks. . Montague grammar. In
Johan van Benthem & Alice ter Meulen (eds.), Handbook of logic and language,
–. Amsterdam: Elsevier.
Peacocke, Christopher. . Necessity and truth theories. Journal of Philosophical
Logic . –.
Peano, Giuseppe. . Arithmetices principia: Nova methodo exposita. Torino:
Bocca.
Percus, Orin. . Constraints on some other variables in syntax. Natural
Language Semantics (). –.
Perry, John R. . Semantics, possible worlds. In E. Craig (ed.), Routledge encyclopedia of philosophy, London: Routledge. URL http://www.rep.
routledge.com/article/U. Preprint http://www-csli.stanford.edu/~john/
PHILPAPERS/posswld.pdf.
Pollock, John. . Subjunctive reasoning. Dordrecht: Reidel.
Portner, Paul. .
The semantics of mood, complementation,
and conversational force. Natural Language Semantics (). –.
doi:./A:.
Portner, Paul. . The progressive in modal semantics. Language ().
–. doi:./.
Portner, Paul. . Modality. Oxford University Press.
Portner, Paul & Barbara H. Partee (eds.). . Formal semantics: The essential
readings. Oxford: Blackwell.
Prior, A. N. . Escapism: The logical basis of ethics. In A. I. Melden (ed.),
Essays in moral philosophy, –. Seattle: University of Washington Press.
Prior, A. N. & Kit Fine. . Worlds, times, and selves. London: Duckworth.
Prior, Arthur. . Now. Noûs . –.



B

Reinhart, Tanya. . Quantiﬁer scope – how labor is divided between QR and
choice functions. Linguistics & Philosophy (). –.
Ross, Jeﬀ. . The semantics of media (Studies in Linguistics and Philosophy
(SLAP) ). Dordrecht: Kluwer.
Rothstein, Susan. . Structuring events: A study in the semantics of lexical aspect Explorations in Semantics. Blackwell. URL http://tinyurl.com/
rothstein-aktionsarten.
Russell, Bertrand. . An inquiry into meaning and truth. London: George
Allen and Unwin.
Saarinen, Esa. . Backwards-looking operators in tense logic and in natural
language. In Jaako Hintikka, I. Niiniluoto & Esa Saarinen (eds.), Essays on
mathematical and philosophical logic, Reidel Dordrecht.
Schlenker, Philippe. . Conditionals as deﬁnite descriptions (A referential analysis). Research on Language and Computation (). –.
doi:./s---.
Segerberg, K. . Two-dimensional modal logic. Journal of Philosophical Logic
. –.
Sextus Empiricus. c. . Outlines of pyrrhonism.
Sharvit, Yael. . How many questions and attitude verbs. University of
Pennsylvania.
Speas, Peggy. . On the syntax and semantics of evidentials. Language and
Linguistics Compass (). –. doi:./j.-X...x.
Spencer, Mary. . Why the “s” in “intension”?
doi:./mind/LXXX...

Mind (). –.

Stalnaker, Robert. . A theory of conditionals. In Nicholas Rescher (ed.),
Studies in logical theory (American Philosophical Quarterly Monograph Series ), –. Oxford: Blackwell.
Stalnaker, Robert. . Indicative conditionals. Philosophia (). –.
doi:./BF.
Stalnaker, Robert. a. Inquiry. MIT Press.
Stalnaker, Robert. b. Inquiry. MIT Press.
Stalnaker, Robert. . Context and content. Oxford: Oxford University Press.


Stanley, Jason & Zoltán Gendler Szabó. . On quantiﬁer domain restriction.
Mind and Language (/). –. doi:./-..
von Stechow, Arnim. . On the proper treatment of tense. Proceedings of
Semantics and Linguistic Theory . URL http://www.sfs.uni-tuebingen.de/
~arnim/Aufsaetze/SALT.pdf.
von Stechow, Arnim. . Tenses in compositional semantics. To be published
in Wolfgang Klein (ed) The Expression of Time in Language. URL http:
//www.sfs.uni-tuebingen.de/~arnim/Aufsaetze/Approaches.pdf.
Steiner, George. . After Babel: Aspects of language and translation. Oxford
University Press rd edn.
Stephenson, Tamina. a. Judge dependence, epistemic modals, and predicates
of personal taste. Linguistics and Philosophy (). –. doi:./s--.
Stephenson, Tamina. b. Towards a theory of subjective meaning: Massachusetts Institute of Technology dissertation. URL http://semanticsarchive.
net/Archive/QxMjkO/Stephenson--thesis.pdf.
Steup, Matthias. . The analysis of knowledge. In Edward N. Zalta (ed.), The
Stanford encyclopedia of philosophy, Fall  edn. URL http://plato.stanford.
edu/archives/fall/entries/knowledge-analysis/.
Suber, Peter. . Paradoxes of material implication. An electronic hand-out for
the course “Symbolic Logic”. URL http://www.earlham.edu/~peters/courses/
log/mat-imp.htm.
Suddendorf, Thomas. . Foresight and evolution of the human mind. Science
(). –. doi:./science..
Suddendorf, Thomas & Michael C. Corballis. . Mental time travel and the
evolution of the human mind. Genetic, Social, and General Psychology Monographs (). –. URL http://cogprints.org///MentalTimeTravel.
txt.
Swanson, Eric. . Modality in language. Philosophy Compass (). –.
doi:./j.-...x.
Tichy, Pavel. . An approach to intensional analysis. Noûs . –.
Varzi, Achille. . Inconsistency without contradiction. Notre Dame Journal of
Formal Logic (). –. doi:./ndjﬂ/.
Vlach, Frank. . Now and then: A formal study in the logic of tense anaphora:
UCLA dissertation.



B

Warmbrod, Ken. . A defense of the limit assumption. Philosophical Studies
(). –. doi:./BF.
Willett, Thomas. . A cross-linguistic survey of the grammaticalization of
evidentiality. Studies in Language (). –.
Williamson, Timothy. . Knowledge and its limits. Oxford: Oxford University
Press.
Wiltschko, Martina. . On the interpretability of tense on D and its consequences for case theory. Lingua (). –. URL http://dx.doi.org/.
/S-()-X.
Winter, Yoad. . Choice functions and the scopal semantics of indeﬁnites.
Linguistics and Philosophy . –.
Wurmbrand, Susi. . Modal verbs must be raising verbs. West Coast Conference
on Formal Linguistics . –. URL http://wurmbrand.uconn.edu/Susi/
Papers%and%handouts_ﬁles/WCCFL.pdf.
Zimmermann, Thomas Ede. . Intensional logic and two-sorted type theory.
Journal of Symbolic Logic . –.
Zucchi, Sandro. . Tense in ﬁction. In Carlo Cecchetto, Gennaro Chierchia
& Maria Teresa Guasti (eds.), Semantic interfaces: Reference, anaphora and
aspect, –. CSLI Publications. URL http://ﬁlosoﬁa.dipaﬁlo.unimi.it/
~bonomi/Zucchi%Tense.pdf.

I S

K  F

I H

MIT S  E

A note about the lecture notes:
The notes for this course have been evolving for years now, starting with some
old notes from the early s by Angelika Kratzer, Irene Heim, and myself,
which have since been modiﬁed and expanded every year by Irene or myself.
Because this version of the notes has not been seen by my co-author, I alone am
responsible for any defects.
– Kai von Fintel, Spring 
This is a work in progress. We may eventually publish these materials as a followup volume to Heim & Kratzer’s Semantics in Generative Grammar, Blackwell
. In the meantime, we encourage the use of these notes in courses at other
institutions. Of course, you need to give full credit to the authors and you may
not use the notes for any commercial purposes. If you use the notes, we would
like to be notiﬁed and we would very much appreciate any comments, criticism,
and advice on these materials. We have already proﬁted from feedback sent in
by several people who have used the notes and the more the better.
Direct your communication to:
Kai von Fintel
Department of Linguistics & Philosophy
Room ·
Massachusetts Institute of Technology
 Massachusetts Avenue
Cambridge,  -
U S  A
ﬁntel@mit.edu
http://kaivonﬁntel.org
Here is the homepage for the course that these notes are designed for:
http://stellar.mit.edu/S/course//sp/.

Advice about using these notes
. These notes presuppose familiarity with the material, concepts, and notation of the Heim & Kratzer textbook.
. There are numerous exercises throughout the notes. It is highly recommended to do all of them and it is certainly necessary to do so if you at all
anticipate doing semantics-related work in the future.
. At the moment, the notes are designed to go along with explanatory lectures.
You should ask questions and make comments as you work through the
notes.
. Students with semantic ambitions should also at an early point start reading
supplementary material (as for example listed at the end of each chapter of
these notes).
. Lastly, prospective semanticists may start thinking about how they would
teach this material.

— T     —

C


Beginnings 
. Displacement 
. An Intensional Semantics in  Easy Steps
. Comments and Complications 
. Supplemental Readings 



 Propositional Attitudes 
. Hintikka’s Idea 
. Accessibility Relations 
. Suplemental Readings 


Modality 
. The Quantiﬁcational Theory of Modality 
. Flavors of Modality 
. *Kratzer’s Conversational Backgrounds 
. Supplementary Readings 

 Conditionals 
. The Material Implication Analysis 
. The Strict Implication Analysis 
. If -Clauses as Restrictors 
Supplemental Readings 


Ordering 
. The Driveway 
. Kratzer’s Solution: Doubly Relative Modality 
. The Paradox of the Good Samaritan 
. Non-Monotonicity of Conditionals 
Supplemental Readings 

 Basics of Tense and Aspect 
. A First Proposal for Tense 
. Are Tenses Referential? 
. The Need for Intervals 
. Aktionsarten 
. The Progressive 

. Tense in Embedded Clauses 
Supplemental Readings 
 DPs and Scope in Modal Contexts 
. De re vs. De dicto as a Scope Ambiguity 
. Raised subjects 
 Beyond de re — de dicto : The Third Reading 
. A Problem: Additional Readings and Scope Paradoxes 
. The Standard Solution: Overt World Variables 
. Alternatives to Overt World Variables 
. Scope, Restrictors, and the Syntax of Movement 
. A Recurring Theme: Historical Overview 
Bibliography 

C O
B
Language is the main instrument
of man’s refusal to accept the world
as it is.
George Steiner, After Babel, p. 

We introduce the idea of extension vs. intension and its main use: taking
us from the actual here and now to past, future, possible, counterfactual situations. We develop a compositional framework for intensional
semantics.

.

Displacement

.

An Intensional Semantics in  Easy Steps



..
..
.

Laying the Foundations
Intensional Operators




Comments and Complications 
..

Why Talk about Other Worlds? 

..

.

Intensions All the Way? 

..
.



The Worlds of Sherlock Holmes 

Supplemental Readings 

Displacement

Hockett () in a famous article (and a follow-up, Hockett & Altmann ())
presented a list of     . This list continues to
play a role in current discussions of animal communication. One of the design

Hockett, Charles F. .
The origin of speech. Scientiﬁc American . –

B



C 

features is . Human language is not restricted to discourse about
the actual here and now.
How does natural language untie us from the actual here and now? One
degree of freedom is given by the ability to name entities and refer to them even
if they are not where we are when we speak:
()

Thomas is in Hamburg.

This kind of displacement is not something we will explore here. We’ll take it
for granted.
Consider a sentence with no names of absent entities in it:
()

*The terms  and  descend from the Latin
modus, “way”, and are ancient
terms pertaining to the way a
proposition holds, necessarily,
contingently, etc.

It is snowing (in Cambridge).

On its own, () makes a claim about what is happening right now here in
Cambridge. But there are devices at our disposal that can be added to (),
resulting in claims about snow in displaced situations. Displacement can occur in
the  dimension and/or in what might be called the * dimension.
Here’s an example of temporal displacement:
()

At noon yesterday, it was snowing in Cambridge.

This sentence makes a claim not about snow now but about snow at noon
yesterday, a diﬀerent time from now.
Here’s an example of modal displacement:
()

See Kratzer (, ) for
more examples of modal
constructions.

If the storm system hadn’t been deﬂected by the jet stream, it would
have been snowing in Cambridge.

This sentence makes a claim not about snow in the actual world but about snow
in the world as it would have been if the storm system hadn’t been deﬂected by
the jet stream, a world distinct from the actual one (where the system did not hit
us), a merely  .
Natural language abounds in modal constructions. () is a so-called  . Here are some other examples:
()

M A
It may be snowing in Cambridge.

()

M A
Possibly, it will snow in Cambridge tomorrow.

()

P A
Jens believes that it is snowing in Cambridge.

 Steiner (: ) writes: “Hypotheticals, ‘imaginaries’, conditionals, the syntax of counterfactuality and contingency may well be the generative centres of human speech”.

§.

A I S   E S

()

H
Jane smokes.

()

G
Bears like honey.



The plan for this course is as follows. In Part , we explore modality and
associated topics. In Part , we explore temporal matters.
In this chapter, we will put in place the basic framework of 
, the kind of semantics that models displacement of the point of
evaluation in temporal and modal dimensions. To do this, we will start with one
rather special example of modal displacement:
()

In the world of Sherlock Holmes, a detective lives at B Baker Street.

() doesn’t claim that a detective lives at B Baker Street in the actual world
(presumably a false claim), but that in the world as it is described in the Sherlock
Holmes stories of Sir Arthur Conan Doyle, a detective lives at B Baker Street
(a true claim, of course). We choose this example rather than one of the more runof-the-mill displacement constructions because we want to focus on conceptual
and technical matters before we do serious empirical work.
The questions we want to answer are: How does natural language achieve
this feat of modal displacement? How do we manage to make claims about other
possible worlds? And why would we want to?
The basic idea of the account we’ll develop is this:
• expressions are assigned their semantic values relative to a possible world;
• in particular, sentences have truth-values in possible worlds;
• in the absence of modal displacement, we evaluate sentences with respect
to the “actual” world, the world in which we are speaking;
• modal displacement changes the world of evaluation;
• displacement is eﬀected by special operators, whose semantics is our primary concern here.
A terminological note: we will call the sister of the intensional operator its
, a useful term introduced by our medieval colleagues.

. An Intensional Semantics in  Easy Steps
..

Laying the Foundations

S : P W. Our ﬁrst step is to introduce possible worlds. This is
not the place to discuss the metaphysics of possible worlds in any depth. Instead,
we will just start working with them and see what they can do for us. Basically, a
possible world is a way that things might have been. In the actual world, there

Check out http://
bakerstreet.org/.



B

C 

are two coﬀee mugs on my desk, but there could have been more or less. So,
there is a possible world — albeit a rather bizarre one — where there are  coﬀee
mugs on my desk. We join Heim & Kratzer in adducing this quote from Lewis
(: f.):
The world we live in is a very inclusive thing. Every stick and every
stone you have ever seen is part of it. And so are you and I. And
so are the planet Earth, the solar system, the entire Milky Way, the
remote galaxies we see through telescopes, and (if there are such
things) all the bits of empty space between the stars and galaxies.
There is nothing so far away from us as not to be part of our world.
Anything at any distance at all is to be included. Likewise the world
is inclusive in time. No long-gone ancient Romans, no long-gone
pterodactyls, no long-gone primordial clouds of plasma are too far
in the past, nor are the dead dark stars too far in the future, to be
part of the same world. . . .
The way things are, at its most inclusive, means the way the entire world is. But things might have been diﬀerent, in ever so many
ways. This book of mine might have been ﬁnished on schedule. Or,
had I not been such a commonsensical chap, I might be defending
not only a plurality of possible worlds, but also a plurality of impossible worlds, whereof you speak truly by contradicting yourself. Or I
might not have existed at all — neither myself, nor any counterparts
of me. Or there might never have been any people. Or the physical
constants might have had somewhat diﬀerent values, incompatible
with the emergence of life. Or there might have been altogether
diﬀerent laws of nature; and instead of electrons and quarks, there
might have been alien particles, without charge or mass or spin
but with alien physical properties that nothing in this world shares.
There are ever so many ways that a world might be: and one of these
many ways is the way that this world is.
Previously, our “metaphysical inventory” included a domain of entities and a
set of two truth-values and increasingly complex functions between entities,
truth-values, and functions thereof. Now, we will add possible worlds to the
inventory. Let’s assume we are given a set W , the set of all possible worlds, which
is a vast space since there are so many ways that things might have been diﬀerent
from the way they are. Each world has as among its parts entities like you and me
and these coﬀee mugs. Some of them may not exist in other possible worlds. So,
strictly speaking each possible worlds has its own, possibly distinctive, domain
of entities. What we will use in our system, however, will be the grand union of
all these world-speciﬁc domains of entities. We will use D to stand for the set of
all possible individuals.

§.

A I S   E S



Among the many possible worlds that there are — according to Lewis, there
is a veritable plenitude of them — is the world as it is described in the Sherlock
Holmes stories by Sir Arthur Conan Doyle. In that world, there is a famous
detective Sherlock Holmes, who lives at B Baker Street in London and has a
trusted sidekick named Dr. Watson. Our sentence In the world of Sherlock Holmes,
a detective lives at B Baker Street displaces the claim that a famous detective
lives at B Baker Street from the actual world to the world as described in the
Sherlock Holmes stories. In other words, the following holds:
()

The sentence In the world of Sherlock Holmes, a detective lives at B
Baker Street is true in a world w iﬀ the sentence a detective lives at B
Baker Street is true in the world as it is described in the Sherlock Holmes
stories.

What this suggests is that we need to make space in our system for having devices
that control in what world a claim is evaluated. This is what we will do now.
S : T E W P. Recall from H& K that we
were working with a semantic interpretation function that was relativized to
an assignment function g, which was needed to take care of pronouns, traces,
variables, etc. From now on, we will relativize the semantic values in our
system to possible worlds as well. What this means is that from now on, our
interpretation function will have two superscripts: a world w and an assignment
g: · w,g .
So, the prejacent embedded in () will have its truth-conditions described
as follows:
()

a famous detective lives at B Baker Street w,g = 
iﬀ a famous detective lives at B Baker Street in world w.

It is customary to refer to the world for which we are calculating the extension
of a given expression as the  . In the absence of any shifting
devices, we would normally evaluate a sentence in the actual world. But then
there are shifting devices such as our in the world of Sherlock Holmes. We will
soon see how they work. But ﬁrst some more pedestrian steps: adding lexical
entries and composition principles that are formulated relative to a possible
world. This will allow us to derive the truth-conditions as stated in () in a
compositional manner.
 We will see in Section .. that this is not quite right. It’ll do for now.
 Recall from H& K, pp.f, that what’s inside the interpretation brackets is a mention of an
object language expression. They make this clear by bold-facing all object language expressions
inside interpretation brackets. In these notes, we will follow common practice in the ﬁeld and
not use a special typographic distinction, but let it be understood that what is interpreted are
object language expressions.

B



C 

S : L E. Among our lexical items, we can distinguish between
items which have a - semantic value and those that are worldindependent. Predicates are typically world-dependent. Here are some sample
entries.
()

Note the ruthless condensation
of the notation in (c) and (d).

For any w ∈ W and any assignment function g:
a.
famous w,g = λx ∈ D. x is famous in w.,
b.
detective w,g = λx ∈ D. x is a detective in w.
c.
lives-at w,g = λx ∈ D. λy ∈ D. y lives-at x in w.

The set of detectives will obviously diﬀer from world to world, and so will the
set of famous individuals and the set of pairs where the ﬁrst element lives at the
second element.
Other items have semantic values which do not diﬀer from world to world.
The most important such items are certain “logical” expressions, such as truthfunctional connectives and determiners:
()

a.
b.
c.
d.

and w,g = λu ∈ Dt . λv ∈ Dt . u = v = .
the w,g = λf ∈ D e,t : ∃!x. f(x) = . the y such that f(y) = .
every w,g = λf e,t . λg e,t . ∀xe : f(x) =  → g(x) = .
a/some w,g = λf e,t . λg e,t . ∃xe : f(x) =  & g(x) = .

Note that there is no occurrence of w on the right-hand side of the entries in
(). That’s the tell-tale sign of the world-independence of the semantics of these
items.
We will also assume that proper names have world-independent semantic
values, that is, they refer to the same individual in any possible world.
()

a.
b.
c.

Noam Chomsky w,g = Noam Chomsky.
Sherlock Holmes w,g = Sherlock Holmes.
B Baker Street w,g = B Baker Street.

S : C P. The old rules of Functional Application,
Predicate Modiﬁcation, and λ-Abstraction can be retained almost intact. We
just need to modify them by adding world-superscripts to the interpretation
function. For example:
 Of course, “λx ∈ D. . . . ” is short for “λx : x ∈ D. . . . ”. Get used to semanticists condensing
their notation whenever convenient!
 Always make sure that you actually understand what the notation means. Here, for example,
we are saying that the semantic value of the word famous with respect to a given possible world
w and a variable assignment g is that function that is deﬁned for an argument x only if x is a
member of the domain of individuals and that, if it is deﬁned, yields the truth-value  if and
only if x is famous in w.

§.

A I S   E S

()

F A (FA)
If α is a branching node and {β, γ} the set of its daughters, then, for
any world w and assignment g: if β w,g is a function whose domain
contains γ w,g , then α w,g = β w,g ( γ w,g ).



The rule simply passes the world parameter down.
S : T. Lastly, we will want to connect our semantic system to the
notion of the    . We ﬁrst adopt the “Appropriateness
Condition” from Heim & Kratzer (p.):
()

A C
A context c is appropriate for an LF φ only if c determines a variable
assignment gc whose domain includes every index which has a free
occurrence in φ.

We then intensionalize Heim & Kratzer’s deﬁnition of truth and falsity of
utterances:
()

T  F C  U
An utterance of a sentence φ in a context c in a possible world w is true
iﬀ φ w,gc =  and false if φ w,gc = .

E .: Compute under what conditions an utterance in possible world
w (which may or may not be the one we are all living in) of the sentence a
famous detective lives at B Baker Street is true. [Since this is the ﬁrst exercise of
the semester, please do this in excrutiating detail, not skipping any steps.]

..

Intensional Operators

So far we have merely “redecorated” our old system inherited from last semester.
We have introduced possible worlds into our inventory, our lexical entries and
our old composition principles. But with the tools we have now, all we can do
so far is to keep track of the world in which we evaluate the semantic value of
an expression, complex or lexical. We will get real mileage once we introduce
  which are capable of shifting the world parameter.
We mentioned that there are a number of devices for modal displacement. As
advertised, for now, we will just focus on a very particular one: the expression
in the world of Sherlock Holmes. We will assume, as seems reasonable, that this
expression is a sentence-modiﬁer both syntactically and semantically.
S : A S E. We begin with a heuristic step. We want
to derive something like the following truth-conditions for our sentence:
()

in the world of Sherlock Holmes,
a famous detective lives at B Baker Street

w,g

=



B

C 

iﬀ the world w as it is described in the Sherlock Holmes stories is such
that there exists a famous detective in w who lives at B Baker Street
in w .
We would get this if in general we had this rule for in the world of Sherlock
Holmes:
()

The diamond ♦ symbol for
possibility is due to C.I. Lewis,
ﬁrst introduced in Lewis &
Langford (), but he made
no use of a symbol for the dual
combination ¬♦¬. The dual
symbol was later devised by
F.B. Fitch and ﬁrst appeared in
print in  in a paper by his
doctoral student Barcan ().
See footnote  of Hughes
& Cresswell (). Another
notation one ﬁnds is L for
necessity and M for possibility,
the latter from the German
möglich ‘possible’.

For any sentence φ, any world w, and any assignment g:
in the world of Sherlock Holmes φ w,g = 
iﬀ the world w as it is described in the Sherlock Holmes stories is such
that φ w ,g = .

This is a so-called  treatment of the meaning of this expression. Instead of giving an explicit semantic value to the expression, we specify
what eﬀect it has on the meaning of a complex expression that contains it. In (),
we do not compute the meaning for in the world of Sherlock Holmes, φ from the
combination of the meanings of its parts, since in the world of Sherlock Holmes
is not given a separate meaning, but in eﬀect triggers a special composition
principle. This format is very common in modal logic systems, which usually
give a syncategorematic semantics for the two modal operators (the necessity
operator and the possibility operator ♦). When one only has a few closed
class expressions to deal with that may shift the world parameter, employing
syncategorematic entries is a reasonable strategy. But we are facing a multitude
of displacement devices. So, we will need to make our system more modular.
So, we want to give in the world of Sherlock Holmes its own meaning and combine that meaning with that of its prejacent by a general composition principle.
The Fregean slogan we adopted says that all composition is function application
(modulo the need for λ-abstraction and the possible need for predicate modiﬁcation). So, what we will want to do is to make () be the result of functional
application. But we can immediately see that it cannot be the result of our usual
rule of functional application, since that would feed to in the world of Sherlock
Holmes the semantic value of a famous detective lives in B Baker Street in w,
which would be a particular truth-value,  if a famous detective lives at B
Baker Street in w and  if there doesn’t. And whatever the semantics of in the
world of Sherlock Holmes is, it is certainly not a truth-functional operator.
So, we need to feed something else to in the world of Sherlock Holmes. At the
same time, we want the operator to be able to shift the evaluation world of its
prejacent. Can we do this?
S : I. We will deﬁne a richer notion of semantic value, the
 of an expression. This will be a function from possible worlds to
 See Heim & Kratzer, Section ., pp. – for a reminder about the status of predicate
modiﬁcation.

§.

A I S   E S



the extension of the expression in that world. The intension of a sentence can
be applied to any world and give the truth-value of the sentence in that world.
Intensional operators take the intension of their prejacent as their argument, that
is we will feed the intension of the embedded sentence to the shifting operator.
The operator will use that intension and apply it to the world it wants the
evaluation to happen in. Voilà.
Now let’s spell that account out. Our system actually provides us with two
kinds of meanings. For any expression α, we have α w,g , the semantic value
of α in w, also known as the  of α in w. But we can also calculate
λw. α w,g , the function that assigns to any world w the extension of α in that
world. This is usually called the  of α. We will sometimes use an
abbreviatory notation for the intension of α:
()

α

g

¢

:= λw. α

w,g

.

It should be immediately obvious that since the deﬁnition of intension abstracts
over the evaluation world, intensions are not world-dependent.,
Note that strictly speaking, it now makes no sense anymore to speak of
“the semantic value” of an expression α. What we have is a semantic system
that allows us to calculate extensions (for a given possible world w) as well as
intensions for all (interpretable) expressions. We will see that when α occurs in a
particular bigger tree, it will always be determinate which of the two “semantic
values” of α is the one that enters into the compositional semantics. So, that
one — whichever one it is, the extension or the intension of α — might then be
called “the semantic value of α in the tree β”.
It should be noted that the terminology of  vs.  is
time-honored but that the possible worlds interpretation thereof is more recent.
The technical notion we are using is certainly less rich a notion of meaning than
tradition assumed.

 The notation with the subscripted cent-sign comes from Montague Grammar. See e.g. Dowty
et al. (: ).
 Since intensions are by deﬁnition not dependent on the choice of a particular world, it makes
no sense to put a world-superscript on the intension-brackets. So don’t ever write “ . . . w,g ”;
¢
we’ll treat that as undeﬁned nonsense.
 The deﬁnition here is simpliﬁed, in that it glosses over the fact that some expressions, in
particular those that contain  , may fail to have an extension in certain
worlds. In such a case, the intension has no extension to map such a world to. Therefore, the
intension will have to be a partial function. So, the oﬃcial, more “pedantic”, deﬁnition will have
to be as follows: α g := λw : α ∈ dom( w,g ). α w,g .
¢
 For example, Frege’s “modes of presentation” are not obviously captured by this possible
worlds implementation of extension/intension.

The Port-Royal logicians distinguished  from
. Leibniz
preferred the term 
rather than .
The notion probably goes
back even further. See Spencer
() for some notes on this.
The possible worlds interpretation is due to Carnap ().

B



C 

S : S T  S D. If we want to be able to
feed the intensions to lexical items like in the world of Sherlock Holmes, we need
to have the appropriate types in our system.
Recall that W is the set of all possible worlds. And recall that D is the set of
all   and thus contains all individuals existing in the actual
world plus all individuals existing in any of the merely possible worlds.
We now expand the set of semantic types, to add intensions. Intensions are
functions from possible worlds to all kinds of extensions. So, basically we want
to add for any kind of extension we have in our system, a corresponding kind of
intension, a function from possible worlds to that kind of extension.
We add a new clause, (c), to the deﬁnition of semantic types:
()

S T
a. e and t are semantic types.
b. If σ and τ are semantic types, then σ, τ is a semantic type.
c. If σ is a semantic type, then s, σ is a semantic type.
d. Nothing else is a semantic type.

We also add a fourth clause to the previous deﬁnition of semantic domains:
()

S D
a. De = D, the set of all possible individuals
b. Dt = {, }, the set of truth-vales
c. If σ and τ are semantic types, then D σ,τ is the set of all functions
from Dσ to Dτ .
d. I: If σ is a type, then D s,σ is the set of all functions
from W to Dσ .

Clause (d) is the addition to our previous system of types. The functions of the
schematic type s, . . . are intensions. Here are some examples of intensions:
• The intensions of sentences are of type s, t , functions from possible
worlds to truth values. These are usually called . Note that
if the function is total, then we can see the sentence as picking out a
set of possible worlds, those in which the sentence is true. More often
than not, however, propositions will be  functions from worlds
to truth-values, that is functions that fail to map certain possible worlds
into either truth-value. This will be the case when the sentence contains
a presupposition trigger, such as the. The famous sentence The King of
 Note a curious feature of this set-up: there is no type s and no associated domain. This
corresponds to the assumption that there are no expressions of English that take as their extension
a possible world, that is, there are no pronouns or names referring to possible worlds. We will
actually question this assumption in a later chapter. For now, we will stay with this more
conventional set-up.

§.

A I S   E S



France is bald has an intension that (at least in the analysis sketched in
Heim & Kratzer) is undeﬁned for any world where there fails to be a
unique King of France.
• The intensions of one-place predicates are of type s, e, t , functions
from worlds to set of individuals. These are usually called .
• The intensions of expressions of type e are of type s, e , functions from
worlds to individuals. These are usually called  .
S : A L E   S. We are ready to formulate the semantic
entry for in the world of Sherlock Holmes:
()

in the world of Sherlock Holmes w,g =
λp s,t . the world w as it is described in the Sherlock Holmes stories
is such that p(w ) = .

That is, in the world of Sherlock Holmes expects as its argument a function of type
s, t , a proposition. It yields the truth-value  iﬀ the proposition is true in the
world as it is described in the Sherlock Holmes stories.
All that’s left to do now is to provide in the world of Sherlock Holmes with a
proposition as its argument. This is the job of a new composition principle.
S : I F A. We add the new rule of
Intensional Functional Application.
()

I F A (IFA)
If α is a branching node and {β, γ} the set of its daughters, then, for
any world w and assignment g: if β w,g is a function whose domain
contains γ g , then α w,g = β w,g ( γ g ).
¢
¢

This is the crucial move. It makes space for expressions that want to take the
intension of their sister as their argument and do stuﬀ to it. Now, everything is
in place. Given (), the semantic argument of in the world of Sherlock Holmes
will not be a truth-value but a proposition. And thus, in the world of Sherlock
Holmes will be able to check the truth-value of its prejacent in various possible
worlds. To see in practice that we have all we need, please do the following
exercise.
E .: Calculate the conditions under which an utterance in a given
possible world w of the sentence in the world of the Sherlock Holmes stories, a
famous detective lives at B Baker Street is true.
 This is not yet the ﬁnal semantics, see Section . for complications. Also, note again the
condensed notation: “λp s,t . . . . ” stands for the fully oﬃcial “λp : p ∈ D s,t . . . . ”.

B



C 

. Comments and Complications
..

Intensions All the Way?

We have seen that to adequately deal with expressions like in the world of
Sherlock Holmes, we need an intensional semantics, one that gives us access to the
extensions of expressions across the multitude of possible worlds. At the same
time, we have kept the semantics for items like and, every, and a unchanged and
extensional. This is not the only way one can set up an intensional semantics.
The following exercise demonstrates this.
E .: Consider the following “intensional” meaning for and:
()

and

w,g

= λp

s,t

. λq s,t . p(w) = q(w) = .

With this semantics, and would operate on the intensions of the two conjoined
sentences. In any possible world w, the complex sentence will be true iﬀ the
component propositions are both true of that world.
Compute the truth-conditions of the sentence In the world of Sherlock Holmes,
Holmes is quick and Watson is slow both with the extensional meaning for and
given earlier and the intensional meaning given here. Is there any diﬀerence in
the results?
There are then at least two ways one could develop an intensional system.
(i) We could “generalize to the worst case” and make the semantics deliver
intensions as the semantic value of an expression. Such systems are common
in the literature (see Cresswell ; Lewis b).
(ii) We could maintain much of the extensional semantics we have developed
so far and extend it conservatively so as to account for non-extensional
contexts.
We have chosen to pursue (ii) over (i), because it allows us to keep the semantics
of extensional expressions simpler. The philosophy we follow is that we will only
move to the intensional sub-machinery when triggered by an expression that
creates a non-extensional context. As the exercise just showed, this is more a
matter of taste than a deep scientiﬁc decision.

..

Why Talk about Other Worlds?

Why would natural language bother having such elaborate mechanisms to talk
about other possible worlds? While having devices for spatial and temporal
displacement (talking about Hamburg or what happened yesterday) seems eminently reasonable, talking about worlds other than the actual world seems only
suitable for poets and the like. So, why?
The solution to this puzzle lies in a fact that our current semantics of the
shifter in the world of Sherlock Holmes does not yet accurately capture: modal

§.

C  C



sentences have empirical content, they make  claims, claims that
are true or false depending on the circumstances in the actual world.
Our example sentence In the world of Sherlock Holmes, a famous detective
lives at  Baker Street is true in this world but it could easily have been false.
There is no reason why Sir Arthur Conan Doyle could not have decided to locate
Holmes’ abode on Abbey Road.
To see that our semantics does not yet capture this fact, notice that in the
semantics we gave for in the world of Sherlock Holmes:
()

in the world of Sherlock Holmes w,g =
λp s,t . the world w as it is described in the Sherlock Holmes stories
is such that p(w ) = .

there is no occurrence of w on the right hand side. This means that the truthconditions for sentences with this shifter are world-independent. In other words,
they are predicted to make non-contingent claims that are either true no-matterwhat or false no-matter-what. This needs to be ﬁxed.
The ﬁx is obvious: what matters to the truth of our sentence is the content
of the Sherlock Holmes stories as they are in the evaluation world. So, we need
the following semantics for our shifter:
()

in the world of Sherlock Holmes w,g =
λp s,t . the world w as it is described in the Sherlock Holmes stories
in w is such that p(w ) = .

We see now that sentences with this shifter do make a claim about the evaluation
world: namely, that the Sherlock Holmes stories as they are in the evaluation
world describe a world in which such-and-such is true. So, what is happening
is that although it appears at ﬁrst as if modal statements concern other possible
worlds and thus couldn’t really be very informative, they actually only talk about
certain possible worlds, those that stand in some relation to what is going on at
the ground level in the actual world. As a crude analogy, consider:
()

My grandmother is sick.

At one level this is a claim about my grandmother. But it is also a claim about me:
namely that I have a grandmother who is sick. Thus it is with modal statements.
They talk about possible worlds that stand in a certain relation to the actual
world and thus they make claims about the actual world, albeit slightly indirectly.

.. The Worlds of Sherlock Holmes
So far, we have played along with colloquial usage in talking of the world of
Sherlock Holmes. But it is important to realize that this is sloppy talk. Lewis
() writes:



B

C 

[I]t will not do to follow ordinary language to the extent of supposing
that we can somehow single out a single one of the worlds [as the
one described by the stories]. Is the world of Sherlock Holmes a
world where Holmes has an even or an odd number of hairs on
his head at the moment when he ﬁrst meets Watson? What is
Inspector Lestrade’s blood type? It is absurd to suppose that these
questions about the world of Sherlock Holmes have answers. The
best explanation of that is that the worlds of Sherlock Holmes are
plural, and the questions have diﬀerent answers at diﬀerent ones.
The usual move at this point is to talk about the set of worlds “ 
the (content of ) Sherlock Holmes stories in w”. We imagine that we ask of each
possible world whether what is going on in it is compatible with the stories as
they were written in our world. Worlds where Holmes lives on Abbey Road are
not compatible. Some worlds where he lives at B Baker Street are compatible
(again not all, because in some such worlds he is not a famous detective but
an obscure violinist). Among the worlds compatible with the stories are ones
where he has an even number of hairs on his head at the moment when he ﬁrst
meets Watson and there are others where he has an odd number of hairs at that
moment.
What the operator in the world of Sherlock Holmes expresses is that its complement is true throughout the worlds compatible with the stories. In other words,
the operator universally quantiﬁes over the compatible worlds. Our next iteration
of the semantics for the operator is therefore this:
()

in the world of Sherlock Holmes w,g =
λp s,t . ∀w compatible with the Sherlock Holmes stories in w :
p(w ) = .

At a very abstract level, the way we parse sentences of the form in the world of
Sherlock Holmes, φ is that both components, the in-phrase and the prejacent,
determine sets of possible worlds and that the set of possible worlds representing
the content of the ﬁction mentioned in the in-phrase is a subset of the set
of possible worlds determined by the prejacent. We will see the same rough
structure of relating sets of possible worlds in other intensional constructions.
This is where we will leave things. There is more to be said about ﬁction
operators like in the world of Sherlock Holmes, but we will just refer to you to the
relevant literature. In particular, one might want to make sense of Lewis’ idea
that a special treatment is needed for cases where the sentence makes a claim
about things that are left open by the ﬁction (no truth-value, perhaps?). One
also needs to ﬁgure out how to deal with cases where the ﬁction is internally
inconsistent. In any case, for our purposes we’re done with this kind of operator.

§.

S R

.

Supplemental Readings



There is considerable overlap between this chapter and Chapter  of Heim &
Kratzer’s textbook:
Heim, Irene & Angelika Kratzer. . Semantics in generative grammar. Blackwell.
Here, we approach intensional semantics from a diﬀerent angle. It would
probably be beneﬁcial if you read H& K’s Chapter  in addition to this chapter
and if you did the exercises in there.
Come to think of it, some other ancillary reading is also recommended. You may
want to look at relevant chapters in other textbooks:
Dowty, David, Robert Wall & Stanley Peters. . Introduction to Montague
semantics. Kluwer. [Chapters & ].
Gamut, L. T. F. . Logic, language, and meaning. Chicago University Press.
[Volume II: Intensional Logic and Logical Grammar].
Chierchia, Gennaro & Sally McConnell-Ginet. . Meaning and grammar: An
introduction to semantics (nd edition). MIT Press. [Chapter : Intensionality].
An encyclopedia article by Perry on possible worlds semantics:
Perry, John R. . Semantics, possible worlds. In E. Craig (ed.), Routledge encyclopedia of philosophy, London: Routledge. URL http://www.rep.
routledge.com/article/U. Preprint http://www-csli.stanford.edu/~john/
PHILPAPERS/posswld.pdf.
A couple of inﬂuential philosophical works on the metaphysics and uses of
possible worlds:
Kripke, Saul. . Naming and necessity. Oxford: Blackwell.
Lewis, David. . On the plurality of worlds. Oxford: Blackwell.
An interesting paper on the origins of the modern possible worlds semantics for
modal logic:
Copeland, B. Jack. . The genesis of possible worlds semantics. Journal of
Philosophical Logic (). –. doi:./A:.
A personal history of formal semantics:
Partee, Barbara H. . Reﬂections of a formal semanticist as of Feb .
Ms. (longer version of introductory essay in  book). URL http://people.
umass.edu/partee/docs/BHP_Essay_Feb.pdf.



B

C 

A must read for students who plan to go on to becoming specialists in semantics,
together with a handbook article putting it in perspective:
Montague, Richard. . The proper treatment of quantiﬁcation in ordinary
English. In Jaako Hintikka, Julius Moravcsik & Patrick Suppes (eds.), Approaches to natural language, –. Dordrecht: Reidel. URL http://www.
blackwellpublishing.com/content/BPL_Images/Content_store/Sample_chapter/
/Portner.pdf. Reprinted in Portner & Partee (), pp. –.
Partee, Barbara H. & Herman L.W. Hendriks. . Montague grammar. In
Johan van Benthem & Alice ter Meulen (eds.), Handbook of logic and language,
–. Amsterdam: Elsevier.
To learn more about discourse about ﬁction, read Lewis:
Lewis, David. . Truth in ﬁction. American Philosophical Quarterly (). –
. URL http://www.jstor.org/stable/. Reprinted with postscripts in
Lewis (), pp. –.
Recent reconsiderations:
Bonomi, Andrea & Sandro Zucchi. . A pragmatic framework for truth
in ﬁction. Dialectica (). –. doi:./j.-..tb.x.
Preprint http://ﬁlosoﬁa.dipaﬁlo.unimi.it/~bonomi/Pragmatic.pdf.
Hanley, Richard. . As good as it gets: Lewis on truth in ﬁction. Australasian
Journal of Philosophy (). –. doi:./.
A while back, there was an entry on Kai’s blog with comments from readers
about indeterminacies in ﬁction:
http://kaivonﬁntel.org/q-the-quantiﬁcational-force-of-ﬁction-operators.
Inconsistencies in ﬁctions and elsewhere are discussed in:
Varzi, Achille. . Inconsistency without contradiction. Notre Dame Journal of
Formal Logic (). –. doi:./ndjﬂ/.
Lewis, David. . Logic for equivocators. Noûs (). –. doi:./.
Reprinted in Lewis (: pp. –).
Some other interesting work on stories and pictures and their content:
Ross, Jeﬀ. . The semantics of media (Studies in Linguistics and Philosophy
(SLAP) ). Dordrecht: Kluwer.
Zucchi, Sandro. . Tense in ﬁction. In Carlo Cecchetto, Gennaro Chierchia
& Maria Teresa Guasti (eds.), Semantic interfaces: Reference, anaphora and
aspect, –. CSLI Publications. URL http://ﬁlosoﬁa.dipaﬁlo.unimi.it/
~bonomi/Zucchi.

§.

S R



Blumson, Ben. . Pictures, perspective and possibility. Philosophical Studies
doi:./s---.
If you’re interested in whether displacement really is an exclusive feature of
human language and cognition, you might want to check out this fairly recent
literature:
Suddendorf, Thomas & Michael C. Corballis. . Mental time travel and the
evolution of the human mind. Genetic, Social, and General Psychology Monographs (). –. URL http://cogprints.org///MentalTimeTravel.
txt.
Suddendorf, Thomas. . Foresight and evolution of the human mind. Science
(). –. doi:./science..
Osvath, Mathias & Peter Gärdenfors. . Oldowan culture and the evolution
of anticipatory cognition. Tech. Rep.  Lund University Cognitive Studies
LUCS, Lund. URL http://www.lucs.lu.se/ftp/pub/LUCS_Studies/LUCS.
pdf.
Astonishingly, Lewis’ doctrine of the reality of plurality of possible worlds is
being taken up here and there by researchers trying to understand quantum
mechanics via the so-called “many worlds” interpretation. See for a start, Kai’s
blog entry on a popular book on the issue, http://kaivonﬁntel.org/many-worlds,
and also MIT physics professor Max Tegmark’s FAQ on the topic, http://space.
mit.edu/home/tegmark/multiverse.html.

— T     —

C T
P A
With the basic framework in place, we now proceed to analyze a number
of intensional constructions. We start with the basic possible worlds
semantics for propositional attitude ascriptions. We talk brieﬂy about
the formal properties of accessibility relations.

.

Hintikka’s Idea 

.

Accessibility Relations 
..
..

.

*Transitivity 

..
.

Reﬂexivity 
*Symmetry 

Suplemental Readings 

Hintikka’s Idea

Expressions like believe, know, doubt, expect, regret, and so on are usually said
to describe  , expressing relations between individuals
(the attitude holder) and propositions (intensions of sentences).
The simple idea is that George believes that Henry is a spy claims that George
believes of the proposition that Henry is a spy that it is true. Note that for the
attitude ascription to be true it does not have to hold that Henry is actually a
spy. But where — in which world(s) — does Henry have to be a spy for it be true
that George believes that Henry is a spy? We might want to be inspired by the
colloquial phrase “in the world according to George” and say that George believes
that Henry is a spy is true iﬀ in the world according to George’s beliefs, Henry is
a spy. We immediately recall from the previous chapter that we need to ﬁx this
idea up by making space for multiple worlds compatible with George’s beliefs
and by tying the truth-conditions to contingent facts about the evaluation world.
That is, what George believes is diﬀerent in diﬀerent possible worlds.
The following lexical entry thus oﬀers itself:

According to Hintikka
(), the term   goes
back to Russell ().
Of course, the possible worlds
semantics for propositional
attitudes was in place long
before the extension to ﬁction contexts was proposed.
Our discussion here has inverted the historical sequence
for pedagogical purposes.

P A


()
It is important to realize the
modesty of this semantics: we
are not trying to ﬁgure out
what belief systems are and
particularly not what their
internal workings are like.
That is the job of psychologists
(and philosophers of mind,
perhaps). For our semantics,
we treat the belief system as a
black box that determines for
each possible world whether it
considers it possible that it is
the world it is located in.

Jaakko Hintikka

believe

C 

w,g

=
λp s,t . λx. ∀w compatible with x s beliefs in w : p(w ) = .

What is going on in this semantics? We conceive of George’s beliefs as a state of
his mind about whose internal structure we will remain agnostic, a matter left
to other cognitive scientists. What we require of it is that it embody opinions
about what the world he is located in looks like. In other words, if his beliefs
are confronted with a particular possible world w , they will determine whether
that world may or may not be the world as they think it is. What we are asking
of George’s mental state is whether any state of aﬀairs, any event, anything in
w is in contradiction with anything that George believes. If not, then w is
compatible with George’s beliefs. For all George believes, w may well be the
world where he lives. Many worlds will pass this criterion, just consider as one
factor that George is unlikely to have any precise opinions about the number of
leaves on the tree in front of my house. George’s belief system determines a set
of worlds compatible with his beliefs: those worlds that are viable candidates for
being the actual world, as far as his belief system is concerned.
Now, George believes a proposition iﬀ that proposition is true in all of the
worlds compatible with his beliefs. If there is just one world compatible with his
beliefs where the proposition is not true, that means that he considers it possible
that the proposition is not true. In such a case, we can’t say that he believes the
proposition. Here is the same story in the words of Hintikka (), the source
for this semantics for propositional attitudes:
My basic assumption (slightly simpliﬁed) is that an attribution of any
propositional attitude to the person in question involves a division
of all the possible worlds (. . . ) into two classes: into those possible
worlds which are in accordance with the attitude in question and
into those which are incompatible with it. The meaning of the
division in the case of such attitudes as knowledge, belief, memory,
perception, hope, wish, striving, desire, etc. is clear enough. For
instance, if what we are speaking of are (say) a’s memories, then
these possible worlds are all the possible worlds compatible with
everything he remembers. [. . . ]
How are these informal observations to be incorporated into
a more explicit semantical theory? According to what I have said,
understanding attributions of the propositional attitude in question
(. . . ) means being able to make a distinction between two kinds
of possible worlds, according to whether they are compatible with
the relevant attitudes of the person in question. The semantical
counterpart to this is of course a function which to a given individual
person assigns a set of possible worlds.
However, a minor complication is in order here. Of course,

§.

H’ I



the person in question may himself have diﬀerent attitudes in the
diﬀerent worlds we are considering. Hence this function in eﬀect
becomes a relation which to a given individual and to a given possible
world µ associates a number of possible worlds which we shall call
the  to µ. The relation will be called the alternativeness
relation. (For diﬀerent propositional attitudes, we have to consider
diﬀerent alternativeness relations.)
E .: Let’s adopt Hintikka’s idea that we can use a function that maps x
and w into the set of worlds w compatible with what x believes in w. Call this
function B. That is,
()

B = λx. λw. {w : w is compatible with what x believes in w}.

Using this notation, our lexical entry for believe would look as follows:
()

believe

w,g

= λp

s,t

. λx. B(x)(w) ⊆ p.

We are here indulging in the usual sloppiness in treating p both as a function
from worlds to truth-values and as the set characterized by that function.
Here now are two “alternatives” for the semantics of believe:
()

A  ( )
believe w,g = λp ∈ D s,t . λx ∈ D. p = B(x)(w) .

()

A  (  )
believe w,g = λp ∈ D s,t . λx ∈ D. p ∩ B(x)(w) = ∅ .

Explain why these do not adequately capture the meaning of believe.
E .: Follow-up: The semantics in () would have made believe into
an existential quantiﬁer of sorts: it would say that some of the worlds compatible
with what the subject believes are such-and-such. You have argued (successfully,
of course) that such an analysis is wrong for believe. But are there attitude
predicates with such an “existential” meaning? Discuss some candidates. If you
can’t ﬁnd any candidates that survive scrutiny, can you speculate why there might
be no existential attitude predicates? [Warning: this is unexplored territory!]
We can also think of belief states as being represented by a function BS, which
maps an individual and a world into a set of propositions: those that the
individual believes. From there, we could calculate the set of worlds compatible
with an individual x’s beliefs in world w by retrieving the set of those possible
worlds in which all of the propositions in BS(x)(w) are true: {w : ∀p ∈
BS(x)(w) : p(w ) = }, which in set talk is simply the big intersection of all the
propositions in the set: ∩BS(x)(w). Our lexical entry then would be:

BS is meant to stand for
‘belief state’, not for what
you might have thought!

P A


()

believe

w,g

= λp

s,t

C 

. λx. ∩ BS(x)(w) ⊆ p.

E .: Imagine that our individual x forms a new opinion. Imagine
that we model this by adding a new proposition p to the pool of opinions. So,
BS(x)(w) now contains one further element. There are now more opinions.
What happens to the set of worlds compatible with x’s beliefs? Does it get bigger
or smaller? Is the new set a subset or superset of the previous set of compatible
worlds?

. Accessibility Relations
Another way of reformulating Hintikka’s semantics for propositional attitudes
is via the notion of an  . We talk of a world w being
accessible from w. Each attitude can be associated with such an accessibility
B
relation. For example, we can introduce the relation wRa w which holds iﬀ w
is compatible with a’s belief state in w. We have then yet another equivalent way
of specifying the lexical entry for believe:
()
Kirill Shklovsky (in class) asked
why we call reﬂexivity, transitivity, and symmetry “formal”
properties of relations. The
idea is that certain properties
are “formal” or “logical”, while
others are more substantial. So,
the fact that the relation “have
the same birthday as” is symmetric seems a more formal
fact about it than the fact that
the relation holds between my
daughter and my brother-inlaw. Nevertheless, one of the
most common ways of characterizing formal/logical notions
(permutation-invariance, if
you’re curious) does not in
fact make symmetry etc. a
formal/logical notion. So,
while intuitively these do seem
to be formal/logical properties, we do not know how to
substantiate that intuition.
See MacFarlane () for
discussion.
We talk here about knowledge
entailing (or even presupposing) truth but we do not
mean to say that knowledge
simply equals true belief. Professors Socrates and Gettier
and their exegetes have further
considerations.

believe

w,g

= λp

s,t

. λx. ∀w : wRB w → p(w ) = .
x

It is proﬁtable to think of diﬀerent attitudes (belief, knowledge, hope, regret,
memory, . . . ) as corresponding to diﬀerent accessibility relations. Recall now that
the linguistic study of determiners beneﬁtted quite a bit from an investigation of
the formal properties of the relations between sets of individuals that determiners
express. We can do the same thing here and ask about the formal properties
of the accessibility relation associated with belief versus the one associated with
knowledge, etc. The obvious properties to think about are reﬂexivity, transitivity,
and symmetry.

..

Reﬂexivity

A relation is reﬂexive iﬀ for any object in the domain of the relation we know that
the relation holds between that object and itself. Which accessibility relations
are reﬂexive? Take knowledge:
()

wRK w iﬀ w is compatible with what x knows in w.
x

We are asking whether for any given possible world w, we know that RK holds
x
between w and w itself. It will hold if w is a world that is compatible with what
we know in w. And clearly that must be so. Take our body of knowledge in w.
The concept of knowledge crucially contains the concept of truth: what we know
must be true. So if in w we know that something is the case then it must be the
case in w. So, w must be compatible with all we know in w. RK is reﬂexive.
x

§.

A R



Now, if an attitude X corresponds to a reﬂexive accessibility relation, then we
can conclude from a Xs that p being true in w that p is true in w. This property
of an attitude predicate is often called . It is to be distinguished
from , which is a property of attitudes which presuppose – rather than
(merely) entail – the truth of their complement.
If we consider the relation RB pairing with a world w those worlds w which
x
are compatible with what x believes in w, we no longer have reﬂexivity: belief is
not a veridical attitude. It is easy to have false beliefs, which means that the actual
world is not in fact compatible with one’s beliefs, which contradicts reﬂexivity.
And many other attitudes as well do not involve veridicality/reﬂexivity: what we
hope may not come true, what we remember may not be what actually happened,
etc.
In modal logic, the correspondence between formal properties of the accessibility relation and the validity of inference patterns is well-studied. What we
have just seen is that reﬂexivity of the accessibility relation corresponds to the
validity of p → p. Other properties correspond to other characteristic patterns.
Let’s see this for transitivity and symmetry.

..

In modal logic notation:
p → p. This pattern
is sometimes called T or
M, as is the corresponding system of modal logic.

The diﬀerence between believe
and know in natural discourse
is quite delicate, especially
when one considers ﬁrst person
uses (I believe the earth is ﬂat
vs. I know the earth is ﬂat).

*Transitivity

Transitivity of the accessibility relation corresponds to the inference p →
p.
The pattern seems not obviously wrong for knowledge: if one knows that p,
doesn’t one thereby know that one knows that p? But before we comment
on that, let’s establish the formal correspondence between transitivity and that
inference pattern. This needs to go in both directions.
not p
p

w3

w2

w1

Figure .: Transitivity
What does it take for the pattern to be valid? Assume that p holds for an
arbitrary world w, i.e. that p is true in all worlds w accessible from w. Now,
the inference is to the fact that p again holds in any world w accessible from
any of those worlds w accessible from w. But what would prevent p from being
false in some w accessible from some w accessible from w? That could only
be prevented from happening if we knew that w itself is accessible from w as

Starred sections are optional.
In the literature on epistemic
modal logic, the pattern is
known as the KK T or
P I.
In general modal logic, it is
the characteristic axiom 
of the modal logic system
S, which is a system that
adds  to the previous axiom
M/T. Thus, S is the logic of
accessibility relations that are
both reﬂexive and transitive.

P A



C 

well, because then we would know from the premiss that p is true in it (since p
is true in all worlds accessible from w). Ah, but w (some world accessible from
a world w accessible from w) is only guaranteed to be accessible from w if the
accessibility relation is transitive (if w is accessible from w and w is accessible
from w , then transitivity ensures that w is accessible from w). This reasoning
has shown that validity of the pattern requires transitivity. The other half of
proving the correspondence is to show that transitivity entails that the pattern is
valid.
The proof proceeds by reductio. Assume that the accessibility relation is
transitive. Assume that (i) p holds for some world w but that (ii)
p doesn’t
hold in w. We will show that this situation cannot obtain. By (i), p is true in all
worlds w accessible from w. By (ii), there is some non-p world w accessible
from some world w accessible from w. But by transitivity of the accessibility
relation, that non-p world w must be accessible from w. And since all worlds
accessible from w are p worlds, w must be a p world, in contradiction to (ii).
So, as soon as we assume transitivity, there is no way for the inference not to go
through.
Now, do any of the attitudes have the transitivity property? It seems rather
obvious that as soon as you believe something, you thereby believe that you
believe it (and so it seems that belief involves a transitive accessibility relation).
And in fact, as soon as you believe something, you believe that you know it. But
one might shy away from saying that knowing something automatically amounts
to knowing that you know it. For example, many are attracted to the idea that to
know something requires that (i) that it is true, (ii) that you believe it, and (iii)
that you are justiﬁed in believing it: the justiﬁed true belief analysis of knowledge.
So, now couldn’t it be that you know something, and thus (?) that you believe
you know it, and thus that you believe that you are justiﬁed in believing it,
but that you are not justiﬁed in believing that you are justiﬁed in believing it?
After all, one’s source of knowledge, one’s reliable means of acquiring knowledge,
might be a mechanism that one has no insight into. So, while one can implicitly
trust (believe) in its reliability, and while it is in fact reliable, one might not have
any means to have trustworthy beliefs about it. [Further worries about the KK
Thesis are discussed by Williamson ().]

..

*Symmetry

What would the consequences be if the accessibility relation were symmetric?
Symmetry of the accessibility relation R corresponds to the validity of the
following principle:

§.

A R

()

Brouwer’s Axiom:



∀p∀w : w ∈ p → ∀w wRw → ∃w [w Rw & w ∈ p]

not p
w2

w3

In modal logic notation:
♦p, known simply as
B in modal logic. The system
that combines T/M with B is
often called Brouwer’s System
(B), after the mathematician
L.E.J. Brouwer, not because
he proposed it but because it
was thought that it had some
connections to his doctrines.

p→

p
w1

Figure .: Symmetry
Here’s the reasoning: Assume that R is in fact symmetric. Pick a world w in
which p is true. Now, could it be that the right hand side of the inference fails
to hold in w? Assume that it does fail. Then, there must be some world w
accessible from w in which ♦p is false. In other words, from that world w
there is no accessible world w in which p is true. But since R is assumed to be
symmetric, one of the worlds accessible from w is w and in w, p is true, which
contradicts the assumption that the inference doesn’t go through. So, symmetry
ensures the validity of the inference.
The other way (validity of the inference requires symmetry): the inference
says that from any p world we only have worlds accessible from which there is
at least one accessible p world. But imagine that p is true in w but not true in
any other world. So, the only way for the conclusion of the inference to hold
automatically is to have a guarantee that w (the only p world) is accessible from
any world accessible from it. That is, we need to have symmetry. QED.
To see whether a particular kind of attitude is based on a symmetric accessibility relation, we can ask whether Brouwer’s Axiom is intuitively valid with respect
to this attitude. If it is not valid, this shows that the accessibility relation can’t
be symmetric. In the case of a knowledge-based accessibility relation (epistemic
accessibility), one can argue that symmetry does not hold:
The symmetry condition would imply that if something happens
to be true in the actual world, then you know that it is compatible
with your knowledge (Brouwer’s Axiom). This will be violated by
any case in which your beliefs are consistent, but mistaken. Suppose
that while p is in fact true, you feel certain that it is false, and so
think that you know that it is false. Since you think you know this,
it is compatible with your knowledge that you know it. (Since we
 Thanks to Bob Stalnaker (pc to Kai von Fintel) for help with the following reasoning.

L.E.J. Brouwer



All one really needs to make
NI valid is to have a E accessibility relation:
any two worlds accessible from
the same world are accessible
from each other. It is a nice
little exercise to prove this, if
you have become interested in
this sort of thing. Note that
all reﬂexive and Euclidean
accessibility relations are
transitive and symmetric as
well — another nice little thing
to prove.

P A

C 

are assuming you are consistent, you can’t both believe that you
know it, and know that you do not). So it is compatible with your
knowledge that you know that not p. Equivalently : you don’t know
that you don’t know that not p. Equivalently: you don’t know that
it’s compatible with your knowledge that p. But by Brouwer’s Axiom,
since p is true, you would have to know that it’s compatible with
your knowledge that p. So if Brouwer’s Axiom held, there would
be a contradiction. So Brouwer’s Axiom doesn’t hold here, which
shows that epistemic accessibility is not symmetric.
Game theorists and theoretical computer scientists who traﬃc in logics of knowledge often assume that the accessibility relation for knowledge is an equivalence
relation (reﬂexive, symmetric, and transitive). But this is appropriate only if
one abstracts away from any error, in eﬀect assuming that belief and knowledge
coincide. One striking consequence of working with an equivalence relation
as the accessibility relation for knowledge is that one predicts the principle of
N I to hold:
()

N I ()
If one doesn’t know that p, then one knows that one doesn’t know that
p. (¬ p → ¬ p).

This surely seems rather dubious: imagine that one strongly believes that p but
that nevertheless p is false, then one doesn’t know that p, but one doesn’t seem
to believe that one doesn’t know that p, in fact one believes that one does know
that p.

. Suplemental Readings
We will come back to propositional attitudes and especially the scope of noun
phrases with respect to them, including the infamous  -  distinction.
Further connections between mathematical properties of accessibility relations
and logical properties of various notions of necessity and possibility are studied
extensively in modal logic:
Hughes, G.E. & M.J. Cresswell. . A new introduction to modal logic. London:
Routledge.
Garson, James. . Modal logic. In Edward N. Zalta (ed.), The Stanford
encyclopedia of philosophy, URL http://plato.stanford.edu/entries/logic-modal/,
 This and the following step rely on the duality of necessity and possibility: q is compatible
with your knowledge iﬀ you don’t know that not q.

§.

S R



especially section  and , “Modal Axioms and Conditions on Frames”, “Map
of the Relationships between Modal Logics”.
A thorough discussion of the possible worlds theory of attitudes, and some of its
potential shortcomings, can be found in Bob Stalnaker’s work:
Stalnaker, Robert. a. Inquiry. MIT Press.
Stalnaker, Robert. . Context and content. Oxford: Oxford University Press.
A quick and informative surveys about the notion of knowledge:
Steup, Matthias. . The analysis of knowledge. In Edward N. Zalta (ed.), The
Stanford encyclopedia of philosophy, Fall  edn. URL http://plato.stanford.
edu/archives/fall/entries/knowledge-analysis/.
Linguistic work on attitudes has often been concerned with various co-occurrence
patterns, particularly which moods (indicative or subjunctive or inﬁnitive) occur
in the complement and whether negative polarity items are licensed in the
complement.
Mood licensing:
Portner, Paul. . The semantics of mood, complementation, and conversational force. Natural Language Semantics (). –. doi:./A:.
NPI-Licensing:
Kadmon, Nirit & Fred Landman. . Any. Linguistics and Philosophy ().
–. doi:./BF.
von Fintel, Kai. . NPI licensing, Strawson entailment, and context dependency. Journal of Semantics (). –. doi:./jos/...
Giannakidou, Anastasia. . Aﬀective dependencies. Linguistics and Philosophy
(). –. doi:./A:.
There is some interesting work out of Amherst rethinking the way attitude
predicates take their complements:
Kratzer, Angelika. . Decomposing attitude verbs. Handout from a talk
honoring Anita Mittwoch on her th birthday at the Hebrew University of Jerusalem July , . URL http://semanticsarchive.net/Archive/
DcwYJkM/attitude-verbs.pdf.
Moulton, Keir. . Clausal complementation and the Wager-class. Proceedings of the North East Linguistics Society . URL http://sites.google.com/
site/keirmoulton/Moultonnelswager.pdf. http://people.umass.edu/keir/
Wager.pdf.
Moulton, Keir. . Natural selection and the syntax of clausal complementation:



P A

C 

University of Massachusetts at Amherst dissertation. URL http://scholarworks.
umass.edu/open_access_dissertations//.
Tamina Stephenson in her MIT dissertation and related work explores the way
attitude predicates interact with epistemic modals and taste predicates in their
complements:
Stephenson, Tamina. a. Judge dependence, epistemic modals, and predicates
of personal taste. Linguistics and Philosophy (). –. doi:./s--.
Stephenson, Tamina. b. Towards a theory of subjective meaning: Massachusetts Institute of Technology dissertation. URL http://semanticsarchive.
net/Archive/QxMjkO/Stephenson--thesis.pdf.
Jon Gajewski in his MIT dissertation and subsequent work explores the distribution of the - property among attitude predicates and traces it back to
presuppositional components of the meaning of the predicates:
Gajewski, Jon. . Neg-raising: Polarity and presupposition: Massachusetts
Institute of Technology dissertation. doi:./.
Gajewski, Jon. . Neg-raising and polarity. Linguistics and Philosophy
doi:./s---z.
Interesting work has also been done on presupposition projection in attitude
contexts:
Asher, Nicholas. . A typology for attitude verbs and their anaphoric properties. Linguistics and Philosophy (). –. doi:./BF.
Heim, Irene. . Presupposition projection and the semantics of attitude verbs.
Journal of Semantics (). –. doi:./jos/...
Geurts, Bart. . Presuppositions and anaphors in attitude contexts. Linguistics
& Philosophy (). –. doi:./A:.

C T
M
We turn to modal auxiliaries and related constructions. The main
diﬀerence from attitude constructions is that their semantics is more
context-dependent. Otherwise, we are still quantifying over possible
worlds.

.

The Quantiﬁcational Theory of Modality 
..
..

.

Syntactic Assumptions 
Quantiﬁcation over Possible Worlds 

Flavors of Modality 
..

Contingency 

..

Epistemic vs. Circumstantial Modality 

..

Contingency Again 

..

Iteration 

..

A technical variant of the analysis 

.
.

.

*Kratzer’s Conversational Backgrounds 
Supplementary Readings 

The Quantiﬁcational Theory of Modality

We will now be looking at modal auxiliaries like may, must, can, have to, etc.
Most of what we say here should carry over straightforwardly to modal adverbs
like maybe, possibly, certainly, etc. We will make certain syntactic assumptions,
which make our work easier but which leave aside many questions that at some
point deserve to be addressed.

M



..

The issue of raising vs. control
will be taken up later. If you
are eager to get started on it
and other questions of the
morphosyntax of modals, read
the handout from an LSA class
Sabine and Kai taught last
summer: http://web.mit.edu/
ﬁntel/lsa-class--handout.
pdf.

We will talk about reconstruction in more detail later.

C 

Syntactic Assumptions

We will assume, at least for the time being, that a modal like may is a 
predicate (rather than a  predicate), i.e., its subject is not its own argument, but has been moved from the subject-position of its inﬁnitival complement.
So, we are dealing with the following kind of structure:
()

a.
b.

Ann may be smart.
[ Ann [ λ [ may [ t be smart ]]]]

Actually, we will be working here with the even simpler structure below, in
which the subject has been reconstructed to its lowest trace position. (E.g.,
these could be generated by deleting all but the lowest copy in the movement
chain.) We will be able to prove that movement of a name or pronoun never
aﬀects truth-conditions, so at any rate the interpretation of the structure in (b)
would be the same as of (). As a matter of convenience, then, we will take
the reconstructed structures, which allow us to abstract away from the (here
irrelevant) mechanics of variable binding.
()

may [ Ann be smart ]

So, for now at least, we are assuming that modals are expressions that take a full
sentence as their semantic argument. Now then, what do modals mean?

..
This idea goes back a long
time. It was famously held
by Leibniz, but there are
precedents in the medieval
literature, see Knuuttila ().
See Copeland () for the
modern history of the possible
worlds analysis of modal
expressions.

Quantiﬁcation over Possible Worlds

The basic idea of the possible worlds semantics for modal expressions is that they
are quantiﬁers over possible worlds. Toy lexical entries for must and may, for
example, would look like this:
()

must

()

may

w,g
w,g

= λp

= λp

s,t

s,t

. ∀w : p(w ) = .
. ∃w : p(w ) = .

This analysis is too crude (in particular, notice that it would make modal sentences non-contingent — there is no occurrence of the evaluation world on the
right hand side!). But it does already have some desirable consequences that we
will seek to preserve through all subsequent reﬁnements. It correctly predicts a
number of intuitive judgments about the logical relations between must and may
and among various combinations of these items and negations. To start with
some elementary facts, we feel that must φ entails may φ, but not vice versa:
 We will assume that even though Ann be smart is a non-ﬁnite sentence, this will not have any
eﬀect on its semantic type, which is that of a sentence, which in turn means that its semantic
value is a truth-value. This is hopefully independent of the (interesting) fact that Ann be smart
on its own cannot be used to make a truth-evaluable assertion.

§.

T Q T  M

()

You must stay.
Therefore, you may stay.



You may stay.
Therefore, you must stay.



()
()

a.
b.



You may stay, but it is not the case that you must stay.
You may stay, but you don’t have to stay.


We judge must φ incompatible with its “inner negation” must [not φ ], but ﬁnd
may φ and may [not φ ] entirely compatible:
()

You must stay, and/but also, you must leave. (leave = not stay).


()

You may stay, but also, you may leave.


We also judge that in each pair below, the (a)-sentence and the (b)-sentences say
the same thing.
()

a.
b.

You must stay.
It is not the case that you may leave.
You aren’t allowed to leave.
(You may not leave.)
(You can’t leave.)

()

a.
b.

You may stay.
It is not the case that you must leave.
You don’t have to leave.
You don’t need to leave.
(You needn’t leave.)

Given that stay and leave are each other’s negations (i.e. leave w,g = not stay w,g ,
and stay w,g = not leave w,g ), the LF-structures of these equivalent pairs of
 The somewhat stilted it is not the case-construction is used in to make certain that negation
takes scope over must. When modal auxiliaries and negation are together in the auxiliary complex
of the same clause, their relative scope seems not to be transparently encoded in the surface
order; speciﬁcally, the scope order is not reliably negation modal. (Think about examples
with mustn’t, can’t, shouldn’t, may not etc. What’s going on here? This is an interesting topic
which we must set aside for now. See the references at the end of the chapter for relevant work.)
With modal main verbs (such as have to), this complication doesn’t arise; they are consistently
inside the scope of clause-mate auxiliary negation. Therefore we can use (b) to (unambiguously)
express the same scope order as (a), without having to resort to a biclausal structure.
 The parenthesized variants of the (b)-sentences are pertinent here only to the extent that we
can be certain that negation scopes over the modal. In these examples, apparently it does, but as
we remarked above, this cannot be taken for granted in all structures of this form.

M



C 

sentences can be seen to instantiate the following schemata:
()

must φ ≡ not [may [not φ]]
must [not ψ] ≡ not [may ψ]

()

More linguistic data regarding
the “parallel logic” of modals
and quantiﬁers can be found
in Larry Horn’s dissertation
(Horn ).

a.
b.
a.
b.

may φ ≡ not [must [not φ]]
may [not ψ] ≡ not [must ψ]

Our present analysis of must, have-to, . . . as universal quantiﬁers and of may, can,
. . . as existential quantiﬁers straightforwardly predicts all of the above judgments,
as you can easily prove.
()

a.
b.

∀xφ ≡ ¬∃¬φ
∀x¬φ ≡ ¬∃xφ

()

a.
b.

∃xφ ≡ ¬∀x¬φ
∃x¬φ ≡ ¬∀xφ

. Flavors of Modality
..

Contingency

We already said that the semantics we started with is too simple-minded. In
particular, we have no dependency on the evaluation world, which would make
modal statements non-contingent. This is not correct.
If one says It may be snowing in Cambridge, that may well be part of useful,
practical advice about what to wear on your upcoming trip to Cambridge. It may
be true or it may be false. The sentence seems true if said in the dead of winter
when we have already heard about a Nor’Easter that is sweeping across New
England. The sentence seems false if said by a clueless Australian acquaintance
of ours in July.
The contingency of modal claims is not captured by our current semantics.
All the may-sentence would claim under that semantics is that there is some
possible world where it is snowing in Cambridge. And surely, once you have
read Lewis’ quote in Chapter , where he asserts the existence of possible worlds
with diﬀerent physical constants than we enjoy here, you must admit that there
have to be such worlds even if it is July. The problem is that in our semantics,
repeated here
()

may

w,g

= λp

s,t

. ∃w : p(w ) = .

there is no occurrence of w on the right hand side. This means that the truthconditions for may-sentences are world-independent. In other words, they make
 In logicians’ jargon, must and may behave as  of each other. For deﬁnitions of “dual”,
see Barwise & Cooper (: ) or Gamut (: vol.,).

§.

F  M



non-contingent claims that are either true whatever or false whatever, and because
of the plenitude of possible worlds they are more likely to be true than false.
This needs to be ﬁxed. But how?
Well, what makes it may be snowing in Cambridge seem true when we know
about a Nor’Easter over New England? What makes it seem false when we
know that it is summer in New England? The idea is that we only consider
possible worlds       . And since
what evidence is available to us diﬀers from world to world, so will the truth of a
may-statement.
()

may

()

must

w,g

= λp. ∃w compatible with the evidence in w : p(w ) = .

w,g

= λp. ∀w compatible with the evidence in w : p(w ) = .

Let us consider a diﬀerent example:
()

You have to be quiet.

Imagine this sentence being said based on the house rules of the particular
dormitory you live in. Again, this is a sentence that could be true or could
be false. Why do we feel that this is a contingent assertion? Well, the house
rules can be diﬀerent from one world to the next, and so we might be unsure
or mistaken about what they are. In one possible world, they say that all noise
must stop at pm, in another world they say that all noise must stop at pm.
Suppose we know that it is : now, and that the dorm we are in has either
one or the other of these two rules, but we have forgotten which. Then, for all
we know, you have to be quiet may be true or it may be false. This suggests a
lexical entry along these lines:
()

have-to

w,g

= λp. ∀w compatible with the rules in w : p(w ) = .

Again, we are tying the modal statement about other worlds down to certain
worlds that stand in a certain relation to actual world: those worlds where the
rules as they are here are obeyed.
A note of caution: it is very important to realize that the worlds compatible
with the rules as they are in w are those worlds where nothing happens that
violates any of the w-rules. This is not at all the same as saying that the worlds
compatible with the rules in w are those worlds where the same rules are in
force. Usually, the rules do not care what the rules are, unless the rules contain
some kind of meta-statement to the eﬀect that the rules have to be the way they
are, i.e. that the rules cannot be changed. So, in fact, a world w in which
nothing happens that violates the rules as they are in w but where the rules are
quite diﬀerent and in fact what happens violates the rules as they are in w is
 From now on, we will leave oﬀ type-speciﬁcations such as that p has to be of type s, t ,
whenever it is obvious what they should be and when saving space is aesthetically called for.

M



C 

nevertheless a world compatible with the rules in w. For example, imagine that
the only relevant rule in w is that students go to bed before midnight. Take a
world w where a particular student goes to bed at : pm but where the rules
are diﬀerent and say that students have to go to bed before  pm. Such a world
w is compatible with the rules in w (but of course not with the rules in w ).
Apparently, there are diﬀerent ﬂavors of modality, varying in what kind of
facts in the evaluation world they are sensitive to. The semantics we gave for
must and may above makes them talk about evidence, while the semantics we
gave for have-to made it talk about rules. But that was just because the examples
were hand-picked. In fact, in the dorm scenario we could just as well have said
You must be quiet. And, vice versa, there is nothing wrong with using it has to
be snowing in Cambridge based on the evidence we have. In fact, many modal
expressions seem to be multiply ambiguous.
Traditional descriptions of modals often distinguish a number of “readings”:
, , , , , . . . . (Beyond “epistemic” and “deontic,” there is a great deal of terminological variety. Sometimes
all non-epistemic readings are grouped together under the term  .)
Here are some initial illustrations.
()

E M
A: Where is John?
B: I don’t know. He may be at home.

()

D M
A: Am I allowed to stay over at Janet’s house?
B: No, but you may bring her here for dinner.

()

C/D M
A: I will plant the rhododendron here.
B: That’s not a good idea. It can grow very tall.

How are may and can interpreted in each of these examples? What do the
interpretations have in common, and where do they diﬀer?
In all three examples, the modal makes an existentially quantiﬁed claim about
possible worlds. This is usually called the   of the claim. What
diﬀers is what worlds are quantiﬁed over. In  modal sentences, we
quantify over worlds compatible with the available evidence. In  modal
sentences, we quantify over worlds compatible with the rules and/or regulations.
And in the  modal sentence, we quantify over the set of worlds
which conform to the laws of nature (in particular, plant biology). What speaker
B in () is saying, then, is that there are some worlds conforming to the laws of
nature in which this rhododendron grows very tall. (Or is this another instance
of an epistemic reading? See below for discussion of the distinction between

§.

F  M



circumstantial readings and epistemic ones.)
How can we account for this variety of readings? One way would be to
write a host of lexical entries, basically treating this as a kind of (more or less
principled) ambiguity. Another way, which is preferred by many people, is to
treat this as a case of context-dependency, as argued in seminal work by Kratzer
(, , , ).
According to Kratzer, what a modal brings with it intrinsically is just a
modal force, that is, whether it is an existential (possibility) modal or a universal
(necessity) modal. What worlds it quantiﬁes over is determined by context. In
essence, the context has to supply a restriction to the quantiﬁer. How can we
implement this idea?
We encountered context-dependency before when we talked about pronouns
and their referential (and E-Type) readings (H& K, chapters –). We treated
referential pronouns as free variables, appealing to a general principle that free
variables in an LF need to be supplied with values from the utterance context. If
we want to describe the context-dependency of modals in a technically analogous
fashion, we can think of their LF-representations as incorporating or subcategorizing for a kind of invisible pronoun, a free variable that stands for a set of
possible worlds. So we posit LF-structures like this:
()

Angelika Kratzer
It is well-known that natural
language quantiﬁcation is in
general subject to contextual restriction. See Stanley & Szabó
() for a recent discussion.

[ I [ I must p s,t ] [ VP you quiet]]

s,t here is a variable over (characteristic functions of ) sets of worlds, which — like
all free variables — needs to receive a value from the utterance context. Possible
values include: the set of worlds compatible with the speaker’s current knowledge; the set of worlds in which everyone obeys all the house rules of a certain
dormitory; and many others. The denotation of the modal itself now has to be
of type st, st, t rather than st, t , thus it will be more like a quantiﬁcational
determiner rather than a complete generalized quantiﬁer. Only after the modal
has been combined with its covert restrictor do we obtain a value of type st, t .

p

()

a.
b.

= have-to w,g = need-to w,g = . . . =
λp ∈ D s,t . λq ∈ D s,t . ∀w ∈ W [p(w) =  → q(w) = ]
may w,g = can w,g = be-allowed-to w,g = . . . =
λp ∈ D s,t . λq ∈ D s,t . ∃w ∈ W [p(w) =  & q(w) = ]
must

w,g

On this approach, the epistemic, deontic, etc. “readings” of individual occurrences of modal verbs come about by a combination of two separate things.
The lexical semantics of the modal itself encodes just a quantiﬁcational force,
a relation between sets of worlds. This is either the subset-relation (universal
quantiﬁcation; necessity) or the relation of non-disjointness (existential quantiﬁcation; possibility). The covert variable next to the modal picks up a contextually
salient set of worlds, and this functions as the quantiﬁer’s restrictor. The labels
“epistemic”, “deontic”, “circumstantial” etc. group together certain conceptually

in set talk: p ⊆ q
in set talk: p ∩ q = ∅

M



C 

natural classes of possible values for this covert restrictor.
Notice that, strictly speaking, there is not just one deontic reading (for
example), but many. A speaker who utters
()

You have to be quiet.

might mean: ‘I want you to be quiet,’ (i.e., you are quiet in all those worlds
that conform to my preferences). Or she might mean: ‘unless you are quiet, you
won’t succeed in what you are trying to do,’ (i.e., you are quiet in all those worlds
in which you succeed at your current task). Or she might mean: ‘the house rules
of this dormitory here demand that you be quiet,’ (i.e., you are quiet in all those
worlds in which the house rules aren’t violated). And so on. So the label “deontic”
appears to cover a whole open-ended set of imaginable “readings”, and which
one is intended and understood on a particular utterance occasion may depend
on all sorts of things in the interlocutors’ previous conversation and tacit shared
assumptions. (And the same goes for the other traditional labels.)

..

Epistemic vs. Circumstantial Modality

Is it all context-dependency? Or do ﬂavors of modality correspond to some sorts
of signals in the structure of sentences? Read the following famous passage from
Kratzer and think about how the two sentences with their very diﬀerent modal
meanings diﬀer in structure:
Quoted from Kratzer (). In
Kratzer (), the hydrangeas
were Zwetschgenbäume ‘plum
trees’. The German word
Zwetschge, by the way, is
etymologically derived from
the name of the city Damascus
(Syria), the center of the
ancient plum trade.

Consider sentences () and ():
()
()

Hydrangeas can grow here.
There might be hydrangeas growing here.

The two sentences diﬀer in meaning in a way which is illustrated by
the following scenario.
“Hydrangeas”
Suppose I acquire a piece of land in a far away country and
discover that soil and climate are very much like at home, where
hydrangeas prosper everywhere. Since hydrangeas are my favorite
plants, I wonder whether they would grow in this place and inquire
about it. The answer is (). In such a situation, the proposition
expressed by () is true. It is true regardless of whether it is or
isn’t likely that there are already hydrangeas in the country we are
considering. All that matters is climate, soil, the special properties
of hydrangeas, and the like. Suppose now that the country we are in
has never had any contacts whatsoever with Asia or America, and
the vegetation is altogether diﬀerent from ours. Given this evidence,
my utterance of () would express a false proposition. What counts

§.

F  M



here is the complete evidence available. And this evidence is not
compatible with the existence of hydrangeas.
() together with our scenario illustrates the pure  reading of the modal can. [. . . ]. () together with our scenario
illustrates the epistemic reading of modals. [. . . ] circumstantial
and epistemic conversational backgrounds involve diﬀerent kinds
of facts. In using an epistemic modal, we are interested in what else
may or must be the case in our world given all the evidence available.
Using a circumstantial modal, we are interested in the necessities
implied by or the possibilities opened up by certain sorts of facts.
Epistemic modality is the modality of curious people like historians,
detectives, and futurologists. Circumstantial modality is the modality of rational agents like gardeners, architects, and engineers. A
historian asks what might have been the case, given all the available
facts. An engineer asks what can be done given certain relevant facts.
Consider also the very diﬀerent prominent meanings of the following two
sentences, taken from Kratzer as well:
()

a.
b.

Cathy can make a pound of cheese out of this can of milk.
Cathy might make a pound of cheese out of this can of milk.

E .: Come up with examples of epistemic, deontic, and circumstantial
uses of the necessity verb have to. Describe the set of worlds that constitutes the
understood restrictor in each of your examples.

.. Contingency Again
We messed up. If you inspect the context-dependent meanings we have on the
table now for our modals, you will see that the right hand sides again do not
mention the evaluation world w. Therefore, we will again have the problem of
not making contingent claims, indirectly about the actual world. This needs to
be ﬁxed. We need a semantics that is both context-dependent and contingent.
The problem, it turns out, is with the idea that the utterance context supplies
a determinate set of worlds as the restrictor. When I understand that you meant
your use of must, in you must be quiet, to quantify over the set of worlds in which
the house rules of our dorm are obeyed, this does not imply that you and I have
to know or agree on which set exactly this is. That depends on what the house
rules in our world actually happen to say, and this may be an open question at
the current stage of our conversation. What we do agree on, if I have understood
your use of must in the way that you intended it, is just that it quantiﬁes over
whatever set of worlds it may be that the house rules pick out.

M


You will of course recognize
that functions of type s, st
are simply a schönﬁnkeled
version of the 
 we introduced in
the previous chapter.

C 

The technical implementation of this insight requires that we think of the
context’s contribution not as a set of worlds, but rather as a function which for
each world it applies to picks out such a set. For example, it may be the function
which, for any world w, yields the set {w : the house rules that are in force in w
are obeyed in w }. If we apply this function to a world w , in which the house
rules read “no noise after  pm”, it will yield a set of worlds in which nobody
makes noise after  pm. If we apply the same function to a world w , in which
the house rules read “no noise after  pm”, it will yield a set of worlds in which
nobody makes noise after  pm.
Suppose, then, that the covert restrictor of a modal predicate denotes such a
function, i.e., its value is of type s, st .
()

[ I’ [ I must R s,st ] [ VP you quiet]]

And the new lexical entries for must and may that will ﬁt this new structure are
these:
()

a.

in set talk: (R(w) ⊆ q

b.
in set talk: (R(w) ∩ q = ∅

= have-to w,g = need-to w,g = . . . =
λR ∈ D s,st . λq ∈ D s,t . ∀w ∈ W [R(w)(w ) =  → q(w ) = ]
may w,g = can w,g = be-allowed-to w,g = . . . =
λR ∈ D s,st . λq ∈ D s,t . ∃w ∈ W [R(w)(w ) =  & q(w ) = ]
must

w,g

Let us see now how this solves the contingency problem.
()

Let w be a world, and assume that the context supplies an assignment g
such that g(R) = λw. λw . the house rules in force in w are obeyed in
w

must R you quiet w,g =
(IFA)
w,g
w
must R
(λw you quiet ) =
(FA)
w,g
w,g
w
must
(R
) (λw you quiet ) =
(lex. entries you, quiet)
must w,g ( R w,g ) (λw . you are quiet in w ) =
(lex. entry must)
w,g
(w)(w ) =  → you are quiet in w =
∀w ∈ W : R
(pronoun rule)
∀w ∈ W : g(R)(w)(w ) =  → you are quiet in w =
(def. of g)
∀w ∈ W [the house rules in force in w are obeyed in w
→ you are quiet in w ]
As we see in the last line of (), the truth-value of () depends on the evaluation
world w.
E .: Describe two worlds w and w so that
must R you quiet w ,g =  and must R you quiet w ,g = .
E .: In analogy to the deontic relation g(R) deﬁned in (), deﬁne an
appropriate relation that yields an epistemic reading for a sentence like You may
be quiet.

§.

F  M

..

Iteration



Consider the following example:
()

You might have to leave.

What does this mean? Under one natural interpretation, we learn that the speaker
considers it possible that the addressee is under the obligation to leave. This
seems to involve one modal embedded under a higher modal. It appears that
this sentence should be true in a world w iﬀ some world w compatible with
what the speaker knows in w is such that every world w in which the rules as
they are in w are followed is such that you leave in w .
Assume the following LF:
()

[ I [ might R ] [ VP [ have-to R ] [ IP you leave]]]

Suppose w is the world for which we calculate the truth-value of the whole
sentence, and the context maps R to the function which maps w to the set of all
those worlds compatible with what is known in w. might says that some of those
worlds are worlds w that make the tree below might true. Now assume further
that the context maps R to the function which assigns to any such world w the
set of all those worlds in which the rules as they are in w are followed. have to
says that all of those worlds are worlds w in which you leave.
In other words, while it is not known to be the case that you have to leave,
for all the speaker knows it might be the case.
E .: Describe values for the covert s, st -variable that are intuitively
suitable for the interpretation of the modals in the following sentences:
()

As far as John’s preferences are concerned, you may stay with us.

()

According to the guidelines of the graduate school, every PhD candidate
must take  credit hours outside his/her department.

()

John can run a mile in  minutes.

()

This has to be the White House.

()

This elevator can carry up to  pounds.

For some of the sentences, diﬀerent interpretations are conceivable depending
on the circumstances in which they are uttered. You may therefore have to
sketch the utterance context you have in mind before describing the accessibility
relation.
E .: Collect two naturally occurring examples of modalized sentences
(e.g., sentences that you overhear in conversation, or read in a newspaper or novel
– not ones that are being used as examples in a linguistics or philosophy paper!),
and give deﬁnitions of values for the covert s, st -variable which account for the

There is more to be said about
which modals can embed
under which other modals.
See for some discussion the
handout mentioned earlier:
http://web.mit.edu/ﬁntel/
lsa-class--handout.pdf.

M



C 

way in which you actually understood these sentences when you encountered
them. (If the appropriate interpretation is not salient for the sentence out of
context, include information about the relevant preceding text or non-linguistic
background.)

..

A technical variant of the analysis

In our account of the contingency of modalized sentences, we adopted lexical entries for the modals that gave them world-dependent extensions of type
s, st , st, t :
()

(repeated from earlier):
For any w ∈ W : must w,g
λR ∈ D s,st . λq ∈ D s,t . ∀w ∈ W [R(w)(w ) =  → q(w ) = ]
(in set talk: λR s,st . λq s,t . (R(w) ⊆ q)).

Unfortunately, this treatment somewhat obscures the parallel between the modals
and the quantiﬁcational determiners, which have world-independent extensions
of type et, et, t .
Let’s explore an alternative solution to the contingency problem, which will
allow us to stick with the world-independent type- st, st, t -extensions that
we assumed for the modals at ﬁrst:
()

(repeated from even earlier):
must w,g = λp ∈ D s,t . λq ∈ D s,t . ∀w ∈ W [p(w) =  → q(w) = ]
(in set talk: λp ∈ D s,t . λq ∈ D s,t . p ⊆ q).

We posit the following LF-representation:
()

[ I [ I must [ R , s,st w*]] [ VP you quiet]]

What is new here is that the covert restrictor is complex. The ﬁrst part, R , s,st ,
is (as before) a free variable of type s, st , which gets assigned an accessibility
relation by the context of utterance. The second part is a special terminal symbol
which is interpreted as picking out the evaluation world:
()

For any w ∈ W : w∗

w,g

= w.

When R , s,st and w* combine (by Functional Application), we obtain a
constituent whose extension is of type s, t (a proposition or set of worlds). This
is the same type as the extension of the free variable p in the previous proposal,
hence suitable to combine with the old entry for must (by FA). However, while
the extension of p was completely ﬁxed by the variable assignment, and did not
 Dowty () introduced an analogous symbol to pick out the evaluation time. We have
chosen the star-notation to allude to this precedent.

§.

*K’ C B



vary with the evaluation world, the new complex constituent’s extension depends
on both the assignment and the world:
()

For any w ∈ W and any assignment g:
R , s,st (w*) w,g = g( , s, st )(w).

As a consequence of this, the extensions of the higher nodes I and I will also
vary with the evaluation world, and this is how we capture the fact that () is
contingent.
Maybe this variant is more appealing. But for the rest of this chapter, we
continue to assume the original analysis as presented earlier. In the next chapter
on conditionals, we will however make crucial use of this way of formulating the
semantics for modals. So, make sure you understand what we just proposed.

. *Kratzer’s Conversational Backgrounds
Angelika Kratzer has some interesting ideas on how accessibility relations are
supplied by the context. She argues that what is really ﬂoating around in a
discourse is a  . Accessibility relations can be
computed from conversational backgrounds (as we shall do here), or one can
state the semantics of modals directly in terms of conversational backgrounds (as
Kratzer does).
A conversational background is the sort of thing that is identiﬁed by phrases
like what the law provides, what we know, etc. Take the phrase what the law
provides. What the law provides is diﬀerent from one possible world to another.
And what the law provides in a particular world is a set of propositions. Likewise,
what we know diﬀers from world to world. And what we know in a particular
world is a set of propositions. The intension of what the law provides is then that
function which assigns to every possible world the set of propositions p such that
the law provides in that world that p. Of course, that doesn’t mean that p holds in
that world itself: the law can be broken. And the intension of what we know will
be that function which assigns to every possible world the set of propositions we
know in that world. Quite generally, conversational backgrounds are functions
of type s, st, t , functions from worlds to (characteristic functions of ) sets of
propositions.
Now, consider:
()

(In view of what we know,) Brown must have murdered Smith.

The in view of -phrase may explicitly signal the intended conversational background. Or, if the phrase is omitted, we can just infer from other clues in the
discourse that such an epistemic conversational background is intended. We will
focus on the case of pure context-dependency.



M

C 

How do we get from a conversational background to an accessibility relation?
Take the conversational background at work in (). It will be the following:
()

λw. λp. p is one of the propositions that we know in w.

This conversational background will assign to any world w the set of propositions
p that in w are known by us. So we have a set of propositions. From that we can
get the set of worlds in which all of the propositions in this set are true. These
are the worlds that are compatible with everything we know. So, this is how we
get an accessibility relation:
()

For any conversational background f of type s, st, t , we deﬁne the
corresponding accessibility relation Rf of type s, st as follows:
Rf := λw. λw . ∀p [f(w)(p) =  → p(w ) = ].

In words, w is f-accessible from w iﬀ all propositions p that are assigned by f to
w are true in w .
Kratzer uses the term   for the conversational background that
determines the set of accessible worlds. We can be sloppy and use this term for a
number of interrelated concepts:
(i) the conversational background (type s, st, t ),
(ii) the set of propositions assigned by the conversational background to a
particular world (type st, t ),
(iii) the accessibility relation (type s, st ) determined by (i),
(iv) the set of worlds accessible from a particular world (type s, t ).
Kratzer calls a conversational background (modal base)  iﬀ it assigns to
any world a set of propositions that are all true in that world. The modal base
what we know is realistic, the modal bases what we believe and what we want are
not.
What follows are some (increasingly technical exercises) on conversational backgrounds.
E .: Show that a conversational background f is realistic iﬀ the corresponding accessibility relation Rf (deﬁned as in ()) is reﬂexive.
E .: Let us call an accessibility relation  if it makes every world
accessible from every world. R is  iﬀ ∀w∀w : w ∈ R(w). What would
the conversational background f have to be like for the accessibility relation Rf
to be trivial in this sense?
E .: The deﬁnition in () speciﬁes, in eﬀect, a function from D s, st,t
to D s,st . It maps each function f of type s, st, t to a unique function Rf of

§.

*K’ C B



type s, st . This mapping is not one-to-one, however. Diﬀerent elements of
D s, st,t may be mapped to the same value in D s,st .
• Prove this claim. I.e., give an example of two functions f and f ’ in D s, st,t
for which () determines Rf = Rf .
• As you have just proved, if every function of type s, st, t qualiﬁes as
a ‘conversational background’, then two diﬀerent conversational backgrounds can collapse into the same accessibility relation. Conceivably,
however, if we imposed further restrictions on conversational backgrounds
(i.e., conditions by which only a proper subset of the functions in D s, st,t
would qualify as conversational backgrounds), then the mapping between
conversational backgrounds and accessibility relations might become oneto-one after all. In this light, consider the following potential restriction:
()

Every conversational background f must be “closed under entailment”; i.e., it must meet this condition:
∀w.∀p [∩f(w) ⊆ p → p ∈ f(w)].

(In words: if the propositions in f(w) taken together entail p, then p
must itself be in f(w).) Show that this restriction would ensure that the
mapping deﬁned in () will be one-to-one.

 In this exercise, we systematically substitute sets for their characteristic functions. I.e., we
pretend that D s,t is the power set of W (i.e., elements of D s,t are sets of worlds), and D st,t is
the power set of D s,t (i.e., elements of D st,t are sets of sets of worlds). On these assumptions,
the deﬁnition in () can take the following form:
(i)

For any conversational background f of type s, st, t ,
we deﬁne the corresponding accessibility relation Rf of type s, st as follows:
Rf := λw. {w : ∀p [p ∈ f(w) → w ∈ p]}.

The last line of this can be further abbreviated to:
(ii)

Rf := λw. ∩ f(w)

This formulation exploits a set-theoretic notation which we have also used in condition () of
the second part of the exercise. It is deﬁned as follows:
(iii)

If S is a set of sets, then ∩S := {x : ∀Y [Y ∈ S → x ∈ Y]}.

M



.

C 

Supplementary Readings

The most important background readings for this chapter are the following two
papers by Kratzer:
Kratzer, Angelika. . The notional category of modality. In Hans-Jürgen
Eikmeyer & Hannes Rieser (eds.), Words, worlds, and contexts: New approaches
in word semantics (Research in Text Theory ), –. Berlin: de Gruyter.
Kratzer, Angelika. . Modality. In Arnim von Stechow & Dieter Wunderlich
(eds.), Semantics: An international handbook of contemporary research, –.
Berlin: de Gruyter.
Kratzer has been updating her classic papers for a volume of her collected
work on modality and conditionals. These are very much worth studying:
http://semanticsarchive.net/Archive/TcNjAM/.
A major new resource on modality is Paul Portner’s book:
Portner, Paul. . Modality. Oxford University Press.
You might also proﬁt from other survey-ish type papers:
von Fintel, Kai. . Modality and language. In Donald M. Borchert (ed.),
Encyclopedia of philosophy – second edition, MacMillan. URL http://mit.edu/
ﬁntel/ﬁntel--modality.pdf.
von Fintel, Kai & Anthony S. Gillies. . An opinionated guide to epistemic
modality. In Tamar Szabó Gendler & John Hawthorne (eds.), Oxford studies
in epistemology: Volume , –. Oxford University Press. URL http://mit.
edu/ﬁntel/ﬁntel-gillies--ose.pdf.
Swanson, Eric. . Modality in language. Philosophy Compass (). –.
doi:./j.-...x.
Hacquard, Valentine. . Modality. Ms, prepared for Semantics: An international handbook of meaning, edited by Klaus von Heusinger, Claudia
Maienbon, and Paul Portner. URL http://ling.umd.edu/~hacquard/papers/
HoS_Modality_Hacquard.pdf.
On the syntax of modals, there are only a few papers of uneven quality. Some
of the more recent work is listed here. Follow up on older references from the
bibliographies in these papers.
Bhatt, Rajesh. . Obligation and possession. In Heidi Harley (ed.), Papers
from the upenn/mit roundtable on argument structure and aspect, vol.  MIT
Working Papers in Linguistics, –. URL http://people.umass.edu/bhatt/
papers/bhatt-haveto.pdf.

§.

S R



Wurmbrand, Susi. . Modal verbs must be raising verbs. West Coast Conference
on Formal Linguistics . –. URL http://wurmbrand.uconn.edu/Susi/
Papers/WCCFL.pdf.
Cormack, Annabel & Neil Smith. . Modals and negation in English. In
Sjef Barbiers, Frits Beukema & Wim van der Wurﬀ (eds.), Modality and its
interaction with the verbal system, –. Benjamins.
Butler, Jonny. . A minimalist treatment of modality. Lingua (). –.
doi:./S-()-.
The following paper explores some issues in the LF-syntax of epistemic modals:
von Fintel, Kai & Sabine Iatridou. . Epistemic containment. Linguistic
Inquiry (). –. doi:./.
Valentine Hacquard’s MIT dissertation is a rich source of cross-linguistic issues
in modality, as is Fabrice Nauze’s Amsterdam dissertation:
Hacquard, Valentine. . Aspects of modality: Massachusetts Institute of
Technology dissertation. URL http://people.umass.edu/hacquard/hacquard_
thesis.pdf.
Nauze, Fabrice. . Modality in typological perspective: Universiteit van Amsterdam dissertation. URL http://www.illc.uva.nl/Publications/Dissertations/
DS--.text.pdf.
The semantics of epistemic modals has become a hot topic recently. Here are the
main references:
DeRose, Keith. . Epistemic possibilities. The Philosophical Review ().
–. doi:./.
Egan, Andy, John Hawthorne & Brian Weatherson. . Epistemic modals in
context. In Gerhard Preyer & Georg Peter (eds.), Contextualism in philosophy:
Knowledge, meaning, and truth, –. Oxford: Oxford University Press.
Egan, Andy. . Epistemic modals, relativism, and assertion. Philosophical
Studies (). –. doi:./s---x.
MacFarlane, John. . Epistemic modals are assessment-sensitive. Ms, University of California, Berkeley, forthcoming in an OUP volume on epistemic modals, edited by Brian Weatherson and Andy Egan. URL http:
//sophos.berkeley.edu/macfarlane/epistmod.pdf.
von Fintel, Kai & Anthony S. Gillies. a. CIA leaks. The Philosophical Review
(). –. doi:./--.
von Fintel, Kai & Anthony S. Gillies. b. Might made right. To appear in a
volume on epistemic modality, edited by Andy Egan and Brian Weatherson,



M

C 

Oxford University Press. URL http://mit.edu/ﬁntel/ﬁntel-gillies--mmr.
pdf.
Stephenson, Tamina. a. Judge dependence, epistemic modals, and predicates
of personal taste. Linguistics and Philosophy (). –. doi:./s--.
A recent SALT paper by Pranav Anand and Valentine Hacquard tackles what
happens to epistemic modals under attitude predicates:
Anand, Pranav & Valentine Hacquard. . Epistemics with attitude. Proceedings of Semantics and Linguistic Theory . doi:/.
Evidentiality is a topic closely related to epistemic modality. Some references:
Willett, Thomas. . A cross-linguistic survey of the grammaticalization of
evidentiality. Studies in Language (). –.
Aikhenvald, Alexandra Y. . Evidentiality. Oxford: Oxford University Press.
Drubig, Hans Bernhard. . On the syntactic form of epistemic modality. Ms,
Universität Tübingen. URL http://www.sfb.uni-tuebingen.de/b/papers/
DrubigModality.pdf.
Blain, Eleanor M. & Rose-Marie Déchaine. . Evidential types: Evidence
from Cree dialects. International Journal of American Linguistics (). –.
doi:./.
McCready, Eric & Norry Ogata. . Evidentiality, modality and probability.
Linguistics and Philosophy (). –. doi:./s---.
Speas, Peggy. . On the syntax and semantics of evidentials. Language and
Linguistics Compass (). –. doi:./j.-X...x.
von Fintel, Kai & Anthony S. Gillies. . Must . . . stay . . . strong! Final
pre-print, to appear in Natural Language Semantics. URL http://mit.edu/
ﬁntel/ﬁntel-gillies--mss.pdf.

C F
C
We integrate conditionals into the semantics of modal expressions that
we are developing. We show that the material implication analysis and
the strict implication analysis are inferior to the restrictor analysis. Our
discussion will remain focussed on some simple questions and we refer
you to the rich literature on conditionals for further topics.

.

The Material Implication Analysis 

.

The Strict Implication Analysis 

.

If -Clauses as Restrictors 

Supplemental Readings 

.

The Material Implication Analysis

Consider the following example:
()

If I am healthy, I will come to class.

The simplest analysis of such conditional constructions is the so-called 
 analysis, which treats if as contributing a truth-function operating
on the truth-values of the two component sentences (which are called the  and  — from Latin — or  and  — from
Greek). The lexical entry for if would look as follows:
()

if = λu ∈ Dt . λv ∈ Dt . u =  or v = .

Applied to example in (), this semantics would predict that the example is
false just in case the antecedent is true, I am healthy, but the consequent false, I
do not come to class. Otherwise, the sentence is true. We will see that there is
 Quoth the Stoic philosopher Philo of Megara: “a true conditional is one which does not have
a true antecedent and a false consequent” (according to Sextus Empiricus (c. : II, –)).

Note that as a truth-functional
connective, this if does not
vary its denotation depending
on the evaluation world.
It’s its arguments that vary
with the evaluation world.

C



C 

much to complain about here. But one should realize that under the assumption
that if denotes a truth-function, this one is the most plausible candidate.
Suber () does a good job of persuading (or at least trying to persuade)
recalcitrant logic students:
After saying all this, it is important to note that material implication
does conform to some of our ordinary intuitions about implication.
For example, take the conditional statement, If I am healthy, I will
come to class. We can symbolize it: H ⊃ C.
The question is: when is this statement false? When will I have
broken my promise? There are only four possibilities:
H C
T T
T F
F T
F F

H⊃ C
?
?
?
?

• In case #, I am healthy and I come to class. I have clearly kept
my promise; the conditional is true.
• In case #, I am healthy, but I have decided to stay home and
read magazines. I have broken my promise; the conditional is
false.
• In case #, I am not healthy, but I have come to class anyway. I
am sneezing all over you, and you’re not happy about it, but I
did not violate my promise; the conditional is true.
• In case #, I am not healthy, and I did not come to class. I did
not violate my promise; the conditional is true.
But this is exactly the outcome required by the material implication.
The compound is only false when the antecedent is true and the
consequence is false (case #); it is true every other time.
Despite the initial plausibility of the analysis, it cannot be maintained. Consider
this example:
()

If there is a major earthquake in Cambridge tomorrow, my house will
collapse.

 The symbol ⊃ which Suber uses here is called the “horseshoe”. We have been using the right
arrow → as the symbol for implication. We think that this is much preferable to the confusing
horseshoe symbol. There is an intimate connection between universal quantiﬁcation, material
implication, and the subset relation, usually symbolized as ⊂, which is the other way round from
the horseshoe. The horseshoe can be traced back to the notation introduced by Peano (), a
capital C standing for ‘conseguenza’ facing backwards. The C facing in the other (more “logical”)
direction was actually introduced ﬁrst by Gergonne (), but didn’t catch on.

§.

T M I A



If we adopt the material implication analysis, we predict that () will be false
just in case there is indeed a major earthquake in Cambridge tomorrow but my
house fails to collapse. This makes a direct prediction about when the negation
of () should be true. A false prediction, if ever there was one:
()

a.
b.

It’s not true that if there is a major earthquake in Cambridge
tomorrow, my house will collapse.
≡ There will be a major earthquake in Cambridge tomorrow, and
my house will fail to collapse.

Clearly, one might think that (a) is true without at all being committed to
what the material implication analysis predicts to be the equivalent statement in
(b). This is one of the inadequacies of the material implication analysis.
These inadequacies are sometimes referred to as the “paradoxes of material
implication”. But that is misleading. As far as logic is concerned, there is nothing
wrong with the truth-function of material implication. It is well-behaved and
quite useful in logical systems. What is arguable is that it is not to be used as a
reconstruction of what conditionals mean in natural language.
E .: Under the assumption that if has the meaning in (), calculate
the truth-conditions predicted for ():
()

a.
b.

No student will succeed if he goofs oﬀ.
No student λx (if x goofs oﬀ, x will succeed)

State the predicted truth-conditions in words and evaluate whether they correspond to the actual meaning of ().

S

S

Conditional
if

S

S

Modal

A problem that is not often raised for the material implication analysis is
how badly it interacts with the analysis of modal expressions, once we look at
Figure .: LF A for
sentences involving both a conditional clause and a modal. Consider:
()
()
If we are on Route , we might be in Lockhart now.
we are on Route 

might

R

we be in Lockhart

S

()

If you keep this fern dry, it cannot grow.

Modal
might

S
R
Conditional

S

We need to consider two possible LFs for these sentences, depending on whether
wider scope is given to the modal or to the conditional clause. For example, in
the margin you see LFs A and B for ().
Figure .: LF B for
The reading for () we have in mind is an epistemic one; imagine for
()
instance that () is uttered in a car by Mary to Susan, while Susan is driving
and Mary is looking at a map. The information provided by the map, together
with other background knowledge, constitutes the relevant context for the modal
might here. The accessibility relation is roughly this:
if

S

we are on Route 

we be in Lockhart


()

C

C 

λw. λw . w is compatible with what the map says in w and what Mary
knows about the geography of the relevant area in w.

Let’s suppose () is uttered in the actual world w and we are interested in its
truth-value at this world. We now proceed to show that neither of the LFs A
and B represent the intuitively natural meaning of () if we assume the material
implication analysis of if.
Consider ﬁrst LF A. There are two respects in which the predicted truthconditions for this LF deviate from intuitive judgment. First, suppose that Susan
and Mary are not on Route  in w . Then () is predicted to be true in w ,
regardless of the geographical facts, e.g. even if Lockhart is nowhere near Route
. This is counterintuitive. Imagine the following quite sensible dialogue:
()

Mary: If we are on Route , we might be in Lockhart now.
Susan (stops the car and looks at the map): You are wrong. Look here,
Route  doesn’t run anywhere near Lockhart.

If Mary concedes Susan’s claim that Route  doesn’t go through Lockhart, she
has to also concede that her original assertion was false. It wouldn’t do for her to
respond: “I know that  runs about  miles east of Lockhart, but maybe we
are not on Route , so I may still be right.” Yet we predict that this should be a
reasonable way for her to defend ().
A second inadequacy is this: we predict that the truth of the consequent of
() is a suﬃcient condition for the truth of () as a whole. If this were right,
it would take very little for () to be true. As long as the map and the rest of
Mary’s knowledge in w don’t rule out the possibility that they are in Lockhart,
we might be in Lockhart will be true in w — regardless, once again, of whether
Lockhart is anywhere near . It should therefore be reasonable for Mary to
continue the dialogue in () with the rejoinder: “But how can you be so sure we
are not in Lockhart?” According to intuitive judgment, however, this would not
be a pertinent remark and certainly would not help Mary defend () against
Susan’s objection.
Now let’s look at LF B, where the modal has widest scope. Given the material
implication analysis of if, this is predicted to mean, in eﬀect: “It might be the
case that we are either in Lockhart or not on Route ”. This truth-condition
is also far too easy to satisfy: All it takes is that the map and the rest of Mary’s
knowledge in w are compatible with Mary and Susan not being on Route ,
or that they are compatible with their being in Lockhart. So as long as it isn’t
certain that they are on Route , Mary should be justiﬁed in asserting (),
regardless, once again, of her information about the relative location of Lockhart
and Route .
E .: Show that similar diﬃculties arise for the analysis of ().

§.

T S I A

.

The Strict Implication Analysis



Some of the problems we encountered would go away if we treated if as introducing a modal meaning. The simplest way to do that would be to treat it as
a universal quantiﬁer over possible worlds. If p, q would simply mean that the
set of p-worlds is a subset of the q-worlds. This kind of analysis is usually called
 . The diﬀerence between if and must would be that if takes
an overt restrictive argument. Here is what the lexical entry for if might look
like:
()

if

w,g

= λp ∈ D

s,t

. λq ∈ D s,t . ∀w : p(w ) =  → q(w ) = .

(in set talk: p ⊆ q)

Applied to (), we would derive the truth-conditions that () is true iﬀ all of
the worlds where there is a major earthquake in Cambridge tomorrow are worlds
where my house collapses.
We immediately note that this analysis has the same problem of noncontingency that we faced with one of our early attempts at a quantiﬁcational
semantics for modals like must and may. The obvious way to ﬁx this here is
to assume that if takes a covert accessibility function as one of its arguments.
The antecedent clause then serves as an additional restrictive device. Here is the
proposal:
()

if

w,g

= λR ∈ D

. λp ∈ D s,t . λq ∈ D s,t .
∀w : (R(w)(w ) =  & p(w ) = ) → q(w ) = .

s, s,t

(in set talk: R(w) ∩ p ⊆ q)

If we understand () as involving an epistemic accessibility relation, it would
claim that among the worlds epistemically accessible from the actual world
(i.e. the worlds compatible with what we know), those where there is a major
earthquake in Cambridge tomorrow are worlds where my house collapses. This
would appear to be quite adequate — although potentially traumatic to me.
E .: Can you come up with examples where a conditional is interpreted
relative to a non-epistemic accessibility relation?
E .: What prediction does the strict implication analysis make about
the negated conditional in (a)?
What happens when we let this analysis loose on ()? We again need to
assess two LFs depending on the relative scope of if and might. Both LFs would
have two covert variables over accessibility relations, one for if and one for might.
Before we can assess the adequacy of the two candidate analyses, we need to

C



C 

decide what the contextually salient values for the accessibility relations might be.
One would think that the epistemic accessibility relation that we have already
encountered is the most likely value, and in fact for both variables.
Next, we need to consider the particular epistemic state that Mary is in.
By assumption, Mary does not know where they are. Nothing in her visual
Figure .: LF A for
environment helps her ﬁgure out where they are. She does see from the map that
()
if they are on Route , one of the towns they might be in is Lockhart. But she
doesn’t know whether they are on Route . Even if they are on , she doesn’t
know that they are and her epistemic state would still be what it is: one of being
lost.
Consider then LF A , with the modal in the scope of the conditional. Here,
we derive the claim that all worlds w compatible with what Mary knows in w
Figure .: LF B for and where they are on  are such that some world w compatible with what
Mary knows in w is such that they are in Lockhart. Is that adequate? Not really.
()
We have just convinced ourselves that whether they are on  or not has no
relevant inﬂuence on Mary’s epistemic state, since she wouldn’t know it either
way. But that means that our analysis would predict that () is true as long as it
is possible as far as Mary knows that they are in Lockhart. Whether they are on
 or not doesn’t change that. So, we would expect () to not be distinct in
truth-value from something like:
S

S

Conditional

S

if

S

Modal

R

we are on Route 

might

R

we be in Lockhart

S

Modal

might

S

R

Conditional

S

S

if

we be in Lockhart

R

we are on Route 

()

If we are on the turnpike, we might be in Lockhart.

But that is not right — Mary knows quite well that if they are on the turnpike,
they cannot be in Lockhart.
Turning to LF B , with the modal having widest scope, doesn’t help us either.
Here, we would derive the claim that it is compatible with what Mary knows
that from being on  it follows (according to what she knows) that they are in
Lockhart. Clearly, that is not what () means. Mary doesn’t consider it possible
that if they are on , she knows that they are in Lockhart. After all, she’s well
aware that she doesn’t know where they are.

. If -Clauses as Restrictors
The problem we have encountered here with the interaction of an if -clause
and the modal operator might is similar to others that have been noted in the
literature. Most inﬂuentially, David Lewis in his paper “Adverbs of Quantiﬁcation” showed how hard it is to ﬁnd an adequate analysis of the interaction of
if -clauses and    like never, rarely, sometimes, often,
usually, always. Lewis proposed that in the cases he was considering, the adverb
is the only operator at work and that the if -clause serves to restrict the adverb.
Thus, it has much the same function that a common noun phrase has in a
determiner-quantiﬁcation.
David Lewis

§.

If -C  R



The if of our restrictive if -clauses should not be regarded as a
sentential connective. It has no meaning apart from the adverb
it restricts. The if in always if . . . , . . . , sometimes if . . . , . . . , and
the rest is on a par with the non-connective and in between . . . and
. . . , with the non-connective or in whether . . . or . . . , or with the
non-connective if in the probability that . . . if . . . . It serves merely to
mark an argument-place in a polyadic construction. (Lewis : )
Building on Lewis’ insight, Kratzer argued for a uniform treatment of if -clauses
as restrictors. She claimed that
the history of the conditional is the story of a syntactic mistake.
There is no two-place if . . . then connective in the logical forms of
natural languages. If -clauses are devices for restricting the domains
of various operators. (Kratzer )
Let us repeat this:
()

K’ T
If -clauses are devices for restricting the domains of various operators.

Kratzer’s Thesis gives a uniﬁed picture of the semantics of conditional clauses.
Note that it is not meant to supplant previous accounts of the meaning of
conditionals. It just says that what those accounts are analyzing is not the
meaning of if itself but the meaning of the operators that if -clauses restrict.
Let us see how this idea helps us with our Lockhart-sentence. The idea is to
deny that there are two quantiﬁers over worlds in (). Instead, the if -clause
merely contributes a further restriction to the modal might. In eﬀect, the modal
is not quantifying over all the worlds compatible with Mary’s knowledge but
only over those where they are on Route . It then claims that at least some of
those worlds are worlds where they are in Lockhart. We cannot anymore derive
the problematic conclusion that it should also be true that if they are on the
turnpike, they might be in Lockhart. In all, we have a good analysis of what ()
means.
What we don’t yet have is a compositional calculation. What does it mean in
structural terms for the if -clause to be restricting the domain of the modal? We
Figure .: LF C for
will assume a structure as in LF C. Here, the if -clause is the sister to what used
()
to be the covert set-of-worlds argument of the modal. As you can see, we have
chosen the variant of the semantics for modals that was discussed in Section ...
The idea now is that the two restrictive devices work together: we just feed to
the modal the intersection of (i) the set of worlds that are R-accessible from the
actual world, and (ii) the set of worlds where they are on Route .
S

S

Modal

we be in Lockhart

might

R w* (if )

S

we are on Route 

E .: To make the composition work, we need to be able to intersect
the set of accessible worlds with the antecedent proposition. This could be



C

C 

done in two ways: (i) a new composition principle, which would be a slight
modiﬁcation of the P M rule, (ii) give if a functional
meaning that accomplishes the intersection. Formulate such a meaning for if.
Alternatively, we could do without the w∗ device and instead give if a
meaning that takes a proposition p and then modiﬁes an accessibility relation to
give a new accessibility relation, which is restricted to p-worlds. Formulate such
a meaning for if.
What about cases like (), now? Here there is no modal operator for the
if -clause to restrict. Should we revert to treating if as an operator on its own?
Kratzer proposes that we should not and that such cases simply involve covert
modal operators.

Supplementary Readings
A short handbook article on conditionals:
von Fintel, Kai. . Conditionals. Ms, prepared for Semantics: An international
handbook of meaning, edited by Klaus von Heusinger, Claudia Maienborn,
and Paul Portner. URL http://mit.edu/ﬁntel/ﬁntel--hsk-conditionals.pdf
Overviews of the philosophical work on conditionals:
Edgington, Dorothy. . On conditionals. Mind (). –. URL
./mind/...
Bennett, Jonathan. . A philosophical guide to conditionals. Oxford University
Press.
A handbook article on the logic of conditionals:
Nute, Donald. . Conditional logic. In Dov Gabbay & Franz Guenthner
(eds.), Handbook of philosophical logic. volume ii, –. Dordrecht: Reidel.
Three indispensable classics:
Lewis, David. . Counterfactuals. Oxford: Blackwell.
Stalnaker, Robert. . A theory of conditionals. In Nicholas Rescher (ed.), Studies in logical theory (American Philosophical Quarterly Monograph Series ),
–. Oxford: Blackwell.
Stalnaker, Robert. . Indicative conditionals. Philosophia (). –.
doi:./BF.

§.

If -C  R



The Restrictor Analysis:
Lewis, David. . Adverbs of quantiﬁcation. In Edward Keenan (ed.), Formal
semantics of natural language, –. Cambridge University Press.
Kratzer, Angelika. . Conditionals. Chicago Linguistics Society (). –.
The application of the restrictor analysis to the interaction of nominal quantiﬁers
and conditionals:
von Fintel, Kai. . Quantiﬁers and ‘if ’-clauses. The Philosophical Quarterly
(). –. doi:./-.. URL http://mit.edu/ﬁntel/
www/qandif.pdf.
von Fintel, Kai & Sabine Iatridou. . If and when If -clauses can restrict
quantiﬁers. Ms, MIT. URL http://mit.edu/ﬁntel/ﬁntel-iatridou--ifwhen.
pdf.
Higginbotham, James. . Conditionals and compositionality. Philosophical
Perspectives (). –. doi:./j.-...x.
Leslie, Sarah-Jane. . If, unless, and quantiﬁcation. In Robert J. Stainton &
Christopher Viger (eds.), Compositionality, context and semantic values: Essays
in honour of Ernie Lepore, –. Springer. doi:./----_.
Huitink, Janneke. b. Quantiﬁed conditionals and compositionality. Ms, to
appear in Language and Linguistics Compass. URL http://user.uni-frankfurt.
de/~huitink/compass-conditionals-ﬁnal.pdf.
Syntax of conditionals:
von Fintel, Kai. . Restrictions on quantiﬁer domains: University of Massachusetts at Amherst dissertation. URL http://semanticsarchive.net/Archive/
jANIwN/ﬁntel--thesis.pdf, Chapter : “Conditional Restrictors”
Iatridou, Sabine. . On the contribution of conditional Then. Natural
Language Semantics (). –. doi:./BF.
Bhatt, Rajesh & Roumyana Pancheva. . Conditionals. In The Blackwell
companion to syntax, vol. , –. Blackwell. URL http://www-rcf.usc.edu/
~pancheva/bhatt-pancheva_syncom.pdf.
A shifty alternative to the restrictor analysis:
Gillies, Anthony S. . On truth-conditions for if (but not quite only if ).
The Philosophical Review (). –. doi:./--.
Gillies, Anthony S. . Iﬃness. Semantics and Pragmatics (). –. doi:./sp...



C

C 

The Belnap alternative:
Belnap, Jr., Nuel D. . Conditional assertion and restricted quantiﬁcation.
Noûs (). –. doi:./.
Belnap, Jr., Nuel D. . Restricted quantiﬁcation and conditional assertion.
In Hugues Leblanc (ed.), Truth, syntax and modality: Proceedings of the Temple
University conference on alternative semantics, vol.  Studies in Logic and the
Foundations of Mathematics, –. Amsterdam: North-Holland.
von Fintel, Kai. . If : The biggest little word. Slides from a plenary
address given at the Georgetown University Roundtable, March , . URL
http://mit.edu/ﬁntel/gurt-slides.pdf.
Huitink, Janneke. . Modals, conditionals and compositionality: Radboud Universiteit Nijmegen dissertation. URL http://user.uni-frankfurt.de/~huitink/
Huitink-dissertation.pdf, Chapters  and  give a nice summary of what we’re
covering in this class, while Chapter  is about the Belnap-method.
Huitink, Janneke. a. Domain restriction by conditional connectives. Ms,
Goethe-University Frankfurt. URL http://semanticsarchive.net/Archive/zgMDMM/
Huitink-domainrestriction.pdf.

C F
O
We have stressed throughout the previous two chapters that there are
numerous parallels between quantiﬁcation over ordinary individuals via
determiner quantiﬁers and quantiﬁcation over possible worlds via modal
operators (including conditionals). Now, we turn to a phenomenon that
(at least at ﬁrst glance) appears to show that there are non-parallels
as well: a sensitivity to an  of the elements in the domain
of quantiﬁcation. We ﬁrst look at this in the context of simple modal
sentences and then we look at conditionals.

.

The Driveway 

.

Kratzer’s Solution: Doubly Relative Modality 

.

The Paradox of the Good Samaritan 

.

Non-Monotonicity of Conditionals 

Supplemental Readings 

.

The Driveway

Consider a typical use of a sentence like ().
()

John must pay a ﬁne.

This is naturally understood in such a way that its truth depends both on facts
about the law and facts about what John has done. For instance, it will be
judged true if (i) the law states that driveway obstructors are ﬁned, and (ii) John
has obstructed a driveway. It may be false either because the law is diﬀerent or
because John’s behavior was diﬀerent.
What accessibility relation provides the implicit restriction of the quantiﬁer
must on this reading of ()? A naïve attempt might go like this:

O


()

C 

λw. λw . [what happened in w up to now is the same as what happened
in w and w conforms to what the law in w demands].

The problem with () is that, unless there were no infractions of the law at
all in w up to now, no world w will be accessible from w. Therefore, () is
predicted to follow logically from the premise that John broke some law. This
does not represent our intuition about its truth conditions.
A better deﬁnition of the appropriate accessibility relation has to be more
complicated:
()

λw. λw . [what happened in w up to now is the same as what happened
in w and w conforms at least as well to what the law in w demands as

does any other world in which what happened up to now is the same as
in w].
() makes explicit that there is an important diﬀerence between the ways in
which facts about John’s behavior on the one hand, and facts about the law on
the other, enter into the truth conditions of sentences like (). Worlds in
which John didn’t do what he did are simply excluded from the domain of must
here. Worlds in which the law isn’t obeyed are not absolutely excluded. Rather,
we restrict the domain to those worlds in which the law is obeyed as well as it
can be, considering what has happened. We exclude only those worlds in which
there are infractions above and beyond those that are shared by all the worlds
in which John has done what he has done. The analysis of () thus crucially
involves the notion of an ordering of worlds: here they are ordered according to
how well they conform to what the law in w demands.

. Kratzer’s Solution: Doubly Relative Modality
Kratzer proposes that modal operators are sensitive to two context-dependent
parameters: a set of accessible worlds (provided by an accessibility function computed from a conversational background, the  ), and a partial ordering
of the accessible worlds (computed from another conversational background,
called the  ).
Let’s see how the analysis applies to the previous example.
• The modal base will be a function that assigns to any evaluation world
a set of propositions describing the relevant circumstances, for example,
what John did. Since in our stipulated evaluation world John obstructed a
driveway, the modal base will assign the proposition that John obstructed
a driveway to this world. The set of worlds accessible from the evaluation
world will thus only contain worlds where John obstructed a driveway.
• The ordering source will be a function that assigns to any evaluation world
a set of propositions P whose truth is demanded by the law. Imagine that

§.

•

•
•
•

K’ S: D R M



for our evaluation world this set of propositions contains (among others)
the following two propositions: (i) nobody obstructs any driveways, (ii)
anybody who obstructs a driveway pays a ﬁne.
The idea is now that such a set P of propositions can be used to order the
worlds in the modal base. For any pair of worlds w and w , we say that
w comes closer than w to the ideal set up by P (in symbols: w <P w ),
iﬀ the set of propositions from P that are true in w is a proper subset of
the set of propositions from P that are true in w .
For our simple example then, any world in modal base where John pays a
ﬁne will count as better than an otherwise similar world where he doesn’t.
Modals then make quantiﬁcational claims about the best worlds in the
modal base (those for which there isn’t a world that is better than them).
In our case, () claims that in the best worlds (among those where John
obstructed a driveway), he pays a ﬁne.

More technically:
()

Given a set of worlds X and a set of propositions P, deﬁne the 
  <P as follows:
∀w , w ∈ X : w <P w iﬀ {p ∈ P : p(w ) = } ⊂ {p ∈ P : p(w ) = }.

()

For a given strict partial order <P on worlds, deﬁne the selection function
maxP that selects the set of <P -best worlds from any set X of worlds:
∀X ⊆ W : maxP (X) = {w ∈ X : ¬∃w ∈ X : w <P w}.

()

must

w,g

= λf

s,st

. λg s,st . λq s,t .

∀w ∈ maxg(w) (∩f(w)) : q(w ) = .

T N: This only works if we can in general assume that the <P
relation has minimal elements, that there always are accessible worlds that
come closest to the P-ideal, worlds that are better than any world they can
be compared with via <P . It is possible, with some imagination, to cook up
scenarios where this assumption fails. This problem has been discussed primarily
in the area of the semantics of conditionals. There, Lewis presents relevant
scenarios and argues that one shouldn’t make this assumption, which he calls the
Limit Assumption. Stalnaker, on the one other hand, defends the assumption
against Lewis’ arguments by saying that in actual practice, in actual natural
language semantics and in actual modal/conditional reasoning, the assumption
is eminently reasonable. Kratzer is persuaded by Lewis’ evidence and does not
make the Limit Assumption; hence her semantics for modals is more convoluted
than what we have in () and (). I will side with Stalnaker, not the least
because it makes life easier. For further discussion, see Lewis (: –) and
Stalnaker (b: Chapter , esp. pp. –); Pollock (), Herzberger
(), and Warmbrod () argue for the Limit Assumption as well.



O

C 

E .: In her handbook article Kratzer (), Kratzer presents a number
of examples of modal statements and sketches an analyses in terms of doubly
relative modality. You should study her examples carefully.

. The Paradox of the Good Samaritan
Prior () introduced the following “Paradox of the Good Samaritan”. Imagine
that someone has been robbed and John is walking by. It is easy to conceive of a
code of ethics that would make the following sentence true:
()

John ought to help the person who was robbed.

In our previous one-factor semantics for modals, we would have said that ()
says that in all of the deontically accessible worlds (those compatible with the
code of ethics) John helps the person who was robbed. Prior’s point was that
under such a semantics, something rather unfortunate holds. Notice that in all
of the worlds where John helps the person who was robbed, someone was robbed
in the ﬁrst place. Therefore, it will be true that in all of the deontically accessible
worlds, someone was robbed. Thus, () will entail:
()

It ought to be the case that someone was robbed.

It clearly would be good not make such a prediction.
The doubly-relative analysis of modality can successfully avoid this unfortunate prediction. We conceive of () as being uttered with respect to a circumstantial modal base that includes the fact that someone was robbed. Among
those already somewhat ethically deﬁcient worlds, the relatively best ones are all
worlds where John helps the victim.
Note that we still have the problematic fact that among the worlds in the
modal base, all are worlds where someone was robbed, and we would thus appear
to still make the unfortunate prediction that () should be true. But this can
now be ﬁxed. For example, we could say that ought p is semantically defective if p
is true throughout the worlds in the modal base. This could be a presupposition
or some other ingredient of meaning. So, with respect to a modal base which
pre-determines that someone was robbed, one couldn’t felicitously say ().
Consequently, saying () would only be felicitous if a diﬀerent modal base
is intended, one that contains both p and non-p worlds. And given a choice
between worlds where someone was robbed and worlds where nobody was
robbed, most deontic ordering sources would probably choose the no-robbery
worlds, which would make () false, as desired.
K’    S P Kratzer () argues that
the restrictor approach to deontic conditionals is the crucial ingredient in the
solution to a conditional version of the Samaritan Paradox:

§.

N-M  C

()

If a murder occurs, the jurors must convene.



Kratzer points out that if one tried to analyze () as a material implication
embedded under deontic necessity, then one quickly runs into a problem. Surely,
one wants the following to be a true statement about the law:
()

There must be no murder.

But this means that in the deontically accessible worlds, all of them have no
murders occurring. Now, this means that in all of the deontically accessible
worlds, any material implication of the form “if a murder occurs, q” will be true
no matter what the consequent is since the antecedent will be false. Since that
is an absurd prediction, () cannot be analyzed as material implication under
deontic necessity. The combination of the restrictor approach to if -clauses and
the doubly-relative theory of modals can rescue us from this problem. () is
analyzed as the deontic necessity modal being restricted by the if -clause. The set
of accessible worlds is narrowed down by the if -clause to only include worlds
in which a murder occurs. The deontic ordering then identiﬁes the best among
those worlds and those are plausibly all worlds where the jurors convene.

.

Non-Monotonicity of Conditionals

The last case discussed takes us straight to the crucial role of the ordering of
worlds in the semantics of conditionals, as we would of course expect under
the analysis of if -clauses as restrictors of modal operators. In this arena, the
discussion usually revolves around the failure of certain inference patterns, which
one would expect a universal quantiﬁer to validate. Here are the most important
ones:
()

L D M (“D E”)
Every A is a B. → Every A & C is a B.

()

T
Every A is a B. Every B is a C. → Every A is a C.

()

C
Every A is a B. → Every non-B is a non-A.

Conditionals were once thought to obey these patterns as well, known in conditional logic as S  A, H S,
and C. But then spectacular counterexamples became known
through the work of Stalnaker and Lewis.



O

C 

()

F  S  A
a. If I strike this match, it will light.
If I dip this match into water and strike it, it will light.
b. If John stole the earrings, he must go to jail.
If John stole the earrings and then shot himself, he must go to jail.
c. If kangaroos had no tails, they would topple over. If kangaroos had
no tails but used crutches, they would topple over.

()

F   H S (T)
a. If Brown wins the election, Smith will retire to private life.
If Smith dies before the election, Brown will win the election.
If Smith dies before the election,Smith will retire to private life.
b. If Hoover had been a Communist, he would have been a traitor.
If Hoover had been born in Russia, he would have been a Communist.
If Hoover had been born in Russia, he would have been a traitor.

()

F  C
a. If it rained, it didn’t rain hard.
If it rained hard, it didn’t rain.
b. (Even) if Goethe hadn’t died in , he would still be dead now.
If Goethe were alive now, he would have died in .

The Goethe example is due to
Kratzer.

Note that these cases involve examples of both “indicative” (epistemic) conditionals and counterfactual conditionals. It is sometimes thought that indicative
conditionals are immune from these kinds of counterexamples, but it is clear that
they are not. Also note that in (b) we have a case of Failure of Strengthening
the Antecedent with a deontic conditional. Deontic counterexamples to the
other patterns seem harder to ﬁnd.
The failure of these inference patterns again indicates that the semantics of
modal operators (restricted by if -clauses) is more complicated than the simple
universal quantiﬁcation we had previously been assuming. The basic idea of
most approaches to this problem is this: the semantics of conditionals is more
complicated than simple universal quantiﬁcation. The conditional does not make
a claim about simply every antecedent world, nor even about every contextually
relevant antecedent world. Instead, in each of the conditional statements, only a
particular subset of the antecedent worlds is quantiﬁed over. Informally, we can
call those the “most highly ranked antecedent worlds”. Consider:
()

If I had struck this match, it would have lit.
If I had dipped this match into water and struck it, it would have lit.

According to the Stalnaker-Lewis account, this inference is semantically invalid.
The premise merely claims that the most highly ranked worlds in which I strike

§.

N-M  C



this match are such that it lights. No claim is made about the most highly ranked
worlds in which I ﬁrst dip this match into water and then strike it. Strengthening
the Antecedent will only be safe if it is additionally known that the strengthened
antecedent is instantiated among the worlds that verify the original antecedent.
The other fallacies receive similar treatments. Transitivity (Hypothetical
Syllogism) fails for the new non-monotonic quantiﬁer because even if all the
most highly rated p-worlds are q-worlds and all the most highly rated q-worlds
are r-worlds, we are not necessarily speaking about the same q-worlds (the qworlds that p takes us to may be rather remote ones). So in the Hoover-example,
we get the following picture: The most highly ranked p-worlds in which Hoover
was born in Russia (but where he retains his level of civic involvement), are all
q-worlds in which he becomes a Communist. On the other hand, the most
highly ranked q-worlds in which he is a Communist (but retaining his having
been born in the United States and being a high level administrator) are all
r-worlds in which he is a traitor. However, the most highly ranked p-worlds do
not get us to the most highly ranked q-worlds, so the Transitive inference does
not go through.
Contraposition fails because the fact that the most highly rated p-worlds are
q-worlds does not preclude a situation where the most highly rated non q-worlds
are also p-worlds. The most highly rated p-worlds in which Goethe didn’t die in
 are all q-worlds where he dies nevertheless (well) before the present. But of
course, the most highly rated (in fact, all) non-q-worlds (where he is alive today)
are also p-worlds where he didn’t die in .
In the conditionals literature, the ordering of worlds is usually given directly
as an evaluation parameter. The typical gloss is that the ordering ranks possible
worlds based on how similar they are to the evaluation world. Kratzer developed
an alternative where the ordering is computed from a set of propositions true in
the evaluation world. Lewis () showed that ordering semantics and premise
semantics are largely notational variants.

Supplementary Readings
The central readings for this chapter are two papers by Kratzer:
Kratzer, Angelika. . Modality. In Arnim von Stechow & Dieter Wunderlich
(eds.), Semantics: An international handbook of contemporary research, –.
Berlin: de Gruyter.
Kratzer, Angelika. . The notional category of modality. In Hans-Jürgen
Eikmeyer & Hannes Rieser (eds.), Words, worlds, and contexts: New approaches
in word semantics (Research in Text Theory ), –. Berlin: de Gruyter.



O

C 

Some work that discusses and uses Kratzer’s two factor semantics for modals:
Frank, Anette. . Context dependence in modal constructions: Universität
Stuttgart dissertation. URL http://www.dfki.de/~frank/papers/header.ps.gz.
von Fintel, Kai & Sabine Iatridou. . What to do if you want to go to
Harlem: Anankastic conditionals and related matters. Ms, MIT. URL http:
//mit.edu/ﬁntel/ﬁntel-iatridou--harlem.pdf.
von Fintel, Kai & Sabine Iatridou. . How to say ought in Foreign: The
composition of weak necessity modals. In Jacqueline Guéron & Jacqueline
Lecarme (eds.), Time and modality (Studies in Natural Language and Linguistic
Theory ), –. Springer. doi:./----.
Some work that discusses whether non-monotonicity could be or might have to
be relegated to a dynamic pragmatic component of meaning:
von Fintel, Kai. . Counterfactuals in a dynamic context. In Michael
Kenstowicz (ed.), Ken Hale: A life in language, –. MIT Press.
von Fintel, Kai. . NPI licensing, Strawson entailment, and context dependency. Journal of Semantics (). –. doi:./jos/...
Gillies, Anthony S. . Counterfactual scorekeeping. Linguistics and Philosophy
(). –. doi:./s---.
Schlenker explored whether the apparent non-monotonicity in conditional is
paralleled in quantiﬁcation over individuals:
Schlenker, Philippe. . Conditionals as deﬁnite descriptions (A referential
analysis). Research on Language and Computation (). –. doi:./s--.

C S
B  T  A
We explore an analysis of tense that treats tenses as intensional operators
manipulating a time parameter of evaluation. The treatment is formally
quite parallel to the treatment of modals in Chapter . We touch on
many basic questions about tense and aspect, without exploring them
fully.

.

A First Proposal for Tense 
..

former 

..

Some Time Adverbials 

..

A Word of Caution 

.

Are Tenses Referential? 

.

The Need for Intervals 

.

Aktionsarten 

.

The Progressive 

.

Tense in Embedded Clauses 

Supplemental Readings 

.

A First Proposal for Tense

Tense logic, or temporal logic, is a branch of logic ﬁrst developed by the aptly
named Arthur Prior in a series of works, in which he proposed treating tense
in a way that is formally quite parallel to the treatment of modality discussed
in Chapter . Since tense logic (and modal logic) typically is formulated at a
high level of abstraction regarding the structure of sentences, it doesn’t concern
itself with the internal make-up of “atomic” sentences and thus treats tenses as
sentential operators (again, in parallel to the way modal operators are typically

See the Stanford Encyclopedia
of Philosophy entry on temporal logic: http://plato.stanford.
edu/entries/logic-temporal/
and the website for Prior
studies: http://www.
prior.aau.dk/index.htm.

B  T  A



C 

treated in modal logic). We will implement a version of Prior’s tense logic in our
framework.
The ﬁrst step is to switch to a version of our intensional semantic system where
instead of a world parameter, the evaluation function is sensitive to a time
parameter (and a variable assignment). Eventually, we will want to deal with
the full complexity and relativize the evaluation function to both worlds and
times, but for now, we will just relativize to times. The composition principles
developed in Chapter  will be adopted mutatis mutandis. Predicates will now
have lexical entries that incorporate their sensitivity to time:
()

tired

t,g

= λx ∈ D. x is tired at t.

It is customary in the literature to introduce a new basic type for times; for
now, we will recycle the designation s as the type for times. Then, for example,
the intension of sentence will again be of type s, t , but now that would be a
temporal proposition, a function from times to truth-values.
In this framework, we can now formulate a very simple-minded ﬁrst analysis
of the present and past tenses and the future auxiliary will. As for (LF) syntax
let’s assume that (complete matrix) sentences are TPs, headed by T (for “tense”).
There are two morphemes of the functional category T, namely PAST (past
tense) and PRES (present tense). The complement of T is an MP or a VP.
MP is headed by M (for “modal”). Morphemes of the category M include the
modal auxiliaries must, can, etc., which we talked about in previous chapters, the
semantically vacuous do (in so-called “do-support” structures), and the future
auxiliary will. Evidently, this is a semantically heterogeneous category, grouped
together solely because of their common syntax (they are all in complementary
distribution with each other). The complement of M is a VP. When the sentence
contains none of the items in the category M, we assume that MP isn’t projected
at all; the complement of T is just a VP in this case. We thus have LF-structures
like the following. (The corresponding surface sentences are given below, and
we won’t be explicit about the derivational relation between these and the LFs.
Assume your favorite theories of syntax and morphology here.)
()

[TP Mary [ T’ PRES [ VP t [ V’ be tired ]]]]
= Mary is tired.

()

[TP Mary [ T’ PAST [ VP t [ V’ be tired ]]]]
= Mary was tired.

 We remain vague for now about what we mean by “times” (points in time? time intervals?).
This will need clariﬁcation, We will also see the need to clarify what we mean by “at” in the
metalanguage in this entry and others.

§.

A F P  T

()

[ TP Mary [ T’ PRES [ MP t [ M’ woll [ VP t [ V’ be tired ]]]]]]
= Mary will be tired.



When we have proper name subjects, we will pretend for simplicity that they are
reconstructed somehow into their VP-internal base position. (We will talk more
about reconstruction later on.)
What are the meanings of PRES, PAST, and will? For PRES, the simplest
assumption is actually that it is semantically vacuous. This means that the
interpretation of the LF in () is identical to the interpretation of the bare VP
Mary be tired:
()

For any time t:
PRES (Mary be tired) t = Mary be tired t =  iﬀ Mary is tired at t.

Does this adequately capture the intuitive truth-conditions of the sentence Mary
is tired ? It does if we make the following general assumption:
()

An utterance of a sentence (= LF) φ that is made at a time t counts as
true iﬀ φ t =  (and as false if φ t = ).

This assumption ensures that (unembedded) sentences are, in eﬀect, interpreted
as claims about the time at which they are uttered (“utterance time” or “speech
time”). If we make this assumption and we stick to the lexical entries we have
adopted, then we are driven to conclude that the present tense has no semantic
job to do. A tenseless VP Mary be tired would in principle be just as good as
() to express the assertion that Mary is tired at the utterance time. Apparently
it is just not well-formed as an unembedded structure, but this fact must be
attributed to principles of syntax rather than semantics.
What about PAST ? When a sentence like () Mary was tired is uttered at a
time t, then what are the conditions under which this utterance is judged to be
true? A quick (and perhaps ultimately wrong) answer is: an utterance of () at
t is true iﬀ there is some time before t at which Mary is tired. This suggests the
following entry:
()

For any time t:
PAST t = λp ∈ D s,t .∃t before t : p(t ) = 

So, the past tense seems to be an existential quantiﬁer over times, restricted to
times before the utterance time.
For will, we can say something completely analogous:

B  T  A


()

C 

For any time t:
will t = λp ∈ D s,t .∃t after t : p(t ) = 

Apparently, PAST and will are semantically alike, even mirror images of each
other, though they are of diﬀerent syntactic categories. The fact that PAST
is the topmost head in its sentence, while will appears below PRES, is due to
the fact that syntax happens to require a T-node in every complete sentence.
Semantically, this has no eﬀect, since PRES is vacuous.
Both () and () presuppose that the set T comes with an intrinsic order.
For concreteness, assume that the relation ‘precedes’ (in symbols: <) is a strict
linear order on T . The relation ‘follows’, of course, can be deﬁned in terms of
‘precedes’ (t follows t iﬀ t precedes t).
There are many things wrong with this simple analysis. We will not have time
here to diagnose most of the problems, much less correct them. But let’s see a
couple of things that work out OK and let’s keep problems and remedies for
later.

..

former

There is a brief discussion on p.  of H& K about the inadequacy of an
extensional semantics for the adjective former as in
()

John is a former teacher.

We can now write a semantics for former. While there are a bunch of people
who are currently teachers, there are others that wre not now teachers but were
at some previous time. The latter are the ones that the predicate former teacher
should be true of, In other words, former teacher is a predicate that is true of
individuals just in case the predicate teacher was true of them at some previous
time (and is not true of them now). So, former needs to be an intensional
operator that “displaces” the evaluation of time of its complement from “now”
to some previous time. To be able to do that, it needs to take the intension of its
complement as its argument. This suggests the following lexical entry:
 Deﬁnition: A relation R is a strict linear order on a set S iﬀ it has the following four properties:
(i) ∀x∀y∀z((Rxy&Ryz) → Rxz) “Transitivity"
(ii) ∀x(¬Rxx) “Irreﬂexivity"
(iii) ∀x∀y(Rxy → ¬Ryx) “Asymmetry", and
(iv) ∀x∀y(x = y → (Rxy ∨ Ryx)) “Connectedness"

§.

A F P  T

()

former = λf ∈ D s, e,t .λx.[f(t)(x) =  & ∃t before t : f(t )(x) = ].



E .: H& K on p. mention the adjective alleged in one breath with
former. Formulate a lexical entry for alleged as used in John is an alleged murderer.
[This will use our original intensional system with a world parameter]

..

Some Time Adverbials

At least to a certain extent, we can also provide a treatment of temporal adverbials
such as:
()

Mary was tired on February , .

The basic idea would be that phrases like on February ,  are propositional
modiﬁers. Propositions are the intensions of sentences. At this point, propositions are functions from times to truth-values. Propositional modiﬁers take a
proposition and return a proposition with the addition of a further condition on
the time argument.
()

on February ,  t
= λp ∈ D s,t . [ p(t) =  & t is part of Feb ,  ]

()

[T’ PAST [ VP [ VP Mary [ V’ be tired]] [ PP on February , ]]]

An alternative would be to treat on February ,  as a “sentence” by itself,
whose intension then would be a proposition.
()

on February ,  t =  iﬀ t is part of February , 

()

on t = λx. t is part of x

To make this work, we would then have to devise a way of combining two
tenseless sentences (Mary be tired and on February , ) into one. We could
do this by positing a silent and or by introducing a new composition rule
(“Propositional Modiﬁcation”?).
Let’s not spend time on such a project.
E .: Imagine that Mary was tired on February ,  is not given the
LF in () but this one:
()

[ T’ [ T’ PAST [ VP Mary [ V’ be tired]]] [ PP on February , ]]

B  T  A



C 

What would the truth-conditions of this LF be? Does this result correspond at
all to a possible reading of this sentence (or any other analogous sentence)? If
not, how could we prevent such an LF from being produced?
E .: When a quantiﬁer appears in a tensed sentence, we might expect
two scope construals. Consider a sentence like this:
()

Every professor (in the department) was a teenager in the Sixties.

We can imagine two LFs:
()

PAST [ [every professor be a teenager] [in the sixties] ]

()

[every professor] λ [ PAST [ [t be a teenager] [in the sixties] ]

Describe the diﬀerent truth-conditions which our system assigns to the two LFs.
Is the sentence ambiguous in this way?
If not this sentence, are there analogous sentences that do have the ambiguity?
E .: The following entry for every makes it a time-insensitive item:
()

every t = λf ∈ D e,t .λg ∈ D e,t .∀x[f(x) =  → g(x) = ]

Consider now two possible variants (we have underlined the portion where they
diﬀer):
()

every t = λf ∈ D e,t .λg ∈ D e,t .∀x at t [f(x) =  → g(x) = ]

()

every t = λf ∈ D e,t .λg ∈ D e,t .∀x[f(x) =  at t → g(x) =  at t]

Does either of these alternative entries make sense? If so, what does it say? Is it
equivalent to our oﬃcial entry? Could it lead to diﬀerent predictions about the
truth-conditions of English sentences?

..

A Word of Caution

Compare the semantics given for former and the one for PAST :
()

former = λf ∈ D s, e,t .λx.[f(t)(x) =  & ∃t before t : f(t )(x) = ].

()

PAST t = λp ∈ D s,t . ∃t before t : p(t ) = 

§.

A T R?



Notice that these entries have an interesting consequence:
()

a.
b.

John is a former teacher.
John was a teacher.

The two sentences in () diﬀer in their truth-conditions. The sentence in
(a) can only be true if John is not a teacher anymore while this is not part of
the truth-conditions of the sentence in (b). To see that this analysis is in fact
correct, consider this:
()

Last night, John was reading a book about tense.
a. !! The authors are former Italians.
b. The authors were Italian.

Consider the past tense in (b). It is not (necessarily) interpreted as claiming
that the authors are not Italian anymore. But this is in fact required by (a).
There are some cases where it seems that the past tense does trigger inferences
that one would not expect from the lexical entry that we gave. Surely, if I tell you
My cousin John was a teacher you will infer that he isn’t a teacher anymore. In fact,
you may even infer that he is not alive anymore. One promising approach that
tries to reconcile a semantics like ours with the possibility of stronger inferences
in some contexts is based on pragmatic considerations, see Musan ().
Examples like the one in () are problematic for widely held naive conceptions
of what the past tense means. One often hears that PAST expresses the fact that
“the time of the reported situation precedes the speech time”. If this were to
mean that the time of the book’s authors being Italian precedes the speech time,
this would presumably wrongly predict that they would have to be not Italian
anymore for the sentence to be true (or usable).

. Are Tenses Referential?
Our semantics for the past tense treats it essentially as an existential quantiﬁer
over times (albeit in the meta-language), the same way we treated possibility
modals as existential quantiﬁers over (accessible) worlds. This seems quite
adequate for examples like (), which seem to display the expected quantiﬁed
meaning:
()

John went to a private school.

B  T  A



C 

All we learn from () is that at some point in the past, whenever it was that
John went to school, he went to a private school.
Partee in her famous paper “Some structural analogies between tenses and
pronouns in English” (Partee ) presented an example where tense appears to
act more “referentially”:
()

I didn’t turn oﬀ the stove.

“When uttered, for instance, halfway down the turnpike, such a sentence clearly
does not mean either that there exists some time in the past at which I did not
turn oﬀ the stove or that there exists no time in the past at which I turned oﬀ
the stove. The sentence clearly refers to a particular time — not a particular
instant, most likely, but a deﬁnite interval whose identity is generally clear from
the extralinguistic context, just as the identity of the he in [He shouldn’t be in
here] is clear from the context.”
Partee here is arguing that neither of the two plausible LFs derivable in our
current system correctly captures the meaning of (). Given that the sentence
contains a past tense (which we have treated as an existential quantiﬁer over past
times) and a negation, we need to consider two possible scopings of the two
operators:
()

a.
b.

PAST NEG I turn oﬀ the stove.
NEG PAST I turn oﬀ the stove.

E .: Show that neither LF in () captures the meaning of ()
correctly.
At this point, we will not develop Partee’s analysis in formally explicit detail. If
tenses refer to times, it would be easiest to give up on the treatment of times as
evaluation parameters and move to a system where times are object language
arguments of time-sensitive expressions. We will see a system of that nature later
on.
In a commentary on Partee’s paper at the same conference it was presented at,
Stalnaker pointed out that the Priorean theory can in fact deal with (), if
one allows the existential quantiﬁer over times to be contextually restricted to
times in the salient interval of Partee leaving her house — since natural language
quantiﬁers are typically subject to contextual restrictions, this is not a problematic
assumption. (Note that Partee formulated her observation in quite a circumspect
way: “The sentence refers to a particular time”; Stalnaker’s suggestion is that the
reference to a particular time is part of the restriction to the quantiﬁer over times
expressed by tense, rather than tense itself being a referring item (of type s).)

§.

T N  I



E .: Assuming a restricted existential quantiﬁcation à la Stalnaker,
which of the LFs in () captures the meaning of () correctly?
Ogihara () argues that the restricted existential quantiﬁcation view is in fact
superior to Partee’s analysis, since Partee’s analysis needs an existential quantiﬁer
anyway. Note that it is clear that the time being referred to is a protracted interval
(the time during which Partee was leaving her house). But the sentence is not
interpreted as saying that this interval is not a time at which she turned oﬀ her
stove, which would have to be a fairly absurd turning-oﬀ-of-the-stove (turning
oﬀ the stove only takes a moment and doesn’t take up a signiﬁcant interval).
Instead, the sentence says that in that salient interval there is no time at which
she turned oﬀ the stove. Clearly, we do need an existential quantiﬁer in there
somewhere and the Priorean theory provides one. Ogihara makes the point
with the following example:
()

John:Did you see Mary?
Bill: Yes, I saw her, but I don’t remember exactly when.

The question and answer in this dialogue concern the issue of whether Bill saw
Mary at some time in a contextually salient interval.

. The Need for Intervals
We have just seen a reason to recognize that natural language can talk not just
about moments of time but also about intervals (connected sets of moments),
which is a fairly trivial fact; after all, what does the year  refer to if not an
interval of time? We have to go even farther, though. It can be shown that we
need the time parameter of the evaluation function to be able to be an interval.
Consider the tenseless clause John build a house and consider a situation where
John starts building a house (the only house he has ever built) on April , 
and ﬁnishes building it on April , . Now, which times do we want to be
times at which “John build a house” is true? If we allow the clause to be true
at moments during the building, we would make it true at other times during
the building (the ones after the ﬁrst times) that John built a house, but that is
wrong. So, the time(s) at which “John build a house” cannot be before April ,
. And clearly, times after April ,  cannot be times at which “John build
a house” is true. So, perhaps, the only time at which “John build a house” is true
 Clearly, the alternative is to say that the existential quantiﬁer is not expressed by tense but
comes from somewhere else, perhaps aspect, perhaps in the lexical meaning of turn oﬀ. We will
not pursue those options here.

Partee  adopts an existential quantiﬁer analysis.

B  T  A



C 

is the moment on April ,  when he ﬁnishes building the house? But then
we would incorrectly predict that on the day before, when he has already been
building the house for almost a year, we can truthfully say that John will build
a house. So, no moment of time can be the time at which “John build a house”
is true. The solution is that the time at which “John build a house” is true is
exactly the interval that starts with the ﬁrst moment of the building project and
ends with the last nail hammered into the wall. Then, we can say before April ,
 that John will build a house and after April , , that John built a house.
What can we say during the building of the house, though? The English present
tense is not correctly used in this circumstance:
()

!!John builds a house.

Our analysis may be read as predicting this fact. Assume that for an unembedded
clause, the time parameter is set to be the speech time. But what is the speech
time? Perhaps, it is the exact interval it takes to utter the particular clause being
evaluated. If so, an example like () can only possibly be true if the speech
interval exactly coincides with the reported event, here the building of the house.
That is, the speaker of () would have to ensure that she starts speaking at
the very ﬁrst moment of John’s building the house, continues speaking rather
slowly, and then ﬁnishes speaking with the very last nail. It is intriguing to note
that sentences like () become acceptable in situations where a sentence is
conceived of as exactly coinciding the event being reported, namely play-by-play
sports commentary (“He passes the ball to Messi”).
What English needs to do instead is to use the progressive:
()

John is building a house.

() expresses that the speech time is included in an interval of John building
a house. Elements that connect the evaluation time to the time at which a
predicate holds are usually called aspectual operators or simply aspects. The
English progressive then is an aspectual expression. We will look closer at its
meaning in a little while.

 We cannot go into this fascinating topic further here, but there is much more to explore about
the peculiar nature of (). Bennett & Partee () assume that the speech time is a moment
and use that assumption to derive the nature of (). Ejerhed () calls the typical use of
(), the “voyeur present”; see also Cooper .

§.

A

.

Aktionsarten



We can distinguish predicates with respect to their temporal proﬁle. The traditional classiﬁcation has four categories:
•
•
•
•

accomplishment predicates
achievement predicates
activity predicates
stative predicates

Accomplishment predicates (build a house, cross the street) describe an event that
has a deﬁned beginning and end (telos, ‘goal’) and takes some amount of time to
ﬁnish. Achievement predicates (reach the summit, notice the problem) also have a
telos but are conceived of as describing an instantaneous event. Accomplishment
predicates and achievement predicates constitute the class of telic predicates.
Activity predicates (run, dance) describe events that are not conceived of as
having a deﬁned goal. Stative predicates (be in New York, know French) describe
states that are true of intervals. The diﬀerence between activity predicates and
stative predicates is often said to turn on whether there is an agent being active
in the described event.
— Read Rothstein : Chapter , pp. – —

. The Progressive
— Read Portner  —

.

Tense in Embedded Clauses

What happens to the time-sensitivity of the verb in a tenseless clause? Consider
ECM complements to verbs of believing:
()

John believed it to be raining.

Evidently, there is some kind of dependency of the time reference in the lower
clause and the higher clause. The simplest approach in our framework would be
to have believe pass down its evaluation time to the lower clause and to assume
that the lower clause doesn’t have a tense operator. Then, whatever time believe
is being interpreted at would be the same time that the lower verb would be
evaluated at.

B  T  A


()

C 

believe w,t = λp s,t .λx. p(w , t), for all worlds w compatible with
what x believes in w at t.

Together with the rest of the system, we predict that () will be true iﬀ there
is a past time t such that it is raining at t in all worlds which conform to what
John believes at t, which seems adequate. Unfortunately, it only seems adequate.
Consider these four worlds:
w rain at am, John awake at am
w rain at am, John awake at am
w rain at am, John awake at am
w rain at am, John awake at am

Assume that in all four worlds, John wakes up, has no idea what time it is, hears a
dripping noise, and says to himself “it is raining (now)”. Which worlds conform
to what John believes at am in w ? In which worlds is it raining at am? Are
the former a subset of the latter? No!
Consider a variant of the story. Everything is the same as above, except that
John wakes up, thinks it is am and says to himself: “It was raining at am.”
Fact: Sentence () is not a true report of John’s beliefs in w in this story. Why
not? There is a description, viz. am, which in fact picks out the time of John’s
thinking, and under which he ascribes rain to that time.
Conclusion: Sentence () unambiguously means that there is a past time t such
that John at t ascribes rain to t under the description “now”. We need to capture
this but the proposal encapsulated in () doesn’t achieve this.
The solution: believe (and other attitude verbs, or perhaps the complementizer
they select) controls not just the world parameter of its prejacent but also the
time parameter.
()

believe w,t = λp s,t .λx. p(w , t ), for all worlds w and t such that
for all that x can tell in w at t, x might be located in w at t .

On this analysis, () means essentially that John located himself at a raining
time. This is intuitively correct.

— More on tense in tensed complement clauses —

§.

T  E C



Supplementary Readings
A nice and gentle introduction to some of the issues discussed in this chapter
comes from Ogihara:
Ogihara, Toshiyuki. . Tense and aspect in truth-conditional semantics.
Lingua (). –. doi:./j.lingua....
Partee’s seminal paper is a must read:
Partee, Barbara H. . Some structural analogies between tenses and pronouns
in English. The Journal of Philosophy (). –. doi:./.
Musan’s work on the pragmatic eﬀects of tense:
Musan, Renate. . Tense, predicates, and lifetime eﬀects. Natural Language
Semantics (). –. doi:./A:.
The three essential works on the progressive:
Dowty, David R. . Toward a semantic analysis of verb aspect and the
english ‘imperfective’ progressive. Linguistics and Philosophy (). –.
doi:./BF.
Landman, Fred. . The progressive. Natural Language Semantics (). –.
doi:./BF.
Portner, Paul. . The progressive in modal semantics. Language ().
–. doi:./.
The ﬁrst chapter of Susan Rothstein’s book on lexical aspect gives a nice overview
of Aktionsarten/aspectual classes:
Rothstein, Susan. . Structuring events: A study in the semantics of lexical aspect Explorations in Semantics. Blackwell. URL http://tinyurl.com/
rothstein-aktionsarten, Chapter : “Verb Classes and Aspectual Classiﬁcation”,
pp. –, available online at http://tinyurl.com/rothstein-aktionsarten.
Concise statements of some of the issues surrounding dependent tenses:
von Stechow, Arnim. . On the proper treatment of tense. Proceedings of
Semantics and Linguistic Theory . URL http://www.sfs.uni-tuebingen.de/
~arnim/Aufsaetze/SALT.pdf.
von Stechow, Arnim. . Tenses in compositional semantics. To be published
in Wolfgang Klein (ed) The Expression of Time in Language. URL http:
//www.sfs.uni-tuebingen.de/~arnim/Aufsaetze/Approaches.pdf.

— T     —

C S
DP  S  M C
We discuss ambiguities that arise when DPs occur in modal contexts.

.

De re vs. De dicto as a Scope Ambiguity 

.

Raised subjects 
..
..

Syntactic “Reconstruction” 

..

.

Examples of de dicto readings for raised subjects 
Some Alternatives to Syntactic Reconstruction 

De re vs. De dicto as a Scope Ambiguity

When a DP appears inside the clausal or VP complement of a modal predicate ,
there is often a so-called de re-de dicto ambiguity. A classic example is (),
which contains the DP a plumber inside the inﬁnitive complement of want.
()

John wants to marry a plumber.

According to the de dicto reading, every possible world in which John gets what
he wants is a world in which there is a plumber whom he marries. According
to the de re reading, there is a plumber in the actual world whom John marries
in every world in which he gets what he wants. We can imagine situations in
which one of the readings is true and the other one false.
For example, suppose John thinks that plumbers make ideal spouses, because
they can ﬁx things around the house. He has never met one so far, but he
 We will be using the terms “modal operator” and “modal predicate” in their widest sense here,
to include modal auxiliaries (“modals”), modal main verbs and adjectives, attitude predicates,
and also modalizing sentence-adverbs like possibly.



DP  S  M C

C 

deﬁnitely wants to marry one. In this scenario, the de dicto reading is true, but
the de re reading is false. What all of John’s desire-worlds have in common is
that they have a plumber getting married to John in them. But it’s not the same
plumber in all those worlds. In fact, there is no particular individual (actual
plumber or other) whom he marries in every one of those worlds.
For a diﬀerent scenario, suppose that John has fallen in love with Robin and
wants to marry Robin. Robin happens to be a plumber, but John doesn’t know
this; in fact, he wouldn’t like it and might even call oﬀ the engagement if he
found out. Here the de re reading is true, because there is an actual plumber,
viz. Robin, who gets married to John in every world in which he gets what he
wants. The de dicto reading is false, however, because the worlds which conform
to John’s wishes actually do not have him marrying a plumber in them. In his
favorite worlds, he marries Robin, who is not a plumber in those worlds.
When confronted with this second scenario, you might, with equal justiﬁcation,
say ‘John wants to marry a plumber’, or ‘John doesn’t want to marry a plumber’.
Each can be taken in a way that makes it a true description of the facts – although,
of course, you cannot assert both in the same breath. This intuition ﬁts well
with the idea that we are dealing with a genuine ambiguity.
Let’s look at another example:
()

John believes that your abstract will be accepted.

Here the relevant DP in the complement clause of the verb believe is your abstract.
Again, we detect an ambiguity, which is brought to light by constructing diﬀerent
scenarios.
 What is behind the Latin terminology “de re” (lit.: ‘of the thing’) and “de dicto” (lit.: ‘of what
is said’)? Apparently, the term “de dicto” is to indicate that on this reading, the words which
I, the speaker, am using to describe the attitude’s content, are the same (at least as far as the
relevant DP is concerned) as the words that the subject herself would use to express her attitude.
Indeed, if we asked the John in our example what he wants, then in the ﬁrst scenario he’d say
“marry a plumber”, but in the second scenario he would not use these words. The term “de re”,
by contrast, indicates that there is a common object (here: Robin) whom I (the speaker) am
talking about when I say “a plumber” in my report and whom the attitude holder would be
referring to if he were to express his attitude in his own words. E.g., in our second scenario, John
might say that he wanted to marry “Robin”, or “this person here” (pointing at Robin). He’d
thus be referring to the same person that I am calling “a plumber”, but wouldn’t use that same
description.
Don’t take this “deﬁnition” of the terms too seriously, though! The terminology is much older
than any precise truth-conditional analysis of the two readings, and it does not, in hindsight,
make complete sense. We will also see below that there are cases where nobody is sure how to
apply the terms in the ﬁrst place, even as purely descriptive labels. So in case of doubt, it is
always wiser to give a longer, more detailed, and less terminology-dependent description of the
relevant truth-conditional judgments.

§.

De re . De dicto   S A



(i) John’s belief may be about an abstract that he reviewed, but since the
abstract is anonymous, he doesn’t know who wrote it. He told me that
there was a wonderful abstract about subjacency in Hindi that is sure to
be accepted. I know that it was your abstract and inform you of John’s
opinion by saying (). This is the de re reading. In the same situation, the
de dicto reading is false: Among John’s belief worlds, there are many worlds
in which your abstract will be accepted is not true or even false. For all he
knows, you might have written, for instance, that terrible abstract about
Antecedent-Contained Deletion, which he also reviewed and is positive
will be rejected.
(ii) For the other scenario, imagine that you are a famous linguist, and John
doesn’t have a very high opinion about the fairness of the abstract selection
process. He thinks that famous people never get rejected, however the
anonymous reviewers judge their submissions. He believes (correctly or
incorrectly – this doesn’t matter here) that you submitted a (unique) abstract.
He has no speciﬁc information or opinion about the abstract’s content and
quality, but given his general beliefs and his knowledge that you are famous,
he nevertheless believes that your abstract will be accepted. This is the de
dicto reading. Here it is true in all of John’s belief worlds that you submitted
a (unique) abstract and it will be accepted. The de re reading of (),
though, may well be false in this scenario. Suppose – to ﬂesh it out further
– the abstract you actually submitted is that terrible one about ACD. That
one surely doesn’t get accepted in every one of John’s belief worlds. There
may be some where it gets in (unless John is certain it can’t be by anyone
famous, he has to allow at least the possibility that it will get in despite its
low quality). But there are deﬁnitely also belief-worlds of his in which it
doesn’t get accepted.
We have taken care here to construct scenarios that make one of the
readings true and the other false. This establishes the existence of two
distinct readings. We should note, however, that there are also many
possible and natural scenarios that simultaneously support the truth of both
readings. Consider, for instance, the following third scenario for sentence
().
(iii) John is your adviser and is fully convinced that your abstract will be
accepted, since he knows it and in fact helped you when you were writing
it. This is the sort of situation in which both the de dicto and the de re
reading are true. It is true, on the one hand, that the sentence your abstract
will be accepted is true in every one of John’s belief worlds (de dicto reading).
And on the other hand, if we ask whether the abstract which you actually
wrote will get accepted in each of John’s belief worlds, that is likewise true
(de re reading).
In fact, this kind of “doubly verifying” scenario is very common when
we look at actual uses of attitude sentences in ordinary conversation. There



DP  S  M C

C 

may even be many cases where communication proceeds smoothly without
either the speaker or the hearer making up their minds as to which of
the two readings they intend or understand. It doesn’t matter, since the
possible circumstances in which their truth-values would diﬀer are unlikely
and ignorable anyway. Still, we can conjure up scenarios in which the two
readings come apart, and our intuitions about those scenarios do support
the existence of a semantic ambiguity.
In the paraphrases by which we have elucidated the two readings of our examples,
we have already given away the essential idea of the analysis that we will adopt:
We will treat de dicto-de re ambiguities as ambiguities of scope. The de dicto
readings, it turns out, are the ones which we predict without further ado if
we assume that the position of the DP at LF is within the modal predicate’s
complement. (That is, it is either in situ or QRed within the complement clause.)
For example:
()

John wants [ [ a plumber] [  to marry t ]]

()

John believes [ the abstract-by-you will-be-accepted]

To obtain the de re readings, we apparently have to QR the DP to a position
above the modal predicate, minimally the VP headed by want or believe.
()

[ a plumber] [ John wants [  to marry t ]]

()

[ the abstract-by-you] [ John believes will-be-accepted]]

E .: Calculate the interpretations of the four structures in ()–(),
and determine their predicted truth-values in each of the (types of ) possible
worlds that we described above in our introduction to the ambiguity.
Some assumptions to make the job easier: (i) Assume that () and () are
evaluated with respect to a variable assignment that assigns John to the number
. This assumption takes the place of a worked out theory of how controlled
PRO is interpreted. (ii) Assume that abstract-by-you is an unanalyzed one-place
predicate. This takes the place of a worked out theory of how genitives with a
non-possessive meaning are to be analyzed.

. Raised subjects
In the examples of de re-de dicto ambiguities that we have looked at so far, the
surface position of the DP in question was inside the modal predicate’s clausal or
VP-complement. We saw that if it stays there at LF, a de dicto reading results,

§.

R 



and if it covertly moves up above the modal operator, we get a de re reading. In
the present section, we will look at cases in which a DP that is superﬁcially higher
than a modal operator can still be read de dicto. In these cases, it is the de re
reading which we obtain if the LF looks essentially like the surface structure, and
the de dicto reading for which we apparently have to posit a non-trivial covert
derivation.

..

Examples of de dicto readings for raised subjects

Suppose I come to my oﬃce one morning and ﬁnd the papers and books on my
desk in diﬀerent locations than I remember leaving them the night before. I say:
()

Somebody must have been here (since last night).

On the assumptions we have been making, somebody is base-generated as the
subject of the VP be here and then moved to its surface position above the modal.
So () has the following S-structure, which is also an interpretable LF.
()

somebody [ λ [ [ must R] [ t have-been-here]]]

What does () mean? The appropriate reading for must here is epistemic, so
suppose the variable R is mapped to the relation λw.λw . w is compatible with
what I believe in w . Let w be the utterance world. Then the truth-condtion
calculated by our rules is as follows.
()

∃x[x is a person in w &
∀w [w is compatible with what I believe in w → x was here in w ]]

But this is not the intended meaning. For () to be true, there has to be a
person who in every world compatible with what I believe was in my oﬃce. In
other words, all my belief-worlds have to have one and the same person coming
to my oﬃce. But this is not what you intuitively understood me to be saying
about my belief-state when I said (). The context we described suggests that I
do not know (or have any opinion about) which person it was that was in my
oﬃce. For all I know, it might have been John, or it might have been Mary, or it
have been this stranger here, or that stranger there. In each of my belief-worlds,
somebody or other was in my oﬃce, but no one person was there in all of them.
I do not believe of anyone in particular that he or she was there, and you did not
understand me to be saying so when I uttered (). What you did understand
me to be claiming, apparently, was not () but ().


()

DP  S  M C

C 

∀w [w is compatible with what I believe in w
→ ∃x [x is a person in w & x was here in w ]]

In other words – to use the terminology we introduced in the last section – the
DP somebody in () appears to have a de dicto reading.
How can sentence () have the meaning in ()? The LF in (), as we saw,
means something else; it expresses a de re reading, which typically is false when
() is uttered sincerely. So there must be another LF. What does it look like
and how is it derived? One way to capture the intended reading, it seems, would
be to generate an LF that’s essentially the same as the underlying structure we
posited for (), i.e., the structure before the subject has raised:
()

[IP e [I [ must R] [ somebody have-been-here]]]

() means precisely () (assuming that the unﬁlled Spec-of-IP position is
semantically vacuous), as you can verify by calculating its interpretation by our
rules. So is () (one of ) the LF(s) for (), and what assumption about syntax
allow it to be generated? Or are there other – perhaps less obvious, but easier to
generate – candidates for the de dicto LF-structure of ()?
Before we get into these question, let’s look at a few more examples. Each of
the following sentences, we claim, has a de dicto reading for the subject, as given
in the accompanying formula. The modal operators in the examples are of a
variety of syntactic types, including modal auxiliaries, main verbs, adjectives, and
adverbs.
()

Everyone in the class may have received an A.
∃w [w conforms to what I believe in w &
∀x[x is in this class in w → x received an A in w ]].

()

At least two semanticists have to be invited.
∀w [w conforms to what is desirable in w
→ ∃ x [x is a semanticist in w & x is invited in w ]].

()

Somebody from New York is expected to win the lottery.
∀w [w conforms to what is expected in w
→ ∃x[x is a person from NY in w & x wins the lottery in w ]]

()

Somebody from New York is likely to win the lottery.
∀w [w is as likely as any other world, given I know in w
→ ∃x[x is a person from NY in w & x wins the lottery in w ]]

 Hopefully the exact analysis of the modal operators likely and probably is not too crucial for

§.
()

R 



One of these two people is probably infected.
∀w [w is as likely as any other world, given what I know in w
→ ∃x[x is one of these two people & x is in infected in w ]]

To bring out the intended de dicto reading of the last example (to pick just one)
imagine this scenario: We are tracking a dangerous virus infection and have
sampled blood from two particular patients. Unfortunately, we were sloppy and
the blood samples ended up all mixed up in one container. The virus count is
high enough to make it quite probable that one of the patients is infected but
because of the mix-up we have no evidence about which one of them it may
be. In this scenario, () appears to be true. It would not be true under a de re
reading, because neither one of the two people is infected in every one of the
likely worlds.
A word of clariﬁcation about our empirical claim: We have been concentrating
on the observation that de dicto readings are available, but have not addressed
the question whether they are the only available readings or coexist with equally
possible de re readings. Indeed, some of the sentences in our list appear to be
ambiguous: For example, it seems that () could also be understood to claim
the present discussion, but you may still be wondering about it. As you see in our formula,
we are thinking of likely (probably) as a kind of epistemic necessity operator, i.e., a universal
quantiﬁer over a set of worlds that is somehow determined by the speaker’s knowledge. (We are
focussing on the “subjective probability” sense of these words. Perhaps there is a also an “objective
probability” reading that is circumstantial rather than epistemic.) What is the diﬀerence then
between likely and e.g. epistemic must (or necessary or I believe that)? Intuitively, ‘it is likely that
p’ makes a weaker claim than ‘it must be the case that p’. If both are universal quantiﬁers, then,
it appears that likely is quantifying over a smaller set than must, i.e., over only a proper subset of
the worlds that are compatible with what I believe. The diﬀerence concerns those worlds that
I cannot strictly rule out but regard as remote possibilities. These worlds are included in the
domain for must, but not in the one for likely. For example, if there was a race between John and
Mary, and I am willing to bet that Mary won but am not completely sure she did, then those
worlds where John won are remote possibilities for me. They are included in the domain of must,
and so I will not say that Mary must have won, but they are not in the domain quantiﬁed over
by likely, so I do say that Mary is likely to have won.
This is only a very crude approximation, of course. For one thing, probability is a gradable
notion. Some things are more probable than others, and where we draw the line between
what’s probable and what isn’t is a vague or context-dependent matter. Even must, necessary etc.
arguably don’t really express complete certainty (because in practice there is hardly anything we
are completely certain of ), but rather just a very high degree of probability. For more discussion
of likely, necessary, and other graded modal concepts in a possible worlds semantics, see e.g.
Kratzer , .
A diﬀerent approach may be that likely quantiﬁes over the same set of worlds as must, but
with a weaker, less than universal, quantiﬁcational force. I.e., ‘it is likely that p’ means something
like p is true in most of the worlds conforming to what I know. A prima facie problem with this
idea is that presumably every proposition is true in inﬁnitely many possible worlds, so how can
we make sense of cardinal notions like ‘more’ and ‘most’ here? But perhaps this can be worked
out somehow.



DP  S  M C

C 

that there is a particular New Yorker who is likely to win (e.g., because he has
bribed everybody). Others arguably are not ambiguous and can only be read de
dicto. This is what von Fintel & Iatridou () claim about sentences like ().
They note that if () also allowed a de re reading, it should be possible to make
coherent sense of ().
()

Everyone in the class may have received an A. But not everybody did.

In fact, () sounds contradictory, which they show is explained if only the
de dicto reading is permitted by the grammar. They conjecture that this is a
systematic property of epistemic modal operators (as opposed to deontic and
other types of modalities). Epistemic operators always have widest scope in their
sentence.
So there are really two challenges here for our current theory. We need to account
for the existence of de dicto readings, and also for the absence, in at least some of
our examples, of de re readings. We will be concerned here exclusively with the
ﬁrst challenge and will set the second aside. We will aim, in eﬀect, to set up the
system so that all sentences of this type are in principle ambiguous, hoping that
additional constraints that we are not investigating here will kick in to exclude
the de re readings where they are missing.
To complicate the empirical picture further, there are also examples where raised
subjects are unambiguously de re. Such cases have been around in the syntactic
literature for a while, and they have recently received renewed attention in the
work of Lasnik and others. To illustrate just one of the systematic restrictions,
negative quantiﬁers like nobody seem to permit only surface scope (i.e., wide
scope) with respect to a modal verb or adjective they have raised over.
()

Nobody from New York is likely to win the lottery.

() does not have a de dicto reading parallel to the one for () above, i.e., it
cannot mean that it is likely that nobody from NY will win. It can only mean
that there is nobody from NY who is likely to win. This too is an issue that we
set aside.
In the next couple of sections, all that we are trying to do is ﬁnd and justify
a mechanism by which the grammar is capable to generate both de re and de
dicto readings for subjects that have raised over modal operators. It is quite
conceivable, of course, that the nature of the additional constraints which often
exclude one reading or the other is ultimately relevant to this discussion and
that a better understanding of them may undermine our conclusions. But this is
something we must leave for further research.

§.

R 

..

Syntactic “Reconstruction”



Given that the de dicto reading of () we are aiming to generate is equivalent to
the formula in (), an obvious idea is that there is an LF which is essentially the
pre-movement structure of this sentence, i.e., the structure prior to the raising of
the subject above the operator. There are a number of ways to make such an LF
available.
One option, most recently defended in Elbourne & Sauerland (), is to
assume that the raising of the subject can happen in a part of the derivation which
only feeds PF, not LF. In that case, the subject simply stays in its underlying
VP-internal position throughout the derivation from DS to LF. (Recall that
quantiﬁers are interpretable there, as they generally are in subject positions.)
Another option is a version of the so-called Copy Theory of movement introduced in Chomsky (). This assumes that movement generally proceeds in
two separate steps, rather than as a single complex operation as we have assumed
so far. Recall that in H& K, it was stipulated that every movement eﬀects the
following four changes:
(i) a phrase α is deleted,
(ii) an index i is attached to the resulting empty node (making it a so-called
trace, which the semantic rule for “Pronouns and Traces” recognizes as a
variable),
(iii) a new copy of α is created somewhere else in the tree (at the “landing site”),
and
(iv) the sister-constituent of this new copy gets another instance of the index i
adjoined to it (which the semantic rule of Predicate Abstraction recognizes
as a binder index).
If we adopt the Copy Theory, we assume instead that there are three distinct
operations:
“Copy”: Create a new copy of α somewhere in the tree, attach an index i to the
original α , and adjoin another instance of i to the sister of the new copy
of α . (= steps (ii), (iii), and (iv) above)
“Delete Lower Copy”: Delete the original α . (= step (i) above)
“Delete Upper Copy”: Delete the new copy of α and both instances of i.
The Copy operation is part of every movement operation, and can happen
anywhere in the syntactic derivation. The Delete operations happen at the end
of the LF derivation and at the end of the PF deletion. We have a choice of
applying either Delete Lower Copy or Delete Upper Copy to each pair of copies,
and we can make this choice independently at LF and at PF. (E.g., we can do
Copy in the common part of the derivation and than Delete Lower Copy at
LF and Delete Upper Copy at PF.) If we always choose Delete Lower Copy at
LF, this system generates exactly the same structures and interpretations as the



DP  S  M C

C 

one from H& K. But if we exercise the Delete Upper Copy option at LF, we are
eﬀectively undoing previous movements, and this gives us LFs with potentially
new interpretations. In the application we are interested in here, we would apply
the Copy step of subject raising before the derivation branches, and then choose
Delete Lower Copy at PF but Delete Upper Copy at LF. The LF will thus look
as if the raising never happened, and it will straightforwardly get the desired de
dicto reading.
If the choice between the two Delete operations is generally optional, we in
principle predict ambiguity wherever there has been movement. Notice, however,
ﬁrst, that the two structures will often be truth-conditionally equivalent (e.g.
when the moved phrase is a name), and second, that they will not always be both
interpretable. (E.g., if we chose Delete Upper Copy after QRing a quantiﬁer
from object position, we’d get an uninterpretable structure, and so this option
is automatically ruled out.) Even so, we predict lots of ambiguity. Speciﬁcally,
since raised subjects are always interpretable in both their underlying and raised
locations, we predict all raising structures where a quantiﬁcational DP has raised
over a modal operator (or over negation or a temporal operator) to be ambiguous.
As we have already mentioned, this is not factually correct, and so there must be
various further constraints that somehow restrict the choices. (Similar comments
apply, of course, to the option we mentioned ﬁrst, of applying raising only on
the PF-branch.)
Yet another solution was ﬁrst proposed by May (): May assumed that QR
could in principle apply in a “downward” fashion, i.e., it could adjoin the moved
phrase to a node that doesn’t contain its trace. Exercising this option with a
raised subject would let us produce the following structure, where the subject
has ﬁrst raised over the modal and then QRed below it.
()

tj λi [ must-R [ someone λj [ ti have been here]]]

As it stands, this structure contains at least one free variable (the trace t j ) and
can therefore not possibly represent any actual reading of this sentence. May
further assumes that traces can in principle be deleted, when their presence is
not required for interpretability. This is not yet quite enough, though to make
() interpretable, at least not within our framework of assumptions, for () is
still not a candidate for an actual reading of ().
()

λi [ must-R [ someone λj [ ti have been here]]]

We would need to assume further that the topmost binder index could be deleted
along with the unbound trace, and also that the indices i and j can be the same,
so that the raising trace t j is bound by the binding-index created by QR. If these

§.

R 



things can be properly worked out somehow, then this is another way to generate
the de dicto reading. Notice that the LF is not exactly the same as on the previous
two approaches, since the subject ends up in an adjoined position rather than in
its original argument position, but this diﬀerence is obviously without semantic
import.
What all of these approaches have in common is that they place the burden of
generating the de dicto reading for raised subjects on the syntactic derivation.
Somehow or other, they all wind up with structures in which the subject is
lower than it is on the surface and thereby falls within the scope of the modal
operator. They also have in common that they take the modal operator (here the
auxiliary, in other cases a main predicate or an adverb) to be staying put. I.e.,
they assume that the de dicto readings are not due to the modal operator being
covertly higher than it seems to be, but to the subject being lower. Approaches
with these features will be said to appeal to “syntactic reconstruction” of the
subject.

.. Some Alternatives to Syntactic Reconstruction
Besides (some version of ) syntactic reconstruction, there are many other ways in
which one try to generate de dicto readings for raised subjects. Here are some
other possibilities that have been suggested and or readily come to mind. We
will see that some of them yield exactly the de dicto reading as we have been
describing it so far, whereas others yield a reading that is very similar but not
quite the same. We will conﬁne ourselves to analyses which involve no or only
minor changes to our system of syntactic and semantic assumptions. Obviously,
if departed from these further, there would be even more diﬀerent options, but
even so, there seem to be quite a few.
. R   ,  :   Conceivably, an LF for
the de dicto reading of () might be derived from the S-structure (=()) by
covertly moving must (and its covert R-argument) up above the subject. This
would have to be a movement which leaves no (semantically non-vacuous) trace.
Given our inventory of composition rules, the only type that the trace could
have to make the structure containing it interpretable would be the type of the
moved operator itself (i.e. st, t ). If it had that type, however, the movement
would be semantically inconsequential, i.e., the structure would mean exactly
 This is a very broad notion of “reconstruction”, where basically any mechanism which puts a
phrase at LF in a location nearer to its underlying site than its surface site is called “reconstruction”.
In some of the literature, the term is used more narrowly. For example, May’s downward QR
is sometimes explicitly contrasted with genuine reconstruction, since it places the quantiﬁer
somewhere else than exactly where it has moved from.



DP  S  M C

C 

the same as (). So this would not be a way to provide an LF for the de dicto
reading. If there was no trace left however (and also no binder index introduced),
we indeed would obtain the de dicto reading.
E .: Prove the claims we just made in the previous paragraph. Why
is no type for the trace other than st, t possible? Why is the movement
semantically inert when this type is chosen? How does the correct intended
meaning arise if there is no trace and binder index?
. R   ,  :     [Requires slightly
modiﬁed inventory of composition rules. Derives an interpretation that is not
quite the same as the de dicto reading we have assumed so far. Rather, it is a
“narrow-Q, R-de-re” interpretation in the sense of Section ?? below.]
. H     ,  :  et, t
this section, read and do the exercise on p./ in H& K]

[Before reading

So far in our discussion, we have taken for granted that the LF which corresponds
to the surface structure, viz. (), gives us the de re reading. This, however, is
correct only on the tacit assumption that the trace of raising is a variable of type
e. If it is part of our general theory that all variables, or at least all interpretable
binder indices (hence all bound variables), in our LFs are of type e, then there
is nothing more here to say. But it is not prima facie obvious that we must or
should make this general assumption, and if we don’t, then the tree in () is not
really one single LF, but the common structure for many diﬀerent ones, which
diﬀer in the type chosen for the trace. Most of the inﬁnitely many semantic
types we might assign to this trace will lead to uninterpretable structures, but
there turns out to be one other choice besides e that works, namely et, t :
()

somebody λ, et,t [ [ must R] [ t, et,t have-been-here]]

() is interpretable in our system, but again, as above, the predicted interpretation is not exactly the de dicto reading as we have been describing it so far, but a
“narrow-Q, R-de-re” reading.
E .: Using higher-type traces to “reverse” syntactic scope-relation is a
trick which can be used quite generally. It is useful to look at a non-intensional
example as a ﬁrst illustration. () contains a universal quantiﬁer and a negation,
and it is scopally ambiguous between the readings in (a) and (b).
()

Everything that glitters is not gold.
a. ∀x[x glitters → ¬x is gold]
b. ¬∀x[x glitters → x is gold]

“surface scope”
“inverse scope”

§.

R 



We could derive the inverse scope reading for () by generating an LF (e.g. by
some version of syntactic reconstruction") in which the every-DP is below not.
Interestingly, however, we can also derive this reading if the every-DP is in its
raised position above not but its trace has the type e, t , t .
Spell out this analysis. (I.e., draw the LF and show how the inverse-scope
interpretation is calculated by our semantic rules.)
E .: Convince yourself that there are no other types for the raising
trace besides e and et, t that would make the structure in () interpretable.
(At least not if we stick exactly to our current composition rules.)
. H     ,  :  s, et, t If we want
to get exactly the de dicto reading that results from syntactic reconstruction out
of a surface-like LF of the form (), we must use an even higher type for the
raising trace, namely s, e, t , t , the type of the intension of a quantiﬁer. As
you just proved in the exercise, this is not possible if we stick to exactly the
composition rules that we have currently available. The problem is in the VP:
the trace in subject position is of type s, e, t , t and its sister is of type e, t .
These two connot combine by either FA or IFA, but it works if we employ
another variant of functional application.
()

Extensionalizing Functional Application (EFA)
If α is a branching node and {β, γ} the set of its daughters, then, for any
world w and assignment g:
if β w,g (w) is a function whose domain contains γ w,g ,
then α w,g = β w,g (w)( γ w,g ).

E .: Calculate the truth-conditions of () under the assumption that
the trace of the subject quantiﬁer is of type s, e, t , t .
C      ? Two of the methods we tried
derived readings in which the raised subject’s quantiﬁcational determiner took
 Notice that the problem here is kind of the mirror image of the problem that led to the
introduction of “Intensional Functional Application” in H& K, ch. . There, we had a function
looking for an argument of type s, t , but the sister node had an extension of type t. IFA allowed
us to, in eﬀect, construct an argument with an added “s” in its type. This time around, we have
to get rid of an “s” rather than adding one; and this is what EFA accomplishes.
So we now have three diﬀerent “functional application”-type rules altogether in our system:
ordinary FA simply applies β w to γ w ; IFA applies β w to λw . γ w ; and EFA applies
β w (w) to γ w . At most one of them will be applicable to each given branching node,
depending on the type of γ w .
Think about the situation. Might there be other variant functional application rules?



DP  S  M C

C 

scope below the world-quantiﬁer in the modal operator, but the raised subject’s
restricting NP still was evaluated in the utterance world (or the evaluation world
for the larger sentence, whichever that may be). It is diﬃcult to assess whether
these readings are actually available for the sentences under consideration, and
we will postpone this question to a later section. We would like to argue here,
however, that even if these readings are available, they cannot be the only readings
that are available for raised subjects besides their wide-scope readings. In other
words, even if we allowed one of the mechanisms that generated these sort of
hybrid readings, we would still need another mechanism that gives us, for at
least some examples, the “real” de dicto readings that we obtain e.g. by syntactic
reconstruction. The relevant examples that show this most clearly involve DPs
with more descriptive content than somebody and whose NPs express clearly
contingent properties.
()

A neat-freak must have been here.

If I say this instead of our original () when I come to my oﬃce in the morning
and interpret the clues on my desk, I am saying that every world compatible
with my beliefs is such that someone who is a neat-freak in that world was here
in that world. Suppose there is a guy, Bill, whom I know slightly but not well
enough to have an opinion on whether or not he is neat. He may or not be, for
all I know. So there are worlds among my belief worlds where he is a neat-freak
and worlds where he is not. I also don’t have an opinion on whether he was or
wasn’t the one who came into my oﬃce last night. He did in some of my belief
worlds and he didn’t in others. I am implying with (), however, that if Bill
isn’t a neat-freak, then it wasn’t him in my oﬃce. I.e., () is telling you that,
even if I have belief-worlds in which Bill is a slob and I have belief-worlds in
which (only) he was in my oﬃce, I do not have any belief-worlds in which Bill is
a slob and the only person who was in my oﬃce. This is correctly predicted if
() expresses the “genuine” de dicto reading in (), but not if it expresses the
“hybrid” reading in ().
()

∀w [w is compatible with what I believe in w →
∃x[x is a neatfreak in w and x was here in w ]]

()

∀w [w is compatible with what I believe in w →
∃x[x is a neatfreak in w and x was here in w ]]

We therefore conclude the mechanisms  and  considered above (whatever
there merits otherwise) cannot supplant syntactic reconstruction or some other
mechanism that yields readings like ().
This leaves only the ﬁrst and fourth options that we looked at as potential competitors to syntactic reconstruction, and we will focus the rest of the discussion
on how we might be able to tease apart the predictions that these mechanisms
imply from the ones of a syntactic reconstruction approach.

§.

R 



As for moving the modal operator, there are no direct bad predictions that we are
aware of with this. But it leads us to expect that we might ﬁnd not only scope
ambiguities involving a modal operator and a DP, but also scope ambiguities
between two modal operators, since one of them might covertly move over the
other. It seems that this never happens. Sentences with stacked modal verbs
seem to be unambiguous and show only those readings where the scopes of the
operators reﬂect their surface hierarchy.
()

a. I have to be allowed to graduate.
b. #I am allowed to have to graduate.

Of course, this might be explained by appropriate constraints on the movement
of modal operators, and such constraints may even come for free in a the right
synatctic theory. Also, we should have a much more comprehensive investigation
of the empirical facts before we reach any verdict. If it is true, however, that
modal operators only engage in scope interaction with DPs and never with each
other, then a theory which does not allow any movement of modals at all could
claim the advantage of having a simple and principled explanation for this fact.
What about the “semantic reconstruction” option, where raised subjects can leave
traces of type s, et, t and thus get narrow scope semantically without ending
up low syntactically? This type of approach has been explored quite thoroughly
and defended with great sophistication. We can only sketch the main objections
to it here and must leave it to the reader to consult the literature for an informed
opinion.

S   C C An example from Fox () (building on Lebeaux  and Heycock ):
()

a.
b.

A student of his seems to David to be at the party.
OK
de re, OK de dicto
A student of David’s seems to him to be at the party.
OK
de re, *de dicto

Sketch of argument: If Cond. C is formulated in terms of c-command relations
and applies at LF, it will distinguish between de re and de dicto readings only if
those involve LFs with diﬀerent hierarchical relations.

R   
()
()

The cat seems to be out of the bag.
?Advantage might have been taken of them.



DP  S  M C

C 

Sketch of argument: If idioms must be constituents at LF in order to receive
their idiomatic interpretations, these cases call for syntactic reconstruction. An
additional mechanism of semantic reconstruction via high-type traces is then at
best redundant.
Tentative conclusion: Syntactic reconstruction (some version of it) provides the
best account of de dicto readings for raised subjects.

C E
B de re — de dicto : T T R
In this chapter, we will see that quantiﬁcational noun phrases in the
scope of a modal operator can receive a reading where their restrictive
predicate is not interpreted in the worlds introduced by the modal
operator (which is what happens in de re readings as well) while at
the same time their quantiﬁcational force takes scope below the modal
operator (which is what happens in de dicto readings as well). This
seemingly paradoxical situation might force whole-sale revisions to our
architecture. We discuss the standard solution (which involves supplying
predicates with world-arguments) and some alternatives.

.

A Problem: Additional Readings and Scope Paradoxes 

.

The Standard Solution: Overt World Variables 
..
..

Lexical entries 

..

Composition Rules 

..

Syntax 

..

The Need for a Binding Theory for World Variables 

..

Two Kinds of World Pronouns 

..
.

Semantic Values 

Excursus: Semantic reconstruction for de dicto
raised subjects? 

Alternatives to Overt World Variables 
..

Indexed Operators 

..

Scoping After All? 

.

Scope, Restrictors, and the Syntax of Movement 

.

A Recurring Theme: Historical Overview 



.

B de re — de dicto : T T R

C 

A Problem: Additional Readings and Scope
Paradoxes

Janet Dean Fodor discussed examples like () in her dissertation ().
()

Mary wanted to buy a hat just like mine.

Fodor observes that () has three readings, which she labels “speciﬁc de re,”
“non-speciﬁc de re,” and “non-speciﬁc de dicto.”
(i) On the “speciﬁc de re” reading, the sentence says that there is a particular
hat which is just like mine such that Mary has a desire to buy it. Say, I
am walking along Newbury Street with Mary. Mary sees a hat in a display
window and wants to buy it. She tells me so. I don’t reveal that I have one
just like it. But later I tell you by uttering ().
(ii) On the “non-speciﬁc de dicto” reading, the sentence says that Mary’s desire
was to buy some hat or other which fulﬁlls the description that it is just
like mine. She is a copycat.
(iii) On the “non-speciﬁc de re” reading, ﬁnally, the sentence will be true, e.g.,
in the following situation: Mary’s desire is to buy some hat or other, and
the only important thing is that it be a Red Sox cap. Unbeknownst to her,
my hat is one of those as well.
The existence of three diﬀerent readings appears to be problematic for the scopal
account of de re-de dicto ambiguities that we have been assuming. It seems that
our analysis allows just two semantically distinct types of LFs: Either the DP
a hat just like mine takes scope below want, as in (), or it takes scope above
want, as in ().
()

Mary wanted [ [a hat-just-like-mine] [  to buy t ]]

()

[a hat-just-like-mine] [ Mary wanted [  to buy t ]]

In the system we have developed so far, () says that in every world w in which
Mary gets what she wants, there is something that she buys in w that’s a hat in
w and like my hat in w . This is Fodor’s “non-speciﬁc de dicto” reading. (),
on the other hand, says that there is some thing x which is a hat in the actual
world and like my hat in the actual world, and Mary buys x in every one of her
desire worlds. That is Fodor’s “speciﬁc de re.” But what about the “non-speciﬁc
de re”? To obtain this reading, it seems that we would have to evaluate the
predicate hat just like mine in the actual world, so as to obtain its actual extension
(in the scenario we have sketched, the set of all Red Sox caps). But the existential

§.

A P: A R  S P



quantiﬁer expressed by the indeﬁnite article in the hat-DP should not take scope
over the modal operator want, but below it, so that we can account for the fact
that in diﬀerent desire-worlds of Mary’s, she buys possibly diﬀerent hats.
There is a tension here: one aspect of the truth-conditions of this reading
suggests that the DP a hat just like mine should be outside of the scope of want,
but another aspect of these truth-conditions compels us to place it inside the
scope of want. We can’t have it both ways, it would seem, which is why this has
been called a “scope paradox”
Another example of this sort, due to Bäuerle (), is ():
()

Georg believes that a woman from Stuttgart loves every member of the
VfB team.

Bäuerle describes the following scenario: Georg has seen a group of men on
the bus. This group happens to be the VfB team (Stuttgart’s soccer team), but
Georg does not know this. Georg also believes (Bäuerle doesn’t spell out on
what grounds) that there is some woman from Stuttgart who loves every one
of these men. There is no particular woman of whom he believes that, so there
are diﬀerent such women in his diﬀerent belief-worlds. Bäuerle notes that ()
can be understood as true in this scenario. But there is a problem in ﬁnding
an appropriate LF that will predict its truth here. First, since there are diﬀerent
women in diﬀerent belief-worlds of Georg’s, the existential quantiﬁer a woman
from Stuttgart must be inside the scope of believe. Second, since (in each belief
world) there aren’t diﬀerent women that love each of the men, but one that loves
them all, the a-DP should take scope over the every-DP. If the every-DP is in
the scope of the a-DP, and the a-DP is in the scope of believe, then it follows
that the every-DP is in the scope of believe. But on the other hand, if we want to
capture the fact that the men in question need not be VfB-members in Georg’s
belief-worlds, the predicate member of the VfB team needs to be outside of the
scope of believe. Again, we have a “scope paradox”.
Before we turn to possible solutions for this problem, let’s have one more
example:
()

Mary hopes that a friend of mine will win the race.

This again seems to have three readings. In Fodor’s terminology, the DP a friend
of mine can be “non-speciﬁc de dicto,” in which case () is true iﬀ in every
world where Mary’s hopes come true, there is somebody who is my friend and
wins. It can also have a “speciﬁc de re” reading: Mary wants John to win, she
doesn’t know John is my friend, but I can still report her hope as in (). But
there is a third option, the “non-speciﬁc de re” reading. To bring out this rather



B de re — de dicto : T T R

C 

exotic reading, imagine this: Mary looks at the ten contestants and says I hope
one of the three on the right wins - they are so shaggy - I like shaggy people. She
doesn’t know that those are my friends. But I could still report her hope as in
().

.

The Standard Solution: Overt World Variables

The scope paradoxes we have encountered can be traced back to a basic design
feature of our system of intensional semantics: the relevant “evaluation world”
for each predicate in a sentence is strictly determined by its LF-position. All
predicates that occur in the (immediate) scope of the same modal operator must
be evaluated in the same possible worlds. E.g. if the scope of want consists of
the clause a friend of mine (to) win, then every desire-world w will be required
to contain an individual that wins in w and is also my friend in w . If we want
to quantify over individuals that are my friends in the actual world (and not
necessarily in all the subject’s desire worlds), we have no choice but to place
friend of mine outside of the scope of want. And if we want to accomplish this
by means of QR, we must move the entire DP a friend of mine.
Not every kind of intensional semantics constrains our options in this way. One
way to visualize what we might want is to write down an LF that looks promising:
()

Mary wantedw [λw [ a hat-just-like-mine w ]λx [  to buyw x ]]

We have annotated each predicate with the world in which we wish to evaluate
it. w is the evaluation world for the entire sentence and it is the world in
which we evaluate the predicates want and hat-just-like-mine. The embedded
sentence contributes a function from worlds to truth-values and we insert an
explicit λ-operator binding the world where the predicate buy is evaluated. The
crucial aspect of () is that the world in which hat-just-like-mine is evaluated is
the matrix evaluation world and not the same world in which its clause-mate
predicate buy is evaluated. This LF thus looks like it might faithfully capture
Fodor’s third reading.
Logical forms with overt world variables such as () are in fact the standard
solution to the problem presented by the third reading. Let us spell out some of
the technicalities. Later, we will consider a couple of alternatives.

§.

T S S: O W V

..

Semantic Values



In this new system, we do not relativize the interpretation function to a possible
world. As in the old extensional system, the basic notion is just “ α ,” i.e., “the
semantic value of α”. (Or “ α g ,” “the semantic value of α under assignment
g”, if α contains free variables.) However, semantic values are no longer always
extensions; some of them still are, but others are intensions. Here are some
representative examples of the types of semantic values for various kinds of words.

..

Lexical entries

()

a.
b.
c.
d.

smart = λw ∈ Ds . λx ∈ De . x is smart in w
likes = λw ∈ Ds . λx ∈ De . λy ∈ De . y likes x in w
teacher = λw ∈ Ds . λx ∈ De . x is a teacher in w
friend = λw ∈ Ds . λx ∈ De . λy ∈ De . y is x’s friend in w

()

a.

believe = λw ∈ Ds . λp ∈ D s,t . λx ∈ D.
∀w [w conforms to what x believes in w → p(w ) = ]
must = λw ∈ Ds . λR ∈ D s,st . λp ∈ D s,t .
∀w [R(w)(w ) =  → p(w ) = ]

b.
()

a.
b.
c.
d.

Ann =Ann
and = λu ∈ Dt . [λv ∈ Dt . u = v = ]
the = λf ∈ D e,t : ∃!x. f(x) = . the y such that f(y) = .
every = λf ∈ D e,t . λg ∈ D e,t . ∀x[f(x) =  → g(x) = ]

The entries in () (for words whose extensions are constant across worlds)
have stayed the same; their semantic values are still extensions. But the ones for
predicates (ordinary ones and modal ones) in () and () have changed; these
items now have as their semantic values what used to be their intensions.

..

Composition Rules

We abolish the special rule of Intensional Functional Application (IFA) and
go back to our old inventory of Functional Application, λ-Abstraction, and
Predicate Modiﬁcation .
 We also abolish the Extensional Functional Application rule (EFA), if we had that one (see
section .. “Semantic Reconstruction”).
 Actually, PM requires a slightly revised formulation: α β g = λw ∈ Ds . λx ∈
De . α g (w)(x) = β g (w)(x) = . But we will not be concerned with the compositional
interpretation of modiﬁer-structures here, so you won’t be needing this rule.



..

B de re — de dicto : T T R

C 

Syntax

What we have at this point does not allow us to interpret even the simplest
syntactic structures. For instance, we can’t interpret the tree in ().
()

[ VP John leave]

The verb’s type is s, et , so it’s looking for a sister node which denotes a world.
John, which denotes an individual, is not a suitable argument.
We get out of this problem by positing more abstract syntactic structures (at the
LF level). Speciﬁcally, we assume that there is a set of covert “world pronouns”
which are generated as sisters to all lexical predicates in LF-structures. Oﬃcially,
the variable would be a pair of an index and the type s. Inoﬃcially, we will use
“w” with a subscripted index, with the understanding that the “w” indicates we
are dealing with a variable of type s. So, the syntax would generate something
like ():
()

[ John [ leave w ]]

The sentence would then obviously have an assignment-dependent extension (a
truth-value), depending on what world the variable assignment assigns to the
world variable with index . In our intensional system of Chapter  — , we
were assuming the following principle:
()

An utterance of a sentence (=LF) φ in world w is true iﬀ φ

w

= .

To achieve the same in our new system, we would have to ensure that the variable
assignment assign the utterance world to the free world variable(s) in the sentence.
Another possibility, which we will adopt here, is to introduce a variable binder
on top of the sentence. We will assume the following kind of syntactic structure
at LF:
()

[λw [ John [ leave w ]]]

The sentence now has as its extension what used to be its intension, a proposition.
The principle of utterance truth now is this:
()

An utterance of a sentence (=LF) φ in world w is true iﬀ φ (w) = .

Now, we have to look at more complex sentences. First, a simple case of
embedding. The sentence is John wants to leave, which now as an LF like this:

§.

T S S: O W V

()

[ λw [ John [[ wants w [[ λw [  [ leave w ]]]]]]]]



E .: Calculate the semantic value of ().
Next, look at an example involving a complex subject, such as the teacher left.
()

[ λw [[ the [ teacher w ]][ left w ]]]

The verb will need a world argument as before. The noun teacher will likewise
need one, so that the can get the required argument of type e, t (not s, et !).
If we co-index the two world variables, we derive as the semantic value for ()
what its intension would have been in old system. But nothing we have said
forces us to co-index the two world variables, which is what will allow us to
derive the third reading for relevant examples.
Consider what happens when the sentence contains both a modal operator and
a complex DP in its complement.
()

Mary wants a friend of mine to win.

There are now three predicates that need world arguments. Furthermore, there
will be two λ-operators binding world variables. We can now represent the
three readings (to make the structures more readable, we’ll leave oﬀ most of the
bracketing and start writing the world arguments as subscripts to the predicates):
()

a.
b.
c.

non-speciﬁc de dicto:
λw Mary wantsw [λw a friend-of-minew leavew ]
speciﬁc de re:
λw [a friend-of-minew ]λ x Mary wantsw [λw x leavew ]]
non-speciﬁc de re:
λw Mary wantsw [λw a friend-of-minew leavew ]

In this new framework, then, we have a way of resolving the apparent “scope
paradoxes” and of acknowledging Fodor’s point that there are two separate
distinctions to be made when DPs interact with modal operators. First, there is
the scopal relation between the DP and the operator; the DP may take wider
scope (Fodor’s “speciﬁc” reading) or narrower scope (“non-speciﬁc” reading) than
the operator. Second, there is the choice of binder for the world-argument of
the DP’s restricting predicate; this may be cobound with the world-argument of
the embedded predicate (Fodor’s “de dicto”) or with the modal operator’s own
world-argument (“de re”). So the de re-de dicto distinction in the sense of Fodor
is not per se a distinction of scope; but it has a principled connection with scope
in one direction: Unless the DP is within the modal operator’s scope, the de



B de re — de dicto : T T R

C 

dicto option (= co-binding the world-pronoun with the embedded predicate’s
world-argument) is in principle unavailable. (Hence “speciﬁc” implies “de re”,
and “de dicto” implies “non-speciﬁc”.) But there is no implication in the other
direction: if the DP has narrow scope w.r.t. to the modal operator, either the
local or the long-distance binding option for its world-pronoun is in principle
available. Hence “non-speciﬁc” readings may be either “de re” or “de dicto”.
For the sake of clarity, we should introduce a diﬀerent terminology than Fodor’s.
The labels “speciﬁc” and “non-speciﬁc” especially have been used in so many
diﬀerent senses by so many diﬀerent people that it is best to avoid them altogether.
So we will refer to Fodor’s “speciﬁc readings” and “non-speciﬁc readings” as
“wide-quantiﬁcation readings” and “narrow-quantiﬁcation readings”, or “narrowQ/wide-Q readings” for short. For the distinction pertaining to the interpretation
of the restricting NP, we will keep the terms “de re” and “de dicto”, but will
amplify them to “restrictor-de re” and “restrictor-de dicto” (“R-de re”/”R-de
dicto”).
E .: For DPs with extensions of type e (speciﬁcally, DPs headed by
the deﬁnite article), there is a truth-conditionally manifest R-de re/R-de dicto
distinction, but no truth-conditionally detectable wide-Q/narrow-Q distinction.
In other words, if we construct LFs analogous to (a-c) above for an example
with a deﬁnite DP, we can always prove that the ﬁrst option (wide scope DP)
and the third option (narrow scope DP with distantly bound world-pronoun)
denote identical propositions. In this exercise, you are asked to show this for the
example in ().
()

John believes that your abstract will be accepted.

.. The Need for a Binding Theory for World Variables
One could in principle imagine some indexings of our LFs that we have not
considered so far. The following LF indexes the predicate of the complement
clause to the matrix λ-operator rather than to the one on top of its own clause.
()

λw John wantsw [λw  leavew ]

Of course, the resulting semantics would be pathological: what John would be
claimed to stand in the wanting relation to is a set of worlds that is either the
entire set W of possible worlds (if the evaluation world is one in which John
leaves) or the empty set (if the evaluation world is one in which John doesn’t
leave). Clearly, the sentence has no such meaning. Do we need to restrict our
system to not generate such an LF? Perhaps not, if the meaning is so absurd that

§.

T S S: O W V



the LF would be ﬁltered out by some overarching rules distinguishing sense from
nonsense.
But the problem becomes real when we look at more complex examples. Here is
one discussed by Percus in important work (Percus ):
()

Mary thinks that my brother is Canadian.

Since the subject of the lower clause is a type e expression, we expect at least two
readings: de dicto and de re, cf. Exercise .. The two LFs are as follows:
()

a.
b.

de dicto
λw Mary thinksw [ (that) λw my brotherw (is) Canadianw ]
de re
λw Mary thinksw [ (that) λw my brotherw (is) Canadianw ]

But as Percus points out, there is another indexing that might be generated:
()

λw Mary thinksw [ (that) w my brotherw (is) Canadianw ]

In (), we have co-indexed the main predicate of the lower clause with the
matrix λ-operator and co-indexed the nominal predicate brother with the embedded λ-operator. That is, in comparison with the de re reading in (b), we
have just switched around the indices on the two predicates in the lower clause.
Note that this LF will not lead to a pathological reading. So, is the predicted
reading one that the sentence actually has? No. For the de re reading, we can
easily convince ourselves that the sentence does have that reading. Here is Percus’
scenario: “My brother’s name is Allon. Suppose Mary thinks Allon is not my
brother but she also thinks that Allon is Canadian.” In such a scenario, our
sentence can be judged as true, as predicted if it can have the LF in (b). But
when we try to ﬁnd evidence that () is a possible LF for our sentence, we fail.
Here is Percus:
If the sentence permitted a structure with this indexing, we would
take the sentence to be true whenever there is some actual Canadian
who Mary thinks is my brother — even when this person is not my
brother in actuality, and even when Mary mistakenly thinks that he
is not Canadian. For instance, we would take the sentence to be
true when Mary thinks that Pierre (the Canadian) is my brother and
naturally concludes — since she knows that I am American — that
Pierre too is American. But in fact we judge the sentence to be false
on this scenario, and so there must be something that makes the



B de re — de dicto : T T R

C 

indexing in () impossible.
Percus then proposes the following descriptive generalization:
()

G X: The situation pronoun that a verb selects for must
be coindexed with the nearest λ above it.

We expect that there will need to be a lot of work done to understand the deeper
sources of this generalization. For fun, we oﬀer the following implementation
(devised by Irene Heim).

.. Two Kinds of World Pronouns
We distinguish two syntactic types of world-pronouns. One type, w-, behaves
like relative pronouns and  in the analysis of H& K, ch. . (pp. ﬀ.): it
is semantically vacuous itself, but can move and leave a trace that is a variable.
The only diﬀerence between w- and  is that the latter leaves a variable
of type e when it moves, whereas the former leaves a variable of type s. The
other type of world-pronoun, w-pro, is analogous to bound-variable personal
pronouns, i.e., it is itself a variable (here of type s). Like a personal pronoun, it
can be coindexed with the trace of an existing movement chain.
With this inventory of world-pronouns, we can capture the essence of Generalization X by stipulating that w-pro is only generated in the immediate scope
of a determiner (i.e., as sister to the determiner’s argument). Everywhere else
where a world-pronoun is needed for interpretability, we must generate a w-
and move it. This (with some tacit assumptions left to the reader to puzzle over)
derives the result that the predicates inside nominals can be freely indexed but
that the ones inside predicates are captured by the closest λ-operator.
As we said, there is plenty more to be explored in the Binding Theory for world
pronouns. The reader is referred to the paper by Percus and the references he
cites.

.. Excursus: Semantic reconstruction for de dicto raised
subjects?
Let us look back at the account of de dicto readings of raised subjects that we
sketched earlier in Section ... We showed that you can derive such readings
 Percus works with situation pronouns rather than world pronouns, an immaterial diﬀerence
for our purposes here.

§.

T S S: O W V



by positing a high type trace for the subject raising, a trace of type s, et, t .
Before the lower predicate can combine with the trace, the semantic value of the
trace has to be extensionalized by being applied to the lower evaluation world
(done via the EFA composition principle). Upstairs the raised subject has to
be combined with the λ-abstract (which will be of type s, et, t , t ) via its
intension.
We then saw recently discovered data suggesting that syntactic reconstruction is
actually what is going on. This, of course, raises the question of why semantic
reconstruction is unavailable (otherwise we wouldn’t expect the data that we
observed).
Fox (: p. , fn. ) mentions two possible explanations:
(i) “traces, like pronouns, are always interpreted as variables that range over
individuals (type e)”,
(ii) “the semantic type of a trace is determined to be the lowest type compatible
with the syntactic environment (as suggested in Beck ())”.
In this excursus, we will brieﬂy consider whether our new framework has something to say about this issue. Let’s ﬁgure out what we would have to do in the new
framework to replicate the account in the section on semantics reconstruction.
Downstairs, we would have a trace of type s, et, t . To calculate its extension,
we do not need recourse to a special composition principle, but can simply give it
a world-argument (co-indexed with the abstractor resulting from the movement
of the w- in the argument position of the lower verb).
Now, what has to happen upstairs? Well, there we need the subject to be of type
s, et, t , the same type as the trace, to make sure that its semantics will enter
the truth-conditions downstairs. But how can we do this?
We need the DP somebody from New York to have as its semantic value an intension, the function from any world to the existential quantiﬁer over individuals
who are people from New York in that world. This is actually hard to do in our
system. It would be possible if (i) the predicate(s) inside the DP received w-
as their argument, and if (ii) that w- were allowed to moved to adjoin to the
DP. If we manage to rule out at least one of the two preconditions on principled
grounds, we would have derived the impossibility of semantic reconstruction as
a way of getting de dicto readings of raised subjects.
(i) may be ruled out by the Binding Theory for world pronominals, when it
gets developed.
(ii) may be ruled out by principled considerations as well. Perhaps, worldabstractors are only allowed at sentential boundaries. See Larson () for
some discussion of recalcitrant cases, one of which is the object position of
so-called intensional transitive verbs, the topic of another section.



B de re — de dicto : T T R

C 

. Alternatives to Overt World Variables
We presented (a variant of ) what is currently the most widely accepted solution to
the scope paradoxes, which required the use of non-locally bound world-variables.
There are some alternatives, one of which is to some extent a “notational variant”,
the others involved syntactic scoping after all.

..

Indexed Operators

It is possible to devise systems where predicates maintain the semantics we originally gave them, according to which they are sensitive to a world of evaluation
parameter. The freedom needed to account for the third reading and further
facts would be created by assuming more sophisticated operators that shift the
evaluation world. Here is a toy example:
()

Mary wants [ a [  friend-of-mine ] leave ]

The idea is that the  “temporarily” shifts the evaluation world back to
what it was “before” the abstraction over worlds triggered by want happened.
This kind of system can be spelled out in as much detail as the world-variable
analysis. Cresswell () proves that the two systems are equivalent in their
expressive power. The decision is therefore a syntactic one. Does natural
language have a multitude of indexed world-shifters or a multitude of indexed
world-variables? Cresswell suspects the former, as did Kamp () who wrote:
I of course exclude the possibility of symbolizing the sentence by
means of explicit quantiﬁcation over moments. Such a symbolization
would certainly be possible; and it would even make the operators P
and F superﬂuous. Such symbolizations, however, are a considerable
departure from the actual form of the original sentences which they
represent — which is unsatisfactory if we want to gain insight into
the semantics of English. Moreover, one can object to symbolizations
involving quantiﬁcation over such abstract objects as moments, if
these objects are not explicitly mentioned in the sentences that are
to be symbolized.
There is some resistance to world-time variables because they are not phonetically
realized. But in an operator-based system, we’ll have non-overt operators all over
the place. So, there is no a priori advantage for either system. We will stick with
the more transparent LFs with world variables.

§.

A  O W V

..

Scoping After All?



Suppose we didn’t give up our previous framework, in which the evaluationworld for any predicate was strictly determined by its LF-position. It turns out
that there is a way (actually, two ways) to derive Fodor’s non-speciﬁc de re reading
in that framework after all.
Recall again what we need. We need a way to evaluate the restrictive predicate
of a DP with respect to the higher evaluation world while at the same time
interpreting the quantiﬁcational force of the DP downstairs in its local clause.
We saw that if we move the DP upstairs, we get the restriction evaluated upstairs
but we also have removed the quantiﬁer from where it should exert its force.
And if we leave the DP downstairs where its quantiﬁcational forces is felt, its
restriction is automatically evaluated down there as well. That is why Fodor’s
reading is paradoxical for the old framework. In fact, though there is no paradox.
Way  Raise the DP upstairs but leave a e, t , t trace. This way the restriction
is evaluated upstairs, then a quantiﬁer extension is calculated, and that
quantiﬁer extension is transmitted to trace position. This is just what we
needed.
Way  Move the NP-complement of a quantiﬁcational D independently of the
containing DP. Then we could generate three distinct LFs for a sentence
like Mary wants a friend of mine to win: two familiar ones, in which the
whole DP a friend of mine is respectively inside and outside the scope
of want, plus a third one, in which the NP friend of mine is outside the
scope of want but the remnant DP a [NP t] has been left behind inside
it:
()

[ [ NP f-o-m] λ [ Mary [ want [ [ DP a t e,t , ] win]]]]

E .: Convince yourself that this third LF represents the narrowquantiﬁcation, restrictor-de re reading (Fodor’s “non-speciﬁc de re").
We have found, then, that it is in principle possible after all to account for narrowQ R-de re readings within our original framework of intensional semantics.
E .: In (), we chose to annotate the trace of the movement of
the NP with the type-label e, t , thus treating it as a variable whose values are
predicate-extensions (characteristic functions of sets of individuals). As we just
saw, this choice led to an interpretable structure. But was it our only possible
choice? Suppose the LF-structure were exactly as in (), except that the
 Something like this was proposed by Groenendijk & Stokhof () in their treatment of
questions with which-DPs.



B de re — de dicto : T T R

C 

trace had been assigned type s, et instead of e, t . Would the tree still be
interpretable? If yes, what reading of the sentence would it express?
E .: We noted in the previous section about the world-pronouns framework that there was a principled reason why restrictor-de dicto readings necessarily
are narrow-quantiﬁcation readings. (Or, in Fodor’s terms, why there is no such
thing as a “speciﬁc de dicto” reading.) In that framework, this was simply a consequence of the fact that bound variables must be in the scope of their binders.
What about the alternative account that we have sketched in the present section?
Does this account also imply that R-de dicto readings are necessarily narrow-Q?

.

Scope, Restrictors, and the Syntax of
Movement

To conclude our discussion of the ambiguities of DPs in the complements of
modal operators, let us consider some implications for the study of LF-syntax.
This will be very inconclusive.
Accepting the empirical evidence for the existence of narrow-Q R-de re readings
which are truth-conditionally distinct from both the wide-Q R-de re and the
narrow-Q R-de dicto readings, we are facing a choice between two types of
theories. One theory, which we have referred to as the “standard” one, uses a
combination of DP-movement and world-pronoun binding; it maintains that
wide-quantiﬁcation readings really do depend on (covert) syntactic movement,
but de re interpretations of the restrictor do not. The other theory, which we may
dub the “scopal” account, removes the restrictor from the scope of the modal
operator, either by QR (combined with an et, t type trace) or by movement of
the NP-restrictor by itself.
In order to adjudicate between these two competing theories, we may want to
inquire whether the R-de re — de dicto distinction exhibits any of the properties
that current syntactic theory would take to be diagnostic of movement. This is a
very complex enterprise, and the few results to have emerged so far appear to be
pointing in diﬀerent directions.
We have already mentioned that it is questionable whether NPs that are complements to D can be moved out of their DPs. Even if it is possible, we might
expect this movement to be similar to the movement of other predicates, such
as APs, VPs, and predicative NPs. Such movements exist, but — as discussed
by Heycock, Fox, and the sources they cite — they typically have no eﬀect on
semantic interpretation and appear to be obligatorily reconstructed at LF. The

§.

S, R,   S  M



type of NP-movement required by the purely scopal theory of R-de re readings
would be exceptional in this respect.
Considerations based on the locality of uncontroversial instances of QR provide
another reason to doubt the plausibility of the scopal theory. May () argued,
on the basis of examples like (), that quantiﬁers do not take scope out of
embedded tensed clauses.
()

a.
b.

Some politician will address every rally in John’s district.
Some politician thinks that he will address every rally in John’s
district.

While in (a) the universal quantiﬁer can take scope over the existential
quantiﬁer in subject position, this seems impossible in (b), where the universal
quantiﬁer would have to scope out of its ﬁnite clause. Therefore, May suggested,
we should not attribute the de re reading in an example like our () to the
operation of QR.
()

John believes that your abstract will be accepted.

As we saw above, the standard theory which appeals to non-locally bound worldpronouns does have a way of capturing the de re reading of () without any
movement, so it is consistent with May’s suggestion. The purely scopal theory
would have to say something more complicated in order to reconcile the facts
about () and (). Namely, it might have to posit that DP-movement is
ﬁnite-clause bound, but NP-movement is not. Or, in the other version, it would
have to say that QR can escape ﬁnite clauses but only if it leaves a et, t type
trace.
Both theories, by the way, have a problem with the fact that May’s ﬁnite-clauseboundedness does not appear to hold for all quantiﬁcational DPs alike. If we
look at the behavior of every, no, and most, we indeed can maintain that there is
no DP-movement out of tensed complements. For example, () could mean
that Mary hopes that there won’t be any friends of mine that win. Or it could
mean (with suitable help from the context) that she hopes that there is nobody
who will win among those shaggy people over there (whom I describe as my
friends). But it cannot mean merely that there isn’t any friend of mine who she
hopes will win.
()

Mary hopes that no friend of mine will win.

So () has R-de dicto and R-de re readings for no friend of mine, but no
wide-quantiﬁcation reading where the negative existential determiner no takes

B de re — de dicto : T T R



C 

matrix scope. Compare this with the minimally diﬀerent inﬁnitival complement
structure, which does permit all three kinds of readings.
()

Mary expects no friend of mine to win.

However, indeﬁnite DPs like a friend of mine, two friends of mine are notoriously
much freeer in the scope options for the existential quantiﬁaction they express.
For instance, even the ﬁnite clause in () seems to be no impediment to a
reading that is not only R-de re but also wide-quantiﬁcational (i.e., it has the
existential quantiﬁer over individuals outscoping the universal world-quantiﬁer).
()

Mary hopes that a friend of mine will win.

The peculiar scope-taking behavior of indeﬁnites (as opposed to universal, proportional, and negative quantiﬁers) has recently been addressed by a number of
authors (Abusch ; Kratzer ; Matthewson ; Reinhart ; Winter
), and there are good prospects for a successful theory that generates even
the wide-Q R-de re readings of indeﬁnites without any recourse to non-local
DP-movement. You are encouraged to read these works, but for our current
purposes here, all we want to point out is that, with respect to the behavior of
indeﬁnites, neither of the two theories we are trying to compare seems to have a
special advantage over the other. This is because wide-Q readings result from
DP-movement according to both theories.
As we mentioned in the previous chapter, a number of recent papers have been
probing the connection between de dicto readings and the eﬀects of Binding
Condition C applying at LF. These authors have converged on the conclusion
that DPs which are read as de dicto behave w.r.t. Binding Theory as if they are
located below the relevant modal predicate at LF, and DPs that are read as de
re (i.e., wide-Q, R-de re) behave as if they are located above. It is natural to
inquire whether the same kind of evidence could also be exploited to determine
the LF-location of the NP-part of a DP which is read as narrow-quantiﬁcational
but restrictor-de re. If this acted for Condition C purposes as if it were below
the attitude verb, it would conﬁrm the standard theory (non-locally bound
world-pronouns), whereas if it acted as if it was scoped out, we’d have evidence
for the scopal account. Sharvit () constructs some of the relevant examples
and reports judgments that actually favor the scopal theory. For example, she
observes that (a) does allow the narrow-Q, R-de re-reading indicated in (b).
()

a.

How many students who like John does he think every professor
talked to?

 Sharvit’s own conclusion, however, is not that her data supports the purely scopal theory.

§.

S, R,   S  M
b.



For which n does John think that every professor talked to n people
in the set of students who actually like John?

More research is required to corroborate this ﬁnding.
As a ﬁnal piece of potentially relevant data, consider a contrast in Marathi
recently discussed by Bhatt ().
()

[ji bai
kican madhe ahe]i Ram-la watte ki [[ti [ti
 woman kitchen in
is
Ram thinks that that woman
bai]i ] kican madhe nahi]
kitchen in not is
‘Ram thinks that the woman who is in the kitchen is not in the
kitchen’

()

Ram-la watte ki [ [ji bai kican madhe ahe]i [[ti [ti bai]i ] kican madhe
nahi] ]
Ram thinks that  woman kitchen in is that woman kitchen in not is
‘Ram thinks that the woman who is in the kitchen is not in the kitchen’

The English translation of both examples has two readings: a (plausible) de re
reading, on which Ram thinks of the woman who is actually in the kitchen
that she isn’t, and an (implausible) de dicto reading, on which Ram has the
contradictory belief that he would express by saying: “the woman in the kitchen
is not in the kitchen”. The Marathi sentence () also allows these two readings,
but () unambiguously expresses the implausible de dicto reading. Bhatt’s
explanation invokes the assumption that covert movement in Hindi cannot
cross a ﬁnite clause boundary. In (), where the correlative clause has moved
overtly, it can stay high or else reconstruct at LF, thus yielding either reading.
But in (), where it has failed to move up overtly, it must also stay low at LF,
and therefore can only be de dicto. What is interesting about this account is
that it crucially relies on a scopal account of the R-de re-R-de dicto distinction.
(Recall that with type-e DPs like deﬁnite descriptions, there is no additional
wide/narrow-Q ambiguity.) If the standard theory with its non-locally bindable
world-pronouns were correct, we would not expect the constraint that blocks
covert movement in () to aﬀect the possibility of a de re reading.
In sum, then, the evidence appears to be mixed. Some observations appear
to favor the currently standard account, whereas others look like they might
conﬁrm the purely scopal account after all. Much more work is needed.



B de re — de dicto : T T R

C 

.

A Recurring Theme: Historical Overview

To recap, the main shape of the phenomenon discussed in this chapter is that the
intensional parameter (time, world) with respect to which the predicate restricting a quantiﬁer is interpreted can be distinct from the one that is introduced
by the intensional operator that immediately scopes over the quantiﬁer. The
crucial cases have the character of a “scope paradox”. This discovery is one that
has been made repeatedly in the history of semantics. It has been made both in
the domain of temporal dependencies and in the domain of modality. Here are
some of the highlights of that history. .
. The now-operator
Prior () noticed a semantic problem with the adverb now. The main
early researchers that addressed the problem were Kamp () and Vlach
(). A good survey was prepared by van Benthem (). Another early
reference is Saarinen (). The simplest scope paradox examples looked
like this:
()

One day all persons now alive will be dead.

While for this example one could say that now is special in always having
access to the utterance time, other examples show that an unbounded
number of times need to be tracked. It became clear in this work that
whether one uses a multitude of indexed now and then-operators or allows
variables over times is a syntactic and not a deep semantic question.
. The actually-operator
The modal equivalent of the Prior-Kamp scope paradox sentence is:
()

It might have been that everyone actually rich was poor.

Crossley & Humberstone () discuss such examples. Double-indexed
systems of modal logic were studied by Segerberg () and Åqvist ().
See also work by Lewis (a), van Inwagen (), and Hazen ().
Indexed actually-operators are discussed by Prior & Fine (), Peacocke
(), and Forbes (, , ).
. The time of nominal predicates
There is quite a bit of work that argues that freedom in the time-dependency
of nominals even occurs when there is no apparent space for temporal
operators. Early work includes Enç (, ). But see also Ejerhed
(). More recently Musan’s dissertation (Musan ) is relevant.
 Some of this history can be found in comments throughout Cresswell’s book (Cresswell ),
which also contains additional references

§.

A R T: H O
()



Every fugitive is back in custody.

. Tense in Nominals
There is some syntactic work on tense in nominals, see for example
Wiltschko ().
. The Fodor-Reading
Examples similar to the ones from Fodor and Bäuerle that we used at
the beginning of this chapter are discussed in many places (Abusch ;
Bonomi ; Farkas ; Hellan ; Ioup ). The point that all
these authors have made is that the NP-predicate restricting a quantiﬁer
may be evaluated in the actual world, even when that quantiﬁer clearly
takes scope below a modal predicate.
Heim (?) gives an example like this:
()

Every time it could have been the case that the player on the left
was on the right instead.

Here, the player on the left must be evaluated with respect to the actual
world. But it is inside a tensed clause, which — as we saw earlier — is
usually considered a scope island for quantiﬁers.
. Explicit World Variables
Systems with explicit world/time variables were introduced by Tichy ()
and Gallin (). A system (Ty) with overt world-variables is used by
Groenendijk & Stokhof in their dissertation on the semantics of questions.
See also Zimmermann () on the expressive power of that system.
. Movement
The idea of getting the third reading via some kind of syntactic scoping
has not been pursued much. But there is an intriguing idea in a paper
by Bricker (), cited by Cresswell (: p. ). Bricker formalizes a
sentence like Everyone actually rich might have been poor as follows:
()

∃X(∀y(Xy ≡ rich y)& ∀y(Xy → poor y))

This is apparently meant to be interpreted as ‘there is a plurality X all of
whose members are rich and it might have been the case that all of the
members of X are poor’. This certainly looks like somehow a syntactic
scoping of the restrictive material inside the universal quantiﬁer out of the
scope of the modal operator has occurred.

— T     —

B
Abusch, Dorit. . The scope of indeﬁnites. Natural Language Semantics ().
–. doi:./BF.
Aikhenvald, Alexandra Y. . Evidentiality. Oxford: Oxford University Press.
Anand, Pranav & Valentine Hacquard. . Epistemics with attitude. Proceedings of Semantics and Linguistic Theory . doi:/.
Åqvist, Lennart. . Modal logic with subjunctive conditionals and
dispositional predicates.
Journal of Philosophical Logic (). –.
doi:./BF.
Asher, Nicholas. . A typology for attitude verbs and their anaphoric properties. Linguistics and Philosophy (). –. doi:./BF.
Barcan, Ruth C. . A functional calculus of ﬁrst order based on strict
implication. Journal of Symbolic Logic (). –. doi:./.
Barwise, John & Robin Cooper. . Generalized quantiﬁers and natural
language. Linguistics and Philosophy (). –. doi:./BF.
Bäuerle, Rainer. . Pragmatisch-semantische aspekte der NP-interpretation.
In M. Faust, R. Harweg, W. Lehfeldt & G. Wienold (eds.), Allgemeine sprachwissenschaft, sprachtypologie und textlinguistik: Festschrift für peter hartmann,
–. Narr Tübingen.
Beck, Sigrid. . Wh–constructions and transparent logical form: Universität
Tübingen dissertation.
Belnap, Jr., Nuel D. . Conditional assertion and restricted quantiﬁcation.
Noûs (). –. doi:./.
Belnap, Jr., Nuel D. . Restricted quantiﬁcation and conditional assertion.
In Hugues Leblanc (ed.), Truth, syntax and modality: Proceedings of the Temple
University conference on alternative semantics, vol.  Studies in Logic and the
Foundations of Mathematics, –. Amsterdam: North-Holland.
Bennett, Jonathan. . A philosophical guide to conditionals. Oxford University
Press.



B

Bennett, Michael & Barbara Partee. . Toward the logic of tense and aspect in
English. Indiana University Linguistics Club.
van Benthem, Johan. . Tense logic and standard logic. Logique et Analyse .
–.
Bhatt, Rajesh. . Obligation and possession. In Heidi Harley (ed.), Papers
from the upenn/mit roundtable on argument structure and aspect, vol.  MIT
Working Papers in Linguistics, –. URL http://people.umass.edu/bhatt/
papers/bhatt-haveto.pdf.
Bhatt, Rajesh. . Locality in apparently non-local relativization: Correlatives
in the modern indo-aryan languages. Handout for Talk Presented at UT
Austin and MIT.
Bhatt, Rajesh & Roumyana Pancheva. . Conditionals. In The Blackwell
companion to syntax, vol. , –. Blackwell. URL http://www-rcf.usc.edu/
~pancheva/bhatt-pancheva_syncom.pdf.
Blain, Eleanor M. & Rose-Marie Déchaine. . Evidential types: Evidence
from Cree dialects. International Journal of American Linguistics (). –.
doi:./.
Blumson, Ben. . Pictures, perspective and possibility. Philosophical Studies
doi:./s---.
Bonomi, Andrea. . Transparency and speciﬁcity in intensional contexts.
In P. Leonardi & M. Santambrogio (eds.), On quine, –. Cambridge
University Press.
Bonomi, Andrea & Sandro Zucchi. . A pragmatic framework for truth
in ﬁction. Dialectica (). –. doi:./j.-..tb.x.
Preprint http://ﬁlosoﬁa.dipaﬁlo.unimi.it/~bonomi/Pragmatic.pdf.
Bricker, Phillip. . Quantiﬁed modal logic and the plural de re. In French,
Uehling & Wettstein (eds.), Contemporary perspectives in the philosophy of
language ii Midwest Studies in Philosophy, Vol. , –. University of
Notre Dame Press.
Butler, Jonny. . A minimalist treatment of modality. Lingua (). –.
doi:./S-()-.
Carnap, Rudolf. . Meaning and necessity: A study in semantics and modal
logic. Chicago: University of Chicago Press.
Chierchia, Gennaro & Sally McConnell-Ginet. . Meaning and grammar:
An introduction to semantics (nd edition). MIT Press.


Chomsky, Noam. . A minimalist program for linguistic theory. In Kenneth
Hale & Samuel Jay Keyser (eds.), The view from building : Essays in linguistics
in honor of sylvain bromberger, –. MIT Press Cambridge, MA.
Cooper, Robin. . Tense and discourse location in situation semantics.
Linguistics and Philosophy (). –. doi:./BF.
Copeland, B. Jack. . The genesis of possible worlds semantics. Journal of
Philosophical Logic (). –. doi:./A:.
Cormack, Annabel & Neil Smith. . Modals and negation in English. In
Sjef Barbiers, Frits Beukema & Wim van der Wurﬀ (eds.), Modality and its
interaction with the verbal system, –. Benjamins.
Cresswell, Max. . Logics and languages. London: Methuen.
Cresswell, Max. . Entities and indices. Kluwer Dordrecht.
Crossley, J. N. & I. L. Humberstone. . The logic of actually. Reports on
Mathematical Logic . –.
DeRose, Keith. . Epistemic possibilities. The Philosophical Review ().
–. doi:./.
Dowty, David. . Tenses, time adverbs, and compositional semantic theory.
Linguistics and Philosophy . –.
Dowty, David, Robert Wall & Stanley Peters. . Introduction to Montague
semantics. Kluwer.
Dowty, David R. . Toward a semantic analysis of verb aspect and the
english ‘imperfective’ progressive. Linguistics and Philosophy (). –.
doi:./BF.
Drubig, Hans Bernhard. . On the syntactic form of epistemic modality. Ms,
Universität Tübingen. URL http://www.sfb.uni-tuebingen.de/b/papers/
DrubigModality.pdf.
Edgington, Dorothy. . On conditionals. Mind (). –. URL
./mind/...
Egan, Andy. . Epistemic modals, relativism, and assertion. Philosophical
Studies (). –. doi:./s---x.
Egan, Andy, John Hawthorne & Brian Weatherson. . Epistemic modals in
context. In Gerhard Preyer & Georg Peter (eds.), Contextualism in philosophy:
Knowledge, meaning, and truth, –. Oxford: Oxford University Press.



B

Ejerhed, Eva. . The syntax and semantics of English tense markers. Monographs from the Institute of Linguistics, University of Stockholm, No. .
Ejerhed, Eva. . Tense as a source of intensional ambiguity. In Frank Heny
(ed.), Ambiguities in intensional contexts, –. Reidel Dordrecht.
Elbourne, Paul & Uli Sauerland. . Total reconstruction, PF movement, and
derivational order. Linguistic Inquiry (). –.
Enç, Mürvet. . Tense without scope: An analysis of nouns as indexicals: University of Wisconsin, Madison dissertation.
Enç, Mürvet. . Towards a referential analysis of temporal expressions.
Linguistics and Philosophy . –.
Farkas, Donka. . Evaluation indices and scope. In Anna Szabolcsi (ed.),
Ways of scope taking, –. Dordrecht: Kluwer.
von Fintel, Kai. . Restrictions on quantiﬁer domains: University of Massachusetts at Amherst dissertation. URL http://semanticsarchive.net/Archive/
jANIwN/ﬁntel--thesis.pdf.
von Fintel, Kai. . Quantiﬁers and ‘if ’-clauses. The Philosophical Quarterly
(). –. doi:./-.. URL http://mit.edu/ﬁntel/
www/qandif.pdf.
von Fintel, Kai. . NPI licensing, Strawson entailment, and context dependency. Journal of Semantics (). –. doi:./jos/...
von Fintel, Kai. . Counterfactuals in a dynamic context. In Michael
Kenstowicz (ed.), Ken Hale: A life in language, –. MIT Press.
von Fintel, Kai. . Modality and language. In Donald M. Borchert (ed.),
Encyclopedia of philosophy – second edition, MacMillan. URL http://mit.edu/
ﬁntel/ﬁntel--modality.pdf.
von Fintel, Kai. . If : The biggest little word. Slides from a plenary
address given at the Georgetown University Roundtable, March , . URL
http://mit.edu/ﬁntel/gurt-slides.pdf.
von Fintel, Kai. . Conditionals. Ms, prepared for Semantics: An international
handbook of meaning, edited by Klaus von Heusinger, Claudia Maienborn, and
Paul Portner. URL http://mit.edu/ﬁntel/ﬁntel--hsk-conditionals.pdf.
von Fintel, Kai & Anthony S. Gillies. . An opinionated guide to epistemic
modality. In Tamar Szabó Gendler & John Hawthorne (eds.), Oxford studies
in epistemology: Volume , –. Oxford University Press. URL http://mit.
edu/ﬁntel/ﬁntel-gillies--ose.pdf.


von Fintel, Kai & Anthony S. Gillies. a. CIA leaks. The Philosophical Review
(). –. doi:./--.
von Fintel, Kai & Anthony S. Gillies. b. Might made right. To appear in a
volume on epistemic modality, edited by Andy Egan and Brian Weatherson,
Oxford University Press. URL http://mit.edu/ﬁntel/ﬁntel-gillies--mmr.
pdf.
von Fintel, Kai & Anthony S. Gillies. . Must . . . stay . . . strong! Final
pre-print, to appear in Natural Language Semantics. URL http://mit.edu/
ﬁntel/ﬁntel-gillies--mss.pdf.
von Fintel, Kai & Sabine Iatridou. . If and when If -clauses can restrict
quantiﬁers. Ms, MIT. URL http://mit.edu/ﬁntel/ﬁntel-iatridou--ifwhen.
pdf.
von Fintel, Kai & Sabine Iatridou. . Epistemic containment. Linguistic
Inquiry (). –. doi:./.
von Fintel, Kai & Sabine Iatridou. . What to do if you want to go to
Harlem: Anankastic conditionals and related matters. Ms, MIT. URL
http://mit.edu/ﬁntel/ﬁntel-iatridou--harlem.pdf.
von Fintel, Kai & Sabine Iatridou. . How to say ought in Foreign: The
composition of weak necessity modals. In Jacqueline Guéron & Jacqueline
Lecarme (eds.), Time and modality (Studies in Natural Language and Linguistic
Theory ), –. Springer. doi:./----.
Fodor, Janet Dean. . The linguistic description of opaque contexts: Massachusetts Institute of Technology dissertation. Published in  by Indiana
University Linguistics Club and in  in the Series “Outstanding Dissertations in Linguistics” by Garland.
Forbes, Graeme. . Physicalism, instrumentalism, and the semantics of modal
logic. Journal of Philosophical Logic . –.
Forbes, Graeme. . The metaphysics of modality. Oxford: Clarendon Press.
Forbes, Graeme. . Languages of possibility. Oxford: Blackwell.
Fox, Danny. . Economy and semantic interpretation. MIT Press.
Frank, Anette. . Context dependence in modal constructions: Universität
Stuttgart dissertation. URL http://www.dfki.de/~frank/papers/header.ps.gz.
Gajewski, Jon. . Neg-raising: Polarity and presupposition: Massachusetts
Institute of Technology dissertation. doi:./.

B



Gajewski, Jon. . Neg-raising and polarity. Linguistics and Philosophy
doi:./s---z.
Gallin, D. . Intensional and higher-order modal logic. North-Holland Amsterdam.
Gamut, L. T. F. . Logic, language, and meaning. Chicago University Press.
Garson, James. . Modal logic. In Edward N. Zalta (ed.), The Stanford
encyclopedia of philosophy, URL http://plato.stanford.edu/entries/logic-modal/.
Gergonne, Joseph Diaz. . Essai de dialectique rationnelle. Annales de
Mathématiques Pures et Appliquées . –. URL http://archive.numdam.
org/article/AMPA_-_____.pdf.
Geurts, Bart. . Presuppositions and anaphors in attitude contexts. Linguistics
& Philosophy (). –. doi:./A:.
Giannakidou, Anastasia. . Aﬀective dependencies. Linguistics and Philosophy
(). –. doi:./A:.
Gillies, Anthony S. . Counterfactual scorekeeping. Linguistics and Philosophy
(). –. doi:./s---.
Gillies, Anthony S. . On truth-conditions for if (but not quite only if ).
The Philosophical Review (). –. doi:./--.
Gillies, Anthony S. .
doi:./sp...

Iﬃness.

Semantics and Pragmatics (). –.

Groenendijk, Jeroen & Martin Stokhof. . Semantic analysis of Whcomplements. Linguistics and Philosophy . –.
Hacquard, Valentine. . Aspects of modality: Massachusetts Institute of
Technology dissertation. URL http://people.umass.edu/hacquard/hacquard_
thesis.pdf.
Hacquard, Valentine. . Modality. Ms, prepared for Semantics: An international handbook of meaning, edited by Klaus von Heusinger, Claudia
Maienbon, and Paul Portner. URL http://ling.umd.edu/~hacquard/papers/
HoS_Modality_Hacquard.pdf.
Hanley, Richard. . As good as it gets: Lewis on truth in ﬁction. Australasian
Journal of Philosophy (). –. doi:./.
Hazen, Allen. . One of the truths about actuality. Analysis . –.


Heim, Irene. . Presupposition projection and the semantics of attitude verbs.
Journal of Semantics (). –. doi:./jos/...
Heim, Irene & Angelika Kratzer. . Semantics in generative grammar. Blackwell.
Hellan, Lars. . On semantic scope. In Ambiguities in intensional contexts,
Dordrecht: Reidel.
Herzberger, Hans. . Counterfactuals and consistency. The Journal of
Philosophy (). –. doi:./.
Higginbotham, James. . Conditionals and compositionality. Philosophical
Perspectives (). –. doi:./j.-...x.
Hintikka, Jaako. . Semantics for propositional attitudes. In J.W. Davis, D.J.
Hockney & W.K. Wilson (eds.), Philosophical logic, –. Dordrecht: Reidel.
Hockett, Charles F. . The origin of speech. Scientiﬁc American . –.
Hockett, Charles F. & Stuart A. Altmann. . A note on design features.
In Thomas A. Sebeok (ed.), Animal communication: Techniques of study and
results of research, –. Indiana University Press.
Horn, Laurence R. . On the semantic properties of the logical operators in
english: UCLA dissertation.
Hughes, G.E. & M.J. Cresswell. . An introduction to modal logic. London:
Methuen.
Hughes, G.E. & M.J. Cresswell. . A new introduction to modal logic. London:
Routledge.
Huitink, Janneke. . Modals, conditionals and compositionality: Radboud Universiteit Nijmegen dissertation. URL http://user.uni-frankfurt.de/~huitink/
Huitink-dissertation.pdf.
Huitink, Janneke. a. Domain restriction by conditional connectives.
Ms, Goethe-University Frankfurt. URL http://semanticsarchive.net/Archive/
zgMDMM/Huitink-domainrestriction.pdf.
Huitink, Janneke. b. Quantiﬁed conditionals and compositionality. Ms, to
appear in Language and Linguistics Compass. URL http://user.uni-frankfurt.
de/~huitink/compass-conditionals-ﬁnal.pdf.
Iatridou, Sabine. . On the contribution of conditional Then. Natural
Language Semantics (). –. doi:./BF.



B

van Inwagen, Peter. . Indexicality and actuality. Philosophical Review .
–.
Ioup, Georgette. . Speciﬁcity and the interpretation of quantiﬁers. Linguistics
and Philosophy (). –.
Kadmon, Nirit & Fred Landman. . Any. Linguistics and Philosophy ().
–. doi:./BF.
Kamp, Hans. . Formal properties of now. Theoria . –.
Knuuttila, Simo. . Medieval theories of modality. In Edward N. Zalta
(ed.), The stanford encyclopedia of philosophy, Center for the Study of Language
and Information. URL http://plato.stanford.edu/archives/fall/entries/
modality-medieval/.
Kratzer, Angelika. . What must and can must and can mean. Linguistics and
Philosophy (). –. doi:./BF.
Kratzer, Angelika. . Semantik der Rede: Kontexttheorie – Modalwörter –
Konditionalsätze. Königstein/Taunus: Scriptor.
Kratzer, Angelika. . The notional category of modality. In Hans-Jürgen
Eikmeyer & Hannes Rieser (eds.), Words, worlds, and contexts: New approaches
in word semantics (Research in Text Theory ), –. Berlin: de Gruyter.
Kratzer, Angelika. . Conditionals. Chicago Linguistics Society (). –.
Kratzer, Angelika. . Modality. In Arnim von Stechow & Dieter Wunderlich
(eds.), Semantics: An international handbook of contemporary research, –.
Berlin: de Gruyter.
Kratzer, Angelika. . Scope or pseudoscope? are there wide-scope indeﬁnites?
In Susan Rothstein (ed.), Events and grammar, –. Kluwer Dordrecht.
Kratzer, Angelika. . Decomposing attitude verbs. Handout from a talk
honoring Anita Mittwoch on her th birthday at the Hebrew University of Jerusalem July , . URL http://semanticsarchive.net/Archive/
DcwYJkM/attitude-verbs.pdf.
Kripke, Saul. . Naming and necessity. Oxford: Blackwell.
Landman, Fred. . The progressive. Natural Language Semantics (). –.
doi:./BF.
Larson, Richard. . The grammar of intensionality. In G. Preyer (ed.), On
logical form, Oxford University Press.


Leslie, Sarah-Jane. . If, unless, and quantiﬁcation. In Robert J. Stainton &
Christopher Viger (eds.), Compositionality, context and semantic values: Essays
in honour of Ernie Lepore, –. Springer. doi:./----_.
Lewis, Clarence Irving & Cooper Harold Langford. . Symbolic logic. New
York: Century.
Lewis, David. a. Anselm and actuality. Noûs . –. Reprinted (with a
postscript) in Lewis (Lewis : pp. –).
Lewis, David. b.
General semantics.
doi:./BF.

Synthese (-). –.

Lewis, David. . Counterfactuals. Oxford: Blackwell.
Lewis, David. . Adverbs of quantiﬁcation. In Edward Keenan (ed.), Formal
semantics of natural language, –. Cambridge University Press.
Lewis, David. . Truth in ﬁction. American Philosophical Quarterly (). –
. URL http://www.jstor.org/stable/. Reprinted with postscripts in
Lewis (), pp. –.
Lewis, David. . Ordering semantics and premise semantics for counterfactuals. Journal of Philosophical Logic (). –. doi:./BF.
Reprinted in David Lewis, Papers in Philosophical Logic, Cambridge: Cambridge University Press, , pp. -.
Lewis, David. .
Logic for equivocators.
Noûs (). –.
doi:./. Reprinted in Lewis (: pp. –).
Lewis, David. . Philosophical papers: Volume . Oxford: Oxford University
Press.
Lewis, David. . On the plurality of worlds. Oxford: Blackwell.
Lewis, David. . Papers in philosophical logic. Cambridge: Cambridge
University Press.
MacFarlane, John. . Logical constants. In Edward N. Zalta (ed.), The
stanford encyclopedia of philosophy, URL http://plato.stanford.edu/archives/
win/entries/logical-constants/.
MacFarlane, John. . Epistemic modals are assessment-sensitive. Ms,
University of California, Berkeley, forthcoming in an OUP volume on
epistemic modals, edited by Brian Weatherson and Andy Egan. URL
http://sophos.berkeley.edu/macfarlane/epistmod.pdf.



B

Matthewson, Lisa. . On the interpretation of wide-scope indeﬁnites. Natural
Language Semantics ().
May, Robert. . The grammar of quantiﬁcation: Massachusetts Institute of
Technology dissertation.
McCready, Eric & Norry Ogata. . Evidentiality, modality and probability.
Linguistics and Philosophy (). –. doi:./s---.
Montague, Richard. . The proper treatment of quantiﬁcation in ordinary English. In Jaako Hintikka, Julius Moravcsik & Patrick Suppes (eds.), Approaches to natural language, –. Dordrecht: Reidel.
URL http://www.blackwellpublishing.com/content/BPL_Images/Content_
store/Sample_chapter//Portner.pdf. Reprinted in Portner &
Partee (), pp. –.
Moulton, Keir. . Clausal complementation and the Wager-class. Proceedings
of the North East Linguistics Society . URL http://sites.google.com/site/
keirmoulton/Moultonnelswager.pdf.
Moulton, Keir. . Natural selection and the syntax of clausal complementation:
University of Massachusetts at Amherst dissertation. URL http://scholarworks.
umass.edu/open_access_dissertations//.
Musan, Renate. . On the temporal interpretation of noun phrases: Massachusetts Institute of Technology dissertation. Published in  in the Series
“Outstanding Dissertations in Linguistics” by Garland.
Musan, Renate. . Tense, predicates, and lifetime eﬀects. Natural Language
Semantics (). –. doi:./A:.
Nauze, Fabrice. . Modality in typological perspective: Universiteit van Amsterdam dissertation. URL http://www.illc.uva.nl/Publications/Dissertations/
DS--.text.pdf.
Nute, Donald. . Conditional logic. In Dov Gabbay & Franz Guenthner
(eds.), Handbook of philosophical logic. volume ii, –. Dordrecht: Reidel.
Ogihara, Toshiyuki. . The semantics of tense in embedded clauses. Linguistic
Inquiry (). –. URL http://www.jstor.org/stable/.
Ogihara, Toshiyuki. . Tense and aspect in truth-conditional semantics.
Lingua (). –. doi:./j.lingua....
Osvath, Mathias & Peter Gärdenfors. . Oldowan culture and the evolution
of anticipatory cognition. Tech. Rep.  Lund University Cognitive Studies
LUCS, Lund. URL http://www.lucs.lu.se/ftp/pub/LUCS_Studies/LUCS.
pdf.


Partee, Barbara H. . Some structural analogies between tenses and pronouns
in English. The Journal of Philosophy (). –. doi:./.
Partee, Barbara H. . Nominal and temporal anaphora. Linguistics and
Philosophy (). –. doi:./BF.
Partee, Barbara H. . Reﬂections of a formal semanticist as of Feb .
Ms. (longer version of introductory essay in  book). URL http://people.
umass.edu/partee/docs/BHP_Essay_Feb.pdf.
Partee, Barbara H. & Herman L.W. Hendriks. . Montague grammar. In
Johan van Benthem & Alice ter Meulen (eds.), Handbook of logic and language,
–. Amsterdam: Elsevier.
Peacocke, Christopher. . Necessity and truth theories. Journal of Philosophical
Logic . –.
Peano, Giuseppe. . Arithmetices principia: Nova methodo exposita. Torino:
Bocca.
Percus, Orin. . Constraints on some other variables in syntax. Natural
Language Semantics (). –.
Perry, John R. . Semantics, possible worlds. In E. Craig (ed.), Routledge encyclopedia of philosophy, London: Routledge. URL http://www.rep.
routledge.com/article/U. Preprint http://www-csli.stanford.edu/~john/
PHILPAPERS/posswld.pdf.
Pollock, John. . Subjunctive reasoning. Dordrecht: Reidel.
Portner, Paul. .
The semantics of mood, complementation,
and conversational force. Natural Language Semantics (). –.
doi:./A:.
Portner, Paul. . The progressive in modal semantics. Language ().
–. doi:./.
Portner, Paul. . Modality. Oxford University Press.
Portner, Paul & Barbara H. Partee (eds.). . Formal semantics: The essential
readings. Oxford: Blackwell.
Prior, A. N. . Escapism: The logical basis of ethics. In A. I. Melden (ed.),
Essays in moral philosophy, –. Seattle: University of Washington Press.
Prior, A. N. & Kit Fine. . Worlds, times, and selves. London: Duckworth.
Prior, Arthur. . Now. Noûs . –.



B

Reinhart, Tanya. . Quantiﬁer scope – how labor is divided between QR and
choice functions. Linguistics & Philosophy (). –.
Ross, Jeﬀ. . The semantics of media (Studies in Linguistics and Philosophy
(SLAP) ). Dordrecht: Kluwer.
Rothstein, Susan. . Structuring events: A study in the semantics of lexical aspect Explorations in Semantics. Blackwell. URL http://tinyurl.com/
rothstein-aktionsarten.
Russell, Bertrand. . An inquiry into meaning and truth. London: George
Allen and Unwin.
Saarinen, Esa. . Backwards-looking operators in tense logic and in natural
language. In Jaako Hintikka, I. Niiniluoto & Esa Saarinen (eds.), Essays on
mathematical and philosophical logic, Reidel Dordrecht.
Schlenker, Philippe. . Conditionals as deﬁnite descriptions (A referential analysis). Research on Language and Computation (). –.
doi:./s---.
Segerberg, K. . Two-dimensional modal logic. Journal of Philosophical Logic
. –.
Sextus Empiricus. c. . Outlines of pyrrhonism.
Sharvit, Yael. . How many questions and attitude verbs. University of
Pennsylvania.
Speas, Peggy. . On the syntax and semantics of evidentials. Language and
Linguistics Compass (). –. doi:./j.-X...x.
Spencer, Mary. . Why the “s” in “intension”?
doi:./mind/LXXX...

Mind (). –.

Stalnaker, Robert. . A theory of conditionals. In Nicholas Rescher (ed.),
Studies in logical theory (American Philosophical Quarterly Monograph Series ), –. Oxford: Blackwell.
Stalnaker, Robert. . Indicative conditionals. Philosophia (). –.
doi:./BF.
Stalnaker, Robert. a. Inquiry. MIT Press.
Stalnaker, Robert. b. Inquiry. MIT Press.
Stalnaker, Robert. . Context and content. Oxford: Oxford University Press.


Stanley, Jason & Zoltán Gendler Szabó. . On quantiﬁer domain restriction.
Mind and Language (/). –. doi:./-..
von Stechow, Arnim. . On the proper treatment of tense. Proceedings of
Semantics and Linguistic Theory . URL http://www.sfs.uni-tuebingen.de/
~arnim/Aufsaetze/SALT.pdf.
von Stechow, Arnim. . Tenses in compositional semantics. To be published
in Wolfgang Klein (ed) The Expression of Time in Language. URL http:
//www.sfs.uni-tuebingen.de/~arnim/Aufsaetze/Approaches.pdf.
Steiner, George. . After Babel: Aspects of language and translation. Oxford
University Press rd edn.
Stephenson, Tamina. a. Judge dependence, epistemic modals, and predicates
of personal taste. Linguistics and Philosophy (). –. doi:./s--.
Stephenson, Tamina. b. Towards a theory of subjective meaning: Massachusetts Institute of Technology dissertation. URL http://semanticsarchive.
net/Archive/QxMjkO/Stephenson--thesis.pdf.
Steup, Matthias. . The analysis of knowledge. In Edward N. Zalta (ed.), The
Stanford encyclopedia of philosophy, Fall  edn. URL http://plato.stanford.
edu/archives/fall/entries/knowledge-analysis/.
Suber, Peter. . Paradoxes of material implication. An electronic hand-out for
the course “Symbolic Logic”. URL http://www.earlham.edu/~peters/courses/
log/mat-imp.htm.
Suddendorf, Thomas. . Foresight and evolution of the human mind. Science
(). –. doi:./science..
Suddendorf, Thomas & Michael C. Corballis. . Mental time travel and the
evolution of the human mind. Genetic, Social, and General Psychology Monographs (). –. URL http://cogprints.org///MentalTimeTravel.
txt.
Swanson, Eric. . Modality in language. Philosophy Compass (). –.
doi:./j.-...x.
Tichy, Pavel. . An approach to intensional analysis. Noûs . –.
Varzi, Achille. . Inconsistency without contradiction. Notre Dame Journal of
Formal Logic (). –. doi:./ndjﬂ/.
Vlach, Frank. . Now and then: A formal study in the logic of tense anaphora:
UCLA dissertation.



B

Warmbrod, Ken. . A defense of the limit assumption. Philosophical Studies
(). –. doi:./BF.
Willett, Thomas. . A cross-linguistic survey of the grammaticalization of
evidentiality. Studies in Language (). –.
Williamson, Timothy. . Knowledge and its limits. Oxford: Oxford University
Press.
Wiltschko, Martina. . On the interpretability of tense on D and its consequences for case theory. Lingua (). –. URL http://dx.doi.org/.
/S-()-X.
Winter, Yoad. . Choice functions and the scopal semantics of indeﬁnites.
Linguistics and Philosophy . –.
Wurmbrand, Susi. . Modal verbs must be raising verbs. West Coast Conference
on Formal Linguistics . –. URL http://wurmbrand.uconn.edu/Susi/
Papers%and%handouts_ﬁles/WCCFL.pdf.
Zimmermann, Thomas Ede. . Intensional logic and two-sorted type theory.
Journal of Symbolic Logic . –.
Zucchi, Sandro. . Tense in ﬁction. In Carlo Cecchetto, Gennaro Chierchia
& Maria Teresa Guasti (eds.), Semantic interfaces: Reference, anaphora and
aspect, –. CSLI Publications. URL http://ﬁlosoﬁa.dipaﬁlo.unimi.it/
~bonomi/Zucchi%Tense.pdf.

Abstract

The syntax of silence: Sluicing, islands, and identity in ellipsis

Jason Merchant
B.A., Yale University, 1991
M.A., University of California, Santa Cruz, 1996
June 1999
Directed by Professors James McCloskey and William Ladusaw

This dissertation investigates one of the most cross-linguistically widespread forms of
ellipsis: sluicing. Its goals are both empirical and theoretical. Emprically, the dissertation
documents sluicing data from thirty-one languages and establishes a number of novel
and partly surprising generalizations, which indicate inter alia that the form of the whremnant in sluicing reaches its position external to the ellipsis site by movement. This
result stands in direct conflict with the contention, first articulated in Ross 1969 and
unchallenged to date, that islands are not respected under sluicing. Theoretically, then,
the dissertation aims to reconcile these apparently contradictory strands of evidence.
The proposal advanced here is that the usual operation of movement is involved
in the derivation of sluicing, and that the IP out of which the wh-remnant is displaced is
deleted at PF. This allows for a maximally simple syntax of ellipsis: it is simply the syntax
of usual clauses, not pronounced. Although the deletion occurs at PF, I argue that the
identity condition on this deletion is essentially semantic, not structural. To this end, I
propose a semantic condition on ellipsis, building on Rooth 1992a but replacing his
structural isomorphism requirement, and show how this proposal solves a number of
problems encountered by structural accounts, including the phenomenon dubbed
‘vehicle change’ by Fiengo and May 1994. The syntactic licensing conditions on IPdeletion and the semantic identification condition are unified by assigning a semantics

that imposes the identity condition to the syntactic feature that licenses the ellipsis. This
general approach —sluicing as wh-movement followed by deletion— directly accounts
for the generalizations concerning the form of the wh-phrase in sluicing.
The behavior of islands under sluicing, it is then argued, falls into two classes. For
one large class of islands, including relative clauses and adjuncts, island insensitivity
under ellipsis is only apparent. The desired interpretations of the elliptical clause can be
generated by using independently needed mechanisms for resolving E-type anaphora
and modal subordination; the wh-movement in these cases remains local, and islandrespecting. For the second class of islands, such as COMP-trace phenomena and left
branch effects, a more surprising conclusion is reached: these island effects arise at PF,
not as a result of constraints on syntactic movement directly, and can be therefore be
repaired by PF-deletion.
The analysis of sluicing defended here thus supports a pluralistic view of
islandhood where various parts of the grammar interact to constrain extractions, and
integrates sluicing into a general theory of ellipsis, dispensing with the sluicing-specific
operations or stipulations previously thought to be necessary.

LINGUIST List 13.1210
Wed May 1 2002
Review: Syntax: Merchant (2001) Syntax of Silence
Editor for this issue: Terence Langendoen <terry

linguistlist.org>

What follows is another discussion note contributed to our Book Discussion Forum. We expect these
discussions to be informal and interactive; and the author of the book discussed is cordially invited to join
in. If you are interested in leading a book discussion, look for books announced on LINGUIST as
"available for discussion." (This means that the publisher has sent us a review copy.) Then contact Simin
Karimi at simin linguistlist.org or Terry Langendoen at terry linguistlist.org.

Directory
1. Jeff Runner, The Syntax of Silence: Sluicing, Islands and the Theory of Ellipsis

Message 1: The Syntax of Silence: Sluicing, Islands and the Theory of Ellipsis
Date: Tue, 30 Apr 2002 08:44:35 -0400
From: Jeff Runner <jrunner1 rochester.rr.com>
Subject: The Syntax of Silence: Sluicing, Islands and the Theory of Ellipsis
Merchant, Jason (2001) The Syntax of Silence: Sluicing, Islands and the
Theory of Ellipsis. Oxford University Press, xv+262pp, hardback ISBN
0-19-924373-5, GBP 47.50; paperback ISBN 0-19-924372-7, GBP 18.99.
Oxford Studies in Theoretical Linguistics 1.
Jeffrey T. Runner, University of Rochester
[Note: The publisher had not yet announced this book on LINGUIST List
at the time that this review was posted. --Eds.]
SYNOPSIS
The book contains five chapters as well as a short introduction and
conclusion. Additionally, there are 17 pages of references, ten and a
half pages of indexes, including a short language index and name and
subject indexes. The first four chapters lay down the data, theoretical
groundwork and an interesting puzzle that set the stage for the fifth
and longest chapter, which is the heart of the book.
Sluicing, which Merchant argues is a type of IP ellipsis, is the topic
of the book. An example is the following (I will use '<x>' to indicate
an elided 'x'):

1. Gary visited somebody but he didn't say who <Gary visited>
His analysis makes the following claims, discussed in more detail below:
(1) Sluicing is a deletion process at the level of phonetic form (PF);
this builds on early work on the topic by Ross (1969). (2) No
morphosyntactic identity condition holds between the "sluiced" IP and
the antecedent IP which licenses it (contra e.g., Fiengo & May 1994):
the only identity condition is one of a particular kind of
semantic/pragmatic mutual entailment between the two IPs. (3) Certain
"islands" for syntactic movement involve features at PF which can be
deleted by Sluicing; certain other islands are true syntactic islands
and cannot be by-passed by Sluicing at PF.
The first chapter, Identity In Ellipsis: Focus and Isomorphism, is 28
pages long. Though the book focuses on Sluicing, Merchant begins by
presenting facts about a better-studied phenomenon, VP-ellipsis,
illustrated in a sentence like (2):
2. Gary visited Bill, and then the next day Mary did <visit Bill>
Following work by Rooth (1992), Swarzschild (1999) and Romero (1998),
Merchant argues that VP-ellipsis obeys a Focus condition, which
essentially states that a VP can be elided if it is GIVEN in a context.
GIVEN means that it is not in focus and that it has a particular type of
salient antecedent in the context. Merchant argues that the relevant
notion of GIVEN, which he calls e-GIVEN, is a mutual entailment
condition holding between the antecedent VP and the elided VP. That is,
it's not just enough for the elided VP to be entailed by the antecedent
VP, but the antecedent VP must be entailed by the elided VP.
This mutual entailment idea is the heart of Merchant's contribution to
the question of the relationship between the antecedent and the elided
material in ellipsis. Many approaches to ellipsis assume that a
morphosyntactic isomorphism condition holds between the antecedent VP
and the elided VP, in place of or in addition to a semantic/pragmatic
condition. Such a condition seems to be at play in failed VP-ellipsis
examples like (3):
3. *Abby was reading the book while Ben was <reading>
If all that were needed was that the antecedent VP entailed the elided
one, (3) should be acceptable, since reading a book entails reading.
The problem is that there is a direct object NP in the antecedent that
is not present in the elided VP. The standard solution to this problem
has been to propose a structural isomorphism condition, requiring that
the antecedent VP and the elided VP have the same syntactic structure.
Merchant discusses the approach along these lines presented in Fiengo &
May (1994), highlighting a challenge for that approach, illustrated in a
VP-ellipsis example like (4):
4. They arrested Alex-i, though he-i thought they wouldn't <arrest
Alex-i>
Without ellipsis this sentence would be ungrammatical since it contains
a Binding Theory Condition C effect: 'Alex-i' in the elided VP is
c-commanded by a coindexed pronoun, 'he-i'. However, the ellipsis seems
to make the problem disappear. Fiengo & May propose what they call
"vehicle change", which essentially allows under particular

circumstances (e.g., in VP-ellipsis) that the pronominal features of a
nominal may vary; this boils down to saying that 'Alex-i' can actually
be a pronoun, 'he-i', for the purposes of ellipsis, thus avoiding the
Condition C effect. Merchant points out that his mutual entailment
condition predicts this without a special vehicle change device. In
Merchant's view the sentence is actually (5):
5. They arrested Alex-i, though he-i thought they wouldn't <arrest
him-i>
Since all that matters is that the two VPs entail each other it is okay
for the elided VP to contain a coindexed pronoun rather than a full
NP--no morphosyntactic isomorphism condition forces the elided VP to
contain the full NP 'Alex-i' (I am glossing over certain details about
what it means for one constituent to entail another--see the book for
these details).
Chapter 2, The Syntax of Sluicing, is 46 pages long and argues that the
"Sluice" is a CP with an IP missing. In particular this chapter argues
that the missing IP contains the same syntactic structure that its overt
counterpart would contain. Merchant suggests that the null IP is
licensed by a particular combination of features in the head C position
of the CP sluice: [+wh, +Q]; this accounts for why sluicing is possible
in embedded questions ([+wh, +Q]) but not in relative clauses ([+wh,
-Q]). His particular proposal is that a special feature in the head I
of IP is what licenses the ellipsis itself. This feature, E, is argued
to move from I to C, to issue deletion instructions at PF (essentially
instructing the parser to "skip the complement of I"), and to impose the
Focus condition, mentioned in chapter 1. Merchant sees it as a positive
step to link the structural, deletion and focus conditions all together
in one feature of the morphosyntax.
One issue that arises is that the C head in Sluicing must always be
empty, which the following English example illustrates:
6. A: Max has invited someone. B: Really, who (*has)?
The Sluice never contains material in C. This is true in English as
well as in languages which more regularly allow material in C, such as
German, Dutch and Danish. It is also true for languages which place
special clitics in the C area, like the South Slavic languages.
Merchant provides two possible explanations for this. One is that the
movement to C "follows" IP-deletion; thus if the IP is deleted there
will be no verb or clitic to move to C. A second possibility is that
the "strong" feature (in the sense of the minimalist program, see
Chomsky 1995) which triggers the movement to C can be deleted at PF
(with the IP-deletion) obviating the need for movement to C (or perhaps
allowing the bare feature to move to C at LF but not at PF). This just
leaves one additional problem, which a movement/feature account cannot
solve: base-generated material, such as an actual complementizer, is
also banned from C in Sluicing. Merchant offers two suggestions. One
is a version of the doubly-filled Comp filter, specially stated to apply
in Sluicing contexts for languages that do not obey such a filter more
generally; and the other is to claim that the complementizers in
question perhaps normally cliticize to their right, so problems would
arise if their complement IP were deleted.
The third chapter, Islands and Form-Identity, contains 21 pages and sets

the stage for subsequent chapters by presenting a puzzle. On the one
hand, the wh-phrase that appears in the Sluice does not appear to have
arrived there by movement, since certain "islands" for movement do not
seem to be respected; this could suggest a base-generation account. On
the other hand, certain "form-identity" generalizations would be
difficult to state if Sluicing is not simply regular wh-movement; in
particular the morphological (e.g., case) form of the wh-phrase is
always what it would be if it were in its un-moved base position and
whether a language allows preposition-stranding under normal wh-movement
exactly predicts whether it allows preposition-stranding under
Sluicing. This would be difficult to ensure on a non-movement approach.
Merchant shows that Sluicing is fine even if the Sluice contains any of
the following types of islands: relative clause, adjunct,
noun-complement, sentential subject, embedded question, coordinate
structure, complementizer-trace, left branch and "derived" position
(=topicalization and subject) islands. An example of a Sluice
containing e.g., a relative clause is (7):
7. They wanted to hire someone who speaks a Balkan language, but I don't
remember which <Balkan language they want to hire someone who speaks>
If lack of sensitivity to islands indicates lack of movement, as is
often assumed, then these facts all point to the claim that Sluicing
does not involve wh-movement.
However, if Sluicing does not involve wh-movement then it becomes
difficult to explain what he calls "form-identity" generalizations. The
first is that the sluiced wh-phrase must bear the case that its
correlate bears; this shows up in case-marking languages like German,
where a verb like 'praise' assigns accusative case to its object but a
verb like 'flatter' assigns dative case; this distinction is maintained
in Sluicing:
8. Er will jemanden loben, aber sie wissen nicht, {*wer/wen/*wem}
'He wants to praise someone, but they don't know {who-*NOM/ACC/*DAT}'
9. Er will jemanden schmeicheln, aber sie wissen nicht, {*wer/*wen/wem}
'He wants to flatter someone, but they don't know {who-*NOM/*ACC/DAT}'
The second form-identity generalization is that a language will allow
preposition-stranding under Sluicing just in case it allows
preposition-stranding under regular wh-movement. He establishes this
generalization by looking at six preposition-stranding languages and 18
non-preposition-stranding languages. Again, this points to Sluicing
being a type of wh-movement. Thus, this chapter sets up a puzzle to be
solved later.
Chapter 4, Deletio nata atque mortua, is 50 pages long and reviews all
extant accounts of Sluicing. Merchant discusses five types of account,
including one of his own from previous work (Merchant 2000c), and shows
where each fails. He begins by discussing Ross's (1969) deletion
account. In favor of such an account is a straightforward explanation
of the identity-form generalizations discussed in Chapter 3. Ross
recognized that (some) islands were not respected by Sluicing and
suggested that grammaticality was calculated across a derivation, so
that if an early violation were later fixed grammaticality would
result. Since the island was deleted via Sluicing the violation was

repaired. However, Merchant points out that the fact that VP-ellipsis
does not repair island violations casts doubt on Ross's type of
analysis:
10. [Everyone wants to hire someone who speaks a different Balkan
language]
*Abby wants to hire someone who speaks Greek, but I don't remember which
(language) Ben does <want to hire someone [who speaks]>
A second analysis of Sluicing is that of "pseudosluicing". The claim
(made by e.g. Erteschik-Shir 1977 and Pollmann 1975) is that Sluicing
examples are produced by a kind of cleft construction:
11. Someone just left--guess who <it was>
Such an analysis would explain the island effects:
12. That he'll hire someone is possible, but I won't divulge who ?(it
will be).
Merchant provides nine arguments from various languages suggesting there
is a difference between Sluicing and this cleft-like "pseudosluicing"
construction. I will not outline them here for reasons of space.
The third type of analysis of Sluicing treats it as involving a
wh-operator that binds a resumptive pronoun. This would explain the
island effects straightfowardly since no movement would be required.
Merchant points out several problems with this idea. For one, Sluices
can occur with wh-words that do not have resumptive counterparts. A
second argument comes from languages like Irish which have a resumptive
pronoun strategy. These languages have a restriction that the
resumptive pronoun cannot be the "highest subject". No such restriction
holds of Sluicing.
The fourth type of analysis Merchant discusses is the best-known recent
one, that of Chung, Ladusaw & McCloskey (1995). Their analysis
base-generates the wh-phrase in CP and assumes that it binds an
indefinite NP, which they analyze as variable (following Heim 1982 and
Kamp 1981). Merchant points out several problems with this approach,
the most significant of which is its inability to account for "contrast"
Sluices like the following. (11) would have a logical form (LF)
representation like (12):
11. She has five CATS, but I don't know how many DOGS.
12. She has five CATS, but I don't know how many DOGS(x) [she has
CATS(x)]
Merchant's account can handle these cases because the mutual entailment
discussed in Chapter 1 and mentioned above replaces focused material
with variables so that the antecedent IP and the deleted IP both
contain: 'she has x'. Thus, they satisfy his e-GIVENness requirement,
which states that the two IPs must entail each other.
While each of the above approaches can account for the lack of island
effects in Sluicing none of them except Ross's can account for the
form-identity generalizations pointed out in Chapter 3. Merchant
(2000c) proposed a version of the Chung et al. (1995) approach that
attempts to capture the form-identity facts. On this view, which is an

LF-copying approach, the indefinite NPs in Sluicing undergo Quantifier
Raising (QR) leaving an IP containing a gap for copying into the
position of the Sluice. This alternative can explain the form-identity
generalizations since it is a movement account; however, it faces the
same problems the structural isomomorphism approach of Fiengo & May,
discussed in Chapter 1. In addition it relies on the dubious assumption
that indefinite NPs can QR out of islands at LF. For these reasons he
ultimately rejects this alternative as well.
The fifth and final chapter, Deletio Redux, contains 70 pages and is
intended to put together an analysis that can deal with everything
brought up in the preceding four chapters. To recap the issues,
Merchant has pointed out that the form-identity generalizations suggest
a movement approach to sluicing, along with PF-deletion; but the
apparent insensitivity to (some) syntactic islands suggests a
non-movement LF-copying approach. Merchant supports the movement plus
PF-deletion approach; thus, much of this chapter is devoted to
explaining the island (partial) insensitivity facts.
The fact that sluicing seems to be insensitive to some islands and not
to others suggests that islands come in different varieties sensitive to
different factors. Merchant develops this view by showing that what
have been called islands indeed fall into three distinct classes. The
first class, selective (or "weak") islands, Merchant claims are not
syntactic at all, but rather are semantic/pragmatic in nature. The
second class, which includes left-branch extraction, COMP-trace effects,
"derived" positions (topicalizations/subjects), and extraction of a
coordinate structure conjunct, Merchant argues are "undone" by PF
deletion. The third class, which includes extraction out of coordinate
structure conjuncts, complex NPs and adjuncts, all involve extraction
out of a propositional domain, which Merchant argues allow for an
analysis exploiting e-type anaphora. He sets aside the first class,
since their account does not interact with the question of whether
sluicing is PF-deletion or LF-copying (though addresses them at the very
end of the chapter). If Sluicing is PF-deletion and the second class of
islands are all due to some PF-feature conflict then the fact that
Sluicing can "undo" this type of island is explained. The third class
of islands are real non-PF islands but the use of e-type anaphora
combined with his pragmatic mutual entailment identity condition
accounts for the apparent violation of these islands. In fact, they are
not violated at all.
Space does not permit a detailed discussion of each island so just a few
examples will be outlined as illustration. Merchant devotes some time
to left-branch extraction, which is a member of the second class of
island types. He follows Kennedy & Merchant (2000a) which claims that
languages vary on whether their NPs can support a [+wh] feature in the
highest nominal projection. Those that can will allow left-branch
violations; those that cannot, like English, can only extract via this
highest nominal specifier if the unsupported [+wh] feature is deleted at
PF, e.g., via Sluicing (or VP-ellipsis). Left-branch extraction is
illustrated in (13). Sluicing seems to "fix" the problem (14)-(15).
13. *I don't know [how detailed]-i he wants [t-i' F[+wh][a t-i [list]].
14. He wants a detailed list, but I don't know how detailed.
15. He wants a detailed list, but I don't know [how detailed]-i <he
wants [t-i' F[+wh] [a t-i list]]>

A potential problem arises with respect to examples like (16):
16. *He wants a list, but I don't know how detailed.
This looks like a case where Sluicing hasn't fixed the left-branch
extraction problem. However, Merchant claims that (16) is not bad
because of a left-branch problem but rather because it violates his
mutual entailment identity condition. Essentially the issue is that
wanting a list does not entail wanting a detailed list.
Merchant points out that left-branch subextractions are not fixed by
Sluicing. Compare (17) and (18):
17. *[How badly]-i did you meet [a guy [t-i short of funds]]?
18. *She met a guy (badly) short of funds, but I don't know how badly.
Thus, whatever is causing the subextraction violation in (17) is not
being undone by PF-deletion. Merchant argues that the degree phrase
itself does not project the relevant FP through whose specifier the
measure phrase would be extracted, thus these remain islands
independently of whether an offending feature is deleted or not.
Another type of island that Merchant discusses in detail is what he
calls derived position islands. This refers to subject islands and
topicalization islands. An example of the latter is (19), with a
passive and with an unaccusative subject; and (20) illustrates how the
island effect is ameliorated by Sluicing:
19. *Guess [which Marx
published/will appear}
20. A biography of one
published/will appear}

Brother] [a biography of t] {is going to be
this year.
of the Marx Brothers {is going to be
this year--guess which!

Merchant's proposal is that (19) is out because extraction has taken
place out of a phrase that is not L-marked (following Chomsky 1986a).
Why is (20) good? Merchant proposes that within the Sluice the movement
to Spec,IP has not taken place and thus the extraction is from the
subject's base position, not its derived position. Since its base
position is L-marked no violation occurs. The question is why does the
subject in the Sluice not need to move to Spec,IP as it would in (19)?
The answer to that is that the feature that would normally drive the
subject-movement to Spec,IP, the EPP feature in I, is "strong" and thus
is uninterpretable at the PF interface. In (20), Sluicing has deleted
the IP, removing the offending feature.
Merchant explores the question of whether having the subject stay within
the VP at Spell-Out has any consequences for the interpretation. He
concludes that it does not, showing that indeed the subject can interact
with modals and negation as usual.
Before turning to the third class of islands that Sluicing seems to be
insensitive to, Merchant proposes an analysis of sentences like (21),
which forms the foundation for the analysis of that last class of
islands. The type of Sluice one might expect for these is in (22); the
problem is that the Sluiced IP contains an unbound trace:
21. The report details what IBM did and why.
22. The report details what-i [IBM did t-i] and why <IBM did t-i>

Merchant argues that (21) is not related to something like (22), but
rather to something like (23), which contains a pronoun in the second
IP:
23. The report details what IBM did and why <IBM did it>
The pronoun in (23) is an e-type pronoun licensed in the usual way (for
such pronouns) by a non-c-commanding quantifier, in this case the
wh-phrase in the antecedent IP.
This sets the stage for an analysis of how Sluicing appears to violate
the third class of islands, what Merchant calls propositional islands.
These include relative clauses, adjuncts and sentential subjects, and
coordinate structure conjuncts. A relative clause example is in (24).
In (25) is an example showing that usually extraction is not possible:
24. They hired someone who speaks a Balkan language--guess which!
25. *Guess which (Balkan language) they hired someone who speaks!
As in the previous examples, instead of relating (24) to something like
(25), Merchant argues that it is in fact related to something like (26),
which contains an e-type pronoun licensed by the quantifier in the
antecedent IP:
26. They hired someone who speaks a Balkan language--guess which-i <she
speaks t-i>
A similar analysis is given to other cases in which Sluicing apparently
violates propositional islands.
CRITICAL EVALUATION
Overall, I think this is an excellent book. It is very carefully
argued. It contains detailed discussion of alternative proposals,
examining their flaws. It brings in a wealth of data from other
languages with Sluicing. I am sure it will make a long-lasting
contribution to the study of both sluicing and ellipsis in general in
natural language.
There are two additional issues I would like to mention--one positive,
one possibly negative. First, part of Merchant's proposal is that the
only parallelism condition on ellipsis is his pragmatic/semantic
e-GIVENness condition and not a morphosyntactic condition. One positive
aspect of this proposal is that a certain amount of speaker variability
might be expected in ellipsis, which I believe is correct. A particular
example comes from Chapter 5 (judgments are Merchant's):
27. *He wants a list, but I don't know how detailed.
(27) and sentences like it seemed perfectly fine to me so I asked a few
other speakers and found that a few liked them and a few did not.
Recall that Merchant's analysis of (27) depends on whether one believes
that wanting a list entails wanting a detailed list. This seems like
something that could vary from speaker to speaker and from context to
context. Note that a morphosyntactic isomorphism requirement would rule
(27) out for everyone, including me and those speakers like me.
The other issue I want to bring up involves extraction out of DPs of

various sorts. It has been noted by a number of people that semantic
properties of the verb as well as referential/quantificational
properties of the DP itself both influence whether extraction out of a
direct object is possible (e.g., Erteschik-Shir 1977, Kuno 1987). For
example, verbs of destruction are worse than verbs of creation; and DPs
containing referential or strong quantifier determiners are worse than
those containing existential determiners:
28. Who did John write/??destroy a/??every/??the book about t?
Sluicing does not seem to care about this:
29. John wrote/destroyed a/every/the book about someone but I don't know
who.
An extension of Merchant's analysis might be to suggest that whatever
blocks (28) involves some strong PF feature, which Sluicing deletes in
(29). This seems somewhat counterintuitive, though, since the problem
in (28) seems likely to be an LF problem, not a PF problem, since the
relevant properties are semantic.
One could argue that (29) is good because the DP in the Sluice is in an
L-related position (cf., the discussion in the last chapter on "derived"
positions), but that would it seems commit one to the claim that the
object of the verb of destruction and the quantificational/referential
object in (28) are in non-L-marked positions at PF, while the object of
the verb of creation and the existentially quantified object are in
L-marked positions at PF. I know of no evidence for PF-differences in
position for these different DP-types. There have been proposals that
the relevant DPs in (28) are indeed in different positions at LF, due to
their interpretations, and that this distinction is what causes the
reduction in grammaticality (e.g., Diesing 1992, Runner 1995, 1998, who
both argue that this is an ECP violation at LF). But if the problem in
(28) is due to an LF violation then it would seem that the LF of the
Sluiced (29) is somehow different. This different type of island has
not been documented here. In addition, it seems to me that an
appropriate analysis of the amelioration caused by Sluicing in (29)
might actually extend naturally to the analysis of other derived
position islands, without having to posit such different LFs for the
otherwise interpretationally identical Sluiced material.
Overall, though, this book is worth reading and I would highly recommend
it.
REFERENCES
Chomsky, N. (1986a) Barriers. Cambridge, MA: MIT Press.
Chomsky, N. (1995) The Minimalist Program. Cambridge, MA: MIT Press.
Chung, S., W. Ladusaw & J. McCloskey (1995) "Sluicing and Logical Form."
Natural Language Semantics 3:239-282.
Diesing, M. (1992) Indefinites. Cambridge, MA: MIT Press.
Erteschik-Shir, N. (1977) On the Nature of Island Constraints.
Bloomington, IN: Indiana University Linguistics Club.
Fiengo, R. & R. May (1994) Indices and Identity. Cambridge, MA: MIT
Press.
Heim, I. (1982) The Semantics of Definite and Indefinite NPs. Ph.D.
dissertation, UMass-Amherst.
Kamp, H. (1981) "A Theory of Truth and Discourse Representation," in J.
Groendijk et al. (eds.), Formal Methods in the Study of Language.

Amsterdam: Mathematisch Centrum, 277-322.
Kennedy, C. & J. Merchant (2000a) "Attributive Comparative Deletion."
Natural Language and Linguistic Theory 18:89-146.
Kuno, S. (1987) Functional Syntax. Chicago: University of Chicago Press.
Merchant, J. (2000c) "LF Movement and Islands in Greek Sluicing."
Journal of Greek Linguistics 1:39-62.
Pollmann, T. (1975) "Een regel die subject en copula deleert?" Spektator
5:282-292.
Romero, M. (1998) Focus and Reconstruction Effects in Wh-Phrases. Ph.D.
dissertation, UMass-Amherst.
Rooth, M. (1992) "A Theory of Focus Interpretation." Natural Language
Semantics 1:75-116.
Ross, J.R. (1969) "Guess Who?", in R. Binnick et al. (eds.) Papers from
the 5th Regional Meeting of the Chicago Linguistic Society. Chicago:
Chicago Linguistic Society, 282-286.
Runner, J. T. (1995) Noun Phrase Licensing and Interpretation. Ph.D.
dissertation, UMass-Amherst.
Runner, J. T. (1998) Noun Phrase Licensing. New York: Garland
Publications.
Swarzschild, R. (1999) "GIVENness, AVOIDF, and Other Constraints on the
Placement of Accent." Natural Language Semantics 7:141-177.
ABOUT THE REVIEWER
Jeffrey T. Runner is an Assistant Professor in the Department of
Linguistics at the University of Rochester and has been teaching there
since 1994. He received his Ph.D. from the University of Massachusetts,
Amherst in 1995. His dissertation focused on direct objects in English,
exploring the relationship between syntactic position and interpretation
at various levels of representation. More recently, in addition to his
continued research on constructions involving objects, he has been
exploring the roles of syntactic structure and context in the domain of
Binding Theory, studying reflexives and pronouns from data collected
experimentally using a head-mounted eye-tracker.
Mail to author|Respond to list|Read more issues|LINGUIST home page|Top of issue

LINGUIST List 13.1210
Wed May 1 2002
Review: Syntax: Merchant (2001) Syntax of Silence
Editor for this issue: Terence Langendoen <terry

linguistlist.org>

What follows is another discussion note contributed to our Book Discussion Forum. We expect these
discussions to be informal and interactive; and the author of the book discussed is cordially invited to join
in. If you are interested in leading a book discussion, look for books announced on LINGUIST as
"available for discussion." (This means that the publisher has sent us a review copy.) Then contact Simin
Karimi at simin linguistlist.org or Terry Langendoen at terry linguistlist.org.

Directory
1. Jeff Runner, The Syntax of Silence: Sluicing, Islands and the Theory of Ellipsis

Message 1: The Syntax of Silence: Sluicing, Islands and the Theory of Ellipsis
Date: Tue, 30 Apr 2002 08:44:35 -0400
From: Jeff Runner <jrunner1 rochester.rr.com>
Subject: The Syntax of Silence: Sluicing, Islands and the Theory of Ellipsis
Merchant, Jason (2001) The Syntax of Silence: Sluicing, Islands and the
Theory of Ellipsis. Oxford University Press, xv+262pp, hardback ISBN
0-19-924373-5, GBP 47.50; paperback ISBN 0-19-924372-7, GBP 18.99.
Oxford Studies in Theoretical Linguistics 1.
Jeffrey T. Runner, University of Rochester
[Note: The publisher had not yet announced this book on LINGUIST List
at the time that this review was posted. --Eds.]
SYNOPSIS
The book contains five chapters as well as a short introduction and
conclusion. Additionally, there are 17 pages of references, ten and a
half pages of indexes, including a short language index and name and
subject indexes. The first four chapters lay down the data, theoretical
groundwork and an interesting puzzle that set the stage for the fifth
and longest chapter, which is the heart of the book.
Sluicing, which Merchant argues is a type of IP ellipsis, is the topic
of the book. An example is the following (I will use '<x>' to indicate
an elided 'x'):

1. Gary visited somebody but he didn't say who <Gary visited>
His analysis makes the following claims, discussed in more detail below:
(1) Sluicing is a deletion process at the level of phonetic form (PF);
this builds on early work on the topic by Ross (1969). (2) No
morphosyntactic identity condition holds between the "sluiced" IP and
the antecedent IP which licenses it (contra e.g., Fiengo & May 1994):
the only identity condition is one of a particular kind of
semantic/pragmatic mutual entailment between the two IPs. (3) Certain
"islands" for syntactic movement involve features at PF which can be
deleted by Sluicing; certain other islands are true syntactic islands
and cannot be by-passed by Sluicing at PF.
The first chapter, Identity In Ellipsis: Focus and Isomorphism, is 28
pages long. Though the book focuses on Sluicing, Merchant begins by
presenting facts about a better-studied phenomenon, VP-ellipsis,
illustrated in a sentence like (2):
2. Gary visited Bill, and then the next day Mary did <visit Bill>
Following work by Rooth (1992), Swarzschild (1999) and Romero (1998),
Merchant argues that VP-ellipsis obeys a Focus condition, which
essentially states that a VP can be elided if it is GIVEN in a context.
GIVEN means that it is not in focus and that it has a particular type of
salient antecedent in the context. Merchant argues that the relevant
notion of GIVEN, which he calls e-GIVEN, is a mutual entailment
condition holding between the antecedent VP and the elided VP. That is,
it's not just enough for the elided VP to be entailed by the antecedent
VP, but the antecedent VP must be entailed by the elided VP.
This mutual entailment idea is the heart of Merchant's contribution to
the question of the relationship between the antecedent and the elided
material in ellipsis. Many approaches to ellipsis assume that a
morphosyntactic isomorphism condition holds between the antecedent VP
and the elided VP, in place of or in addition to a semantic/pragmatic
condition. Such a condition seems to be at play in failed VP-ellipsis
examples like (3):
3. *Abby was reading the book while Ben was <reading>
If all that were needed was that the antecedent VP entailed the elided
one, (3) should be acceptable, since reading a book entails reading.
The problem is that there is a direct object NP in the antecedent that
is not present in the elided VP. The standard solution to this problem
has been to propose a structural isomorphism condition, requiring that
the antecedent VP and the elided VP have the same syntactic structure.
Merchant discusses the approach along these lines presented in Fiengo &
May (1994), highlighting a challenge for that approach, illustrated in a
VP-ellipsis example like (4):
4. They arrested Alex-i, though he-i thought they wouldn't <arrest
Alex-i>
Without ellipsis this sentence would be ungrammatical since it contains
a Binding Theory Condition C effect: 'Alex-i' in the elided VP is
c-commanded by a coindexed pronoun, 'he-i'. However, the ellipsis seems
to make the problem disappear. Fiengo & May propose what they call
"vehicle change", which essentially allows under particular

circumstances (e.g., in VP-ellipsis) that the pronominal features of a
nominal may vary; this boils down to saying that 'Alex-i' can actually
be a pronoun, 'he-i', for the purposes of ellipsis, thus avoiding the
Condition C effect. Merchant points out that his mutual entailment
condition predicts this without a special vehicle change device. In
Merchant's view the sentence is actually (5):
5. They arrested Alex-i, though he-i thought they wouldn't <arrest
him-i>
Since all that matters is that the two VPs entail each other it is okay
for the elided VP to contain a coindexed pronoun rather than a full
NP--no morphosyntactic isomorphism condition forces the elided VP to
contain the full NP 'Alex-i' (I am glossing over certain details about
what it means for one constituent to entail another--see the book for
these details).
Chapter 2, The Syntax of Sluicing, is 46 pages long and argues that the
"Sluice" is a CP with an IP missing. In particular this chapter argues
that the missing IP contains the same syntactic structure that its overt
counterpart would contain. Merchant suggests that the null IP is
licensed by a particular combination of features in the head C position
of the CP sluice: [+wh, +Q]; this accounts for why sluicing is possible
in embedded questions ([+wh, +Q]) but not in relative clauses ([+wh,
-Q]). His particular proposal is that a special feature in the head I
of IP is what licenses the ellipsis itself. This feature, E, is argued
to move from I to C, to issue deletion instructions at PF (essentially
instructing the parser to "skip the complement of I"), and to impose the
Focus condition, mentioned in chapter 1. Merchant sees it as a positive
step to link the structural, deletion and focus conditions all together
in one feature of the morphosyntax.
One issue that arises is that the C head in Sluicing must always be
empty, which the following English example illustrates:
6. A: Max has invited someone. B: Really, who (*has)?
The Sluice never contains material in C. This is true in English as
well as in languages which more regularly allow material in C, such as
German, Dutch and Danish. It is also true for languages which place
special clitics in the C area, like the South Slavic languages.
Merchant provides two possible explanations for this. One is that the
movement to C "follows" IP-deletion; thus if the IP is deleted there
will be no verb or clitic to move to C. A second possibility is that
the "strong" feature (in the sense of the minimalist program, see
Chomsky 1995) which triggers the movement to C can be deleted at PF
(with the IP-deletion) obviating the need for movement to C (or perhaps
allowing the bare feature to move to C at LF but not at PF). This just
leaves one additional problem, which a movement/feature account cannot
solve: base-generated material, such as an actual complementizer, is
also banned from C in Sluicing. Merchant offers two suggestions. One
is a version of the doubly-filled Comp filter, specially stated to apply
in Sluicing contexts for languages that do not obey such a filter more
generally; and the other is to claim that the complementizers in
question perhaps normally cliticize to their right, so problems would
arise if their complement IP were deleted.
The third chapter, Islands and Form-Identity, contains 21 pages and sets

the stage for subsequent chapters by presenting a puzzle. On the one
hand, the wh-phrase that appears in the Sluice does not appear to have
arrived there by movement, since certain "islands" for movement do not
seem to be respected; this could suggest a base-generation account. On
the other hand, certain "form-identity" generalizations would be
difficult to state if Sluicing is not simply regular wh-movement; in
particular the morphological (e.g., case) form of the wh-phrase is
always what it would be if it were in its un-moved base position and
whether a language allows preposition-stranding under normal wh-movement
exactly predicts whether it allows preposition-stranding under
Sluicing. This would be difficult to ensure on a non-movement approach.
Merchant shows that Sluicing is fine even if the Sluice contains any of
the following types of islands: relative clause, adjunct,
noun-complement, sentential subject, embedded question, coordinate
structure, complementizer-trace, left branch and "derived" position
(=topicalization and subject) islands. An example of a Sluice
containing e.g., a relative clause is (7):
7. They wanted to hire someone who speaks a Balkan language, but I don't
remember which <Balkan language they want to hire someone who speaks>
If lack of sensitivity to islands indicates lack of movement, as is
often assumed, then these facts all point to the claim that Sluicing
does not involve wh-movement.
However, if Sluicing does not involve wh-movement then it becomes
difficult to explain what he calls "form-identity" generalizations. The
first is that the sluiced wh-phrase must bear the case that its
correlate bears; this shows up in case-marking languages like German,
where a verb like 'praise' assigns accusative case to its object but a
verb like 'flatter' assigns dative case; this distinction is maintained
in Sluicing:
8. Er will jemanden loben, aber sie wissen nicht, {*wer/wen/*wem}
'He wants to praise someone, but they don't know {who-*NOM/ACC/*DAT}'
9. Er will jemanden schmeicheln, aber sie wissen nicht, {*wer/*wen/wem}
'He wants to flatter someone, but they don't know {who-*NOM/*ACC/DAT}'
The second form-identity generalization is that a language will allow
preposition-stranding under Sluicing just in case it allows
preposition-stranding under regular wh-movement. He establishes this
generalization by looking at six preposition-stranding languages and 18
non-preposition-stranding languages. Again, this points to Sluicing
being a type of wh-movement. Thus, this chapter sets up a puzzle to be
solved later.
Chapter 4, Deletio nata atque mortua, is 50 pages long and reviews all
extant accounts of Sluicing. Merchant discusses five types of account,
including one of his own from previous work (Merchant 2000c), and shows
where each fails. He begins by discussing Ross's (1969) deletion
account. In favor of such an account is a straightforward explanation
of the identity-form generalizations discussed in Chapter 3. Ross
recognized that (some) islands were not respected by Sluicing and
suggested that grammaticality was calculated across a derivation, so
that if an early violation were later fixed grammaticality would
result. Since the island was deleted via Sluicing the violation was

repaired. However, Merchant points out that the fact that VP-ellipsis
does not repair island violations casts doubt on Ross's type of
analysis:
10. [Everyone wants to hire someone who speaks a different Balkan
language]
*Abby wants to hire someone who speaks Greek, but I don't remember which
(language) Ben does <want to hire someone [who speaks]>
A second analysis of Sluicing is that of "pseudosluicing". The claim
(made by e.g. Erteschik-Shir 1977 and Pollmann 1975) is that Sluicing
examples are produced by a kind of cleft construction:
11. Someone just left--guess who <it was>
Such an analysis would explain the island effects:
12. That he'll hire someone is possible, but I won't divulge who ?(it
will be).
Merchant provides nine arguments from various languages suggesting there
is a difference between Sluicing and this cleft-like "pseudosluicing"
construction. I will not outline them here for reasons of space.
The third type of analysis of Sluicing treats it as involving a
wh-operator that binds a resumptive pronoun. This would explain the
island effects straightfowardly since no movement would be required.
Merchant points out several problems with this idea. For one, Sluices
can occur with wh-words that do not have resumptive counterparts. A
second argument comes from languages like Irish which have a resumptive
pronoun strategy. These languages have a restriction that the
resumptive pronoun cannot be the "highest subject". No such restriction
holds of Sluicing.
The fourth type of analysis Merchant discusses is the best-known recent
one, that of Chung, Ladusaw & McCloskey (1995). Their analysis
base-generates the wh-phrase in CP and assumes that it binds an
indefinite NP, which they analyze as variable (following Heim 1982 and
Kamp 1981). Merchant points out several problems with this approach,
the most significant of which is its inability to account for "contrast"
Sluices like the following. (11) would have a logical form (LF)
representation like (12):
11. She has five CATS, but I don't know how many DOGS.
12. She has five CATS, but I don't know how many DOGS(x) [she has
CATS(x)]
Merchant's account can handle these cases because the mutual entailment
discussed in Chapter 1 and mentioned above replaces focused material
with variables so that the antecedent IP and the deleted IP both
contain: 'she has x'. Thus, they satisfy his e-GIVENness requirement,
which states that the two IPs must entail each other.
While each of the above approaches can account for the lack of island
effects in Sluicing none of them except Ross's can account for the
form-identity generalizations pointed out in Chapter 3. Merchant
(2000c) proposed a version of the Chung et al. (1995) approach that
attempts to capture the form-identity facts. On this view, which is an

LF-copying approach, the indefinite NPs in Sluicing undergo Quantifier
Raising (QR) leaving an IP containing a gap for copying into the
position of the Sluice. This alternative can explain the form-identity
generalizations since it is a movement account; however, it faces the
same problems the structural isomomorphism approach of Fiengo & May,
discussed in Chapter 1. In addition it relies on the dubious assumption
that indefinite NPs can QR out of islands at LF. For these reasons he
ultimately rejects this alternative as well.
The fifth and final chapter, Deletio Redux, contains 70 pages and is
intended to put together an analysis that can deal with everything
brought up in the preceding four chapters. To recap the issues,
Merchant has pointed out that the form-identity generalizations suggest
a movement approach to sluicing, along with PF-deletion; but the
apparent insensitivity to (some) syntactic islands suggests a
non-movement LF-copying approach. Merchant supports the movement plus
PF-deletion approach; thus, much of this chapter is devoted to
explaining the island (partial) insensitivity facts.
The fact that sluicing seems to be insensitive to some islands and not
to others suggests that islands come in different varieties sensitive to
different factors. Merchant develops this view by showing that what
have been called islands indeed fall into three distinct classes. The
first class, selective (or "weak") islands, Merchant claims are not
syntactic at all, but rather are semantic/pragmatic in nature. The
second class, which includes left-branch extraction, COMP-trace effects,
"derived" positions (topicalizations/subjects), and extraction of a
coordinate structure conjunct, Merchant argues are "undone" by PF
deletion. The third class, which includes extraction out of coordinate
structure conjuncts, complex NPs and adjuncts, all involve extraction
out of a propositional domain, which Merchant argues allow for an
analysis exploiting e-type anaphora. He sets aside the first class,
since their account does not interact with the question of whether
sluicing is PF-deletion or LF-copying (though addresses them at the very
end of the chapter). If Sluicing is PF-deletion and the second class of
islands are all due to some PF-feature conflict then the fact that
Sluicing can "undo" this type of island is explained. The third class
of islands are real non-PF islands but the use of e-type anaphora
combined with his pragmatic mutual entailment identity condition
accounts for the apparent violation of these islands. In fact, they are
not violated at all.
Space does not permit a detailed discussion of each island so just a few
examples will be outlined as illustration. Merchant devotes some time
to left-branch extraction, which is a member of the second class of
island types. He follows Kennedy & Merchant (2000a) which claims that
languages vary on whether their NPs can support a [+wh] feature in the
highest nominal projection. Those that can will allow left-branch
violations; those that cannot, like English, can only extract via this
highest nominal specifier if the unsupported [+wh] feature is deleted at
PF, e.g., via Sluicing (or VP-ellipsis). Left-branch extraction is
illustrated in (13). Sluicing seems to "fix" the problem (14)-(15).
13. *I don't know [how detailed]-i he wants [t-i' F[+wh][a t-i [list]].
14. He wants a detailed list, but I don't know how detailed.
15. He wants a detailed list, but I don't know [how detailed]-i <he
wants [t-i' F[+wh] [a t-i list]]>

A potential problem arises with respect to examples like (16):
16. *He wants a list, but I don't know how detailed.
This looks like a case where Sluicing hasn't fixed the left-branch
extraction problem. However, Merchant claims that (16) is not bad
because of a left-branch problem but rather because it violates his
mutual entailment identity condition. Essentially the issue is that
wanting a list does not entail wanting a detailed list.
Merchant points out that left-branch subextractions are not fixed by
Sluicing. Compare (17) and (18):
17. *[How badly]-i did you meet [a guy [t-i short of funds]]?
18. *She met a guy (badly) short of funds, but I don't know how badly.
Thus, whatever is causing the subextraction violation in (17) is not
being undone by PF-deletion. Merchant argues that the degree phrase
itself does not project the relevant FP through whose specifier the
measure phrase would be extracted, thus these remain islands
independently of whether an offending feature is deleted or not.
Another type of island that Merchant discusses in detail is what he
calls derived position islands. This refers to subject islands and
topicalization islands. An example of the latter is (19), with a
passive and with an unaccusative subject; and (20) illustrates how the
island effect is ameliorated by Sluicing:
19. *Guess [which Marx
published/will appear}
20. A biography of one
published/will appear}

Brother] [a biography of t] {is going to be
this year.
of the Marx Brothers {is going to be
this year--guess which!

Merchant's proposal is that (19) is out because extraction has taken
place out of a phrase that is not L-marked (following Chomsky 1986a).
Why is (20) good? Merchant proposes that within the Sluice the movement
to Spec,IP has not taken place and thus the extraction is from the
subject's base position, not its derived position. Since its base
position is L-marked no violation occurs. The question is why does the
subject in the Sluice not need to move to Spec,IP as it would in (19)?
The answer to that is that the feature that would normally drive the
subject-movement to Spec,IP, the EPP feature in I, is "strong" and thus
is uninterpretable at the PF interface. In (20), Sluicing has deleted
the IP, removing the offending feature.
Merchant explores the question of whether having the subject stay within
the VP at Spell-Out has any consequences for the interpretation. He
concludes that it does not, showing that indeed the subject can interact
with modals and negation as usual.
Before turning to the third class of islands that Sluicing seems to be
insensitive to, Merchant proposes an analysis of sentences like (21),
which forms the foundation for the analysis of that last class of
islands. The type of Sluice one might expect for these is in (22); the
problem is that the Sluiced IP contains an unbound trace:
21. The report details what IBM did and why.
22. The report details what-i [IBM did t-i] and why <IBM did t-i>

Merchant argues that (21) is not related to something like (22), but
rather to something like (23), which contains a pronoun in the second
IP:
23. The report details what IBM did and why <IBM did it>
The pronoun in (23) is an e-type pronoun licensed in the usual way (for
such pronouns) by a non-c-commanding quantifier, in this case the
wh-phrase in the antecedent IP.
This sets the stage for an analysis of how Sluicing appears to violate
the third class of islands, what Merchant calls propositional islands.
These include relative clauses, adjuncts and sentential subjects, and
coordinate structure conjuncts. A relative clause example is in (24).
In (25) is an example showing that usually extraction is not possible:
24. They hired someone who speaks a Balkan language--guess which!
25. *Guess which (Balkan language) they hired someone who speaks!
As in the previous examples, instead of relating (24) to something like
(25), Merchant argues that it is in fact related to something like (26),
which contains an e-type pronoun licensed by the quantifier in the
antecedent IP:
26. They hired someone who speaks a Balkan language--guess which-i <she
speaks t-i>
A similar analysis is given to other cases in which Sluicing apparently
violates propositional islands.
CRITICAL EVALUATION
Overall, I think this is an excellent book. It is very carefully
argued. It contains detailed discussion of alternative proposals,
examining their flaws. It brings in a wealth of data from other
languages with Sluicing. I am sure it will make a long-lasting
contribution to the study of both sluicing and ellipsis in general in
natural language.
There are two additional issues I would like to mention--one positive,
one possibly negative. First, part of Merchant's proposal is that the
only parallelism condition on ellipsis is his pragmatic/semantic
e-GIVENness condition and not a morphosyntactic condition. One positive
aspect of this proposal is that a certain amount of speaker variability
might be expected in ellipsis, which I believe is correct. A particular
example comes from Chapter 5 (judgments are Merchant's):
27. *He wants a list, but I don't know how detailed.
(27) and sentences like it seemed perfectly fine to me so I asked a few
other speakers and found that a few liked them and a few did not.
Recall that Merchant's analysis of (27) depends on whether one believes
that wanting a list entails wanting a detailed list. This seems like
something that could vary from speaker to speaker and from context to
context. Note that a morphosyntactic isomorphism requirement would rule
(27) out for everyone, including me and those speakers like me.
The other issue I want to bring up involves extraction out of DPs of

various sorts. It has been noted by a number of people that semantic
properties of the verb as well as referential/quantificational
properties of the DP itself both influence whether extraction out of a
direct object is possible (e.g., Erteschik-Shir 1977, Kuno 1987). For
example, verbs of destruction are worse than verbs of creation; and DPs
containing referential or strong quantifier determiners are worse than
those containing existential determiners:
28. Who did John write/??destroy a/??every/??the book about t?
Sluicing does not seem to care about this:
29. John wrote/destroyed a/every/the book about someone but I don't know
who.
An extension of Merchant's analysis might be to suggest that whatever
blocks (28) involves some strong PF feature, which Sluicing deletes in
(29). This seems somewhat counterintuitive, though, since the problem
in (28) seems likely to be an LF problem, not a PF problem, since the
relevant properties are semantic.
One could argue that (29) is good because the DP in the Sluice is in an
L-related position (cf., the discussion in the last chapter on "derived"
positions), but that would it seems commit one to the claim that the
object of the verb of destruction and the quantificational/referential
object in (28) are in non-L-marked positions at PF, while the object of
the verb of creation and the existentially quantified object are in
L-marked positions at PF. I know of no evidence for PF-differences in
position for these different DP-types. There have been proposals that
the relevant DPs in (28) are indeed in different positions at LF, due to
their interpretations, and that this distinction is what causes the
reduction in grammaticality (e.g., Diesing 1992, Runner 1995, 1998, who
both argue that this is an ECP violation at LF). But if the problem in
(28) is due to an LF violation then it would seem that the LF of the
Sluiced (29) is somehow different. This different type of island has
not been documented here. In addition, it seems to me that an
appropriate analysis of the amelioration caused by Sluicing in (29)
might actually extend naturally to the analysis of other derived
position islands, without having to posit such different LFs for the
otherwise interpretationally identical Sluiced material.
Overall, though, this book is worth reading and I would highly recommend
it.
REFERENCES
Chomsky, N. (1986a) Barriers. Cambridge, MA: MIT Press.
Chomsky, N. (1995) The Minimalist Program. Cambridge, MA: MIT Press.
Chung, S., W. Ladusaw & J. McCloskey (1995) "Sluicing and Logical Form."
Natural Language Semantics 3:239-282.
Diesing, M. (1992) Indefinites. Cambridge, MA: MIT Press.
Erteschik-Shir, N. (1977) On the Nature of Island Constraints.
Bloomington, IN: Indiana University Linguistics Club.
Fiengo, R. & R. May (1994) Indices and Identity. Cambridge, MA: MIT
Press.
Heim, I. (1982) The Semantics of Definite and Indefinite NPs. Ph.D.
dissertation, UMass-Amherst.
Kamp, H. (1981) "A Theory of Truth and Discourse Representation," in J.
Groendijk et al. (eds.), Formal Methods in the Study of Language.

Amsterdam: Mathematisch Centrum, 277-322.
Kennedy, C. & J. Merchant (2000a) "Attributive Comparative Deletion."
Natural Language and Linguistic Theory 18:89-146.
Kuno, S. (1987) Functional Syntax. Chicago: University of Chicago Press.
Merchant, J. (2000c) "LF Movement and Islands in Greek Sluicing."
Journal of Greek Linguistics 1:39-62.
Pollmann, T. (1975) "Een regel die subject en copula deleert?" Spektator
5:282-292.
Romero, M. (1998) Focus and Reconstruction Effects in Wh-Phrases. Ph.D.
dissertation, UMass-Amherst.
Rooth, M. (1992) "A Theory of Focus Interpretation." Natural Language
Semantics 1:75-116.
Ross, J.R. (1969) "Guess Who?", in R. Binnick et al. (eds.) Papers from
the 5th Regional Meeting of the Chicago Linguistic Society. Chicago:
Chicago Linguistic Society, 282-286.
Runner, J. T. (1995) Noun Phrase Licensing and Interpretation. Ph.D.
dissertation, UMass-Amherst.
Runner, J. T. (1998) Noun Phrase Licensing. New York: Garland
Publications.
Swarzschild, R. (1999) "GIVENness, AVOIDF, and Other Constraints on the
Placement of Accent." Natural Language Semantics 7:141-177.
ABOUT THE REVIEWER
Jeffrey T. Runner is an Assistant Professor in the Department of
Linguistics at the University of Rochester and has been teaching there
since 1994. He received his Ph.D. from the University of Massachusetts,
Amherst in 1995. His dissertation focused on direct objects in English,
exploring the relationship between syntactic position and interpretation
at various levels of representation. More recently, in addition to his
continued research on constructions involving objects, he has been
exploring the roles of syntactic structure and context in the domain of
Binding Theory, studying reflexives and pronouns from data collected
experimentally using a head-mounted eye-tracker.
Mail to author|Respond to list|Read more issues|LINGUIST home page|Top of issue

Abstract

The syntax of silence: Sluicing, islands, and identity in ellipsis

Jason Merchant
B.A., Yale University, 1991
M.A., University of California, Santa Cruz, 1996
June 1999
Directed by Professors James McCloskey and William Ladusaw

This dissertation investigates one of the most cross-linguistically widespread forms of
ellipsis: sluicing. Its goals are both empirical and theoretical. Emprically, the dissertation
documents sluicing data from thirty-one languages and establishes a number of novel
and partly surprising generalizations, which indicate inter alia that the form of the whremnant in sluicing reaches its position external to the ellipsis site by movement. This
result stands in direct conflict with the contention, first articulated in Ross 1969 and
unchallenged to date, that islands are not respected under sluicing. Theoretically, then,
the dissertation aims to reconcile these apparently contradictory strands of evidence.
The proposal advanced here is that the usual operation of movement is involved
in the derivation of sluicing, and that the IP out of which the wh-remnant is displaced is
deleted at PF. This allows for a maximally simple syntax of ellipsis: it is simply the syntax
of usual clauses, not pronounced. Although the deletion occurs at PF, I argue that the
identity condition on this deletion is essentially semantic, not structural. To this end, I
propose a semantic condition on ellipsis, building on Rooth 1992a but replacing his
structural isomorphism requirement, and show how this proposal solves a number of
problems encountered by structural accounts, including the phenomenon dubbed
‘vehicle change’ by Fiengo and May 1994. The syntactic licensing conditions on IPdeletion and the semantic identification condition are unified by assigning a semantics

that imposes the identity condition to the syntactic feature that licenses the ellipsis. This
general approach —sluicing as wh-movement followed by deletion— directly accounts
for the generalizations concerning the form of the wh-phrase in sluicing.
The behavior of islands under sluicing, it is then argued, falls into two classes. For
one large class of islands, including relative clauses and adjuncts, island insensitivity
under ellipsis is only apparent. The desired interpretations of the elliptical clause can be
generated by using independently needed mechanisms for resolving E-type anaphora
and modal subordination; the wh-movement in these cases remains local, and islandrespecting. For the second class of islands, such as COMP-trace phenomena and left
branch effects, a more surprising conclusion is reached: these island effects arise at PF,
not as a result of constraints on syntactic movement directly, and can be therefore be
repaired by PF-deletion.
The analysis of sluicing defended here thus supports a pluralistic view of
islandhood where various parts of the grammar interact to constrain extractions, and
integrates sluicing into a general theory of ellipsis, dispensing with the sluicing-specific
operations or stipulations previously thought to be necessary.

CORRIGENDUM 1

Corrigendum to Tic Tac TOE: Effects of predictability and importance on acoustic
prominence in language production

Duane G. Watson
University of Illinois Urbana Champaign

Jennifer E. Arnold
University of North Carolina Chapel Hill

Michael K. Tanenhaus
University of Rochester

Manuscript Dated: August 21, 2009

Corresponding Author:
Duane G. Watson
dgwatson@illinois.edu
603 East Daniel St., Champaign, IL 61820
+1 217 333 0280

CORRIGENDUM 2
Watson, Arnold & Tanenhaus (2008) used the game of Tic Tac Toe to investigate the link
between predictability, importance, and acoustic prominence. We reported that in moves
that were unpredictable (and unimportant) the target word, which was the number of a
cell in a 3 by 3 grid, was produced with longer duration and a higher F0 than the target
word in moves that were predictable (and important). We also reported that the target
word in moves that were predictable (and important) was produced with greater intensity.
We concluded that importance and predictability were independent factors in acoustic
prominence. We found evidence that predictability was linked to speaker-centered
processes while importance may be linked to marking important information for the
listener.
After publication, we found an error in the scripts used to extract acoustic
information from the target number. The data reported in Watson, Arnold and Tanenhaus
did not just contain information from the cell number, but also from segments preceding
it (usually the entire utterance, although not always). Here we report the correct
intensity, F0, and duration for the cell number as well as the entire utterance.
Table 1 presents the means of the acoustic measures of the entire utterance. In an
analysis of the entire utterance, predictable moves were reliably shorter than
unpredictable moves, F1(1,19)=51.39, p <.001; F2(1,8)=157.74, p <.001. In contrast,
important moves were produced with greater intensity than unimportant moves,
F(1,19)=20.20, p < .001; F2(1,8)=12.67, p<.01. There was a numerical trend for F0 to
have a higher minimum and lower maximum over the entire utterance in the predictable

CORRIGENDUM 3
condition, but the differences by condition were not consistently reliable by both subjects
and items.1
In an analysis of just the cell number (Table 2), unpredictable moves were still
reliably longer than unpredictable moves, F1(1,19)=30.25, p < .001; F2(1,8)=50.06, p
<.001. Unpredictable moves had greater intensity then predictable moves although this
was only reliable by subjects and not items, F1(1,19)=6.17, p < .05; F2<1. There were no
differences in F0.
Thus, the corrected results still support our original conclusion that importance
and predictability independently influence acoustic realization. However, this is true only
when we consider the utterance as a whole. For the cell number itself, we only found an
effect of predictability.
From these data, we can conclude two things. The first is that effects of
predictability are not necessarily localized to individual words: both the unpredictable
cell number and the words that preceded it were lengthened. This may reflect effects of
planning processes being engaged during the production of the words that preceded the
target, supported by the fact that the rate of disfluency in these moves was greater. In
fact, several models of speech production attribute the reduction associated with
predictable material with speaker-centered production processes (e.g. Bell et al., 2009,
Bard et al., 2001). Because these words are less difficult to access, they are produced
with less prominence. Second, these data suggest that factors that correlate with
prominence, such as duration and intensity, can fractionate at the utterance level, and that
intensity might play a special role in marking importance information. Although we do
1

For maximum F0, F1<1; F2(1,8)=8.01, p <.05. For minimum F0, F1(1,19)=6.41, p
<.05; F2(1,8)=1.57.

CORRIGENDUM 4
not find evidence for this divergence at the word level in this dataset, more recent work
suggests that this is also possible. Lam, Watson and Arnold (2008, 2009) independently
manipulated repetition and predictability. They found that repeated words (which are
easy to access) are reduced regardless of predictability, and words that are unexpected
tend to be uttered with greater intensity, regardless of repetition.
Overall, these data suggest that both predictability and importance can influence
the acoustic properties of an utterance but in different ways.

References
Bard, E.G., Anderson, A.H., Sotillo, C. Aylett, M., Doherty-Sneddon, G., & Newlands,
A.(2000) Controlling the intelligibility of referring expressions in dialogue.
Journal of Memory and Language. 42(1), 1-22.

Bell, A., Brenier, J., Gregory, M., Girand, C. and Jurafsky, D. (2009). Predictability
Effects on Durations of Content and Function Words in Conversational English.
Journal of Memory and Language 60:1, 92-111.

Lam, T., Watson, D.G., & Arnold, J. (2008, April). Effects of repeated mention and
predictability on the production of acoustic prominence. Poster presented at
ETAP 2008: Experimental and Theoretical Advances in Prosody, Ithaca, NY.

CORRIGENDUM 5
Lam, T., Watson, D. G., & Arnold, J. (2009, March). Do repeated mention and
expectancy independently affect prosodic prominence? Poster presentation at
CUNY 2009: Conference on Human Sentence Processing Conference, Davis, CA.

Watson, D., Arnold, J. & Tanenhaus, M. (2008). Tic Tac TOE: Effects of predictability
and importance on acoustic prominence in language production. Cognition, 106,
1548-1557.

CORRIGENDUM 6

Table 1. The duration, intensity, and F0 of the entire move.
Important/Predictable

Unimportant/Unpredictable

Duration (ms)

1688 (59)

2732 (173)

Intensity (db)

67.71 (0.84)

67.08 (0.79)

Minimum F0 (Hz)

107 (9)

106 (9)

Maximum F0 (Hz)

188 (15)

189 (15)

Average F0 (Hz)

144 (11)

144 (10)

CORRIGENDUM 7

Table 2. The duration, intensity and F0 of the cell numbers.
Important/Predictable

Unimportant/Unpredictable

Duration (ms)

479 (23)

537 (22)

Intensity (db)

62.83 (.75)

63.25 (.74)

Minimum F0 (Hz)

115 (7)

112 (7)

Maximum F0 (Hz)

159 (11)

161 (11)

Average F0 (Hz)

135 (9)

134 (8)

CORRIGENDUM 1

Corrigendum to Tic Tac TOE: Effects of predictability and importance on acoustic
prominence in language production

Duane G. Watson
University of Illinois Urbana Champaign

Jennifer E. Arnold
University of North Carolina Chapel Hill

Michael K. Tanenhaus
University of Rochester

Manuscript Dated: August 21, 2009

Corresponding Author:
Duane G. Watson
dgwatson@illinois.edu
603 East Daniel St., Champaign, IL 61820
+1 217 333 0280

CORRIGENDUM 2
Watson, Arnold & Tanenhaus (2008) used the game of Tic Tac Toe to investigate the link
between predictability, importance, and acoustic prominence. We reported that in moves
that were unpredictable (and unimportant) the target word, which was the number of a
cell in a 3 by 3 grid, was produced with longer duration and a higher F0 than the target
word in moves that were predictable (and important). We also reported that the target
word in moves that were predictable (and important) was produced with greater intensity.
We concluded that importance and predictability were independent factors in acoustic
prominence. We found evidence that predictability was linked to speaker-centered
processes while importance may be linked to marking important information for the
listener.
After publication, we found an error in the scripts used to extract acoustic
information from the target number. The data reported in Watson, Arnold and Tanenhaus
did not just contain information from the cell number, but also from segments preceding
it (usually the entire utterance, although not always). Here we report the correct
intensity, F0, and duration for the cell number as well as the entire utterance.
Table 1 presents the means of the acoustic measures of the entire utterance. In an
analysis of the entire utterance, predictable moves were reliably shorter than
unpredictable moves, F1(1,19)=51.39, p <.001; F2(1,8)=157.74, p <.001. In contrast,
important moves were produced with greater intensity than unimportant moves,
F(1,19)=20.20, p < .001; F2(1,8)=12.67, p<.01. There was a numerical trend for F0 to
have a higher minimum and lower maximum over the entire utterance in the predictable

CORRIGENDUM 3
condition, but the differences by condition were not consistently reliable by both subjects
and items.1
In an analysis of just the cell number (Table 2), unpredictable moves were still
reliably longer than unpredictable moves, F1(1,19)=30.25, p < .001; F2(1,8)=50.06, p
<.001. Unpredictable moves had greater intensity then predictable moves although this
was only reliable by subjects and not items, F1(1,19)=6.17, p < .05; F2<1. There were no
differences in F0.
Thus, the corrected results still support our original conclusion that importance
and predictability independently influence acoustic realization. However, this is true only
when we consider the utterance as a whole. For the cell number itself, we only found an
effect of predictability.
From these data, we can conclude two things. The first is that effects of
predictability are not necessarily localized to individual words: both the unpredictable
cell number and the words that preceded it were lengthened. This may reflect effects of
planning processes being engaged during the production of the words that preceded the
target, supported by the fact that the rate of disfluency in these moves was greater. In
fact, several models of speech production attribute the reduction associated with
predictable material with speaker-centered production processes (e.g. Bell et al., 2009,
Bard et al., 2001). Because these words are less difficult to access, they are produced
with less prominence. Second, these data suggest that factors that correlate with
prominence, such as duration and intensity, can fractionate at the utterance level, and that
intensity might play a special role in marking importance information. Although we do
1

For maximum F0, F1<1; F2(1,8)=8.01, p <.05. For minimum F0, F1(1,19)=6.41, p
<.05; F2(1,8)=1.57.

CORRIGENDUM 4
not find evidence for this divergence at the word level in this dataset, more recent work
suggests that this is also possible. Lam, Watson and Arnold (2008, 2009) independently
manipulated repetition and predictability. They found that repeated words (which are
easy to access) are reduced regardless of predictability, and words that are unexpected
tend to be uttered with greater intensity, regardless of repetition.
Overall, these data suggest that both predictability and importance can influence
the acoustic properties of an utterance but in different ways.

References
Bard, E.G., Anderson, A.H., Sotillo, C. Aylett, M., Doherty-Sneddon, G., & Newlands,
A.(2000) Controlling the intelligibility of referring expressions in dialogue.
Journal of Memory and Language. 42(1), 1-22.

Bell, A., Brenier, J., Gregory, M., Girand, C. and Jurafsky, D. (2009). Predictability
Effects on Durations of Content and Function Words in Conversational English.
Journal of Memory and Language 60:1, 92-111.

Lam, T., Watson, D.G., & Arnold, J. (2008, April). Effects of repeated mention and
predictability on the production of acoustic prominence. Poster presented at
ETAP 2008: Experimental and Theoretical Advances in Prosody, Ithaca, NY.

CORRIGENDUM 5
Lam, T., Watson, D. G., & Arnold, J. (2009, March). Do repeated mention and
expectancy independently affect prosodic prominence? Poster presentation at
CUNY 2009: Conference on Human Sentence Processing Conference, Davis, CA.

Watson, D., Arnold, J. & Tanenhaus, M. (2008). Tic Tac TOE: Effects of predictability
and importance on acoustic prominence in language production. Cognition, 106,
1548-1557.

CORRIGENDUM 6

Table 1. The duration, intensity, and F0 of the entire move.
Important/Predictable

Unimportant/Unpredictable

Duration (ms)

1688 (59)

2732 (173)

Intensity (db)

67.71 (0.84)

67.08 (0.79)

Minimum F0 (Hz)

107 (9)

106 (9)

Maximum F0 (Hz)

188 (15)

189 (15)

Average F0 (Hz)

144 (11)

144 (10)

CORRIGENDUM 7

Table 2. The duration, intensity and F0 of the cell numbers.
Important/Predictable

Unimportant/Unpredictable

Duration (ms)

479 (23)

537 (22)

Intensity (db)

62.83 (.75)

63.25 (.74)

Minimum F0 (Hz)

115 (7)

112 (7)

Maximum F0 (Hz)

159 (11)

161 (11)

Average F0 (Hz)

135 (9)

134 (8)

NIH Public Access
Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

NIH-PA Author Manuscript

Published in final edited form as:
Cognition. 2008 March ; 106(3): 1548–1557.

Tic Tac TOE: Effects of predictability and importance on acoustic
prominence in language production
Duane G. Watson,
Department of Psychology, Beckman Institute for Advanced Science and Technology, University of
Illinois Urbana-Champaign
Jennifer E. Arnold, and
Department of Psychology, University of North Carolina Chapel Hill
Michael K. Tanenhaus
Department of Brain & Cognitive Sciences, University of Rochester

Abstract
NIH-PA Author Manuscript

Importance and predictability each have been argued to contribute to acoustic prominence. To
investigate whether these factors are independent or two aspects of the same phenomenon, naïve
participants played a verbal variant of Tic Tac Toe. Both importance and predictability contributed
independently to the acoustic prominence of a word, but in different ways. Predictable game moves
were shorter in duration and had less pitch excursion than less predictable game moves, whereas
intensity was higher for important game moves. These data also suggest that acoustic prominence is
affected by both speaker-centered processes (speaker effort) and listener-centered processes (intent
to signal important information to the listener).
It is widely assumed that prosodically prominent words play a role in signaling the information
status of entities in a discourse (Chafe, 1974; Bolinger, 1986; Prince, 1981; Jackendoff,
1972; Pierrehumbert & Hirschberg, 1990). For example, the word violin in (1b) receives
emphasis, and this is related to it being the information requested by the speaker.
1)
a.

What does Alessandra play?

NIH-PA Author Manuscript

b. Alessandra plays the VIOLIN.
Although information status is clearly related to whether or not a word receives a pitch accent,
the nature of this relationship is less clear. Two different factors have been claimed to provide
an account of this relationship: 1) the importance of the information to the goals of the
interlocutors and 2) the predictability of the information in a given context.
Bolinger (1972, 1986) argued that the most informative words in a sentence receive an accent
and some version of this view has been used to understand differences in accent type (e.g.
Pierrehumbert & Hirschberg, 1990) as well as the ways in which prominence signals the

Corresponding Author: Duane Watson, Department of Psychology, 603 E Daniel St, Champaign IL, 61820, Email: dgwatson@uiuc.edu,
Phone: (217) 333-0280.
Comments welcome. Please do not circulate without permission
Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers
we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting
proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could
affect the content, and all legal disclaimers that apply to the journal pertain.

Watson et al.

Page 2

NIH-PA Author Manuscript

information structure of a sentence (e.g. Gussenhoven, 1983; Selkirk, 1996; Schwarszchild,
1999). The word “violin” in (1b) receives an accent because, as the answer to the question in
(1a), it is the most important part of the sentence.
Researchers have also argued that acoustic phenomena associated with pitch accenting,
especially duration, are a function of how predictable information is in a discourse. Words that
are statistically predictable from the preceding linguistic context tend to be produced with
shorter duration (Bell et al., 2003; Gregory et al., 1999). In addition, Gregory (2002) found
that words were more likely to be produced without a pitch accent when they were predictable
given their context. Related findings have shown that words that are in predictable contexts
tend to have lower intelligibility (e.g. Bard & Aylett, 1999; Lieberman, 1963; Fowler &
Housum, 1987), which is associated with a reduction in duration.
Importance and predictability might be two aspects of the same phenomenon. Information that
is predictable tends to be less important and information that is not predictable tends to be more
important (Shannon, 1951). Our primary goal is to explore whether these factors are really two
aspects of the same thing or whether one or both of these factors contribute independently to
the acoustic realization of a word.

NIH-PA Author Manuscript

A second goal is to understand whether acoustic prominence is speaker or listener centered.
By marking information that is important, the speaker may help the listener coordinate the
information structure of the utterance. In contrast, effects of predictability, could be the result
of either speaker-centered or listener-centered processes. On the one hand, speakers might aim
to produce intelligible language when there is less information from the context to help the
listener, as in the case of unpredictable words (e.g., Lieberman, 1963). On the other hand,
predictable words might be less prominent because they require less effort by the speaker.
Speakers face the challenge of preparing and uttering their conversational contributions in real
time, often while also completing other, nonlinguistic tasks. When these demands require
speakers to plan complex utterances or prepare upcoming words, they tend to produce disfluent
words like “um” or “uh” (Clark & Fox Tree, 2002), repeat themselves (Clark & Wasow,
1998), and produce intonational phrase boundaries (Watson & Gibson, 2004). These demands
also result in longer durations for words (Bell et al., 2003; Gregory et al., 1999), as does the
production of lower-frequency words (Gahl, 2006). This suggests that when speech is effortful,
word durations are longer, contributing to the impression of acoustic prominence.

NIH-PA Author Manuscript

We used a variation of the game of Tic Tac Toe to separate the predictions of importance and
predictability as well as to understand the cognitive processes underlying prominence. Tic Tac
Toe is traditionally played on a 3 × 3 grid. Players take turns placing a mark in one of the cells
of the grid. The goal of the game is for players to position their marks so that they make a
continuous line of three cells vertically, horizontally, or diagonally. An opponent can prevent
a win by blocking the completion of the opponent’s line. In our variant of the game, players
placed objects on a board. In order to induce participants to produce utterances that were usable
for analysis, participants had their own playing boards and faced away from each other, so that
verbal communication was required. Each player had a group of red objects and a group of
blue objects, and was randomly assigned a color. Each of the cells in the grid was labeled with
a number from 1 to 9, so that the players could indicate cell position by number. The number
was the critical target word used in the analysis.
There are two reasons why this variant of Tic Tac Toe is useful for separating effects of
importance and predictability on acoustic prominence. First, defining importance within the
context of Tic Tac Toe is straightforward. An utterance that introduces a game move that wins
or blocks the win of a game can be defined as more important than an utterance introducing a
move that does not win or block the win of a game. This is particularly advantageous because

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 3

NIH-PA Author Manuscript

defining importance is difficult in most conversations. Importance can vary depending on the
task, conversation, and intentions of the interlocutor--a point that Bolinger (1972) aptly
summarized with the title of his classic article: “Accent is predictable (if you are a mind
reader)”. Within the context of Tic Tac Toe, however, importance is easily operationalized.
Second, Tic Tac Toe allows us to separate contributions of predictability from contributions
of importance in acoustic prominence because moves that are important are highly predictable.
An importance-based account predicts that a move that is important should have relatively high
acoustic prominence. In contrast, a predictability-based account predicts that such a move
should have relatively low acoustic prominence because it is highly predictable.
Consider the following example, illustrated in Figures 1 and 2. Figure 1 displays a game state
in which the utterance of the blue player’s move is of relatively little importance. Because the
red player has only one object on the board, the blue player can place the balloon at almost any
location and not lose (or win) the game. At the same time, the move the blue player is going
to make is not very predictable.

NIH-PA Author Manuscript

In contrast, in a game state such as the one in Figure 2, a move to cell one would be very
important since it is crucial in blocking a win by the red player. At the same time, the game
state makes this move highly predictable. Under an importance-based account, one would
expect the utterance introducing this move to be acoustically prominent because it is critical
to not losing the game. By contrast, the predictability account predicts the utterance introducing
the move to be less acoustically prominent. Over a series of games, we measured how players
pronounced the numbers of the square they were placing their marker in (one through nine),
and compared these pronunciations in cases where the moves were important/highly
predictable and non-important/less predictable.
To understand whether prominence is associated with speaker-centered or listener-centered
processes, linguistic events associated with planning such as disfluencies and intonational
boundaries were measured. If the acoustic marking of either predictability or importance is
speaker-centered, they should co-occur with indicators of speaker difficulty.

NIH-PA Author Manuscript

In sum, within the context of this game, predictability and importance make differing
predictions. If predictability is the primary factor in acoustic prominence, then moves that are
highly important (but highly predictable) should be less acoustically prominent than moves
that are less important (but less predictable). If importance is the critical factor, than we expect
the opposite pattern. Of course, because predictability has been found to correlate with duration
(e.g. Gregory, 2002) and importance has been argued to correlate with pitch, duration, and
intensity (e.g. Bolinger, 1972, 1986), it is possible that one might see contributions of both of
these factors either in concert or in opposition.

Method
Participants
10 pairs (20 participants) of students from the University of Illinois Urbana-Champaign
participated in the experiment for course credit.
Materials and Procedure
Participants were seated at separate tables so that they could not see each other. Each participant
had their own game board and both were given two groups of blue and red paper cut outs of
objects so that they could mark their moves and the moves of their partner. The players were
assigned to the red and blue objects randomly. In order to vary the starting game state,
participants were given a card that instructed them where to place their first piece. These initial
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 4

NIH-PA Author Manuscript

trials were not included in the analysis. Both participants were free to place their objects at any
location after this initial turn. All participants were familiar with the game Tic Tac Toe, and
played 20 games.
16 bit recordings were made of the utterances at 44.1 KHz and were analyzed using the Praat
analysis software (Boersma & Weenik, 2005). A move was labeled as important if it was
necessary for winning the game or preventing a loss on the next turn. All “important” moves
were also more predictable, and the less important moves were less predictable. There were a
total of 642 important moves and 715 non-important moves. In order to control for idiosyncratic
acoustic differences between words and speakers, utterances were only included in the analysis
if the speaker had produced the move in both important and non-important contexts. Using
these criteria, 9.3% of the cell numbers went unmatched. These were not included in the
analysis.

NIH-PA Author Manuscript

We hypothesized that if importance or predictability effects result from speaker-interrnal
processing constraints, they should correlate with markers of production difficulty. We
therefore examined each utterance for the following three factors: 1) was there an intonational
boundary or pause immediately before the prepositional phrase denoting the move location
(e.g., “[boundary] on number five”), or before the number word (“on [boundary] five”); 2) was
there any disfluency in the utterance (um, uh, repeats, repairs, saying “thiy” for “the” or “ay”
for “a”, or hesitations (“hmmm”, “mmm”, “ooooh”); and 3) the duration of the object phrase.

Results
The means for maximum F0, minimum F0, intensity and duration are presented in Table 1. In
this analysis, there was a reliable effect of intensity in the direction predicted by importance.
Cell numbers produced in the context of important moves had higher overall intensity than
numbers produced in non-important moves, F1(1,19)=9.92, p < .01; F2(1,8)=20.53, p < .01.
The opposite pattern was found for duration, in keeping with the predictability hypothesis.
Moves that were not predictable contained cell numbers that were produced with longer
duration than predictable moves, F1(1,19)=55.51, p <.001; F2(1,8)=109.78, p < .001.

NIH-PA Author Manuscript

Measures of F0 change and amplitude also patterned with the duration data. The pitch excursion
(maximum F0) was higher when the move was not predictable F1(1,19)=4.80, p <. 05; F(1,8)
=15.08, p < .01. There was also a greater difference in pitch range (the difference between the
maximum and minimum F0) for non-predictable moves than predictable moves, F1(1,19)
=7.18, p < .05; F2(1,8)=24.54, p < .01. One might argue that the source of these F0 differences
is attributable to duration. If the rising F0 for two words of unequal length have the same slope
and starting point, the pitch excursion will be higher for the longer word because the time over
which the F0 has to rise is greater. However, the minimum F0 in the non-important/nonpredictable conditions was lower than in the important/predictable conditions, F1(1,19)=8.22,
p < .05; F2(1,8)=10.85, p < .05, suggesting that the larger pitch difference for non-important
moves was driven by an expanded pitch range rather than just differences in duration.
One consequence of Tic Tac Toe is that the predictability of game moves changes as the game
progresses. As more and more spaces are filled on a game board, the predictability of any
potential move increases. Thus, it is possible that the differences between important and nonimportant moves reflect the changing predictability of moves across the game rather than
differences between the predictability and importance of a move given a game state. To see if
this was in fact the case, data from the sixth move of the game and higher were examined in a
sub-analysis. All of these moves occur relatively late in the game when the majority of the cells
have been filled, and consequently, should be less affected by predictability based on the time
point in the game. The F0, duration, and intensity of the cell numbers in these moves are in
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 5

NIH-PA Author Manuscript

presented in Table 2. After the sixth move of the game, cell numbers produced in nonpredictable contexts were significantly longer than moves produced in predictable contexts F1
(1,19)=38.45, p < .001; F2(1,8)=22.61, p< .01. There were also significant effects of intensity,
with predictable/important moves having a greater intensity than non-predictable/nonimportant moves, F1(1,19)=17.97, p < .001; F2(1,8)=11.59, p < .01. There were no differences
in F0.

NIH-PA Author Manuscript

These data suggest that both predictability and importance contribute to the acoustic realization
of a word. While important moves were produced with higher intensity, non-predicable, nonimportant moves were produced with higher F0 and duration. An analysis of the measures of
production difficulty suggests that the latter may be due to effortful planning. Speakers had
more production difficulty in the non-predictable/non-important moves, as shown in Table 3.
There was a higher likelihood of pausing immediately before the target number word (F1(1,19)
= 35.94, p < .001; F2(1,8) = 52.07, p < .001). Speakers were also more likely to produce a
disfluency in non-predictable than predictable moves (F1(1,19) = 35.60, p < .001; F2 (1,8) =
24.71, p < .001). Even though the target word always came at the end of the utterance, we
expected that some planning would occur earlier in the utterance (cf. Clark & Wasow, 1998).
We therefore examined the object phrase for evidence of planning and production difficulty,
assuming that more words would be produced and that the produced words would have longer
duration when more planning time was necessary. As predicted, the object phrase was longer
in duration when the move was non-predictable than when it was predictable, (F1(1,15) = 19.71,
p < .01, F2(1,5) = 28.91, p < .001).

Discussion
These results suggest that both importance and predictability play a role in the acoustic
realization of a word. Duration is longer and pitch movement is greater for non-predictable
words while intensity is greater for important words.
One potential concern is that the effects of intensity may have been the result of paralinguistic
factors such as emotional excitement related to winning the game. How one might distinguish
between the linguistic and paralinguistic factors that drive prominence is a question of
considerable debate (see Ladd, 1996 for a discussion). However, it is important to keep in mind
that important trials did not consist of only emotionally exciting wins, but also the relatively
more routine cases where a win was blocked. Moreover, the speaker’s personal reaction to the
importance of their utterance is not inconsistent with the proposal that importance drives
acoustic prominence.

NIH-PA Author Manuscript

These results also suggest that acoustic prominence is the result of both speaker and listenercentered processes. Disfluencies and intonational boundaries, which are linked to production
difficulty, occurred more frequently in the non-predictable moves. This suggests that both
duration and F0, which were also higher in these conditions, are linked to speaker-centered
processes. It is unknown whether duration and pitch effects are the byproducts of planning
processes, such that they are the result of more effortful production, or whether they actually
facilitate the production of lexical items that are difficult to access. In the case of duration, a
longer word might facilitate production by providing more processing time. These data also
suggest that acoustic prominence may be used overtly to assist listeners. The intensity of the
target word, which marked important information, did not correlate with disfluencies and
intonational boundaries, suggesting only a weak link with production processes. Therefore,
marking the target word with higher intensity may have been done to assist the listener.
The more general question of whether speaker preferences are listener centered or speaker
centered is a topic of considerable debate in the psycholinguistics literature (see Arnold, in

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 6

NIH-PA Author Manuscript

press). Recent work suggests that speakers’ production choices are often made without listener
needs in mind, both with respect to prosody (Schafer et al., 2001; Kraljic & Brennan, 2005),
and syntactic choices (Arnold, Wasow, Asudeh, & Alrenga, 2004; Ferreira & Dell, 2000, but
see Haywood, Pickering, & Branigan, 2005). However, research also suggests that listeners
pay attention to prominences (Arnold, 2007; Dahan, Tanenhaus & Chambers, 2002), perhaps
because a speaker’s production choices create systematic patterns in the input that listeners
learn implicitly. Moreover, many researchers have argued that speaker’s and listener’s mental
states are closely aligned in conversation (Clark, 1996; Pickering & Garrod, 2004; BrownSchmidt, Campana & Tanenhaus, 2005). Studying acoustic prominence in this context may
shed light on understanding the interplay between speaker and listener requirements.
An important challenge for future research is to understand how listeners are able to compute
prominence, given that prominence can be conveyed by multiple acoustic dimensions, some
of which may fractionate under different circumstances. Ladd (1996), among others, has
warned against interpreting any one acoustic factor such as F0, for example, as a transducer of
discourse status. The data presented here are consistent with this claim. They suggest that a
complex gestalt of acoustic features work together to convey prominence, with different factors
affecting the acoustic signal in different ways (e.g. importance affects intensity and
predictability affects duration and F0).

NIH-PA Author Manuscript

Acknowledgements
This project was supported by NIH grant HD-27206 to M. Tanenhaus and NIH grant HD-41522 to J. Arnold. The first
author was supported by NSF grant SES-0208484. We would like to thank Jill Thorson, Shin-Yi Lao, and Abhishek
Shroff for help with data collection and analysis.

References

NIH-PA Author Manuscript

Arnold JE. Reference Production: Production-internal and Addressee-oriented Processes. Language and
Cognitive Processes. in press
Arnold, JE. THE BACON not the bacon: How children and adults understand accented and unaccented
noun phrases. University of North Carolina; Chapel Hill: 2007. Unpublished manuscript
Arnold JE, Wasow T, Asudeh A, Alrenga P. Avoiding attachment ambiguities: The role of constituent
ordering. Journal of Memory and Language 2004;51:55–70.
Bard, EG.; Aylett, MP. The dissociation of deaccenting, givenness and syntactic role in sponteaneous
speech. Proceedings of ICPhS-99; San Francisco. 1999.
Bell A, Jurafsky D, Fosler-Lussier E, Girand C, Gregory M, Gildea D. Effects of disfluencies,
predictability, and utterance position on word form variation in English conversation. Journal of the
Acoustical Society of America 2003;113(2):1001–1024. [PubMed: 12597194]
Boersma, P.; Weenink, D. Praat: Doing phonetics by computer (Version 4.5.01) [Computer program].
Amsterdam: Institute of Phonetic Sciences; 2005. Retrieved from http://www.praat.org/
Bolinger D. Accent is predictable (if you’re a mind-reader). Language 1972;48:633–644.
Bolinger, D. Intonation and Its Parts: Melody in Spoken English. Stanford, CA: Stanford University
Press; 1986.
Brown-Schmidt, S.; Campana, E.; Tanenhaus, MK. Real-time reference resolution by naïve participants
during a task-based unscripted conversation. In: Trueswell, JC.; Tanenhaus, MK., editors. Worldsituated language processing: Bridging the language as product and language as action traditions.
Cambridge: MIT Press; 2005. p. 153-171.
Chafe W. Language and consciousness. Language 1974;50:111–133.
Clark, HH. Using language. Cambridge University Press; 1996.
Clark HH, Fox Tree JE. Using uh and um in spontaneous speech. Cognition 2002;84:73–111. [PubMed:
12062148]
Clark HH, Wasow T. Repeating words in spontaneous speech. Cognitive Psychology 1998;37:201–242.
[PubMed: 9892548]

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 7

NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript

Dahan D, Tanenhaus MK, Chambers CG. Accent and reference resolution in spoken-language
comprehension. Journal of Memory and Language 2002;47:292–314.
Ferreira VS, Dell GS. Effect of ambiguity and lexical availability on syntactic and lexical production.
Cognitive Psychology 2000;40:296–340. [PubMed: 10888342]
Fowler CA, Housum J. Talkers’ signaling of “new” and “old” words in speech and listeners’ perception
and use of the distinction. Journal of Memory and Language 1987;26:489–504.
Gahl, S. Is frequency a property of phonological forms? Evidence from spontaneous speech. Paper
presented at the19th annual CUNY conference on human sentence processing; New York City, NY.
2006.
Gregory, ML. Linguistic Informativeness and Speech Production: An Investigation of Contextual and
Discourse Pragmatic Effects on Phonological Variation. University of Colorado Boulder; 2002.
Unpublished doctoral dissertation
Gregory, ML.; Raymond, W.; Bell, A.; Jurafsky, D. Effects of informativeness on word duration in
conversation. Vancouver, B.C: Society for Text and Discourse; 1999.
Gussenhoven, C. A semantic analysis of the nuclear tones of English. Bloomington, Ind: Indiana
University Linguistics Club; 1983.
Haywood S, Pickering MJ, Branigan HP. Do Speakers Avoid Ambiguities During Dialogue?
Psychological Science 2005;16(5):362–366. [PubMed: 15869694]
Jackendoff, R. Semantic interpretation in Generative Grammar. Cambridge, MA: MIT Press; 1972.
Kraljic T, Brennan SE. Prosodic disambiguation of syntactic structure: For the speaker or for the
addressee? Cognitive Psychology 2005;50:194–231. [PubMed: 15680144]
Ladd, DR. Intonational Phonology. Cambridge: Cambridge University Press; 1996.
Lieberman P. Some effects of the semantic and grammatical context on the production and perception of
speech. Language and Speech 1963;6:172–175.
Pickering MJ, Garrod S. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences
2004;27
Pierrehumbert, J.; Hirschberg, J. The meaning of intonational contours in the interpretation of discourse.
In: Cohen, PR.; Morgan, J.; Pollack, ME., editors. Intentions in Communication. Cambridge, MA:
MIT Press; 1990. p. 271-311.
Prince, EF. Toward a taxonomy of given-new information. In: Cole, P., editor. Radical Pragmatics. New
York, NY: Academic Press; 1981. p. 223-255.
Schafer, AJ.; Speer, SR.; Warren, P.; White, SD. Prosodic influences on the production and
comprehension of syntactic ambiguity in a game-based conversation task. Fourteenth Annual CUNY
Conference on Human Sentence Processing; Philadelphia, PA. 2001.
Schwarzschild R. GIVENness, Avoid F and other constraints on the placement of focus. Natural
Language Semantics 1999;7:141–177.
Selkirk, EO. Sentence prosody: Intonation, stress and phrasing. In: Goldsmith, JA., editor. The handbook
of phonological theory. Cambridge, Mass., USA: Blackwell; 1996. p. 550-569.
Shannon CE. Prediction and entropy of printed english. Bell System Technical Journal 1951;30:50–64.
Watson D, Gibson E. The relationship between intonational phrasing and syntactic structure in language
production. Language and Cognitive Processes 2004;19:713–755.

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 8

NIH-PA Author Manuscript
NIH-PA Author Manuscript

Figure 1.

A Tic Tac Toe game state in which the blue player’s move can be described as of relatively
low importance for the goals of the game since the placement of the blue player’s balloon
cannot win or prevent the win of a game.

NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 9

NIH-PA Author Manuscript
NIH-PA Author Manuscript

Figure 2.

A Tic Tac Toe game state in which the move blue player’s move is important. In order to
prevent a loss of the game, the blue player must place the balloon in cell 1.

NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 10

Table 1

The F0, intensity and duration of the cell numbers for each game move. Standard errors are in parentheses.

NIH-PA Author Manuscript

Important/Predictable
Maximum F0 (Hz)
Minimum F0 (Hz)
Intensity (db)
Duration (msec)

Non-important/Non-predictable

224.27 (10.38)
105.41 (6.18)
67.16 (.83)
1617 (64)

240.67 (12.10)
99.86 (5.20)
66.50 (.79)
2585 (149)

NIH-PA Author Manuscript
NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 11

Table 2

The F0, intensity and duration of the cell numbers after the sixth move of the game. Standard errors are in
parentheses.

NIH-PA Author Manuscript

Important/Predictable
Maximum F0 (Hz)
Minimum F0 (Hz)
Intensity (db)
Duration (msec)

Non-important/Non-predictable

227.66 (12.75)
103.51 (5.79)
67.09 (.79)
1557 (76)

222.20 (12.98)
101.79 (5.92)
66.00 (.87)
2206 (115)

NIH-PA Author Manuscript
NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 12

Table 3

Percentage of disfluencies and pauses in the game moves. Standard errors are in parentheses.

NIH-PA Author Manuscript

Important/Predictable
% disfluent utterances
% pauses before target word or target PP
Duration of object phrase (msec)

Non-important/Non-predictable

9.79 (2.15)
15.10 (2.86)
552 (51)

25.36 (3.68)
37.28 (3.53)
889 (91)

NIH-PA Author Manuscript
NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

NIH Public Access
Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

NIH-PA Author Manuscript

Published in final edited form as:
Cognition. 2008 March ; 106(3): 1548–1557.

Tic Tac TOE: Effects of predictability and importance on acoustic
prominence in language production
Duane G. Watson,
Department of Psychology, Beckman Institute for Advanced Science and Technology, University of
Illinois Urbana-Champaign
Jennifer E. Arnold, and
Department of Psychology, University of North Carolina Chapel Hill
Michael K. Tanenhaus
Department of Brain & Cognitive Sciences, University of Rochester

Abstract
NIH-PA Author Manuscript

Importance and predictability each have been argued to contribute to acoustic prominence. To
investigate whether these factors are independent or two aspects of the same phenomenon, naïve
participants played a verbal variant of Tic Tac Toe. Both importance and predictability contributed
independently to the acoustic prominence of a word, but in different ways. Predictable game moves
were shorter in duration and had less pitch excursion than less predictable game moves, whereas
intensity was higher for important game moves. These data also suggest that acoustic prominence is
affected by both speaker-centered processes (speaker effort) and listener-centered processes (intent
to signal important information to the listener).
It is widely assumed that prosodically prominent words play a role in signaling the information
status of entities in a discourse (Chafe, 1974; Bolinger, 1986; Prince, 1981; Jackendoff,
1972; Pierrehumbert & Hirschberg, 1990). For example, the word violin in (1b) receives
emphasis, and this is related to it being the information requested by the speaker.
1)
a.

What does Alessandra play?

NIH-PA Author Manuscript

b. Alessandra plays the VIOLIN.
Although information status is clearly related to whether or not a word receives a pitch accent,
the nature of this relationship is less clear. Two different factors have been claimed to provide
an account of this relationship: 1) the importance of the information to the goals of the
interlocutors and 2) the predictability of the information in a given context.
Bolinger (1972, 1986) argued that the most informative words in a sentence receive an accent
and some version of this view has been used to understand differences in accent type (e.g.
Pierrehumbert & Hirschberg, 1990) as well as the ways in which prominence signals the

Corresponding Author: Duane Watson, Department of Psychology, 603 E Daniel St, Champaign IL, 61820, Email: dgwatson@uiuc.edu,
Phone: (217) 333-0280.
Comments welcome. Please do not circulate without permission
Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers
we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting
proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could
affect the content, and all legal disclaimers that apply to the journal pertain.

Watson et al.

Page 2

NIH-PA Author Manuscript

information structure of a sentence (e.g. Gussenhoven, 1983; Selkirk, 1996; Schwarszchild,
1999). The word “violin” in (1b) receives an accent because, as the answer to the question in
(1a), it is the most important part of the sentence.
Researchers have also argued that acoustic phenomena associated with pitch accenting,
especially duration, are a function of how predictable information is in a discourse. Words that
are statistically predictable from the preceding linguistic context tend to be produced with
shorter duration (Bell et al., 2003; Gregory et al., 1999). In addition, Gregory (2002) found
that words were more likely to be produced without a pitch accent when they were predictable
given their context. Related findings have shown that words that are in predictable contexts
tend to have lower intelligibility (e.g. Bard & Aylett, 1999; Lieberman, 1963; Fowler &
Housum, 1987), which is associated with a reduction in duration.
Importance and predictability might be two aspects of the same phenomenon. Information that
is predictable tends to be less important and information that is not predictable tends to be more
important (Shannon, 1951). Our primary goal is to explore whether these factors are really two
aspects of the same thing or whether one or both of these factors contribute independently to
the acoustic realization of a word.

NIH-PA Author Manuscript

A second goal is to understand whether acoustic prominence is speaker or listener centered.
By marking information that is important, the speaker may help the listener coordinate the
information structure of the utterance. In contrast, effects of predictability, could be the result
of either speaker-centered or listener-centered processes. On the one hand, speakers might aim
to produce intelligible language when there is less information from the context to help the
listener, as in the case of unpredictable words (e.g., Lieberman, 1963). On the other hand,
predictable words might be less prominent because they require less effort by the speaker.
Speakers face the challenge of preparing and uttering their conversational contributions in real
time, often while also completing other, nonlinguistic tasks. When these demands require
speakers to plan complex utterances or prepare upcoming words, they tend to produce disfluent
words like “um” or “uh” (Clark & Fox Tree, 2002), repeat themselves (Clark & Wasow,
1998), and produce intonational phrase boundaries (Watson & Gibson, 2004). These demands
also result in longer durations for words (Bell et al., 2003; Gregory et al., 1999), as does the
production of lower-frequency words (Gahl, 2006). This suggests that when speech is effortful,
word durations are longer, contributing to the impression of acoustic prominence.

NIH-PA Author Manuscript

We used a variation of the game of Tic Tac Toe to separate the predictions of importance and
predictability as well as to understand the cognitive processes underlying prominence. Tic Tac
Toe is traditionally played on a 3 × 3 grid. Players take turns placing a mark in one of the cells
of the grid. The goal of the game is for players to position their marks so that they make a
continuous line of three cells vertically, horizontally, or diagonally. An opponent can prevent
a win by blocking the completion of the opponent’s line. In our variant of the game, players
placed objects on a board. In order to induce participants to produce utterances that were usable
for analysis, participants had their own playing boards and faced away from each other, so that
verbal communication was required. Each player had a group of red objects and a group of
blue objects, and was randomly assigned a color. Each of the cells in the grid was labeled with
a number from 1 to 9, so that the players could indicate cell position by number. The number
was the critical target word used in the analysis.
There are two reasons why this variant of Tic Tac Toe is useful for separating effects of
importance and predictability on acoustic prominence. First, defining importance within the
context of Tic Tac Toe is straightforward. An utterance that introduces a game move that wins
or blocks the win of a game can be defined as more important than an utterance introducing a
move that does not win or block the win of a game. This is particularly advantageous because

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 3

NIH-PA Author Manuscript

defining importance is difficult in most conversations. Importance can vary depending on the
task, conversation, and intentions of the interlocutor--a point that Bolinger (1972) aptly
summarized with the title of his classic article: “Accent is predictable (if you are a mind
reader)”. Within the context of Tic Tac Toe, however, importance is easily operationalized.
Second, Tic Tac Toe allows us to separate contributions of predictability from contributions
of importance in acoustic prominence because moves that are important are highly predictable.
An importance-based account predicts that a move that is important should have relatively high
acoustic prominence. In contrast, a predictability-based account predicts that such a move
should have relatively low acoustic prominence because it is highly predictable.
Consider the following example, illustrated in Figures 1 and 2. Figure 1 displays a game state
in which the utterance of the blue player’s move is of relatively little importance. Because the
red player has only one object on the board, the blue player can place the balloon at almost any
location and not lose (or win) the game. At the same time, the move the blue player is going
to make is not very predictable.

NIH-PA Author Manuscript

In contrast, in a game state such as the one in Figure 2, a move to cell one would be very
important since it is crucial in blocking a win by the red player. At the same time, the game
state makes this move highly predictable. Under an importance-based account, one would
expect the utterance introducing this move to be acoustically prominent because it is critical
to not losing the game. By contrast, the predictability account predicts the utterance introducing
the move to be less acoustically prominent. Over a series of games, we measured how players
pronounced the numbers of the square they were placing their marker in (one through nine),
and compared these pronunciations in cases where the moves were important/highly
predictable and non-important/less predictable.
To understand whether prominence is associated with speaker-centered or listener-centered
processes, linguistic events associated with planning such as disfluencies and intonational
boundaries were measured. If the acoustic marking of either predictability or importance is
speaker-centered, they should co-occur with indicators of speaker difficulty.

NIH-PA Author Manuscript

In sum, within the context of this game, predictability and importance make differing
predictions. If predictability is the primary factor in acoustic prominence, then moves that are
highly important (but highly predictable) should be less acoustically prominent than moves
that are less important (but less predictable). If importance is the critical factor, than we expect
the opposite pattern. Of course, because predictability has been found to correlate with duration
(e.g. Gregory, 2002) and importance has been argued to correlate with pitch, duration, and
intensity (e.g. Bolinger, 1972, 1986), it is possible that one might see contributions of both of
these factors either in concert or in opposition.

Method
Participants
10 pairs (20 participants) of students from the University of Illinois Urbana-Champaign
participated in the experiment for course credit.
Materials and Procedure
Participants were seated at separate tables so that they could not see each other. Each participant
had their own game board and both were given two groups of blue and red paper cut outs of
objects so that they could mark their moves and the moves of their partner. The players were
assigned to the red and blue objects randomly. In order to vary the starting game state,
participants were given a card that instructed them where to place their first piece. These initial
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 4

NIH-PA Author Manuscript

trials were not included in the analysis. Both participants were free to place their objects at any
location after this initial turn. All participants were familiar with the game Tic Tac Toe, and
played 20 games.
16 bit recordings were made of the utterances at 44.1 KHz and were analyzed using the Praat
analysis software (Boersma & Weenik, 2005). A move was labeled as important if it was
necessary for winning the game or preventing a loss on the next turn. All “important” moves
were also more predictable, and the less important moves were less predictable. There were a
total of 642 important moves and 715 non-important moves. In order to control for idiosyncratic
acoustic differences between words and speakers, utterances were only included in the analysis
if the speaker had produced the move in both important and non-important contexts. Using
these criteria, 9.3% of the cell numbers went unmatched. These were not included in the
analysis.

NIH-PA Author Manuscript

We hypothesized that if importance or predictability effects result from speaker-interrnal
processing constraints, they should correlate with markers of production difficulty. We
therefore examined each utterance for the following three factors: 1) was there an intonational
boundary or pause immediately before the prepositional phrase denoting the move location
(e.g., “[boundary] on number five”), or before the number word (“on [boundary] five”); 2) was
there any disfluency in the utterance (um, uh, repeats, repairs, saying “thiy” for “the” or “ay”
for “a”, or hesitations (“hmmm”, “mmm”, “ooooh”); and 3) the duration of the object phrase.

Results
The means for maximum F0, minimum F0, intensity and duration are presented in Table 1. In
this analysis, there was a reliable effect of intensity in the direction predicted by importance.
Cell numbers produced in the context of important moves had higher overall intensity than
numbers produced in non-important moves, F1(1,19)=9.92, p < .01; F2(1,8)=20.53, p < .01.
The opposite pattern was found for duration, in keeping with the predictability hypothesis.
Moves that were not predictable contained cell numbers that were produced with longer
duration than predictable moves, F1(1,19)=55.51, p <.001; F2(1,8)=109.78, p < .001.

NIH-PA Author Manuscript

Measures of F0 change and amplitude also patterned with the duration data. The pitch excursion
(maximum F0) was higher when the move was not predictable F1(1,19)=4.80, p <. 05; F(1,8)
=15.08, p < .01. There was also a greater difference in pitch range (the difference between the
maximum and minimum F0) for non-predictable moves than predictable moves, F1(1,19)
=7.18, p < .05; F2(1,8)=24.54, p < .01. One might argue that the source of these F0 differences
is attributable to duration. If the rising F0 for two words of unequal length have the same slope
and starting point, the pitch excursion will be higher for the longer word because the time over
which the F0 has to rise is greater. However, the minimum F0 in the non-important/nonpredictable conditions was lower than in the important/predictable conditions, F1(1,19)=8.22,
p < .05; F2(1,8)=10.85, p < .05, suggesting that the larger pitch difference for non-important
moves was driven by an expanded pitch range rather than just differences in duration.
One consequence of Tic Tac Toe is that the predictability of game moves changes as the game
progresses. As more and more spaces are filled on a game board, the predictability of any
potential move increases. Thus, it is possible that the differences between important and nonimportant moves reflect the changing predictability of moves across the game rather than
differences between the predictability and importance of a move given a game state. To see if
this was in fact the case, data from the sixth move of the game and higher were examined in a
sub-analysis. All of these moves occur relatively late in the game when the majority of the cells
have been filled, and consequently, should be less affected by predictability based on the time
point in the game. The F0, duration, and intensity of the cell numbers in these moves are in
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 5

NIH-PA Author Manuscript

presented in Table 2. After the sixth move of the game, cell numbers produced in nonpredictable contexts were significantly longer than moves produced in predictable contexts F1
(1,19)=38.45, p < .001; F2(1,8)=22.61, p< .01. There were also significant effects of intensity,
with predictable/important moves having a greater intensity than non-predictable/nonimportant moves, F1(1,19)=17.97, p < .001; F2(1,8)=11.59, p < .01. There were no differences
in F0.

NIH-PA Author Manuscript

These data suggest that both predictability and importance contribute to the acoustic realization
of a word. While important moves were produced with higher intensity, non-predicable, nonimportant moves were produced with higher F0 and duration. An analysis of the measures of
production difficulty suggests that the latter may be due to effortful planning. Speakers had
more production difficulty in the non-predictable/non-important moves, as shown in Table 3.
There was a higher likelihood of pausing immediately before the target number word (F1(1,19)
= 35.94, p < .001; F2(1,8) = 52.07, p < .001). Speakers were also more likely to produce a
disfluency in non-predictable than predictable moves (F1(1,19) = 35.60, p < .001; F2 (1,8) =
24.71, p < .001). Even though the target word always came at the end of the utterance, we
expected that some planning would occur earlier in the utterance (cf. Clark & Wasow, 1998).
We therefore examined the object phrase for evidence of planning and production difficulty,
assuming that more words would be produced and that the produced words would have longer
duration when more planning time was necessary. As predicted, the object phrase was longer
in duration when the move was non-predictable than when it was predictable, (F1(1,15) = 19.71,
p < .01, F2(1,5) = 28.91, p < .001).

Discussion
These results suggest that both importance and predictability play a role in the acoustic
realization of a word. Duration is longer and pitch movement is greater for non-predictable
words while intensity is greater for important words.
One potential concern is that the effects of intensity may have been the result of paralinguistic
factors such as emotional excitement related to winning the game. How one might distinguish
between the linguistic and paralinguistic factors that drive prominence is a question of
considerable debate (see Ladd, 1996 for a discussion). However, it is important to keep in mind
that important trials did not consist of only emotionally exciting wins, but also the relatively
more routine cases where a win was blocked. Moreover, the speaker’s personal reaction to the
importance of their utterance is not inconsistent with the proposal that importance drives
acoustic prominence.

NIH-PA Author Manuscript

These results also suggest that acoustic prominence is the result of both speaker and listenercentered processes. Disfluencies and intonational boundaries, which are linked to production
difficulty, occurred more frequently in the non-predictable moves. This suggests that both
duration and F0, which were also higher in these conditions, are linked to speaker-centered
processes. It is unknown whether duration and pitch effects are the byproducts of planning
processes, such that they are the result of more effortful production, or whether they actually
facilitate the production of lexical items that are difficult to access. In the case of duration, a
longer word might facilitate production by providing more processing time. These data also
suggest that acoustic prominence may be used overtly to assist listeners. The intensity of the
target word, which marked important information, did not correlate with disfluencies and
intonational boundaries, suggesting only a weak link with production processes. Therefore,
marking the target word with higher intensity may have been done to assist the listener.
The more general question of whether speaker preferences are listener centered or speaker
centered is a topic of considerable debate in the psycholinguistics literature (see Arnold, in

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 6

NIH-PA Author Manuscript

press). Recent work suggests that speakers’ production choices are often made without listener
needs in mind, both with respect to prosody (Schafer et al., 2001; Kraljic & Brennan, 2005),
and syntactic choices (Arnold, Wasow, Asudeh, & Alrenga, 2004; Ferreira & Dell, 2000, but
see Haywood, Pickering, & Branigan, 2005). However, research also suggests that listeners
pay attention to prominences (Arnold, 2007; Dahan, Tanenhaus & Chambers, 2002), perhaps
because a speaker’s production choices create systematic patterns in the input that listeners
learn implicitly. Moreover, many researchers have argued that speaker’s and listener’s mental
states are closely aligned in conversation (Clark, 1996; Pickering & Garrod, 2004; BrownSchmidt, Campana & Tanenhaus, 2005). Studying acoustic prominence in this context may
shed light on understanding the interplay between speaker and listener requirements.
An important challenge for future research is to understand how listeners are able to compute
prominence, given that prominence can be conveyed by multiple acoustic dimensions, some
of which may fractionate under different circumstances. Ladd (1996), among others, has
warned against interpreting any one acoustic factor such as F0, for example, as a transducer of
discourse status. The data presented here are consistent with this claim. They suggest that a
complex gestalt of acoustic features work together to convey prominence, with different factors
affecting the acoustic signal in different ways (e.g. importance affects intensity and
predictability affects duration and F0).

NIH-PA Author Manuscript

Acknowledgements
This project was supported by NIH grant HD-27206 to M. Tanenhaus and NIH grant HD-41522 to J. Arnold. The first
author was supported by NSF grant SES-0208484. We would like to thank Jill Thorson, Shin-Yi Lao, and Abhishek
Shroff for help with data collection and analysis.

References

NIH-PA Author Manuscript

Arnold JE. Reference Production: Production-internal and Addressee-oriented Processes. Language and
Cognitive Processes. in press
Arnold, JE. THE BACON not the bacon: How children and adults understand accented and unaccented
noun phrases. University of North Carolina; Chapel Hill: 2007. Unpublished manuscript
Arnold JE, Wasow T, Asudeh A, Alrenga P. Avoiding attachment ambiguities: The role of constituent
ordering. Journal of Memory and Language 2004;51:55–70.
Bard, EG.; Aylett, MP. The dissociation of deaccenting, givenness and syntactic role in sponteaneous
speech. Proceedings of ICPhS-99; San Francisco. 1999.
Bell A, Jurafsky D, Fosler-Lussier E, Girand C, Gregory M, Gildea D. Effects of disfluencies,
predictability, and utterance position on word form variation in English conversation. Journal of the
Acoustical Society of America 2003;113(2):1001–1024. [PubMed: 12597194]
Boersma, P.; Weenink, D. Praat: Doing phonetics by computer (Version 4.5.01) [Computer program].
Amsterdam: Institute of Phonetic Sciences; 2005. Retrieved from http://www.praat.org/
Bolinger D. Accent is predictable (if you’re a mind-reader). Language 1972;48:633–644.
Bolinger, D. Intonation and Its Parts: Melody in Spoken English. Stanford, CA: Stanford University
Press; 1986.
Brown-Schmidt, S.; Campana, E.; Tanenhaus, MK. Real-time reference resolution by naïve participants
during a task-based unscripted conversation. In: Trueswell, JC.; Tanenhaus, MK., editors. Worldsituated language processing: Bridging the language as product and language as action traditions.
Cambridge: MIT Press; 2005. p. 153-171.
Chafe W. Language and consciousness. Language 1974;50:111–133.
Clark, HH. Using language. Cambridge University Press; 1996.
Clark HH, Fox Tree JE. Using uh and um in spontaneous speech. Cognition 2002;84:73–111. [PubMed:
12062148]
Clark HH, Wasow T. Repeating words in spontaneous speech. Cognitive Psychology 1998;37:201–242.
[PubMed: 9892548]

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 7

NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript

Dahan D, Tanenhaus MK, Chambers CG. Accent and reference resolution in spoken-language
comprehension. Journal of Memory and Language 2002;47:292–314.
Ferreira VS, Dell GS. Effect of ambiguity and lexical availability on syntactic and lexical production.
Cognitive Psychology 2000;40:296–340. [PubMed: 10888342]
Fowler CA, Housum J. Talkers’ signaling of “new” and “old” words in speech and listeners’ perception
and use of the distinction. Journal of Memory and Language 1987;26:489–504.
Gahl, S. Is frequency a property of phonological forms? Evidence from spontaneous speech. Paper
presented at the19th annual CUNY conference on human sentence processing; New York City, NY.
2006.
Gregory, ML. Linguistic Informativeness and Speech Production: An Investigation of Contextual and
Discourse Pragmatic Effects on Phonological Variation. University of Colorado Boulder; 2002.
Unpublished doctoral dissertation
Gregory, ML.; Raymond, W.; Bell, A.; Jurafsky, D. Effects of informativeness on word duration in
conversation. Vancouver, B.C: Society for Text and Discourse; 1999.
Gussenhoven, C. A semantic analysis of the nuclear tones of English. Bloomington, Ind: Indiana
University Linguistics Club; 1983.
Haywood S, Pickering MJ, Branigan HP. Do Speakers Avoid Ambiguities During Dialogue?
Psychological Science 2005;16(5):362–366. [PubMed: 15869694]
Jackendoff, R. Semantic interpretation in Generative Grammar. Cambridge, MA: MIT Press; 1972.
Kraljic T, Brennan SE. Prosodic disambiguation of syntactic structure: For the speaker or for the
addressee? Cognitive Psychology 2005;50:194–231. [PubMed: 15680144]
Ladd, DR. Intonational Phonology. Cambridge: Cambridge University Press; 1996.
Lieberman P. Some effects of the semantic and grammatical context on the production and perception of
speech. Language and Speech 1963;6:172–175.
Pickering MJ, Garrod S. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences
2004;27
Pierrehumbert, J.; Hirschberg, J. The meaning of intonational contours in the interpretation of discourse.
In: Cohen, PR.; Morgan, J.; Pollack, ME., editors. Intentions in Communication. Cambridge, MA:
MIT Press; 1990. p. 271-311.
Prince, EF. Toward a taxonomy of given-new information. In: Cole, P., editor. Radical Pragmatics. New
York, NY: Academic Press; 1981. p. 223-255.
Schafer, AJ.; Speer, SR.; Warren, P.; White, SD. Prosodic influences on the production and
comprehension of syntactic ambiguity in a game-based conversation task. Fourteenth Annual CUNY
Conference on Human Sentence Processing; Philadelphia, PA. 2001.
Schwarzschild R. GIVENness, Avoid F and other constraints on the placement of focus. Natural
Language Semantics 1999;7:141–177.
Selkirk, EO. Sentence prosody: Intonation, stress and phrasing. In: Goldsmith, JA., editor. The handbook
of phonological theory. Cambridge, Mass., USA: Blackwell; 1996. p. 550-569.
Shannon CE. Prediction and entropy of printed english. Bell System Technical Journal 1951;30:50–64.
Watson D, Gibson E. The relationship between intonational phrasing and syntactic structure in language
production. Language and Cognitive Processes 2004;19:713–755.

Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 8

NIH-PA Author Manuscript
NIH-PA Author Manuscript

Figure 1.

A Tic Tac Toe game state in which the blue player’s move can be described as of relatively
low importance for the goals of the game since the placement of the blue player’s balloon
cannot win or prevent the win of a game.

NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 9

NIH-PA Author Manuscript
NIH-PA Author Manuscript

Figure 2.

A Tic Tac Toe game state in which the move blue player’s move is important. In order to
prevent a loss of the game, the blue player must place the balloon in cell 1.

NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 10

Table 1

The F0, intensity and duration of the cell numbers for each game move. Standard errors are in parentheses.

NIH-PA Author Manuscript

Important/Predictable
Maximum F0 (Hz)
Minimum F0 (Hz)
Intensity (db)
Duration (msec)

Non-important/Non-predictable

224.27 (10.38)
105.41 (6.18)
67.16 (.83)
1617 (64)

240.67 (12.10)
99.86 (5.20)
66.50 (.79)
2585 (149)

NIH-PA Author Manuscript
NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 11

Table 2

The F0, intensity and duration of the cell numbers after the sixth move of the game. Standard errors are in
parentheses.

NIH-PA Author Manuscript

Important/Predictable
Maximum F0 (Hz)
Minimum F0 (Hz)
Intensity (db)
Duration (msec)

Non-important/Non-predictable

227.66 (12.75)
103.51 (5.79)
67.09 (.79)
1557 (76)

222.20 (12.98)
101.79 (5.92)
66.00 (.87)
2206 (115)

NIH-PA Author Manuscript
NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

Watson et al.

Page 12

Table 3

Percentage of disfluencies and pauses in the game moves. Standard errors are in parentheses.

NIH-PA Author Manuscript

Important/Predictable
% disfluent utterances
% pauses before target word or target PP
Duration of object phrase (msec)

Non-important/Non-predictable

9.79 (2.15)
15.10 (2.86)
552 (51)

25.36 (3.68)
37.28 (3.53)
889 (91)

NIH-PA Author Manuscript
NIH-PA Author Manuscript
Cognition. Author manuscript; available in PMC 2009 March 1.

The Interaction between Information and Intonation Structure:
Prosodic Marking of Theme and Rheme
Max M. Louwerse (mlouwers@memphis.edu)a
Patrick Jeuniaux (pjeuniau@memphis.edu)a
Bin Zhang (bzhang@memphis.edu)b

Department of Psychology / Institute for Intelligent Systemsa
Department of Computer Science / Institute for Intelligent Systemsb
Memphis, TN 38152 USA

Jie Wu (jie82.wu@gmail.com)b
SpeechGear, Inc.
Northfield, MN 55057 USA

Mohammed E. Hoque (mehoque@mit.edu)
Media Lab / MIT
MA 02139 USA
Abstract
Several studies have investigated the relation between information
structure and intonation structure. Few studies however have
investigated this relationship empirically using natural face-to-face
conversations. The current study explores this relation using a large
corpus of face-to-face conversations on a map navigation task. In
this task dialogue partners sometimes do and sometimes do not have
common ground, depending on the differences between their maps.
The corpus is therefore ideal to investigate differences between
given (theme) and new information (rheme). The current paper
presents a technique of automated speech segmentation and
transcript time stamping and applies this technique to determine
prosodic differences in information structure. Confirming several
theoretical studies it shows that the average pitch of the rheme in a
turn is significantly higher than the average pitch of the phrasal
theme of that turn, showing the relation between information and
intonation structure.
Keywords: information structure, intonation structure, theme;
rheme; prosody; pitch, multimodal communication.

Introduction
Multimodal communication is comprised of various
modalities, both linguistic (intonation and information
structure) and non-linguistic (facial expressions, eye gaze and
gesture). Despite the deceptively simple appearance of these
communicative tools in human-human face-to-face
conversation, relatively little is understood about their
interaction and alignment. The current paper focuses on the
relation between the two linguistic modalities: theme and
rheme in language and the prosody in speech.
Knowing the nature of the relation between these
modalities can shed light on various areas of cognitive
science. From a psychological perspective, an understanding
of the interplay of modalities can help us understand language
and communication (Clark, 1996). Limited experimental
research is available that can help determine whether

modalities can be substituted or whether they are
complementary (Doherty-Sneddon, et al., 1997).
From an educational perspective, an understanding of
modalities can help answer questions regarding student
motivation, interest, and confusion, as well as how instructors
and tutors can monitor and respond to these cognitive states
(Kort, Reilly & Picard, 2001). But with little information
available on the conditions under which students use
modalities, tapping into students’ cognitive states is difficult
(Graesser, et al., in press).
From a computational perspective, an understanding of the
interplay between modalities can help in the development of
animated conversational agents (Louwerse, Graesser, Lu &
Mitchell, 2005). These agents maximize the availability of
both linguistic (semantics, syntax) and paralinguistic
(pragmatic, sociological) features (Cassell & Thórisson,
1999; Massaro & Cohen, 1994; Picard, 1997). But without
experimental data on multimodal communication, the
guidelines for implementing human-like multimodal behavior
in agents are missing (Cassell, et al., 1994).
In an ongoing project on multimodal communication in
humans and agents, we are investigating the interaction
between dialogue act, speech, eye gaze, facial movements,
gesture, and map drawing. The project aims to determine how
these modalities are aligned, whether, and if so when, these
modalities are observed, and whether the correct use of these
channels actually aids comprehension.
Due to the inherent complexity of multimodal
communication, controlling for genre, topic, and goals during
unscripted dialogue is crucial. With these concerns in mind,
we used the Map Task scenario (Anderson, et al., 1991), a
restricted-domain, route-communication task. In the Map
Task scenario it is possible for experimenters to determine
exactly what each participant knows at any given time. In this
scenario, the Instruction Giver (IG) coaches the Instruction
Follower (IF) through a route on the map.

Figure 1. Examples maps for the IG (left) and the IF (right)
By way of instruction, participants are told that they and
their interlocutors have maps of the same location, but drawn
by different explorers, and so are potentially different in
detail.
Sixteen different maps were used, each varying according
to the presentation of landmarks, route shape, and method of
distortion in the IF map. For instance, IF’s maps were
distorted with blurred out portions of the map, as shown in
Figure 1. The goal of these differences between maps was to
elicit dialogue between the participants in a controlled
environment whereby dialogue partners sometimes do and
sometimes do not have common ground, depending on the
differences between their maps. These discrepancies in
common ground can be resolved through multimodal
communication. Dialogue partners can maintain common
ground by using different modalities including eye gaze,
facial expressions, gestures, content information or
intonation. Elsewhere (Louwerse et al., 2006; 2007) we have
reported on the relation between both linguistic and nonlinguistic modalities. The current paper investigates the
relationship between these two modalities and tests whether
information structure can predict patterns in intonation
structure.

Information and Intonation Structure
Several studies have discussed the relationship between
information and intonation structure (Halliday, 1967;
Pierrehumbert & Hirschberg, 1990). In Steedman’s (2000)
Combinatory Categorial Grammar (CCG), theme and rheme
are defined as the basic elements of information structure.
Steedman distinguishes the shared topic between interlocutors
as the theme and the new information introduced into the
dialogue as the rheme. Theme and rheme can next be divided
into focus and background. The focus (or contrast) provides
alternatives that distinguish the referent of a referring

expression from the alternatives that the context affords. The
background is everything else. The following exchange, taken
from the multimodal communication corpus (Louwerse, et al.,
2006; 2007), illustrates these concepts (Example 1). The IG
starts speaking and the IF’s reply is analyzed in terms of
theme/rheme, background/focus.
Example 1
IG: then you’re gonna- ok. Then you’re gonna stop. OK
and now you’re gonna start curving down and when you go
down, do you see a purple rectangular alien to the left?
IF: uh… is it right above a blue rectangular alien?
background
theme

focus

background
rheme

Steedman (2003) made the claim that theme and rheme can
be discriminated in terms of pitch accents, and adds that
theme and rheme expose a particular intonation pattern
dependent on the common grounds between the speakers. The
common ground can vary for instance in function of the
agreement between participants. Table 1 illustrates
Steedman’s proposal.
Table 1 Pitch Accent Patterns
Agree
Theme
L+H*
Rheme
H* or (H*+L)

Disagree
L*+H
L* or (H+L*)

L, H, H*, L* are the transcription conventions for
intonation and prosody as described in Pierrehumbert (1990).
“H” and “L” represent “high” and “low” tone, and “*”

denotes that the tone is aligned with a stressed syllable. “+” is
a “followed-by” notation. As for the interplay between focus
and background, Steedman’s prediction is that focus is
marked by prominence in pitch compared to the background
and is also emphasized. On the other hand, background is
usually unaccented and can even be omitted entirely from
conversations. In other words, the theme/rheme partitioning
determines the overall intonation pattern, whereas the
focus/background partitioning determines the placement of
pitch accents.
Despite the fact that there are a number of studies making
the link between intonation and information structure at a
theoretical level, there is relatively little research that has
investigated this link empirically using naturally occurring
speech outside of an experimental setting. An exception is
Calhoun (2006) who conducted a series of production and
perception experiments, showing that differences in pitch
mapped onto differences in theme and rheme and extended
this conclusion with evidence from corpus linguistic data
using the Switchboard corpus. More specifically, Calhoun
showed focus is signaled through the alignment of words with
prosodic structure.
For the purpose of the current paper, we will however not
discriminate between focus/background, because the
utterances of interest are phrases which the focus is part of.
Take for instance Example 1. Instead of saying “uh is it right
above a blue rectangular alien?” the IF could say “blue
rectangular alien” where “blue” and “rectangular sign” are
similar to a theme/rheme pair.
After a manual inspection of a sample of conversations
from the Multimodal Map Task corpus, Guhe, Steedman,
Bard and Louwerse (2006) observed that, on average, rheme
has a higher pitch than the theme (see Example 2).
Example 2
IG: OK. Do you have a black triangular sign?
IF: No, I have a red triangular sign
In this example the common ground between the speakers
is confined to a triangular sign, which is the theme of the
dialogue. However, it happens that the speakers don’t agree
on its color red. Within the IF’s utterance red is found to
conceive a higher pitch than “triangular sign”.
The current study extends Guhe et al.’s study by taking the
same corpus, automatically segmenting the turns and words
in the speech, and automatically identifying theme and rheme
in the transcripts in order to test whether they differ in terms
of prosody in natural face-to-face communication.

Turn Segmentation
Various spoken cues have been used over the years to
segment turns, including pitch ranges, preceding pauses,
speaking rate, amplitude and pitch contour (Brown, 1983;
Grosz & Hirschberg, 1992; Swerts & Ostendorf, 1995).
In this paper, we have used pauses as the initial parameter
to detect the beginning and end of a turn in a natural
conversation. In the data collection, we used the Marantz
PMD670 recorder which enables recording of speech of IG

and IF on separate audio channels. Pauses were analyzed
using the upper intensity limit and minimum duration of
silences. In measurement of intensity, minimum pitch
specifies the minimum periodicity frequency in any signal. In
our case, 75 Hz for minimum pitch yielded a sharp contour
for the intensity. Audio segments with intensity values less
than its mean intensity were classified as pauses. We thereby
used mean intensity for each channel rather than a pre-set
threshold. This enabled our pause detection system to
properly adapt to the diverse set of voice properties of the
participants. Any audio segment with silences more than .4
second was denoted as pauses. However, the extracted turns
were manually inspected to account for different kinds of
pauses in the speech signal (e.g. hesitations vs. end of turn).
The speech processing software Praat (Boersma & Weenink,
2006) was used to perform all calculations to identify these
pause regions.
The pause detection algorithm was used separately on the
right and left channels of each audio file to detect time-stamp
information of turns for both IG and IF. Two audio channels
contain separate information for IG and IF, respectively.
Using the pause detection algorithm, the beginning and
ending time of each turn for both IG and IF are stored
separately. Later, the time stamp information for both IG and
IF are merged into one file to potentially detect and discard
segments where two participants speak at the same time
(overlapping speech being difficult to analyze). Examples are
given below (Case 1 and 2).
Case 1 depicts the ideal cases where one of the participants
is silent while other participant is speaking. Case 2 introduces
the challenge of segmenting a conversation as two people
speak at the same time. Due to a few cases of both of the
participants speaking at the same time, it was not possible to
attain 100% accuracy in segmenting the audio files in turn
level. The chosen audio files, each containing a little more
than 80 turns, were processed using the proposed turn
detection framework based on pauses. For each audio file, our
system was able to map a turn as defined in the transcript into
the corresponding audio segment more than 90% of the time
with an average of 93% accuracy rate for all the speech files.
Case 1:
IF: is it right above………….a blue rectangular alien
Start of a turn

pause

continuation of the turn

IG: …………… (pause)……………………………...
Segmented turn:
IF: is it right above (...) a blue rectangular alien

Case 2:
IG: Go right………...okay…..........then.. go straight
turn1 | pause | turn2 | pause |

turn3

IF: …………..okay………………ummm…okay…
pause |turn1|

pause

1)
2)
3)

Adjacent IG and IF turn pairs were selected.
Two windows of size N were chosen in the turns (see
Example 3).
These two windows shifted for the two whole turns.
Within the window it was determined whether there
was a match of N-1 words. If this was the case, the pair
was considered as a potential theme/rheme pair.

| noisy data

Segmented turns:
IG: Go right
IF: Okay
IG: Okay (...) then. go straight.

Word Segmentation
In order to segment the words of each turn, the
Lumenvox’s (www.lumenvox.com) Speech Recognition
Engine was used, a flexible API that performs speech
recognition on audio data from any audio source. One of the
strengths of the Lumenvox system is that it is speakerindependent. Spontaneous speech can thus be segmented and
recognized based on an acoustic model and a language model.
The system provides an API to identify the starting and
ending time for every recognized speech unit in the output.
This suggests that we have the necessary information to
identify the starting and ending times of the leaf node (i.e. the
word) of the parse tree induced from the grammar.
A significantly small number of words are used in IF’s
turns. Indeed, 70% of these turns only contained less than
three words, which is a reasonable representation of the
conversational nature of the Map Task corpus, given the fact
that IFs are generally waiting for instructions and
acknowledging the information (Louwerse & Crossley,
2006). The IGs, on the other hand, used longer sentences with
around 50% of the IG’s turns consisting of more than 10
words. The Lumenvox ASR performs well on shorter streams
of speech, but like any other ASR systems, lacks in
performance on longer streams of speech, even when the
verbatim transcript is available and used for the only purpose
of “recognizing” this specific stream. Average performance
for turns with more than 10 words was low at 18.51%
accuracy and satisfactory for turns ranging between 1-9
words 67.2%. IF turns typically fell in the latter range.

Contrast Marking
As described earlier, Guhe et al. (2006) observed that
theme and rheme can be distinguished by their pitch features
with which the corresponding words are realized. Guhe et al.
therefore predicted that rheme has a significantly higher pitch
than theme. A small sample of turns marking contrast
confirms this prediction. In the current study we are using an
automated approach to extract contrasts from the multimodal
Map Task transcripts, and the segmentation techniques
proposed above are thereby used to help identify the speech
units from the corpora. Contrastive cases were selected using
the following algorithm:

Example 3
IG: We're drawing parallel to the bottom of the page
again almost. Uh. there are three purple bugs.
IF: I see three white bugs.
In the current experiment, we set the window size to N=3. In
Example 3, the two turns will be chosen because “three
purple bugs” and “three white bugs” have two words in
common (i.e. N-1=2). This algorithm narrowed down the
25,000 turns of the 258 conversations to 458 turns, all of
which were potential candidates for theme-rheme pairs.
In order to precisely derive the pitch information, we
needed to filter out noise in the speech data. The pitch for
human vocals typically ranges from 100 Hz to 150 Hz for
men, and from 170 Hz to 220 Hz for women. We
conservatively filtered out the sound information outside the
[75-300] range that was caused by noise or non-speech
related sounds.

Results
The average of pitch across four different types of speech
segments was computed: 1) the rheme 2) the head of the
phrase that formed the theme (e.g. the head of the NP), 3) the
phrase itself (e.g. the NP) and the 4) whole turn containing
the theme/rheme pair. In Example 3, the rheme is white, the
head word of the theme phrase is bugs, the theme phrase is
three bugs and the whole turn is I see three white bugs.
Following Guhe, et al. (2006) we predicted the average pitch
for rheme to be higher than the average pitch computed on
the theme segments (head, phrase and turn).
Table 4 presents the results of the analysis. All pitch
information showed the expected patterns with the pitch for
rheme being higher than the pitch for theme. The difference
did not reach significance at the turn level, reached marginal
significance in a one-tailed test at the head level (t (45) =
1.42, p = .08) and significance at the phrase level (t (45) =
1.81, p = .04).
Table 4: Mean and SD of pitch of theme and rheme
Theme
Rheme
Mean
SD
Mean
SD
Head
158.02
48.67
165.58
43.75
Phrase
156.80
44.53
Turn
163.44
39.61

Conclusion

References

The current study has explored the relation between
information and intonation structure. Several studies have
investigated this relation, but few have done this empirically
using natural face-to-face conversations. We have used a
large corpus of face-to-face conversations on a map
navigation task. In this task dialogue partners sometimes do
and sometimes do not have common ground, depending on
the differences between their maps. The corpus is therefore
ideal to investigate differences between given (theme) and
new information (rheme).
We presented a technique of automated speech
segmentation and transcript time stamping and applied this
technique to determine prosodic differences in information
structure. Confirming the argument made in a number of
theoretical studies the results show that the average pitch of
the rheme in a turn is significantly higher than the average
pitch of the phrasal theme of that turn in natural face-to-fae
communication.
Automated sentence segmentation based on pauses and
word segmentation based on automatic speech recognition
techniques were employed to help mine the prosodic features
of contrasts. Future work includes how to improve the speech
segmentation techniques. A proposed method is that, instead
of using turns for word segmentation, dialogues acts are used.
This would boost the performance of the word segmentation,
but would on the other hand, put an extra burden on the
dialogue act segmentation. An alternative possibility is to
adopt a recent sentence segmentation tool, nailon (Edlund &
Heldner, 2006), which segments continuous streams of
speech based on the fusion of prosodic features such as
pauses, duration of voicing, intensity, pitch, pseudo-syllable
durations, and intonation patterns.
The current study focused on the two linguistic modalities
of information and intonation structure. Louwerse et al.
(2007) provided insight into how eye gaze, facial movements,
speech features, map drawings, and dialogue structures
correlate with each other and which dialogue acts best predict
the expression of a particular modality. Evidence of a
mapping between linguistic modalities as well as between
non-linguistic modalities is emerging, however, the exact
nature of the alignment and whether these modalities add or
substitute information remains an open research question.

Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G. M.,
Garrod, et al. (1991). The HCRC Map Task Corpus.
Language and Speech, 34, 351-366.
Boersma, P., & Weenink, D. (2006). Praat: Doing phonetics
by computer (Version 4.4.06) [Computer program].
Retrieved January 30, 2006, from http://www.praat.org/
Brown, G. (1983). Prosodic structures and the Given/New
distinction. In D. R. Ladd & A. Cutler (Eds.), Prosody:
Models and measurements (pp. 67–77). Berlin: Springer
Butterworth, B. (1975). Hesitation and semantic planning in
speech. Journal of Psycholinguistic research, 4, 75-87.
Calhoun, S. (2006). Information structure and the prosodic
structure of English: A probabilistic relationship. PhD
Dissertation, University of Edinburgh.
Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn,
B., Becket, T., Douville, B., Prevost, S., & Stone, M.
(1994) Animated Conversation: Rule-Based Generation of
Facial Expression, Gesture and Spoken Intonation for
Multiple Conversational Agents. Proceedings of
SIGGRAPH '94, 413-420.
Cassell, J., & Thórisson, K. R. (1999). The power of a nod
and a glance: Envelope vs. emotional feedback in animated
conversational agents. Applied Artificial Intelligence, 13,
519-538.
Clark, H. H. (1996). Using language. Cambridge: Cambridge
University Press.
Cole, R. A., et al. (1997). Survey of the state of the art in
human language technology. New York, NY, USA:
Cambridge University Press.
Demuynck, K., & Laureys, T. (2002). A comparison of
different approaches to automatic speech segmentation. In
P. Sojka, I. Kopecek, & K. Pala (Eds.), Proceedings of the
5th International Conference on Text, Speech and
Dialogue (TSD 2002) (pp. 277-284). New York: Springer.
Doherty-Sneddon, G., Anderson, A. H., O'Malley, C.,
Langton, S., Garrod, S., & Bruce, V. (1997). Face-to-face
and video-mediated communication: A comparison of
dialogue structure and task performance. Journal of
Experimental Psychology: Applied, 3, 105-125.
Edlund, J., & Heldner, M. (2006): /nailon/ - software for
online analysis of prosody. In Proceedings of Interspeech
2006 ICSLP . Pittsburgh, PA, USA.
Graesser, A.C., D’Mello, S.K., Craig, S.D., Witherspoon, A.,
Sullins, J., McDaniel, B., & Gholson, B. (in press). The
relationship between affect states and dialogue patterns
during interactions with AutoTutor. Journal of Interactive
Learning Research.
Grosz, B. & Hirschberg, J. (1992). Some intonational
characteristics of discourse structure. In Proceedings of the
International Conference on Spoken Language Processing.
ICSLP.
Guhe, M., Steedman, M., Bard, E. G., & Louwerse, M. M.
(2006). Prosodic marking of contrasts in information
structure. In Proceedings of BranDial 2006: The 10th
Workshop on the Semantics and Pragmatics of Dialogue,

Acknowledgments
This research was supported by grant NSF-IIS-0416128.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the funding institution. We
would like to thank Ellen Bard, Art Graesser, Markus Guhe
and Mark Steedman for their help on this project and Nick
Benesh, Gwyneth Lewis, Divya Vargheese, Shinobu
Watanabe and Megan Zirnstein for their help in the data
collection and analyses.

University of Potsdam, Germany; September 11th-13th
2006.
Halliday, M.A.K. (1967). Intonation and grammar in British
English. The Hague: Mouton.
Hirschberg, J., & Nakatani, C. (1996). A prosodic analysis of
discourse segments in direction-giving monologues. In
Proceedings of the 34th Annual meeting (pp. 286-293).
Morristown, NJ, USA: Association for Computational
Linguistics.
Jurafsky D., & Martin J. (2000). Speech and language
processing. An introduction to natural language
processing, computational linguistics and speech
recognition. Upper Saddle River, NJ: Prentice-Hall, Inc.
Kort, B., Reilly, R., & Picard, R. W. (2001). An affective
model of interplay between emotions and learning:
Reengineering educational pedagogy-building a learning
companion. In Proceedings of the International Conference
on Advanced Learning Technologies (ICALT 2001),
Madison Wisconsin, August 2001.
Louwerse, M. M, Jeuniaux, P., Hoque, M. E., Wu, J., Lewis,
G. (2006). Multimodal Communication in ComputerMediated Map Task Scenarios. In R. Sun & N. Miyake
(Eds.), Proceedings of the 28th Annual Conference of the
(pp.
1717-1722).
Cognitive
Science
Society
Mahwah, NJ: Erlbaum.
Louwerse, M.M., Benesh, N., Hoque, M.E., Jeuniaux, P.,
Lewis, G. , Wu, J., & Zirnstein, M. (2007). Multimodal
communication in face-to-face conversations. Proceedings
of the 28th Annual Conference of the Cognitive Science
Society (pp. 1235-1240). Mahwah, NJ: Erlbaum.
Louwerse, M.M. & Crossley, S.A. (2006). Dialog act
classification using n-gram algorithms. In Proceedings of
the 19th International Florida Artificial Intelligence
Research Society.

Louwerse, M.M., Graesser, A.C., Lu, S., & Mitchell, H.H.
(2005). Social cues in animated conversational agents.
Applied Cognitive Psychology, 19, 1-12.
Massaro, D. W., & Cohen, M. M. (1994). Visual,
orthographic, phonological, and lexical influences in
reading. Journal of Experimental Psychology: Human
Perception and Performance, 20, 1107- 1128.
Picard, R. (1997). Affective computing. Cambridge, MA: MIT
Press.
Pierrehumbert, J., & Hirschberg, J. (1990). The meaning of
intonational contours in the interpretation of discourse. In
P. Cohen, J. Morgan & M. Pollarck (Eds.). Intentions in
Communication (pp. 271-311). Cambridge, MA: MIT
Press.
Silverman, B. W. (1986). Density Estimation for Statistics
and Data Analysis. New York: Chapman and Hall.
Steedman, M. (2000). The syntactic process. Cambridge,
MA: MIT Press.
Steedman, M. (2003). Information-Structural Semantics for
English Intonation. LSA Summer Institute Workshop on
Topic and Focus. Santa Barbara, July 2001.
Steedman, M., & Kruijff-Korbayová, I. (2001). Introduction:
Two dimensions of information structure in relation to
discourse structure and discourse semantics. In I. KruijffKorbayová & Steedman (Eds.), Proceedings of the ESSLLI
2001 Workshop on Information Structure, Discourse
Structure and Discourse Semantics, (pp. 1-6).
Swerts, M., & Ostendorf, M. (1995). Discourse prosody in
human-machine interactions. In Proceedings of the ESCA
Tutorial and Research Workshop on Spoken Dialogue
Systems - Theories and Applications.
Woodbury, A. C. (1987). Rhetorical structure in a central
Alaskan Yupik Eskimo traditional narrative. In J. Sherzer
& A. Woodbury (Eds.) Native American Discourse: poetics
and rhetoric (pp. 176-239). Cambridge, UK: Cambridge
University Press.

The Interaction between Information and Intonation Structure:
Prosodic Marking of Theme and Rheme
Max M. Louwerse (mlouwers@memphis.edu)a
Patrick Jeuniaux (pjeuniau@memphis.edu)a
Bin Zhang (bzhang@memphis.edu)b

Department of Psychology / Institute for Intelligent Systemsa
Department of Computer Science / Institute for Intelligent Systemsb
Memphis, TN 38152 USA

Jie Wu (jie82.wu@gmail.com)b
SpeechGear, Inc.
Northfield, MN 55057 USA

Mohammed E. Hoque (mehoque@mit.edu)
Media Lab / MIT
MA 02139 USA
Abstract
Several studies have investigated the relation between information
structure and intonation structure. Few studies however have
investigated this relationship empirically using natural face-to-face
conversations. The current study explores this relation using a large
corpus of face-to-face conversations on a map navigation task. In
this task dialogue partners sometimes do and sometimes do not have
common ground, depending on the differences between their maps.
The corpus is therefore ideal to investigate differences between
given (theme) and new information (rheme). The current paper
presents a technique of automated speech segmentation and
transcript time stamping and applies this technique to determine
prosodic differences in information structure. Confirming several
theoretical studies it shows that the average pitch of the rheme in a
turn is significantly higher than the average pitch of the phrasal
theme of that turn, showing the relation between information and
intonation structure.
Keywords: information structure, intonation structure, theme;
rheme; prosody; pitch, multimodal communication.

Introduction
Multimodal communication is comprised of various
modalities, both linguistic (intonation and information
structure) and non-linguistic (facial expressions, eye gaze and
gesture). Despite the deceptively simple appearance of these
communicative tools in human-human face-to-face
conversation, relatively little is understood about their
interaction and alignment. The current paper focuses on the
relation between the two linguistic modalities: theme and
rheme in language and the prosody in speech.
Knowing the nature of the relation between these
modalities can shed light on various areas of cognitive
science. From a psychological perspective, an understanding
of the interplay of modalities can help us understand language
and communication (Clark, 1996). Limited experimental
research is available that can help determine whether

modalities can be substituted or whether they are
complementary (Doherty-Sneddon, et al., 1997).
From an educational perspective, an understanding of
modalities can help answer questions regarding student
motivation, interest, and confusion, as well as how instructors
and tutors can monitor and respond to these cognitive states
(Kort, Reilly & Picard, 2001). But with little information
available on the conditions under which students use
modalities, tapping into students’ cognitive states is difficult
(Graesser, et al., in press).
From a computational perspective, an understanding of the
interplay between modalities can help in the development of
animated conversational agents (Louwerse, Graesser, Lu &
Mitchell, 2005). These agents maximize the availability of
both linguistic (semantics, syntax) and paralinguistic
(pragmatic, sociological) features (Cassell & Thórisson,
1999; Massaro & Cohen, 1994; Picard, 1997). But without
experimental data on multimodal communication, the
guidelines for implementing human-like multimodal behavior
in agents are missing (Cassell, et al., 1994).
In an ongoing project on multimodal communication in
humans and agents, we are investigating the interaction
between dialogue act, speech, eye gaze, facial movements,
gesture, and map drawing. The project aims to determine how
these modalities are aligned, whether, and if so when, these
modalities are observed, and whether the correct use of these
channels actually aids comprehension.
Due to the inherent complexity of multimodal
communication, controlling for genre, topic, and goals during
unscripted dialogue is crucial. With these concerns in mind,
we used the Map Task scenario (Anderson, et al., 1991), a
restricted-domain, route-communication task. In the Map
Task scenario it is possible for experimenters to determine
exactly what each participant knows at any given time. In this
scenario, the Instruction Giver (IG) coaches the Instruction
Follower (IF) through a route on the map.

Figure 1. Examples maps for the IG (left) and the IF (right)
By way of instruction, participants are told that they and
their interlocutors have maps of the same location, but drawn
by different explorers, and so are potentially different in
detail.
Sixteen different maps were used, each varying according
to the presentation of landmarks, route shape, and method of
distortion in the IF map. For instance, IF’s maps were
distorted with blurred out portions of the map, as shown in
Figure 1. The goal of these differences between maps was to
elicit dialogue between the participants in a controlled
environment whereby dialogue partners sometimes do and
sometimes do not have common ground, depending on the
differences between their maps. These discrepancies in
common ground can be resolved through multimodal
communication. Dialogue partners can maintain common
ground by using different modalities including eye gaze,
facial expressions, gestures, content information or
intonation. Elsewhere (Louwerse et al., 2006; 2007) we have
reported on the relation between both linguistic and nonlinguistic modalities. The current paper investigates the
relationship between these two modalities and tests whether
information structure can predict patterns in intonation
structure.

Information and Intonation Structure
Several studies have discussed the relationship between
information and intonation structure (Halliday, 1967;
Pierrehumbert & Hirschberg, 1990). In Steedman’s (2000)
Combinatory Categorial Grammar (CCG), theme and rheme
are defined as the basic elements of information structure.
Steedman distinguishes the shared topic between interlocutors
as the theme and the new information introduced into the
dialogue as the rheme. Theme and rheme can next be divided
into focus and background. The focus (or contrast) provides
alternatives that distinguish the referent of a referring

expression from the alternatives that the context affords. The
background is everything else. The following exchange, taken
from the multimodal communication corpus (Louwerse, et al.,
2006; 2007), illustrates these concepts (Example 1). The IG
starts speaking and the IF’s reply is analyzed in terms of
theme/rheme, background/focus.
Example 1
IG: then you’re gonna- ok. Then you’re gonna stop. OK
and now you’re gonna start curving down and when you go
down, do you see a purple rectangular alien to the left?
IF: uh… is it right above a blue rectangular alien?
background
theme

focus

background
rheme

Steedman (2003) made the claim that theme and rheme can
be discriminated in terms of pitch accents, and adds that
theme and rheme expose a particular intonation pattern
dependent on the common grounds between the speakers. The
common ground can vary for instance in function of the
agreement between participants. Table 1 illustrates
Steedman’s proposal.
Table 1 Pitch Accent Patterns
Agree
Theme
L+H*
Rheme
H* or (H*+L)

Disagree
L*+H
L* or (H+L*)

L, H, H*, L* are the transcription conventions for
intonation and prosody as described in Pierrehumbert (1990).
“H” and “L” represent “high” and “low” tone, and “*”

denotes that the tone is aligned with a stressed syllable. “+” is
a “followed-by” notation. As for the interplay between focus
and background, Steedman’s prediction is that focus is
marked by prominence in pitch compared to the background
and is also emphasized. On the other hand, background is
usually unaccented and can even be omitted entirely from
conversations. In other words, the theme/rheme partitioning
determines the overall intonation pattern, whereas the
focus/background partitioning determines the placement of
pitch accents.
Despite the fact that there are a number of studies making
the link between intonation and information structure at a
theoretical level, there is relatively little research that has
investigated this link empirically using naturally occurring
speech outside of an experimental setting. An exception is
Calhoun (2006) who conducted a series of production and
perception experiments, showing that differences in pitch
mapped onto differences in theme and rheme and extended
this conclusion with evidence from corpus linguistic data
using the Switchboard corpus. More specifically, Calhoun
showed focus is signaled through the alignment of words with
prosodic structure.
For the purpose of the current paper, we will however not
discriminate between focus/background, because the
utterances of interest are phrases which the focus is part of.
Take for instance Example 1. Instead of saying “uh is it right
above a blue rectangular alien?” the IF could say “blue
rectangular alien” where “blue” and “rectangular sign” are
similar to a theme/rheme pair.
After a manual inspection of a sample of conversations
from the Multimodal Map Task corpus, Guhe, Steedman,
Bard and Louwerse (2006) observed that, on average, rheme
has a higher pitch than the theme (see Example 2).
Example 2
IG: OK. Do you have a black triangular sign?
IF: No, I have a red triangular sign
In this example the common ground between the speakers
is confined to a triangular sign, which is the theme of the
dialogue. However, it happens that the speakers don’t agree
on its color red. Within the IF’s utterance red is found to
conceive a higher pitch than “triangular sign”.
The current study extends Guhe et al.’s study by taking the
same corpus, automatically segmenting the turns and words
in the speech, and automatically identifying theme and rheme
in the transcripts in order to test whether they differ in terms
of prosody in natural face-to-face communication.

Turn Segmentation
Various spoken cues have been used over the years to
segment turns, including pitch ranges, preceding pauses,
speaking rate, amplitude and pitch contour (Brown, 1983;
Grosz & Hirschberg, 1992; Swerts & Ostendorf, 1995).
In this paper, we have used pauses as the initial parameter
to detect the beginning and end of a turn in a natural
conversation. In the data collection, we used the Marantz
PMD670 recorder which enables recording of speech of IG

and IF on separate audio channels. Pauses were analyzed
using the upper intensity limit and minimum duration of
silences. In measurement of intensity, minimum pitch
specifies the minimum periodicity frequency in any signal. In
our case, 75 Hz for minimum pitch yielded a sharp contour
for the intensity. Audio segments with intensity values less
than its mean intensity were classified as pauses. We thereby
used mean intensity for each channel rather than a pre-set
threshold. This enabled our pause detection system to
properly adapt to the diverse set of voice properties of the
participants. Any audio segment with silences more than .4
second was denoted as pauses. However, the extracted turns
were manually inspected to account for different kinds of
pauses in the speech signal (e.g. hesitations vs. end of turn).
The speech processing software Praat (Boersma & Weenink,
2006) was used to perform all calculations to identify these
pause regions.
The pause detection algorithm was used separately on the
right and left channels of each audio file to detect time-stamp
information of turns for both IG and IF. Two audio channels
contain separate information for IG and IF, respectively.
Using the pause detection algorithm, the beginning and
ending time of each turn for both IG and IF are stored
separately. Later, the time stamp information for both IG and
IF are merged into one file to potentially detect and discard
segments where two participants speak at the same time
(overlapping speech being difficult to analyze). Examples are
given below (Case 1 and 2).
Case 1 depicts the ideal cases where one of the participants
is silent while other participant is speaking. Case 2 introduces
the challenge of segmenting a conversation as two people
speak at the same time. Due to a few cases of both of the
participants speaking at the same time, it was not possible to
attain 100% accuracy in segmenting the audio files in turn
level. The chosen audio files, each containing a little more
than 80 turns, were processed using the proposed turn
detection framework based on pauses. For each audio file, our
system was able to map a turn as defined in the transcript into
the corresponding audio segment more than 90% of the time
with an average of 93% accuracy rate for all the speech files.
Case 1:
IF: is it right above………….a blue rectangular alien
Start of a turn

pause

continuation of the turn

IG: …………… (pause)……………………………...
Segmented turn:
IF: is it right above (...) a blue rectangular alien

Case 2:
IG: Go right………...okay…..........then.. go straight
turn1 | pause | turn2 | pause |

turn3

IF: …………..okay………………ummm…okay…
pause |turn1|

pause

1)
2)
3)

Adjacent IG and IF turn pairs were selected.
Two windows of size N were chosen in the turns (see
Example 3).
These two windows shifted for the two whole turns.
Within the window it was determined whether there
was a match of N-1 words. If this was the case, the pair
was considered as a potential theme/rheme pair.

| noisy data

Segmented turns:
IG: Go right
IF: Okay
IG: Okay (...) then. go straight.

Word Segmentation
In order to segment the words of each turn, the
Lumenvox’s (www.lumenvox.com) Speech Recognition
Engine was used, a flexible API that performs speech
recognition on audio data from any audio source. One of the
strengths of the Lumenvox system is that it is speakerindependent. Spontaneous speech can thus be segmented and
recognized based on an acoustic model and a language model.
The system provides an API to identify the starting and
ending time for every recognized speech unit in the output.
This suggests that we have the necessary information to
identify the starting and ending times of the leaf node (i.e. the
word) of the parse tree induced from the grammar.
A significantly small number of words are used in IF’s
turns. Indeed, 70% of these turns only contained less than
three words, which is a reasonable representation of the
conversational nature of the Map Task corpus, given the fact
that IFs are generally waiting for instructions and
acknowledging the information (Louwerse & Crossley,
2006). The IGs, on the other hand, used longer sentences with
around 50% of the IG’s turns consisting of more than 10
words. The Lumenvox ASR performs well on shorter streams
of speech, but like any other ASR systems, lacks in
performance on longer streams of speech, even when the
verbatim transcript is available and used for the only purpose
of “recognizing” this specific stream. Average performance
for turns with more than 10 words was low at 18.51%
accuracy and satisfactory for turns ranging between 1-9
words 67.2%. IF turns typically fell in the latter range.

Contrast Marking
As described earlier, Guhe et al. (2006) observed that
theme and rheme can be distinguished by their pitch features
with which the corresponding words are realized. Guhe et al.
therefore predicted that rheme has a significantly higher pitch
than theme. A small sample of turns marking contrast
confirms this prediction. In the current study we are using an
automated approach to extract contrasts from the multimodal
Map Task transcripts, and the segmentation techniques
proposed above are thereby used to help identify the speech
units from the corpora. Contrastive cases were selected using
the following algorithm:

Example 3
IG: We're drawing parallel to the bottom of the page
again almost. Uh. there are three purple bugs.
IF: I see three white bugs.
In the current experiment, we set the window size to N=3. In
Example 3, the two turns will be chosen because “three
purple bugs” and “three white bugs” have two words in
common (i.e. N-1=2). This algorithm narrowed down the
25,000 turns of the 258 conversations to 458 turns, all of
which were potential candidates for theme-rheme pairs.
In order to precisely derive the pitch information, we
needed to filter out noise in the speech data. The pitch for
human vocals typically ranges from 100 Hz to 150 Hz for
men, and from 170 Hz to 220 Hz for women. We
conservatively filtered out the sound information outside the
[75-300] range that was caused by noise or non-speech
related sounds.

Results
The average of pitch across four different types of speech
segments was computed: 1) the rheme 2) the head of the
phrase that formed the theme (e.g. the head of the NP), 3) the
phrase itself (e.g. the NP) and the 4) whole turn containing
the theme/rheme pair. In Example 3, the rheme is white, the
head word of the theme phrase is bugs, the theme phrase is
three bugs and the whole turn is I see three white bugs.
Following Guhe, et al. (2006) we predicted the average pitch
for rheme to be higher than the average pitch computed on
the theme segments (head, phrase and turn).
Table 4 presents the results of the analysis. All pitch
information showed the expected patterns with the pitch for
rheme being higher than the pitch for theme. The difference
did not reach significance at the turn level, reached marginal
significance in a one-tailed test at the head level (t (45) =
1.42, p = .08) and significance at the phrase level (t (45) =
1.81, p = .04).
Table 4: Mean and SD of pitch of theme and rheme
Theme
Rheme
Mean
SD
Mean
SD
Head
158.02
48.67
165.58
43.75
Phrase
156.80
44.53
Turn
163.44
39.61

Conclusion

References

The current study has explored the relation between
information and intonation structure. Several studies have
investigated this relation, but few have done this empirically
using natural face-to-face conversations. We have used a
large corpus of face-to-face conversations on a map
navigation task. In this task dialogue partners sometimes do
and sometimes do not have common ground, depending on
the differences between their maps. The corpus is therefore
ideal to investigate differences between given (theme) and
new information (rheme).
We presented a technique of automated speech
segmentation and transcript time stamping and applied this
technique to determine prosodic differences in information
structure. Confirming the argument made in a number of
theoretical studies the results show that the average pitch of
the rheme in a turn is significantly higher than the average
pitch of the phrasal theme of that turn in natural face-to-fae
communication.
Automated sentence segmentation based on pauses and
word segmentation based on automatic speech recognition
techniques were employed to help mine the prosodic features
of contrasts. Future work includes how to improve the speech
segmentation techniques. A proposed method is that, instead
of using turns for word segmentation, dialogues acts are used.
This would boost the performance of the word segmentation,
but would on the other hand, put an extra burden on the
dialogue act segmentation. An alternative possibility is to
adopt a recent sentence segmentation tool, nailon (Edlund &
Heldner, 2006), which segments continuous streams of
speech based on the fusion of prosodic features such as
pauses, duration of voicing, intensity, pitch, pseudo-syllable
durations, and intonation patterns.
The current study focused on the two linguistic modalities
of information and intonation structure. Louwerse et al.
(2007) provided insight into how eye gaze, facial movements,
speech features, map drawings, and dialogue structures
correlate with each other and which dialogue acts best predict
the expression of a particular modality. Evidence of a
mapping between linguistic modalities as well as between
non-linguistic modalities is emerging, however, the exact
nature of the alignment and whether these modalities add or
substitute information remains an open research question.

Anderson, A., Bader, M., Bard, E., Boyle, E., Doherty, G. M.,
Garrod, et al. (1991). The HCRC Map Task Corpus.
Language and Speech, 34, 351-366.
Boersma, P., & Weenink, D. (2006). Praat: Doing phonetics
by computer (Version 4.4.06) [Computer program].
Retrieved January 30, 2006, from http://www.praat.org/
Brown, G. (1983). Prosodic structures and the Given/New
distinction. In D. R. Ladd & A. Cutler (Eds.), Prosody:
Models and measurements (pp. 67–77). Berlin: Springer
Butterworth, B. (1975). Hesitation and semantic planning in
speech. Journal of Psycholinguistic research, 4, 75-87.
Calhoun, S. (2006). Information structure and the prosodic
structure of English: A probabilistic relationship. PhD
Dissertation, University of Edinburgh.
Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn,
B., Becket, T., Douville, B., Prevost, S., & Stone, M.
(1994) Animated Conversation: Rule-Based Generation of
Facial Expression, Gesture and Spoken Intonation for
Multiple Conversational Agents. Proceedings of
SIGGRAPH '94, 413-420.
Cassell, J., & Thórisson, K. R. (1999). The power of a nod
and a glance: Envelope vs. emotional feedback in animated
conversational agents. Applied Artificial Intelligence, 13,
519-538.
Clark, H. H. (1996). Using language. Cambridge: Cambridge
University Press.
Cole, R. A., et al. (1997). Survey of the state of the art in
human language technology. New York, NY, USA:
Cambridge University Press.
Demuynck, K., & Laureys, T. (2002). A comparison of
different approaches to automatic speech segmentation. In
P. Sojka, I. Kopecek, & K. Pala (Eds.), Proceedings of the
5th International Conference on Text, Speech and
Dialogue (TSD 2002) (pp. 277-284). New York: Springer.
Doherty-Sneddon, G., Anderson, A. H., O'Malley, C.,
Langton, S., Garrod, S., & Bruce, V. (1997). Face-to-face
and video-mediated communication: A comparison of
dialogue structure and task performance. Journal of
Experimental Psychology: Applied, 3, 105-125.
Edlund, J., & Heldner, M. (2006): /nailon/ - software for
online analysis of prosody. In Proceedings of Interspeech
2006 ICSLP . Pittsburgh, PA, USA.
Graesser, A.C., D’Mello, S.K., Craig, S.D., Witherspoon, A.,
Sullins, J., McDaniel, B., & Gholson, B. (in press). The
relationship between affect states and dialogue patterns
during interactions with AutoTutor. Journal of Interactive
Learning Research.
Grosz, B. & Hirschberg, J. (1992). Some intonational
characteristics of discourse structure. In Proceedings of the
International Conference on Spoken Language Processing.
ICSLP.
Guhe, M., Steedman, M., Bard, E. G., & Louwerse, M. M.
(2006). Prosodic marking of contrasts in information
structure. In Proceedings of BranDial 2006: The 10th
Workshop on the Semantics and Pragmatics of Dialogue,

Acknowledgments
This research was supported by grant NSF-IIS-0416128.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the funding institution. We
would like to thank Ellen Bard, Art Graesser, Markus Guhe
and Mark Steedman for their help on this project and Nick
Benesh, Gwyneth Lewis, Divya Vargheese, Shinobu
Watanabe and Megan Zirnstein for their help in the data
collection and analyses.

University of Potsdam, Germany; September 11th-13th
2006.
Halliday, M.A.K. (1967). Intonation and grammar in British
English. The Hague: Mouton.
Hirschberg, J., & Nakatani, C. (1996). A prosodic analysis of
discourse segments in direction-giving monologues. In
Proceedings of the 34th Annual meeting (pp. 286-293).
Morristown, NJ, USA: Association for Computational
Linguistics.
Jurafsky D., & Martin J. (2000). Speech and language
processing. An introduction to natural language
processing, computational linguistics and speech
recognition. Upper Saddle River, NJ: Prentice-Hall, Inc.
Kort, B., Reilly, R., & Picard, R. W. (2001). An affective
model of interplay between emotions and learning:
Reengineering educational pedagogy-building a learning
companion. In Proceedings of the International Conference
on Advanced Learning Technologies (ICALT 2001),
Madison Wisconsin, August 2001.
Louwerse, M. M, Jeuniaux, P., Hoque, M. E., Wu, J., Lewis,
G. (2006). Multimodal Communication in ComputerMediated Map Task Scenarios. In R. Sun & N. Miyake
(Eds.), Proceedings of the 28th Annual Conference of the
(pp.
1717-1722).
Cognitive
Science
Society
Mahwah, NJ: Erlbaum.
Louwerse, M.M., Benesh, N., Hoque, M.E., Jeuniaux, P.,
Lewis, G. , Wu, J., & Zirnstein, M. (2007). Multimodal
communication in face-to-face conversations. Proceedings
of the 28th Annual Conference of the Cognitive Science
Society (pp. 1235-1240). Mahwah, NJ: Erlbaum.
Louwerse, M.M. & Crossley, S.A. (2006). Dialog act
classification using n-gram algorithms. In Proceedings of
the 19th International Florida Artificial Intelligence
Research Society.

Louwerse, M.M., Graesser, A.C., Lu, S., & Mitchell, H.H.
(2005). Social cues in animated conversational agents.
Applied Cognitive Psychology, 19, 1-12.
Massaro, D. W., & Cohen, M. M. (1994). Visual,
orthographic, phonological, and lexical influences in
reading. Journal of Experimental Psychology: Human
Perception and Performance, 20, 1107- 1128.
Picard, R. (1997). Affective computing. Cambridge, MA: MIT
Press.
Pierrehumbert, J., & Hirschberg, J. (1990). The meaning of
intonational contours in the interpretation of discourse. In
P. Cohen, J. Morgan & M. Pollarck (Eds.). Intentions in
Communication (pp. 271-311). Cambridge, MA: MIT
Press.
Silverman, B. W. (1986). Density Estimation for Statistics
and Data Analysis. New York: Chapman and Hall.
Steedman, M. (2000). The syntactic process. Cambridge,
MA: MIT Press.
Steedman, M. (2003). Information-Structural Semantics for
English Intonation. LSA Summer Institute Workshop on
Topic and Focus. Santa Barbara, July 2001.
Steedman, M., & Kruijff-Korbayová, I. (2001). Introduction:
Two dimensions of information structure in relation to
discourse structure and discourse semantics. In I. KruijffKorbayová & Steedman (Eds.), Proceedings of the ESSLLI
2001 Workshop on Information Structure, Discourse
Structure and Discourse Semantics, (pp. 1-6).
Swerts, M., & Ostendorf, M. (1995). Discourse prosody in
human-machine interactions. In Proceedings of the ESCA
Tutorial and Research Workshop on Spoken Dialogue
Systems - Theories and Applications.
Woodbury, A. C. (1987). Rhetorical structure in a central
Alaskan Yupik Eskimo traditional narrative. In J. Sherzer
& A. Woodbury (Eds.) Native American Discourse: poetics
and rhetoric (pp. 176-239). Cambridge, UK: Cambridge
University Press.

RINDI

weight and add
in log domain

Dialogue
model

move type
likelihoods
from intonation

move type
likelihoods

from speech
recognition

Viterbi
decoder

move type
sequence

Utterance #

Move Type

Position

i-2

Speaker Role
giver

instruct

start

instruct

Game Type

other

follower

ready

middle

instruct

i-1

giver

instruct

middle

instruct

i

giver

acknowledge end

instruct

Boston University Computer Science Technical Report No. 2005-12

Real Time Eye Tracking and Blink Detection with USB Cameras
Michael Chau and Margrit Betke
Computer Science Department
Boston University
Boston, MA 02215, USA
{mikechau, betke@cs.bu.edu}
May 12, 2005

Abstract

1

Introduction

A human-computer interface (HCI) system designed for use
by people with severe disabilities is presented. People that
are severely paralyzed or afﬂicted with diseases such as
ALS (Lou Gehrig’s disease) or multiple sclerosis are unable to move or control any parts of their bodies except for
their eyes. The system presented here detects the user’s eye
blinks and analyzes the pattern and duration of the blinks,
using them to provide input to the computer in the form of
a mouse click. After the automatic initialization of the system occurs from the processing of the user’s involuntary eye
blinks in the ﬁrst few seconds of use, the eye is tracked in
real time using correlation with an online template. If the
user’s depth changes signiﬁcantly or rapid head movement
occurs, the system is automatically reinitialized. There are
no lighting requirements nor ofﬂine templates needed for
the proper functioning of the system. The system works with
inexpensive USB cameras and runs at a frame rate of 30
frames per second. Extensive experiments were conducted
to determine both the system’s accuracy in classifying voluntary and involuntary blinks, as well as the system’s ﬁtness
in varying environment conditions, such as alternative camera placements and different lighting conditions. These experiments on eight test subjects yielded an overall detection
accuracy of 95.3%.

A great deal of computer vision research is dedicated to
the implementation of systems designed to detect user
movements and facial gestures [1, 2, 4, 5, 6, 15, 16]. In
many cases, such systems are created with the speciﬁc goal
of providing a way for people with disabilities or limited
motor skills to be able to use computer systems, albeit in
much simpler applications [1, 15, 16]. The motivation for
the system proposed here is to provide an inexpensive,
unobtrusive means for disabled people to interact with
simple computer applications in a meaningful way that
requires minimal effort.
This goal is accomplished using a robust algorithm
based on the work by Grauman et al. [11, 12]. Some of
these methods are implemented here, while some have been
enhanced or modiﬁed to the end of simpliﬁed initialization
and more efﬁcient maintenance of the real time tracking.
The automatic initialization phase is triggered by the
analysis of the involuntary blinking of the current user of
the system, which creates an online template of the eye
to be used for tracking. This phase occurs each time the
current correlation score of the tracked eye falls below a
deﬁned threshold in order to allow the system to recover
and regain its accuracy in detecting the blinks. This system
can be utilized by users that are able to voluntarily blink
and have a use for applications that require mouse clicks as
input (e.g. switch and scanning programs/games [22]).
A thorough survey on work related to eye and blink
detection methods is presented by Grauman et al., as well
as Magee et al. [12, 16]. Since the implementation of the
1

BlinkLink blink detection system by Grauman et al., a
number of signiﬁcant contributions and advancements have
been made in the HCI ﬁeld. Gorodnichy and Roth present
communication interfaces that operate using eye blinks
[8, 9, 10]. Motion analysis methods and frame differencing
techniques used to locate the eyes are used Bhaskar et al.
and Gorodnichy [3, 8, 9]. Detecting eye blinking in the
presence of spontaneous movements as well as occlusion
and out-of-plane motion is discussed by Moriyama et al.
[19]. Methods for locating eyes using gradients and luminance and color information with templates are presented
by Rurainsky and Eisert [21]. Miglietta et al. present
results of a study involving the use of an eyeglass frame
worn by the patients in an Intenstive Care Unit that detects
eye blinks to operate a switch system [18]. There still have
not been many blink detection related systems designed to
work with inexpensive USB webcams [7, 8]. There have,
however, been a number of other feature detection systems
that use more expensive and less portable alternatives, such
as digital and IR cameras for video input [3, 19, 21, 23].
Aside from the portability concerns, these systems are
also typically unable to achieve the desirable higher frame
rates of approximately 30 fps that are common with USB
cameras.

image differencing

tracker
lost

thresholding

detect and analyze
blinking

opening

track eye

label connected
components

create eye
template

filter out
infeasible pairs

return location of best
candidate for eye pair

Figure 1: Overview of the main stages in the system.

2.1 Initialization
Naturally, the ﬁrst step in analyzing the blinking of the user
is to locate the eyes. To accomplish this, the difference
image of each frame and the previous frame is created and
then thresholded, resulting in a binary image showing the
regions of movement that occurred between the two frames.

The main contribution of this paper is to provide a
robust reimplementation of the system described by Grauman et al. [11] that is able to run in real time at 30 frames
per second on readily available and affordable webcams.
As mentioned, most systems dealing with motion analysis
required the use of rather expensive equipment and highend video cameras. However, in recent years, inexpensive
webcams manufactured by companies such as Logitech
have become ubiquitous, facilitating the incorporation of
these motion analysis systems on a more widespread basis.
The system described here is an accurate and useful tool
to give handicapped people another alternative to interface
with computer systems.

Next, a 3x3 star-shaped convolution kernel is passed
over the binary difference image in an Opening morphological operation [14]. This functions to eliminate a great
deal of noise and naturally-occurring jitter that is present
around the user in the frame due to the lighting conditions
and the camera resolution, as well as the possibility of
background movement. In addition, this Opening operation
also produces fewer and larger connected components in
the vicinity of the eyes (when a blink happens to occur),
which is crucial for the efﬁciency and accuracy of the next
phase (see Figure 2).

2 Methods
The algorithm used by the system for detecting and analyzing blinks is initialized automatically, dependent only upon
the inevitability of the involuntary blinking of the user.
Motion analysis techniques are used in this stage, followed
by online creation of a template of the open eye to be used
for the subsequent tracking and template matching that is
carried out at each frame. A ﬂow chart depicting the main
stages of the system is shown in Figure 1.

A recursive labeling procedure is applied next to recover
the number of connected components in the resultant binary
image. Under the circumstances in which this system was
optimally designed to function, in which the users are
for the most part paralyzed, this procedure yields only a
few connected components, with the ideal number being
two (the left eye and the right eye). In the case that other
movement has occurred, producing a much larger number
of components, the system discards the current binary
2

2.2 Template Creation

A

B

C

If the previous stage results in a pair of components that
passes the set of ﬁlters, then it is a good indication that the
user’s eyes have been successfully located. At this point,
the location of the larger of the two components is chosen
for creation of the template. Since the size of the template
that is to be created is directly proportional to the size of
the chosen component, the larger one is chosen for the
purpose of having more brightness information, which will
result in more accurate tracking and correlation scores (see
Figure 3).

D

Since the system will be tracking the user’s open eye,
it would be a mistake to create the template at the instant
that the eye was located, since the user was blinking at this
moment. Thus, once the eye is believed to be located, a
timer is triggered. After a small number of frames elapse,
which is judged to be the approximate time needed for the
user’s eye to become open again after an involuntary blink,
the template of the user’s open eye is created. Therefore,
during initialization, the user is assumed to be blinking at a
normal rate of one involuntary blink every few moments.
Again, no ofﬂine templates are necessary and the creation
of this online template is completely independent of any
past templates that may have been created during the run of
the system.

Figure 2: Motion analysis phase: (A) User at frame f .
(B) User at frame f + 1, having just blinked. (C) Initial
difference of the two frames f and f +1. Note the great deal
of noise in the background due to the lighting conditions
and camera properties. (D) Difference image used to locate
the eyes after performing the Opening operation.
image and waits to process the next involuntary blink in
order to maintain efﬁciency and accuracy in locating the
eyes.
Given an image with a small number of connected
components output from the previous processing steps,
the system is able to proceed efﬁciently by considering
each pair of components as a possible match for the user’s
left and right eyes. The ﬁltering of unlikely eye pair
matches is based on the computation of six parameters
for each component pair: the width and height of each
of the two components and the horizontal and vertical
distance between the centroids of the two components. A
number of experimentally-derived heuristics are applied to
these statistics to pinpoint the exact pair that most likely
represents the user’s eyes. For example, if there is a large
difference in either the width or height of each of the two
components, then they likely are not the user’s eyes. As an
additional example of one of these many ﬁlters, if there is
a large vertical distance between the centroids of the two
components, then they are also not likely to be the user’s
eyes, since such a property would not be humanly possible.
Such observations not only lead to accurate detection of
the user’s eyes, but also speed up the search greatly by
eliminating unlikely components immediately.

Figure 3: Open eye templates: Note the diversity in the appearance of some of the open templates that were used during user experiments. Working templates range from very
small to large in overall size, as well very tight around the
eye to a larger area surrounding the eye, including the eyebrow.

2.3 Eye Tracking
As noted by Grauman et al., the use of template matching
is necessary for the desired accuracy in analyzing the user’s
blinking since it allows the user some freedom to move
around slightly [11]. Though the primary purpose of such
a system is to serve people with paralysis, it is a desirable
3

feature to allow for some slight movement by the user or
the camera that would not be feasible if motion analysis
were used alone.
The normalized correlation coefﬁcient, also implemented in the system proposed by Grauman et al., is used
to accomplish the tracking [11]. This measure is computed
at each frame using the following formula:
x,y
x,y

A

B

C

D

¯
¯
[f (x,y)−fu,v ][t(x−u,y−v)−t]

¯
[f (x,y)−fu,v ]2

x,y

¯
[t(x−u,y−v)−t]2

where f (x, y) is the brightness of the video frame at
¯
the point (x, y), fu,v is the average value of the video
frame in the current search region, t(x, y) is the brightness
¯
of the template image at the point (x, y), and t is the
average value of the template image. The result of this
computation is a correlation score between -1 and 1 that
indicates the similarity between the open eye template and
all points in the search region of the video frame. Scores
closer to 0 indicate a low level of similarity, while scores
closer to 1 indicate a probable match for the open eye
template. A major beneﬁt of using this similarity measure
to perform the tracking is that it is insensitive to constant
changes in ambient lighting conditions. The Results section
shows that the eye tracking and blink detection works just
as well in the presence of both very dark and bright lighting.

Figure 4: Sample frames of a typical session: (A) The system is in this state during the motion analysis phase. The red
rectangle represents the region that is considered during the
frame differencing and labeling of connected components.
(B) The system enters this state once the eye is located and
remains this way as long as the eye is not believed to be
lost. The green rectangle represents the region at which the
open eye template was selected and the red rectangle now
represents the drastically reduced search space for performing the correlation. (C) User at frame f , with eyes already
closed for the deﬁned voluntary blink duration and (D) user
at frame f + 1, opening his eyes, with a yellow dot being
drawn on the eye to indicate that a voluntary blink just occurred.

Since this method requires an extensive amount of
computation and is performed 30 times per second, the
search region is restricted to a small area around the user’s
eye (see Figure 4). This reduced search space allows the
system to remain running smoothly in real time since it
drastically reduces the computation needed to perform the
correlation search at each frame.

Close examination of the correlation scores over time for a
number of different users of the system reveals rather clear
boundaries that allow for the detection of the blinks. As the
user’s eye is in the normal open state, very high correlation
scores of about 0.85 to 1.0 are reported. As the user blinks,
the scores fall to values of about 0.5 to 0.55. Finally, a very
important range to note is the one containing scores below
about 0.45. Scores in this range normally indicate that the
tracker has lost the location of the eye. In such cases, the
system must be reinitialized to relocate and track the new
position of the eye.

2.4 Blink Detection
The detection of blinking and the analysis of blink duration
are based solely on observation of the correlation scores
generated by the tracking at the previous step using the
online template of the user’s eye. As the user’s eye closes
during the process of a blink, its similarity to the open
eye template decreases. Likewise, it regains its similarity to the template as the blink ends and the user’s eye
becomes fully open again. This decrease and increase in
similarity corresponds directly to the correlation scores
returned by the template matching procedure (see Figure 5).

Given these ranges of correlation scores and knowledge of what they signify derived from experimentation
and observation across a number of test subjects, the system
detects voluntary blinks by using a timer that is triggered
each time the correlation scores fall below the threshold of
scores that represent an open eye. If the correlation scores

4

correlation scores over time

1.2

correlation score

1
1.0

0.8
0.8

0.6
0.6

0.4
0.4

0.2
0.2

0.0
0
0 10

1

30

60

90

120

150

180

210

240

270

19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 226 235 244 253 262 271 280

time (in frames)

Figure 6: System interface: Notable features include the
ability for the user to deﬁne the voluntary blink length, the
ability to reset the tracking at any time, should a poor or
unexpected template location be chosen, and the ability to
save the session to a video ﬁle.

Figure 5: Correlation scores for the open eye template plotted over time (in frames). The scores form a clear waveform, as noted by Grauman et al., which is useful in deriving a threshold to be used for classifying the user’s eyes as
being open or closed at each frame [11]. In this example,
there were three short blinks followed by three long blinks,
three short blinks again, and ﬁnally one more long blink.

detector accuracy should yield correspondingly high
accuracy results for the usability tests, subject to the user’s
understanding and capabilities in carrying out the given
tasks, such as simple reaction time and matching games, as
described by Grauman et al. [11].

remain below this threshold and above the threshold that
results in reinitialization of the system for a deﬁned number
of frames that can be set by the user, then a voluntary blink
is judged to have occurred, causing a mouse click to be
issued to the operating system.

Therefore, the experiments conducted for this system
were more focused on detector accuracy, since this is
a more standard measure of the overall accuracy of the
system across a broad range of users. In order to measure
the detection accuracy, test subjects were seated in front of
the computer, approximately 2 feet away from the camera.
Subjects were instructed to act naturally, but were asked not
to turn their heads or move too abruptly, since this could
potentially lead to repeated reinitialization of the system,
making it difﬁcult to test the accuracy. In addition, this
constraint allowed for a closer simulation of the system’s
target audience of handicapped users.

3 Experiments
The system was primarily developed and tested on a Windows XP PC with an Intel Pentium IV 2.8 GHz processor
and 1 GB RAM. Video was captured with a Logitech
Quickcam Pro 4000 webcam at 30 frames per second.
All video was processed as grayscale images of 320 x
240 pixels using various utilities from the Intel OpenCV
and Image Processing libraries, as well as the Microsoft
DirectX SDK [13, 20, 17]. Figure 6 shows the interface for
the system. The experiments were conducted with eight
test subjects at two different locations (see Figure 7).

Similar to the tests done by Grauman et al., subjects
were also asked to blink random test patterns that were
determined prior to the start of the session [11]. For example, subjects were asked to blink two short blinks followed
by a long (voluntary) blink, or were asked to blink twice
voluntarily followed by a short (involuntary) blink. These
test results serve to show how well the system distinguishes
between the voluntary and involuntary blinks, which is the
crux of the problem. Tests involving the voluntary blink
length parameter were also conducted, with values ranging
from 5 to 20 frames (1/6 of a second to 2/3 of a second).

Reviewing the work done by Grauman et al., it is apparent
that similar results were obtained with experiments based
on testing the accuracy of the system and experiments
based on testing the usability of the system as a switch
input device [11]. Intuitively, this makes sense as good

In addition, as a further contribution, numerous other

5

Summary of results

experiments were also conducted to determine the ﬁtness of
the system under varying circumstances, such as alternative
camera placements, lighting conditions, and distance to the
camera. Such considerations are crucial when ruminating
on the possible deployment of such a system in a clinical
setting. As mentioned in the Introduction, an eye blink
detection device based on the use of infrared goggles
has been tested with a switch program in a hospital [18],
where a number of potential problems could arise with this
system, such as the wide range of possible orientations
of the user and distances to the camera. Some of the
experiments conducted aim to simulate these conditions in
order to gain insight into the plausibility of utilizing this
system for a diverse population of handicapped users.

- total blinks analyzed
- overall system measures
total missed blinks
total false positives
detector accuracy
- experimental system measures
total missed blinks
total false positives
detector accuracy
voluntary blink length
missed blinks
false positives

2288
43
64
95.3%
125
173
87.4%

5
10
20
1.09% 1.01% 2.53%
1.49% 1.44% 2.80%

Figure 8: The experimental system measures include the experiments involving the adjustments in the voluntary blink
length parameter, while the overall system measures disregard these outliers, which are detailed in the table.

Video of each test session was captured online and
post-processed to determine how well the system performed. The number of voluntary and involuntary blinks
detected by the system were written to a log ﬁle during
the session. Afterwards, the actual number of times the
user blinked voluntarily and involuntarily were counted
manually by reviewing the video of the session. False
positives and missed blinks were also noted.

double the number of blinks were missed (58), and nearly
double the number of false positives were detected (64).
This leads to the choice of the word “natural” to describe
the default blink length of 10 frames (1/3 of a second). The
test subjects found this to be the most intuitive length of
time to consider as the prolonged blink, with lower values
being too close to the involuntary length, and with higher
values such as 20 frames (2/3 of a second) producing an
unnatural feeling that was too long to be useful as a switch
input. This feeling was well-founded, as this longer blink
length lead to a severe degradation in the detector accuracy.
Nearly all of the misses and false positives in these sessions
were caused by users not holding their voluntary blinks
long enough for the system to correctly classify them.

4 Results
A large volume of data was collected in order to assess the
system accuracy. Compared to the 204 blinks provided
in the sequences by Grauman et al. [11], a total of 2,288
true blinks by the eight test subjects were analyzed in the
experiments for this system. Disregarding the sessions involving the testing of the voluntary blink length parameter
for reasons to be discussed later, there were 43 missed
blinks and 64 false positives, for an overall accuracy rate of
95.3%. Incorporating all sessions and experiments, there
were 125 missed blinks and 173 false positives, for an
accuracy rate of 87.4%. See Figure 8 for a summary of the
main results of the experiments.

In fact, the other experiments, designed to test how
well the system would fair in an environment whose
conditions are not known a priori, only resulted in 20
missed blinks and 31 false positives (see Figures 9 and 10).
Thus, the vast majority of missed blinks and false positives
across all experiments can be attributed to poor choices in
the voluntary blink length, which should not be considered
a problem for the accuracy of the system since these trials
were purely experimental and a length of approximately
10 frames (1/3 of a second) is known to be ideal for high
performance.

The ﬁrst rate of 95.3% should be considered as the overall
accuracy measure of the system because of the nature of
some of the extended experiments that inherently function
to reduce the accuracy rate. For example, in sessions tested
with the default, most natural voluntary blink length of 10
frames (1/3 of a second), there were only 23 missed blinks
and 33 false positives out of 1,242 blinks. On the other
hand, in sessions tested with a voluntary blink length of 20
frames (2/3 of a second), out of 504 such blinks, more than

6

Figure 7: Sample frames from sessions for each of the eight test subjects.
7

Figure 9: Sample frames from sessions testing alternate positions of the camera. The system still works accurately
with the camera placed well below the user’s face, as well
as with the camera rotated as much as about 45 degrees.

Figure 10: Sample frames from sessions testing varying
lighting conditions. The system still works accurately in
exceedingly bright and dark environments.

5 Discussion and Conclusions

Another improvement is this system’s compatibility
with inexpensive USB cameras, as opposed to the highresolution Sony EVI-D30 color video CCD camera used
by Grauman et al. [11]. These Logitech USB cameras are
more affordable and portable, and perhaps most importantly, support a higher real-time frame rate of 30 frames
per second.

The system proposed in this paper provides a binary switch
input alternative for people with disabilities similar to the
one presented by Grauman et al. [11]. However, some
signiﬁcant improvements and contributions were made
over such predecessor systems.

The reliability of the system has been shown with the
high accuracy results reported in the previous section.
In addition to the extensive testing that was conducted
to retrieve these results, additional considerations and
circumstances that are important for such a system were
tested that were not treated experimentally by Grauman
et al. [11]. One such consideration is the performance of
the system under different lighting conditions (see Figure
10). The experiments indicate that the system performs
equally well in extreme lighting conditions (i.e. with all
lights turned off, leaving the computer monitor as the only
light source, and with a lamp aimed directly at the video
camera). The accuracy percentages in these cases were
approximately the same as those that were retrieved in
normal lighting conditions.

The automatic initialization phase (involving the motion analysis work) is greatly simpliﬁed in this system,
with no loss of accuracy in locating the user’s eyes and
choosing a suitable open eye template. Given the reasonable assumption that the user is positioned anywhere
from about 1 to 2 feet away from the camera, the eyes
are detected within moments. As the distance increases
beyond this amount, the eyes can still be detected in
some cases, but it may take a longer time to occur since
the candidate pairs are much smaller and start to fail
the tests designed to pick out the likely components that
represent the user’s eyes. In all of the experiments in
which the subjects were seated between 1 and 2 feet
from the camera, it never took more than three involuntary
blinks by the user before the eyes were located successfully.
8

Another important consideration is the placement and
orientation of the camera with respect to the user (see
Figure 9). This was tested carefully to determine how
much freedom is available when setting up the camera,
a potentially crucial point when considering a clinical
environment, especially an Intensive Care Unit, which is
a prime setting that would beneﬁt from this system [18].
Aside from horizontal offset and orientation of the camera,
another issue of concern is the vertical offset of the camera
in relation to the user’s eyes. The experiments showed
that placing the camera below the user’s head resulted in
desirable functioning of the system. However, if the camera
is placed too high above the user’s head, in such a way
that it is aiming down at the user at a signiﬁcant angle, the
blink detection is no longer as accurate. This is caused by
the very small amount of variation in correlation scores
as the user blinks, since nearly all that is visible to the
camera is the eyelid of the user. Thus, when positioning
the camera, it is beneﬁcial to the detection accuracy to
maximize the degree of variation between the open and
closed eye images of the user. Finally, with respect to the
clinical environment, this system provides an unobtrusive
alternative to the one tested by Miglietta et al., which
required the user to wear a set of eyeglass frames for blink
detection [18]. This is an important point, considering the
additional discomfort that such an apparatus may bring to
the patients.

Figure 11: Experiment with a user wearing glasses. In some
cases, overwhelming glare from the computer monitor prevented the eyes from being located (left). With just the right
maneuvering by the user, the system was sometimes able to
ﬁnd and track the eye (right).

Acknowledgments
The work was supported by the National Science Foundation with grants IIS-0093367, IIS-0308213, IIS-0329009,
and EIA-0202067.

References
[1] M. Betke, J. Gips, and P. Fleming. The camera mouse:
Visual tracking of body features to provide computer
access for people with severe disabilities. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 10:1, pages 1–10, March 2002.

Some tests were also conducted with users wearing
glasses (see Figure 11), which exposed somewhat of a limitation with the system. In some situations, glare from the
computer monitor prevented the eyes from being located
in the motion analysis phase. Users were sometimes able
to maneuver their heads and position their eyes in such a
way that the glare was minimized, resulting in successful
location of the eyes, but this is not a reasonable expectation
for severely disabled people that may be operating with the
system.

[2] M. Betke, W. Mullally, and J. Magee. Active detection
of eye scleras in real time. Proceedings of the IEEE
CVPR Workshop on Human Modeling, Analysis and
Synthesis (HMAS 2000), Hilton Head Island, SC, June
2000.
[3] T.N. Bhaskar, F.T. Keat, S. Ranganath, and Y.V.
Venkatesh. Blink detection and eye tracking for eye
localization. Proceedings of the Conference on Convergent Technologies for Asia-Paciﬁc Region (TENCON 2003), pages 821–824, Bangalore, Inda, October
15-17 2003.

With the rapid advancement of technology and hardware in
use by modern computers, the proposed system could potentially be utilized not just by handicapped people, but by
the general population as an additional binary input. Higher
frame rates and ﬁner camera resolutions could lead to more
robust eye detection that is less restrictive on the user, while
increased processing power could be used to enhance the
tracking algorithm to more accurately follow the user’s eye
and recover more gracefully when it is lost. The ease of
use and potential for rapid input that this system provides
could be used to enhance productivity by incorporating it to
generate input for a task in any general software program.

[4] R.L. Cloud, M. Betke, and J. Gips. Experiments with a
camera-based human-computer interface system. Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (UI4ALL 2002), pages 103–110, Paris, France,
October 2002.
[5] S. Crampton and M. Betke. Counting ﬁngers in real
time: A webcam-based human-computer interface

9

with game applications. Proceedings of the Conference on Universal Access in Human-Computer Interaction (afﬁliated with HCI International 2003), pages
1357–1361, Crete, Greece, June 2003.

[15] J. Lombardi and M. Betke. A camera-based eyebrow
tracker for hands-free computer control via a binary
switch. Proceedings of the 7th ERCIM Workshop,
User Interfaces For All (UI4ALL 2002), pages 199–
200, Paris, France, October 2002.

[6] C. Fagiani, M. Betke, and J. Gips. Evaluation of tracking methods for human-computer interaction. Proceedings of the IEEE Workshop on Applications in
Computer Vision (WACV 2002), pages 121–126, Orlando, Florida, December 2002.

[16] J.J. Magee, M.R. Scott, B.N. Waber, and M. Betke.
Eyekeys: A real-time vision interface based on gaze
detection from a low-grade video camera. Proceedings of the IEEE Workshop on Real-Time Vision for
Human-Computer Interaction (RTV4HCI), Washington, D.C., July 2004.

[7] D.O. Gorodnichy. On importance of nose for face
tracking. Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition
(FG 2002), pages 188–196, Washington, D.C., May
20-21 2002.

[17] Microsoft directx 8 sdk.
http://www.microsoft.com/downloads.
[18] M.A. Miglietta, G. Bochicchio, and T.M. Scalea.
Computer-assisted communication for criticcally ill
patients: a pilot study. The Journal of TRAUMA Injury, Infection, and Critical Care, Vol. 57, pages 488–
493, September 2004.

[8] D.O. Gorodnichy. Second order change detection,
and its application to blink-controlled perceptual interfaces. Proceedings of the IASTED Conference on
Visualization, Imaging and Image Processing (VIIP
2003), pages 140–145, Benalmadena, Spain, September 8-10 2003.

[19] T. Moriyama, T. Kanade, J.F. Cohn, J. Xiao, Z. Ambadar, J. Gao, and H. Imamura. Automatic recognition
of eye blinking in spontaneously occurring behavior.
Proceedings of the International Conference on Pattern Recognition (ICPR 2002), Vol. IV, pages 78–81,
Quebec City, Canada, 2002.

[9] D.O. Gorodnichy. Towards automatic retrieval of
blink-based lexicon for persons suffered from brainstem injury using video cameras. Proceedings of the
CVPR Workshop on Face Processing in Video (FPIV
2004), Washington, D.C., June 28 2004.

[20] Opencv library.
http://sourceforge.net/projects/opencvlibrary.

[10] D.O. Gorodnichy and G. Roth. Nouse use your nose
as a mouse perceptual vision technology for handsfree games and interfaces. Proceedings of the International Conference on Vision Interface (VI 2002), Calgary, Canada, May 27-29 2002.

[21] J. Rurainsky and P. Eisert. Eye center localization
using adaptive templates. Proceedings of the CVPR
Workshop on Face Processing in Video (FPIV 2004),
Washington, D.C., June 28 2004.

[11] K. Grauman, M. Betke, J. Gips, and G. Bradski.
Communication via eye blinks - detection and duration analysis in real time. Proceedings of the IEEE
Computer Vision and Pattern Recognition Conference (CVPR 2001), Vol. 2, pages 1010–1017, Kauai,
Hawaii, December 2001.

[22] Simtech publications.
http://hsj.com/products.html.
[23] X. Wei, Z. Zhu, L. Yin, and Q. Ji. A real-time face
tracking and animation system. Proceedings of the
CVPR Workshop on Face Processing in Video (FPIV
2004), Washington, D.C., June 28 2004.

[12] K. Grauman, M. Betke, J. Lombardi, J. Gips, and
G. Bradski. Communication via eye blinks and eyebrow raises: Video-based human-computer interaces.
Universal Access In The Information Society, 2(4),
pages 359–373, November 2003.
[13] Intel image processing library (ipl).
http://developer.intel.com/software/products/perﬂib/ijl.
[14] R. Jain, R. Kasturi, and B.G. Schunck. Machine Vision. Mc-Graw Hill, New York, 1995.

10

Boston University Computer Science Technical Report No. 2005-12

Real Time Eye Tracking and Blink Detection with USB Cameras
Michael Chau and Margrit Betke
Computer Science Department
Boston University
Boston, MA 02215, USA
{mikechau, betke@cs.bu.edu}
May 12, 2005

Abstract

1

Introduction

A human-computer interface (HCI) system designed for use
by people with severe disabilities is presented. People that
are severely paralyzed or afﬂicted with diseases such as
ALS (Lou Gehrig’s disease) or multiple sclerosis are unable to move or control any parts of their bodies except for
their eyes. The system presented here detects the user’s eye
blinks and analyzes the pattern and duration of the blinks,
using them to provide input to the computer in the form of
a mouse click. After the automatic initialization of the system occurs from the processing of the user’s involuntary eye
blinks in the ﬁrst few seconds of use, the eye is tracked in
real time using correlation with an online template. If the
user’s depth changes signiﬁcantly or rapid head movement
occurs, the system is automatically reinitialized. There are
no lighting requirements nor ofﬂine templates needed for
the proper functioning of the system. The system works with
inexpensive USB cameras and runs at a frame rate of 30
frames per second. Extensive experiments were conducted
to determine both the system’s accuracy in classifying voluntary and involuntary blinks, as well as the system’s ﬁtness
in varying environment conditions, such as alternative camera placements and different lighting conditions. These experiments on eight test subjects yielded an overall detection
accuracy of 95.3%.

A great deal of computer vision research is dedicated to
the implementation of systems designed to detect user
movements and facial gestures [1, 2, 4, 5, 6, 15, 16]. In
many cases, such systems are created with the speciﬁc goal
of providing a way for people with disabilities or limited
motor skills to be able to use computer systems, albeit in
much simpler applications [1, 15, 16]. The motivation for
the system proposed here is to provide an inexpensive,
unobtrusive means for disabled people to interact with
simple computer applications in a meaningful way that
requires minimal effort.
This goal is accomplished using a robust algorithm
based on the work by Grauman et al. [11, 12]. Some of
these methods are implemented here, while some have been
enhanced or modiﬁed to the end of simpliﬁed initialization
and more efﬁcient maintenance of the real time tracking.
The automatic initialization phase is triggered by the
analysis of the involuntary blinking of the current user of
the system, which creates an online template of the eye
to be used for tracking. This phase occurs each time the
current correlation score of the tracked eye falls below a
deﬁned threshold in order to allow the system to recover
and regain its accuracy in detecting the blinks. This system
can be utilized by users that are able to voluntarily blink
and have a use for applications that require mouse clicks as
input (e.g. switch and scanning programs/games [22]).
A thorough survey on work related to eye and blink
detection methods is presented by Grauman et al., as well
as Magee et al. [12, 16]. Since the implementation of the
1

BlinkLink blink detection system by Grauman et al., a
number of signiﬁcant contributions and advancements have
been made in the HCI ﬁeld. Gorodnichy and Roth present
communication interfaces that operate using eye blinks
[8, 9, 10]. Motion analysis methods and frame differencing
techniques used to locate the eyes are used Bhaskar et al.
and Gorodnichy [3, 8, 9]. Detecting eye blinking in the
presence of spontaneous movements as well as occlusion
and out-of-plane motion is discussed by Moriyama et al.
[19]. Methods for locating eyes using gradients and luminance and color information with templates are presented
by Rurainsky and Eisert [21]. Miglietta et al. present
results of a study involving the use of an eyeglass frame
worn by the patients in an Intenstive Care Unit that detects
eye blinks to operate a switch system [18]. There still have
not been many blink detection related systems designed to
work with inexpensive USB webcams [7, 8]. There have,
however, been a number of other feature detection systems
that use more expensive and less portable alternatives, such
as digital and IR cameras for video input [3, 19, 21, 23].
Aside from the portability concerns, these systems are
also typically unable to achieve the desirable higher frame
rates of approximately 30 fps that are common with USB
cameras.

image differencing

tracker
lost

thresholding

detect and analyze
blinking

opening

track eye

label connected
components

create eye
template

filter out
infeasible pairs

return location of best
candidate for eye pair

Figure 1: Overview of the main stages in the system.

2.1 Initialization
Naturally, the ﬁrst step in analyzing the blinking of the user
is to locate the eyes. To accomplish this, the difference
image of each frame and the previous frame is created and
then thresholded, resulting in a binary image showing the
regions of movement that occurred between the two frames.

The main contribution of this paper is to provide a
robust reimplementation of the system described by Grauman et al. [11] that is able to run in real time at 30 frames
per second on readily available and affordable webcams.
As mentioned, most systems dealing with motion analysis
required the use of rather expensive equipment and highend video cameras. However, in recent years, inexpensive
webcams manufactured by companies such as Logitech
have become ubiquitous, facilitating the incorporation of
these motion analysis systems on a more widespread basis.
The system described here is an accurate and useful tool
to give handicapped people another alternative to interface
with computer systems.

Next, a 3x3 star-shaped convolution kernel is passed
over the binary difference image in an Opening morphological operation [14]. This functions to eliminate a great
deal of noise and naturally-occurring jitter that is present
around the user in the frame due to the lighting conditions
and the camera resolution, as well as the possibility of
background movement. In addition, this Opening operation
also produces fewer and larger connected components in
the vicinity of the eyes (when a blink happens to occur),
which is crucial for the efﬁciency and accuracy of the next
phase (see Figure 2).

2 Methods
The algorithm used by the system for detecting and analyzing blinks is initialized automatically, dependent only upon
the inevitability of the involuntary blinking of the user.
Motion analysis techniques are used in this stage, followed
by online creation of a template of the open eye to be used
for the subsequent tracking and template matching that is
carried out at each frame. A ﬂow chart depicting the main
stages of the system is shown in Figure 1.

A recursive labeling procedure is applied next to recover
the number of connected components in the resultant binary
image. Under the circumstances in which this system was
optimally designed to function, in which the users are
for the most part paralyzed, this procedure yields only a
few connected components, with the ideal number being
two (the left eye and the right eye). In the case that other
movement has occurred, producing a much larger number
of components, the system discards the current binary
2

2.2 Template Creation

A

B

C

If the previous stage results in a pair of components that
passes the set of ﬁlters, then it is a good indication that the
user’s eyes have been successfully located. At this point,
the location of the larger of the two components is chosen
for creation of the template. Since the size of the template
that is to be created is directly proportional to the size of
the chosen component, the larger one is chosen for the
purpose of having more brightness information, which will
result in more accurate tracking and correlation scores (see
Figure 3).

D

Since the system will be tracking the user’s open eye,
it would be a mistake to create the template at the instant
that the eye was located, since the user was blinking at this
moment. Thus, once the eye is believed to be located, a
timer is triggered. After a small number of frames elapse,
which is judged to be the approximate time needed for the
user’s eye to become open again after an involuntary blink,
the template of the user’s open eye is created. Therefore,
during initialization, the user is assumed to be blinking at a
normal rate of one involuntary blink every few moments.
Again, no ofﬂine templates are necessary and the creation
of this online template is completely independent of any
past templates that may have been created during the run of
the system.

Figure 2: Motion analysis phase: (A) User at frame f .
(B) User at frame f + 1, having just blinked. (C) Initial
difference of the two frames f and f +1. Note the great deal
of noise in the background due to the lighting conditions
and camera properties. (D) Difference image used to locate
the eyes after performing the Opening operation.
image and waits to process the next involuntary blink in
order to maintain efﬁciency and accuracy in locating the
eyes.
Given an image with a small number of connected
components output from the previous processing steps,
the system is able to proceed efﬁciently by considering
each pair of components as a possible match for the user’s
left and right eyes. The ﬁltering of unlikely eye pair
matches is based on the computation of six parameters
for each component pair: the width and height of each
of the two components and the horizontal and vertical
distance between the centroids of the two components. A
number of experimentally-derived heuristics are applied to
these statistics to pinpoint the exact pair that most likely
represents the user’s eyes. For example, if there is a large
difference in either the width or height of each of the two
components, then they likely are not the user’s eyes. As an
additional example of one of these many ﬁlters, if there is
a large vertical distance between the centroids of the two
components, then they are also not likely to be the user’s
eyes, since such a property would not be humanly possible.
Such observations not only lead to accurate detection of
the user’s eyes, but also speed up the search greatly by
eliminating unlikely components immediately.

Figure 3: Open eye templates: Note the diversity in the appearance of some of the open templates that were used during user experiments. Working templates range from very
small to large in overall size, as well very tight around the
eye to a larger area surrounding the eye, including the eyebrow.

2.3 Eye Tracking
As noted by Grauman et al., the use of template matching
is necessary for the desired accuracy in analyzing the user’s
blinking since it allows the user some freedom to move
around slightly [11]. Though the primary purpose of such
a system is to serve people with paralysis, it is a desirable
3

feature to allow for some slight movement by the user or
the camera that would not be feasible if motion analysis
were used alone.
The normalized correlation coefﬁcient, also implemented in the system proposed by Grauman et al., is used
to accomplish the tracking [11]. This measure is computed
at each frame using the following formula:
x,y
x,y

A

B

C

D

¯
¯
[f (x,y)−fu,v ][t(x−u,y−v)−t]

¯
[f (x,y)−fu,v ]2

x,y

¯
[t(x−u,y−v)−t]2

where f (x, y) is the brightness of the video frame at
¯
the point (x, y), fu,v is the average value of the video
frame in the current search region, t(x, y) is the brightness
¯
of the template image at the point (x, y), and t is the
average value of the template image. The result of this
computation is a correlation score between -1 and 1 that
indicates the similarity between the open eye template and
all points in the search region of the video frame. Scores
closer to 0 indicate a low level of similarity, while scores
closer to 1 indicate a probable match for the open eye
template. A major beneﬁt of using this similarity measure
to perform the tracking is that it is insensitive to constant
changes in ambient lighting conditions. The Results section
shows that the eye tracking and blink detection works just
as well in the presence of both very dark and bright lighting.

Figure 4: Sample frames of a typical session: (A) The system is in this state during the motion analysis phase. The red
rectangle represents the region that is considered during the
frame differencing and labeling of connected components.
(B) The system enters this state once the eye is located and
remains this way as long as the eye is not believed to be
lost. The green rectangle represents the region at which the
open eye template was selected and the red rectangle now
represents the drastically reduced search space for performing the correlation. (C) User at frame f , with eyes already
closed for the deﬁned voluntary blink duration and (D) user
at frame f + 1, opening his eyes, with a yellow dot being
drawn on the eye to indicate that a voluntary blink just occurred.

Since this method requires an extensive amount of
computation and is performed 30 times per second, the
search region is restricted to a small area around the user’s
eye (see Figure 4). This reduced search space allows the
system to remain running smoothly in real time since it
drastically reduces the computation needed to perform the
correlation search at each frame.

Close examination of the correlation scores over time for a
number of different users of the system reveals rather clear
boundaries that allow for the detection of the blinks. As the
user’s eye is in the normal open state, very high correlation
scores of about 0.85 to 1.0 are reported. As the user blinks,
the scores fall to values of about 0.5 to 0.55. Finally, a very
important range to note is the one containing scores below
about 0.45. Scores in this range normally indicate that the
tracker has lost the location of the eye. In such cases, the
system must be reinitialized to relocate and track the new
position of the eye.

2.4 Blink Detection
The detection of blinking and the analysis of blink duration
are based solely on observation of the correlation scores
generated by the tracking at the previous step using the
online template of the user’s eye. As the user’s eye closes
during the process of a blink, its similarity to the open
eye template decreases. Likewise, it regains its similarity to the template as the blink ends and the user’s eye
becomes fully open again. This decrease and increase in
similarity corresponds directly to the correlation scores
returned by the template matching procedure (see Figure 5).

Given these ranges of correlation scores and knowledge of what they signify derived from experimentation
and observation across a number of test subjects, the system
detects voluntary blinks by using a timer that is triggered
each time the correlation scores fall below the threshold of
scores that represent an open eye. If the correlation scores

4

correlation scores over time

1.2

correlation score

1
1.0

0.8
0.8

0.6
0.6

0.4
0.4

0.2
0.2

0.0
0
0 10

1

30

60

90

120

150

180

210

240

270

19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 226 235 244 253 262 271 280

time (in frames)

Figure 6: System interface: Notable features include the
ability for the user to deﬁne the voluntary blink length, the
ability to reset the tracking at any time, should a poor or
unexpected template location be chosen, and the ability to
save the session to a video ﬁle.

Figure 5: Correlation scores for the open eye template plotted over time (in frames). The scores form a clear waveform, as noted by Grauman et al., which is useful in deriving a threshold to be used for classifying the user’s eyes as
being open or closed at each frame [11]. In this example,
there were three short blinks followed by three long blinks,
three short blinks again, and ﬁnally one more long blink.

detector accuracy should yield correspondingly high
accuracy results for the usability tests, subject to the user’s
understanding and capabilities in carrying out the given
tasks, such as simple reaction time and matching games, as
described by Grauman et al. [11].

remain below this threshold and above the threshold that
results in reinitialization of the system for a deﬁned number
of frames that can be set by the user, then a voluntary blink
is judged to have occurred, causing a mouse click to be
issued to the operating system.

Therefore, the experiments conducted for this system
were more focused on detector accuracy, since this is
a more standard measure of the overall accuracy of the
system across a broad range of users. In order to measure
the detection accuracy, test subjects were seated in front of
the computer, approximately 2 feet away from the camera.
Subjects were instructed to act naturally, but were asked not
to turn their heads or move too abruptly, since this could
potentially lead to repeated reinitialization of the system,
making it difﬁcult to test the accuracy. In addition, this
constraint allowed for a closer simulation of the system’s
target audience of handicapped users.

3 Experiments
The system was primarily developed and tested on a Windows XP PC with an Intel Pentium IV 2.8 GHz processor
and 1 GB RAM. Video was captured with a Logitech
Quickcam Pro 4000 webcam at 30 frames per second.
All video was processed as grayscale images of 320 x
240 pixels using various utilities from the Intel OpenCV
and Image Processing libraries, as well as the Microsoft
DirectX SDK [13, 20, 17]. Figure 6 shows the interface for
the system. The experiments were conducted with eight
test subjects at two different locations (see Figure 7).

Similar to the tests done by Grauman et al., subjects
were also asked to blink random test patterns that were
determined prior to the start of the session [11]. For example, subjects were asked to blink two short blinks followed
by a long (voluntary) blink, or were asked to blink twice
voluntarily followed by a short (involuntary) blink. These
test results serve to show how well the system distinguishes
between the voluntary and involuntary blinks, which is the
crux of the problem. Tests involving the voluntary blink
length parameter were also conducted, with values ranging
from 5 to 20 frames (1/6 of a second to 2/3 of a second).

Reviewing the work done by Grauman et al., it is apparent
that similar results were obtained with experiments based
on testing the accuracy of the system and experiments
based on testing the usability of the system as a switch
input device [11]. Intuitively, this makes sense as good

In addition, as a further contribution, numerous other

5

Summary of results

experiments were also conducted to determine the ﬁtness of
the system under varying circumstances, such as alternative
camera placements, lighting conditions, and distance to the
camera. Such considerations are crucial when ruminating
on the possible deployment of such a system in a clinical
setting. As mentioned in the Introduction, an eye blink
detection device based on the use of infrared goggles
has been tested with a switch program in a hospital [18],
where a number of potential problems could arise with this
system, such as the wide range of possible orientations
of the user and distances to the camera. Some of the
experiments conducted aim to simulate these conditions in
order to gain insight into the plausibility of utilizing this
system for a diverse population of handicapped users.

- total blinks analyzed
- overall system measures
total missed blinks
total false positives
detector accuracy
- experimental system measures
total missed blinks
total false positives
detector accuracy
voluntary blink length
missed blinks
false positives

2288
43
64
95.3%
125
173
87.4%

5
10
20
1.09% 1.01% 2.53%
1.49% 1.44% 2.80%

Figure 8: The experimental system measures include the experiments involving the adjustments in the voluntary blink
length parameter, while the overall system measures disregard these outliers, which are detailed in the table.

Video of each test session was captured online and
post-processed to determine how well the system performed. The number of voluntary and involuntary blinks
detected by the system were written to a log ﬁle during
the session. Afterwards, the actual number of times the
user blinked voluntarily and involuntarily were counted
manually by reviewing the video of the session. False
positives and missed blinks were also noted.

double the number of blinks were missed (58), and nearly
double the number of false positives were detected (64).
This leads to the choice of the word “natural” to describe
the default blink length of 10 frames (1/3 of a second). The
test subjects found this to be the most intuitive length of
time to consider as the prolonged blink, with lower values
being too close to the involuntary length, and with higher
values such as 20 frames (2/3 of a second) producing an
unnatural feeling that was too long to be useful as a switch
input. This feeling was well-founded, as this longer blink
length lead to a severe degradation in the detector accuracy.
Nearly all of the misses and false positives in these sessions
were caused by users not holding their voluntary blinks
long enough for the system to correctly classify them.

4 Results
A large volume of data was collected in order to assess the
system accuracy. Compared to the 204 blinks provided
in the sequences by Grauman et al. [11], a total of 2,288
true blinks by the eight test subjects were analyzed in the
experiments for this system. Disregarding the sessions involving the testing of the voluntary blink length parameter
for reasons to be discussed later, there were 43 missed
blinks and 64 false positives, for an overall accuracy rate of
95.3%. Incorporating all sessions and experiments, there
were 125 missed blinks and 173 false positives, for an
accuracy rate of 87.4%. See Figure 8 for a summary of the
main results of the experiments.

In fact, the other experiments, designed to test how
well the system would fair in an environment whose
conditions are not known a priori, only resulted in 20
missed blinks and 31 false positives (see Figures 9 and 10).
Thus, the vast majority of missed blinks and false positives
across all experiments can be attributed to poor choices in
the voluntary blink length, which should not be considered
a problem for the accuracy of the system since these trials
were purely experimental and a length of approximately
10 frames (1/3 of a second) is known to be ideal for high
performance.

The ﬁrst rate of 95.3% should be considered as the overall
accuracy measure of the system because of the nature of
some of the extended experiments that inherently function
to reduce the accuracy rate. For example, in sessions tested
with the default, most natural voluntary blink length of 10
frames (1/3 of a second), there were only 23 missed blinks
and 33 false positives out of 1,242 blinks. On the other
hand, in sessions tested with a voluntary blink length of 20
frames (2/3 of a second), out of 504 such blinks, more than

6

Figure 7: Sample frames from sessions for each of the eight test subjects.
7

Figure 9: Sample frames from sessions testing alternate positions of the camera. The system still works accurately
with the camera placed well below the user’s face, as well
as with the camera rotated as much as about 45 degrees.

Figure 10: Sample frames from sessions testing varying
lighting conditions. The system still works accurately in
exceedingly bright and dark environments.

5 Discussion and Conclusions

Another improvement is this system’s compatibility
with inexpensive USB cameras, as opposed to the highresolution Sony EVI-D30 color video CCD camera used
by Grauman et al. [11]. These Logitech USB cameras are
more affordable and portable, and perhaps most importantly, support a higher real-time frame rate of 30 frames
per second.

The system proposed in this paper provides a binary switch
input alternative for people with disabilities similar to the
one presented by Grauman et al. [11]. However, some
signiﬁcant improvements and contributions were made
over such predecessor systems.

The reliability of the system has been shown with the
high accuracy results reported in the previous section.
In addition to the extensive testing that was conducted
to retrieve these results, additional considerations and
circumstances that are important for such a system were
tested that were not treated experimentally by Grauman
et al. [11]. One such consideration is the performance of
the system under different lighting conditions (see Figure
10). The experiments indicate that the system performs
equally well in extreme lighting conditions (i.e. with all
lights turned off, leaving the computer monitor as the only
light source, and with a lamp aimed directly at the video
camera). The accuracy percentages in these cases were
approximately the same as those that were retrieved in
normal lighting conditions.

The automatic initialization phase (involving the motion analysis work) is greatly simpliﬁed in this system,
with no loss of accuracy in locating the user’s eyes and
choosing a suitable open eye template. Given the reasonable assumption that the user is positioned anywhere
from about 1 to 2 feet away from the camera, the eyes
are detected within moments. As the distance increases
beyond this amount, the eyes can still be detected in
some cases, but it may take a longer time to occur since
the candidate pairs are much smaller and start to fail
the tests designed to pick out the likely components that
represent the user’s eyes. In all of the experiments in
which the subjects were seated between 1 and 2 feet
from the camera, it never took more than three involuntary
blinks by the user before the eyes were located successfully.
8

Another important consideration is the placement and
orientation of the camera with respect to the user (see
Figure 9). This was tested carefully to determine how
much freedom is available when setting up the camera,
a potentially crucial point when considering a clinical
environment, especially an Intensive Care Unit, which is
a prime setting that would beneﬁt from this system [18].
Aside from horizontal offset and orientation of the camera,
another issue of concern is the vertical offset of the camera
in relation to the user’s eyes. The experiments showed
that placing the camera below the user’s head resulted in
desirable functioning of the system. However, if the camera
is placed too high above the user’s head, in such a way
that it is aiming down at the user at a signiﬁcant angle, the
blink detection is no longer as accurate. This is caused by
the very small amount of variation in correlation scores
as the user blinks, since nearly all that is visible to the
camera is the eyelid of the user. Thus, when positioning
the camera, it is beneﬁcial to the detection accuracy to
maximize the degree of variation between the open and
closed eye images of the user. Finally, with respect to the
clinical environment, this system provides an unobtrusive
alternative to the one tested by Miglietta et al., which
required the user to wear a set of eyeglass frames for blink
detection [18]. This is an important point, considering the
additional discomfort that such an apparatus may bring to
the patients.

Figure 11: Experiment with a user wearing glasses. In some
cases, overwhelming glare from the computer monitor prevented the eyes from being located (left). With just the right
maneuvering by the user, the system was sometimes able to
ﬁnd and track the eye (right).

Acknowledgments
The work was supported by the National Science Foundation with grants IIS-0093367, IIS-0308213, IIS-0329009,
and EIA-0202067.

References
[1] M. Betke, J. Gips, and P. Fleming. The camera mouse:
Visual tracking of body features to provide computer
access for people with severe disabilities. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 10:1, pages 1–10, March 2002.

Some tests were also conducted with users wearing
glasses (see Figure 11), which exposed somewhat of a limitation with the system. In some situations, glare from the
computer monitor prevented the eyes from being located
in the motion analysis phase. Users were sometimes able
to maneuver their heads and position their eyes in such a
way that the glare was minimized, resulting in successful
location of the eyes, but this is not a reasonable expectation
for severely disabled people that may be operating with the
system.

[2] M. Betke, W. Mullally, and J. Magee. Active detection
of eye scleras in real time. Proceedings of the IEEE
CVPR Workshop on Human Modeling, Analysis and
Synthesis (HMAS 2000), Hilton Head Island, SC, June
2000.
[3] T.N. Bhaskar, F.T. Keat, S. Ranganath, and Y.V.
Venkatesh. Blink detection and eye tracking for eye
localization. Proceedings of the Conference on Convergent Technologies for Asia-Paciﬁc Region (TENCON 2003), pages 821–824, Bangalore, Inda, October
15-17 2003.

With the rapid advancement of technology and hardware in
use by modern computers, the proposed system could potentially be utilized not just by handicapped people, but by
the general population as an additional binary input. Higher
frame rates and ﬁner camera resolutions could lead to more
robust eye detection that is less restrictive on the user, while
increased processing power could be used to enhance the
tracking algorithm to more accurately follow the user’s eye
and recover more gracefully when it is lost. The ease of
use and potential for rapid input that this system provides
could be used to enhance productivity by incorporating it to
generate input for a task in any general software program.

[4] R.L. Cloud, M. Betke, and J. Gips. Experiments with a
camera-based human-computer interface system. Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (UI4ALL 2002), pages 103–110, Paris, France,
October 2002.
[5] S. Crampton and M. Betke. Counting ﬁngers in real
time: A webcam-based human-computer interface

9

with game applications. Proceedings of the Conference on Universal Access in Human-Computer Interaction (afﬁliated with HCI International 2003), pages
1357–1361, Crete, Greece, June 2003.

[15] J. Lombardi and M. Betke. A camera-based eyebrow
tracker for hands-free computer control via a binary
switch. Proceedings of the 7th ERCIM Workshop,
User Interfaces For All (UI4ALL 2002), pages 199–
200, Paris, France, October 2002.

[6] C. Fagiani, M. Betke, and J. Gips. Evaluation of tracking methods for human-computer interaction. Proceedings of the IEEE Workshop on Applications in
Computer Vision (WACV 2002), pages 121–126, Orlando, Florida, December 2002.

[16] J.J. Magee, M.R. Scott, B.N. Waber, and M. Betke.
Eyekeys: A real-time vision interface based on gaze
detection from a low-grade video camera. Proceedings of the IEEE Workshop on Real-Time Vision for
Human-Computer Interaction (RTV4HCI), Washington, D.C., July 2004.

[7] D.O. Gorodnichy. On importance of nose for face
tracking. Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition
(FG 2002), pages 188–196, Washington, D.C., May
20-21 2002.

[17] Microsoft directx 8 sdk.
http://www.microsoft.com/downloads.
[18] M.A. Miglietta, G. Bochicchio, and T.M. Scalea.
Computer-assisted communication for criticcally ill
patients: a pilot study. The Journal of TRAUMA Injury, Infection, and Critical Care, Vol. 57, pages 488–
493, September 2004.

[8] D.O. Gorodnichy. Second order change detection,
and its application to blink-controlled perceptual interfaces. Proceedings of the IASTED Conference on
Visualization, Imaging and Image Processing (VIIP
2003), pages 140–145, Benalmadena, Spain, September 8-10 2003.

[19] T. Moriyama, T. Kanade, J.F. Cohn, J. Xiao, Z. Ambadar, J. Gao, and H. Imamura. Automatic recognition
of eye blinking in spontaneously occurring behavior.
Proceedings of the International Conference on Pattern Recognition (ICPR 2002), Vol. IV, pages 78–81,
Quebec City, Canada, 2002.

[9] D.O. Gorodnichy. Towards automatic retrieval of
blink-based lexicon for persons suffered from brainstem injury using video cameras. Proceedings of the
CVPR Workshop on Face Processing in Video (FPIV
2004), Washington, D.C., June 28 2004.

[20] Opencv library.
http://sourceforge.net/projects/opencvlibrary.

[10] D.O. Gorodnichy and G. Roth. Nouse use your nose
as a mouse perceptual vision technology for handsfree games and interfaces. Proceedings of the International Conference on Vision Interface (VI 2002), Calgary, Canada, May 27-29 2002.

[21] J. Rurainsky and P. Eisert. Eye center localization
using adaptive templates. Proceedings of the CVPR
Workshop on Face Processing in Video (FPIV 2004),
Washington, D.C., June 28 2004.

[11] K. Grauman, M. Betke, J. Gips, and G. Bradski.
Communication via eye blinks - detection and duration analysis in real time. Proceedings of the IEEE
Computer Vision and Pattern Recognition Conference (CVPR 2001), Vol. 2, pages 1010–1017, Kauai,
Hawaii, December 2001.

[22] Simtech publications.
http://hsj.com/products.html.
[23] X. Wei, Z. Zhu, L. Yin, and Q. Ji. A real-time face
tracking and animation system. Proceedings of the
CVPR Workshop on Face Processing in Video (FPIV
2004), Washington, D.C., June 28 2004.

[12] K. Grauman, M. Betke, J. Lombardi, J. Gips, and
G. Bradski. Communication via eye blinks and eyebrow raises: Video-based human-computer interaces.
Universal Access In The Information Society, 2(4),
pages 359–373, November 2003.
[13] Intel image processing library (ipl).
http://developer.intel.com/software/products/perﬂib/ijl.
[14] R. Jain, R. Kasturi, and B.G. Schunck. Machine Vision. Mc-Graw Hill, New York, 1995.

10

Running title: Prosodic influences in a conversation task

Prosodic influences on the production and comprehension of syntactic
ambiguity in a game-based conversation task
Amy J. Schafer
University of Hawai‘i
Shari R. Speer
Ohio State University
and
Paul Warren
Victoria University of Wellington

Please address correspondence to:
Amy J. Schafer
Department of Linguistics, 569 Moore Hall
University of Hawai‘i at M¯ noa
a
1890 East-West Rd
Honolulu, HI 96822
aschafer@hawaii.edu

1

Schafer, Speer, & Warren

2

There is now considerable evidence that listeners are sensitive to prosodic
structure in their syntactic analysis of spoken language (for reviews see Cutler,
Dahan & Donselaar, 1997; Warren, 1999). Some recent research suggests that the
prosodic contrasts investigated in comprehension research are not produced
consistently by speakers, but may directly depend on ambiguity levels in the
discourse situation. When naïve untrained speakers produced disambiguating
prosody in situations where two or more syntactic parses were plausible, they did
so less reliably if the discourse context contained other disambiguating
information (Allbritton, McKoon & Ratcliff, 1996; Snedeker & Trueswell, 2003;
Straub, 1997). Earlier studies similarly demonstrated stronger prosodic
disambiguation when speakers were explicitly instructed to disambiguate
(Cooper, Paccia & LaPointe, 1978; Lehiste, 1973), as well as situational
dependence, with speakers reducing the length of a description with repeated
mention (Clark & Schober, 1992), or producing reduced forms of words on
repetition in discourse context (Fowler & Housum, 1987).
However, these studies showing variable prosodic disambiguation may not
be representative of typical speech situations. In most of these, the speakers read
sentences aloud in paragraph contexts or as (imagined) instructions to listeners
who provided no spoken response.1 Reading tasks--and the prosody produced in
them--may differ from spontaneous speech, not least because the pragmatic goals
and production constraints of reader-listener pairs differ markedly from those of
interacting speaker-listener pairs. Production studies (Ayers, 1994; Butterworth,
1975) have highlighted some of the differences between the prosodic structures of
read speech and those of spontaneous speech: Read speech tends to have fewer
and shorter pauses, and fewer prosodic phrases. Thus, reading studies might
provide a poor guide to the distribution and size of prosodic boundaries in
spontaneous speech and therefore to the extent and nature of disambiguation in
spontaneous speech (Mazuka, Misono & Kondo, 2001). Studies of conversational
language have shown sharp differences in production and comprehension between
conversing speaker-hearer pairs and non-interacting speakers or overhearers.
Speakers in conversation designed their utterances to reflect the knowledge they
had in common with their listeners and to accommodate feedback from listeners
about how well they were being understood (Brennan, 1990; Clark & WilkesGibbs, 1986). Overhearers were less accurate than conversing listeners in
identifying speakers' intended referents, even when these were visually available
objects (Clark & Schober, 1992). Conversational effects such as these are
strengthened when speaker-listener pairs are aware of the need to cooperate
(Schober, Conrad & Fricker, 2000).
An advantage of using scripted tasks to study correspondences between
prosodic and syntactic structure is that they allow experimenter control of lexical

Running title: Prosodic influences in a conversation task

3

and syntactic content, and make it possible to carefully compare alternative
resolutions of ambiguous utterances. Some spontaneous speech tasks have been
designed to give a certain degree of control over the range of utterances produced,
such as map tasks (Anderson et al., 1991), route descriptions (Levelt & Cutler,
1983), or tangram tasks (Clark & Wilkes-Gibbs, 1986). However, even these
tasks do not elicit multiple renditions from the same speaker of a targeted
syntactic contrast.
The current research employed a cooperative game task, involving a set of
predetermined expressions that were used to negotiate the movement of
gamepieces around a board. These expressions contained a range of syntactic
ambiguities, although not every expression was ambiguous. In this chapter we
focus on PP attachments, as in the sentence I want to change the position of the
square with the triangle. Depending on whether the PP attaches high (to modify
the verb) or low (to modify the noun square), the utterance might mean "use the
triangle to move the square" or "move the combined square+triangle piece",
corresponding to two legitimate commands in our games.
Previous small-scale studies of PP ambiguities with read materials have
revealed more pausing and pre-pausal lengthening before the PP when it attaches
high (Lehiste, Olive & Streeter, 1976; Straub, 1997; Warren, 1985). Of interest to
our discussion above, production studies that have included disambiguating
contexts have shown evidence of both the maintenance of prosodic contrasts
(Price, Ostendorf, Shattuck-Hufnagel & Fong, 1991) and their reduction (Cooper
& Paccia-Cooper, 1980; Snedeker & Trueswell, 2003; Straub, 1997), as have
listener judgments from these studies. Studies of prosodic boundary location in PP
ambiguities have shown that low attachment interpretations were most likely
when a prosodic boundary preceded the direct object NP, and high attachments
when one followed the NP (Pynte & Prieur, 1996; Schafer, 1997).
Our game task allowed manipulation of the degree of contextual
determination of one meaning of an ambiguity over another. Straub (1997), for
instance, has proposed that the production system will allocate resources to
prosodic disambiguation when other sources of disambiguating information
would not be available for the listener in the resulting utterance. This comports
with findings that prosodic disambiguation is less marked when utterances are
read with disambiguating contexts. In our task, a number of information sources
potentially helped disambiguate between the different PP attachments (see
Warren, Schafer, Speer & White, 2000). Here, we examine two types of variation
of situational ambiguity. One reflects the configuration of gamepieces on the
playing boards and the preceding discourse. The other is linked to players'
potential awareness of the PP attachment contrast, which might result in increased
disambiguation as time spent playing the game increased.
Two experiments provide the relevant data for PP utterances. Experiment

Schafer, Speer, & Warren

4

1 presents acoustic and intonational analyses of productions by naïve speakers.
Experiment 2 considers the categorization by a second set of naïve listeners of
utterances isolated from the game context. The combination allows us to
investigate separately the extent to which speakers alter their prosody to reflect
syntactic and situational factors, and the extent to which listeners use whatever
prosodic cues are present to recover the intended syntactic form. The
transcriptions generated in experiment 1, encoding phonological distinctions such
as the presence or absence of a prosodic boundary, allow us to relate production
patterns to claims made in the comprehension literature. We can also evaluate
whether any given token, considered in isolation, is one which we would expect
to bias comprehension, on the basis of claims about the prosody-syntax interface
(e.g., Schafer, 1997; Selkirk, 1984, Carlson, Clifton & Frazier, 2001).
Our combined analyses allow us to evaluate three aspects of situational
effects on prosodic form. As situational ambiguity increases, i) does the
proportion of utterances pronounced with disambiguating prosody rise, as
determined by categories of prosodic transcriptions; ii) does the strength of the
acoustic cues for disambiguation rise, regardless of phonological categorization;
iii) do speakers' productions become more effective in helping listeners recover
the intended syntactic structure?
Prosodic Assumptions
We assume the analysis of prosodic structure in American English
proposed in Beckman & Pierrehumbert (1986) (following Pierrehumbert (1980)).
Each utterance is composed of one or more intonation phrases, each of which is
made up of one or more intermediate phrases. We collectively refer to intonation
phrases and intermediate phrases as prosodic phrases. The ends of prosodic
phrases in American English carry edge tones, typically associated with changes
in fundamental frequency. They also show final lengthening--increased duration
for the final syllable of the phrase--and can be followed by a silent interval. These
durational effects tend to be more extreme for intonation phrases than for
intermediate phrases (Wightman, Shattuck-Hufnagel, Ostendorf & Price, 1992).
The edges of prosodic phrases are also associated with changes in segmental
articulation (e.g., Keating, Cho, Fougeron & Hsu, to appear), and with resetting of
the pitch range.
Experiment 1
Our production study included both phonological and acoustic phonetic
analyses of utterances such as (1) to (4), exploring the syntactic and situational
determination of the prosodic realization of PP ambiguities by naïve speakers in
our game task.
(1)
I want to change the position of the square with the triangle.

Running title: Prosodic influences in a conversation task

5

(2)
I am able to confirm the move of the square with the triangle.
(3)
I want to change the position of the square with the cylinder.
(4)
I am able to confirm the move of the square with the cylinder.
We conducted our experiments with the following hypotheses in mind.
1. Syntactic determination. We predicted a difference in the realizations of
high (VP) and low (NP) attachments of the PP. The high attachment was
predicted to be reflected in a stronger prosodic boundary before the PP than found
in the low attachment sentences.
2. Illocutionary force. In our game task, one speaker (the Driver) issued
instructions, such as (1), while another (the Slider) followed these instructions and
confirmed that moves had taken place, using utterances such as (2).
Disambiguation was potentially more crucial in Driver utterances, since the
incorrect move could otherwise have been chosen. If prosodic realization is
sensitive to such pragmatic factors, disambiguation should be greater for Driver
than for Slider utterances.
3. Level of situational ambiguity 1: gamepiece contrast. Our game
included square with the cylinder sequences, in which the only interpretation in
the context of the game was that of a high attachment, since there was no
combined square+cylinder piece. Situational sensitivity predicts that the features
that indicate high attachment would not be as clearly marked in the cylinder
utterances as in the triangle utterances.
4. Level of situational ambiguity 2: gameboard configuration. In the game
there were configurations of the pieces on the board which resulted in the Driver's
use of (1) being truly ambiguous, biased toward one interpretation or the other, or
unambiguous, as defined below. If speakers are sensitive to situational
constraints, then we should expect greater disambiguation for ambiguous
situations than for biased or unambiguous ones.
Procedure
In our cooperative game task two players used scripted sentences to
negotiate moves of gamepieces from starting positions to goals. By observing
gamepiece moves, the experimenter was able to identify each PP utterance as an
intended high or low attachment utterance. Neither player could see the board
used by the other, although they knew they had identical gamepieces. The design
of the boards and the rules of the game encouraged negotiation and the strategic
use of moves. The Driver's role was to tell the Slider which piece to move, to
inform the Slider when he or she had moved incorrectly, and to indicate when a
gamepiece had reached its goal. The Slider's role was to choose directions to
move in and to report moves back to the Driver, but the Slider was also required
to ask the Driver for more information when necessary. Players were restricted to
uttering sentences from a provided list, but chose freely from this list to best

Schafer, Speer, & Warren

6

match their communicative needs. Through repeated use of the sentences over the
course of the experiment, players became increasingly familiar with the sentence
forms and less dependent on reading processes. Further information about the
methodology is provided in Warren et al. (2000).
Situational ambiguity levels for gameboard configurations
We defined three levels of situational ambiguity for the gameboard
configurations. Ambiguous – Disregarding prosody and any underlying syntactic
or lexical preferences, sentence (1) could with equal likelihood be interpreted with
high or low attachment. Unambiguous - The global ambiguity could refer to only
one legal move. For example, the square was in its goal and no triangle could be
used to move it out. Biased - Both interpretations of the utterance were possible,
but one was more likely. For example, the players had just moved a triangle next
to a square, so that using the triangle to move the square would be an obvious
next move.
Subjects
Eight pairs of subjects, all native speakers of American English naïve to
the purposes of the experiment, were recorded at the University of Kansas. All
subject pairs played as many games as they could within two hours, using
multiple boards, and exchanging Driver/Slider roles between games. Subjects
wore head-mounted microphones, and their utterances were recorded. Further
details of the participants, excluded participants, and excluded productions appear
in Warren et al. (2000).
Transcription Methods
All PP sentences were excised from the game context, placed in separate
audio files, and assigned coded filenames that masked the speaker's intended
syntactic structure. The prosody was transcribed by a team of five transcribers,
trained to use the English ToBI (Tones and Break Indices) transcription system
(Beckman & Ayers, 1997). All were native speakers of English. Each transcriber
analyzed a subset of the utterances, using auditory information and visual
inspection of waveform displays, F0 tracks, and if desired, spectrograms.
Reliability across transcribers was determined on the basis of a subset on which
all five overlapped, using the reliability metric of Pitrelli, Beckman & Hirschberg
(1994). There was at least 94% agreement on the presence of pitch accents, phrase
accents (indicating an intermediate phrase boundary), and boundary tones
(indicating an intonational phrase boundary).
Results
As mentioned above, this chapter focuses on the effect of the gameboard
configuration manipulation. Therefore, in this section we report only the results

Running title: Prosodic influences in a conversation task

7

for Driver utterances containing the phrase the square with the triangle, returning
to the Driver versus Slider comparison and triangle versus cylinder comparison in
the general discussion.
Transcription results. There was substantial variation in the intonational
and durational patterns that were produced for the sequence the position of the
square in sentence (1), both within and across speakers. In data from 13 speakers,
we found 63 distinct patterns on 79 high-attached utterances, and 87 distinct
patterns on 101 low-attached utterances. This indicates that the exact prosodic
form cannot be predicted solely on the basis of morphosyntactic structure.
We assigned the transcribed utterances to three groups to evaluate the
relationships among syntactic structure, situational ambiguity, and prosodic
disambiguation. The first group contained all utterances with a stronger prosodic
boundary at the end of square (i.e., immediately prior to the PP) than at any other
location in the sentence. Boundary strength was determined by the phonological
category of the boundary (i.e., word, intermediate phrase, or intonation phrase
boundary). The second group had been pronounced with the strongest boundary at
a location other than at the end of square. The third group contained utterances in
which the boundary at the end of square and at least one other boundary were of
equal strength, and these were the strongest boundaries in the sentence.
Previous production results have shown longer duration for the prosodic
boundary preceding high PP attachments than low ones (e.g., Warren, 1985). In
the comprehension domain, Schafer (1997) and Carlson et al. (2001) have argued
that prosodic disambiguation is influenced by the pattern of prosodic boundary
strengths in the preceding material. Both proposals predict that pronunciations of
(1) should be biased toward high attachment when the strongest prosodic
boundary in the sentence is located at the end of square.2, 3
Our transcription results suggest that the pattern of relative boundary
strengths was strongly influenced by the intended syntactic structure. The
strongest boundary followed square for 57% of the high attachment utterances,
versus 7% for low attachment. There is a potential concern that the "low-attached"
utterances could have been produced with lexicalization of the phrase the square
with the triangle.4 The prosodic evidence concerning lexicalization is complex
(Liberman & Sproat, 1992) and beyond the scope of this chapter. However, the
existence of lexicalized utterances would not affect the hypotheses for the highattached sentences, which are our focus for the assessment of effects of situational
ambiguity on prosody.5
The distribution of transcription patterns for high-attached tokens by level
of gameboard ambiguity is given in Table 1. Similar percentages of tokens were
pronounced with the strongest boundary following square in ambiguous, biased,
and unambiguous game situations, with the highest percentage in the

Schafer, Speer, & Warren

8

unambiguous situation. The results indicate that speakers pronounced the PP
sentence with a variety of prosodic structures, which ranged across prosodies
expected to be more and less indicative of the syntactic structure. A substantial
portion of the variability in boundary strength patterns can be explained by the
intended syntax, but none of it seems to be explained by the level of situational
ambiguity.6
-- insert Table 1 about here -Duration results. The transcription patterns in Table 1 do not exclude the
possibility of significant effects of situational ambiguity on prosodic
disambiguation, since matching phonological structures may have systematically
differing phonetic realizations. For example, the silent interval of an intonation
phrase boundary in a critical position could be reliably longer in utterances
produced in ambiguous situations than in unambiguous ones. Using digitized
speech waveforms, we compared the durations of the word square, of any
following pause, and of the combined square + pause sequence. Each was
significantly longer for high-attached versions of (1) (Warren et al., 2000),
providing clear support for the prediction that, in general, speakers would reflect
the intended interpretation of the PP sentences in their prosody.
To examine whether the syntactic effect on prosody was modulated by
situational ambiguity we looked at durational data in the three ambiguity levels
described above. The overall mean durations of square + pause for these
ambiguity levels for each of the high and low attachment conditions for 13
speakers in the Driver role are shown in Figure 1.
-- insert Figure 1 about here -The variable number of tokens making up these data (see Figure 1) made
the comparison of overall means rather unreliable. In particular, the breakdown by
ambiguity level left some speakers with very small or empty cells for some
conditions. Therefore, we restricted our statistical analysis to those speakers with
at least one instance in each ambiguity x attachment condition. The resulting
ANOVAs were consequently for high attachment data only, from just 11 of our
speakers. They showed no effect of ambiguity level on the duration of square, of
the following pause, or of square + pause (Warren et al., 2000).
Thus, the duration results, like the transcription results, show that speakers
in our task marked the syntactic difference between high and low attachments of
PPs with some consistency. Yet the lengthening of the word and pause before the
PP in high attachments, compared with low attachments, did not depend on the
level of situational ambiguity.

Running title: Prosodic influences in a conversation task

9

Experiment 2
In the second experiment, game task materials collected in the production
study were presented to listeners in a categorization task in order to determine
whether the prosodic patterns identified in the production study would be useful
to listeners faced with interpreting the utterances. High and low attachment tokens
of sentence (1) were presented to listeners as complete sentences in a forcedchoice task in which they selected between paraphrases indicating high versus
low attachment. Nineteen native speakers of Midwestern American English from
the University of Kansas took part in this experiment. None of them had
previously taken part in the production experiment described above.
Hypotheses
If speakers produce prosodic structures that reflect syntactic structure, and
that are useful to listeners, then percentages of correct classification in the
comprehension experiment should be above chance for both high and low
attachment sentences.
Further, if speakers increase prosodic disambiguation to reflect situational
need, then correct categorization should be higher for tokens produced in the
ambiguous condition than in the biased condition, and higher for tokens produced
in the biased condition than in the unambiguous condition. Note that this would
imply the use of further prosodic cues to disambiguation than just the boundary
strength and durational differences measured in Experiment 1, which did not
reliably distinguish levels of ambiguity.
Results
The percentages of correct classifications are given in Figure 2. The
overall classification was greater than chance, showing that listeners were able to
make use of distinctions that reflect syntactic structure. The percent correct scores
for each condition and for each individual participant were subjected to an arcsine
transformation, (2arcsine÷p), and entered into an ANOVA with attachment and
ambiguity level as factors. This revealed a significant main effect of attachment
type (F[1,18] = 5.80, p<0.027), with more correct classifications for high than for
low attachments (76% vs. 64% overall). This main effect may reflect a slight
overall bias towards high attachments of the PP.
-- insert Figure 2 about here -There was also a significant interaction of attachment type and ambiguity
level (F[2,36] = 5.133, p<0.011), reflecting the fact that there was no effect of
ambiguity for the high attachment condition, but a significant effect for the low

Schafer, Speer, & Warren

10

attachment condition. This latter effect resulted from the lower correct score for
ambiguous than for biased or unambiguous items. That is, it was low attachments
produced in the ambiguous situation that showed the least evidence of prosodic
disambiguation.7 The absence of a main effect of ambiguity level fails to support
the hypothesis that speakers produce different degrees of prosodic disambiguation
according to differences in situational ambiguity. It supports the conclusion from
the production experiment that our speakers tended to disambiguate the PP
structure, and they did so regardless of the ambiguity of the situation.
Production Sequence Analysis
Given the extent to which interacting speakers can alter their productions
over the course of some tasks (e.g., Clark & Schober, 1992), we might expect that
speakers in the game task would have changed their use of prosody across the
experiment. They presumably became more aware of the contrast between high
and low PP attachments as play continued, especially since the design of the
games elicited the first production of each attachment in an unambiguous
configuration. They also received evidence, directly after each PP production, of
whether their conversation partner had interpreted the sentence correctly or not.
Each of these factors might result in a tendency toward stronger disambiguation at
the end of the task than at the beginning. Therefore, we reanalyzed the listener
categorization results to examine whether categorization improved across the
production sequence of Experiment 1. Since each speaker produced at least 5
utterances for each attachment, the percentage of correct categorizations was
determined for the first through fourth and last utterance for each of the
attachment sequences.
The results are shown in Figure 3. ANOVAs revealed only a marginal
effect of utterance sequence on categorization (nor were there any systematic
effects with a breakdown into ambiguity classes). We wish to emphasize that the
lack of a significant effect cannot be attributed to a high degree of consistency
within each speaker's prosodic productions. Each speaker produced utterances
that received high percentages of correct categorizations and ones that received
low percentages. The average difference across speakers between the utterance
with the highest percentage of correct categorization and that with the lowest was
35% for high attachments and 49% for low attachments. As with the other results,
there was considerable variation within each speaker, but this variation does not
appear to be explained by situational ambiguity, as determined by either the
gameboard configuration or a presumed rising awareness of the PP contrast across
the course of the game.
-- insert Figure 3 about here --

Running title: Prosodic influences in a conversation task

11

General Discussion and Conclusions
Our analyses found strong and consistent evidence that prosodic structure
reflected syntactic structure, at least in the majority of productions, but no
evidence that prosodic disambiguation was modulated by situational need.
Transcription, duration, and listener categorization results all showed syntactic
effects, but gave no indication that prosodic disambiguation increased with
situational ambiguity. Similarly, the investigation of sequence effects
demonstrated that speakers' productions at the beginning of the task were just as
biasing as those from the end of the task.
Other analyses from our game have produced similar results (Schafer,
Speer, Warren & White, 2000; Warren et al., 2000). Speakers strongly
disambiguated an early/late closure contrast in our game, which was produced
with quite limited situational ambiguity. The durational pattern for cylinder PP
sentences, which in our game received referential support for only the highattached interpretation, matched the durational pattern for high-attached triangle
PP sentences. There was no apparent reduction of disambiguation for the cylinder
sentences, even though the intended interpretation was unambiguous throughout
the game. We also found equally strong durational effects of syntax in PP
utterances by Sliders, who were confirming a move, as in utterances by Drivers,
who were introducing a move. Thus, across two syntactic ambiguities and
multiple types of analyses our results consistently show prosodic reflections of
syntactic attachment, unaffected by situational ambiguity.
These results contrast sharply with those from previously published
research on prosodic disambiguation, recent work by Snedeker and Trueswell
(2003), and other tests of situational effects on production (but see also Ferreira &
Dell, 2000). We believe there are several reasons to be cautious in generalizing
from the previous prosody results to spontaneous discourse situations. As noted in
the introduction, the previous studies either did not include a conversation partner
or allowed very limited interaction, and most relied much more heavily than our
task on reading processes. Although our task did not elicit fully spontaneous
speech, we believe that the utterances we collected are much more similar to
spontaneous speech than those in other studies.8 In addition, we believe our task
was extremely effective in clearly establishing a syntactic interpretation of the
ambiguous sentence for the speaker in a manner that did not have unintended
consequences on the prosodic structure of the utterance. The use of biasing
linguistic contexts in some of the previous work might not have always resulted in
the speaker recovering the syntactic structure intended by the experimenter. In our
study, the speaker's intended meaning was always unambiguously demonstrated
to the experimenter by an associated move of a gamepiece. Further, it is quite
likely that certain discourse contexts can induce focal structures that impact the
prosody-syntax correspondences. Schafer and Jun (2001) have demonstrated that

Schafer, Speer, & Warren

12

prosodic reflections of PP attachment in English can be affected by changes in
focal structure. We believe that such factors were minimized in our task, but may
have had significant effects in some of the previous studies.
In this chapter we have been most concerned with effects of situational
ambiguity on prosody. We looked for its effects with an experimental design that
we hoped would be quite representative of everyday speech. The levels of
situational ambiguity in the game fluctuated because of the preceding discourse
and because of actions performed on objects in the discourse context. Some of the
experimental materials received referential support for both PP interpretations
(the triangle utterances), and others received referential support for just one
interpretation throughout the game (the cylinder utterances). We believe that
discourse situations such as these should be highly informative with respect to the
relative strengths of grammatical constraints on prosodic form (such as prosodysyntax correspondences) and tendencies in speakers to alter the prosodic
disambiguation they provide in response to situational needs in non-experimental
contexts. Nevertheless, much research remains to be done in this area, and there is
a particular need to analyze the prosody found in truly spontaneous speech
produced for a range of sentence forms and a range of discourse contexts.
Although we did not find effects of situational ambiguity on prosodic
disambiguation in any of our comparisons, we did see an effect of the discourse
situation on utterance form. Speakers tended to have faster rates of speech when
playing the game in the Slider role than in the Driver role (Warren et al., 2000),
suggesting that they may have been more deliberate when they were directing the
course of action than when they were confirming it. (Recall that players switched
roles after each game.) However, as mentioned above, this difference did not
seem to affect the degree of prosodic disambiguation in the Driver versus Slider
role.
There are certainly cases—including some in research cited above—in
which speakers employ a disambiguating prosodic structure in an attempt to
indicate one interpretation over another. Speakers also make conscious and
unconscious choices to be generally clearer in certain speech situations, and may
therefore do such things as alter their rate of speech in response to the audience.
Such changes may have indirect effects on prosodic disambiguation, e.g., the
inclusion of stronger prosodic boundaries in several positions within a sentence
when it is uttered in a more deliberate style. Nevertheless, we believe that the
production of sentence prosody is primarily controlled by grammatical factors,
such as phonosyntactic constraints relating prosodic form to syntactic form,
phonological constraints governing the length or weight of prosodic units, and
semantic/pragmatic constraints relating information/discourse structure and
prosody. Under this view, most prosodic disambiguation of syntax in everyday
speech is not disambiguation per se, but the regular application of grammatical

Running title: Prosodic influences in a conversation task

constraints. In such a model, we should expect that the degree of prosodic
disambiguation found in most speech depends very little on the degree of
situational ambiguity, but very much on the grammatical structures involved, as
found in our game task.

13

Schafer, Speer, & Warren

14

References
Allbritton, D.W., McKoon, G., & Ratcliff, R. (1996). Reliability of prosodic cues
for resolving syntactic ambiguity. Journal of Experimental Psychology:
Learning, Memory, & Cognition, 22, 714-735.
Anderson, A.H., Bader, M., Boyle, E., Bard, E.G., Doherty, G., Garrod, S., Isard,
S.D., Kowtko, J., McAllister, J., Miller, J., Sotillo, C., Thompson, H. S. &
Weinert, R. (1991). The HCRC map task corpus. Language and Speech, 34,
351-366.
Ayers, G. (1994). Discourse functions of pitch range in spontaneous and read
speech. OSU Working Papers in Linguistics, 44, 1-49.
Beckman, M.E. & Ayers, G. (1997). Guidelines for ToBI labelling. Ms.
Columbus, OH: Ohio State University
Beckman, M.E. & Pierrehumbert, J.B. (1986). Intonational structure in Japanese
and English. Phonology, 3, 255-309.
Brennan, S.E. (1990). Seeking and Providing Evidence for Mutual
Understanding. Doctoral dissertation, Stanford University.
Butterworth, B. (1975). Hesitation and semantic planning in speech. Journal of
Psycholinguistic Research, 4, 57-87.
Carlson, K., Clifton, C., & Frazier, L. (2001). Prosodic boundaries in adjunct
attachment. Journal of Memory and Language, 45, 58-81.
Clark, H. & Schober, M.F. (1992). Understanding by addressees and overhearers.
In H. Clark (Ed.) Arenas of Language Use. Chicago: University of Chicago
Press. 176-197.
Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a collaborative process.
Cognition, 22, 1-39.
Cooper, W.E. & Paccia-Cooper, J. (1980). Syntax and Speech. Cambridge, MA:
Harvard University Press.
Cooper W.E., Paccia, J.M., & LaPointe, S.G. (1978). Hierarchical coding in
speech timing. Cognitive Psychology, 10, 154-177.
Cutler, A., Dahan, D., & Donselaar, W. van (1997). Prosody in the comprehension of
spoken language: A literature review. Language and Speech, 40, 141-201.
Ferreira, V.S. & Dell, G.S. (2000). The effect of ambiguity and lexical availability
on syntactic and lexical production. Cognitive Psychology, 40, 296-340.
Fowler, C. & Housum, J. (1987). Talkers' signaling of "new" and "old" words in
speech and listeners' perception and use of the distinction. Journal of Memory
and Language, 26, 489-504.
Gee, J. & Grosjean, F. (1983). Performance structures: A psycholinguistic and
linguistic appraisal. Cognitive Psychology, 15, 411-458.
Keating, P. Cho, T. Fougeron, C. & Hsu, C.-S. (to appear). Domain-initial
articulatory strengthening in four languages. In J. Local, R. Ogden & R.
Temple (Eds.) Papers in Laboratory Phonology VI. Cambridge: Cambridge

Running title: Prosodic influences in a conversation task

15

University Press.
Lehiste, I. (1973). Phonetic disambiguation of syntactic ambiguity. Glossa, 7,
103-122.
Lehiste, I., Olive, J.P., & Streeter, L.A. (1976). Role of duration in
disambiguating syntactically ambiguous sentences. Journal of the Acoustical
Society of America, 60, 1199-1202.
Levelt, W.J.M. & Cutler, A. (1983). Prosodic marking in speech repair. Journal of
Semantics, 2, 205-217.
Liberman, M. & Sproat, R. (1992). The stress and structure of modified noun
phrases in English. In Sag, I. & Szabolcsi, A. (Eds.) Lexical Matters.
Stanford: CSLI. 131 – 181.
Mazuka, R., Misono, Y. & Kondo, T. (2001). Differences in levels of
informativeness of prosodic cues to resolve syntactic ambiguity. Presented at
the Fourteenth Annual CUNY Conference on Human Sentence Processing,
University of Pennsylvania, Philadelphia, PA, March 2001.
Nespor, M.A., & Vogel, I. (1986). Prosodic Phonology. Boston, MA: Kluwer.
Pierrehumbert, J.B. (1980). The Phonology and Phonetics of English Intonation.
Doctoral dissertation, MIT.
Pitrelli J., Beckman, M., & Hirschberg, J. (1994). Evaluation of prosodic
transcription labeling reliability in the ToBI framework. Proceedings of the
International Conference on Spoken Language Processing, Yokohama,
Japan, September 1994. 123-126.
Price, P., Ostendorf, M., Shattuck-Hufnagel, S., & Fong, C. (1991). The use of prosody in
syntactic disambiguation. Journal of the Acoustical Society of America, 90, 29562970.
Pynte, J. & Prieur, B. (1996). Prosodic breaks and attachment decisions in sentence
parsing. Language and Cognitive Processes, 11, 165-192.
Schafer, A.J. (1997). Prosodic Parsing: The Role of Prosody in Sentence
Comprehension. Doctoral dissertation, University of Massachusetts.
Schafer, A.J. & Jun, S.-A. (2001). Effects of focus on prosodic reflections of
phrase structure in American English. Presented at The Prosody in Processing
Workshop, Utrecht University, Utrecht, Netherlands. July 2001.
Schafer, A.J., Speer, S.R., Warren, P., & White, S.D. (2000). Intonational
disambiguation in sentence production and comprehension. Journal of
Psycholinguistic Research, 29, 169-182.
Schober, M.F., Conrad, F.G., & Fricker, S.S. (2000). Listeners often don't
recognize when their conceptions differ from speakers'. Presented at the
Annual Meeting of the Psychonomics Society, New Orleans, LA.
Selkirk, E. O. (1984). Phonology and Syntax: The Relation between Sound and
Structure. Cambridge, MA: MIT Press.
Snedeker, J., & Trueswell, J. (2003). Using prosody to avoid ambiguity: Effects

Schafer, Speer, & Warren

16

of speaker awareness and referential context. Journal of Memory and
Language, 48, 103–130.
Straub, K.A. (1997). The Production of Prosodic Cues and their Role in the
Comprehension of Syntactically Ambiguous Sentences. Doctoral dissertation,
University of Rochester.
Warren, P. (1985). The Temporal Organisation and Perception of Speech.
Doctoral dissertation, University of Cambridge.
Warren, P. (1999). Prosody and language processing. In S. Garrod & M.
Pickering (Eds.), Language Processing. Hove: Psychology Press. 155-188.
Warren, P., Schafer, A.J., Speer, S.R., & White, S.D. (2000). Prosodic resolution
of prepositional phrase ambiguity in ambiguous and unambiguous situations.
UCLA Working Papers in Phonetics, 99, 5-33.
Wightman, C.W., Shattuck-Hufnagel, S., Ostendorf, M., & Price, P.J. (1992).
Segmental durations in the vicinity of prosodic phrase boundaries. Journal of
the Acoustical Society of America, 92, 1707-1717.

Running title: Prosodic influences in a conversation task

17

Footnotes
*
This research was supported by NIH research grants DC-00029 and MH51768, NZ/USA Cooperative Science Programme grant CSP95/01, and Marsden
Fund grant VUW604. We thank Kelly Barrow, Karen Carmody, Amanda Fisch,
Christa Hansen, Lauren Kling, Jenny Kneale, Jennifer Ludlow, Cara Prall, Lisa
Rief, Shari Sokol, Aaron Soltz, Jill Story, David White, and Gerald Whiteside for
assistance with running subjects and measuring data.
1.
Recent work by Snedeker and Trueswell (to appear) employed a task in
which the speaker uttered a series of commands involving the manipulation of a
set of toys to a listener separated by a screen. Interaction between the two
participants was limited to the speaker asking if the listener was ready. In this task
the experimental materials were presented as printed text and acted-out toy
manipulations. The textual stimulus was then removed and the command
produced by the speaker from memory.
2.
Schafer and Carlson et al. differ in their predictions about several
boundary strength patterns, such as a pattern with intonation phrase boundaries at
the end of both position and square. In Schafer's proposal, this pronunciation
would bias listeners toward high attachment; in Carlson et al.'s proposal it would
not. Since more finely graded analyses are beyond the scope of this paper, we
focus on the cases where there is consensus.
3.
The location of prosodic boundaries is likely influenced by several other
factors than the intended attachment site. For example, there is some tendency to
produce a prosodic boundary at the midpoint of an utterance (e.g., Gee &
Grosjean, 1983). Utterances with the strongest prosodic boundary at the end of
square are unlikely to be showing solely the influence of this tendency, given the
late location of the boundary.
4.
We thank Gary Dell for first mentioning this possibility to us, as well as
Jesse Snedeker, Mike Tanenhaus, and John Trueswell.
5.
In addition to being unaffected by the possibility of lexicalization, the
high-attached tokens were more evenly distributed across the three gameboard
configurations and had the widest distribution across our three boundary pattern
groups.
6.
The high-attached utterances with strong prosodic breaks located prior to
the end of square may reflect the pressure to balance the lengths of prosodic
phrases and to avoid long prosodic phrases (e.g., Gee & Grosjean, 1983; Nespor

Schafer, Speer, & Warren

18

& Vogel, 1986). Phonological factors such as these likely account for some of the
remaining variability in prosodic boundary location.
7.
It is possible that this reflects a choice by the speakers to produce more
deliberate pronunciations in the low-attachment ambiguous-situation condition.
See Warren et al. (2000) for further discussion of this possibility. We note,
though, that very few tokens were produced in this condition in Experiment 1.
Therefore, the stimuli tested in this condition in the comprehension study might
not accurately reflect the range of prosodic patterns that would be found in a
larger sample.
8.
Snedeker and Trueswell's task, like ours, produced speech that was less
dependent on reading processes than that of previous tasks. However, our task
involved greater interaction between participants than theirs, seems to have
included a greater range of syntactic structures in the discourse situation, and
required more varied interaction with the objects in the discourse situation.

Running title: Prosodic influences in a conversation task

19

Table 1. Number (and percentages) of high-attached versions of (1) for each
gameboard configuration pronounced with the strongest prosodic boundary in the
sentence located at the end of square, the ends of square and at least one other
word (two or more boundaries of equal phonological strength), or the end of some
word other than square.
Figure 1. Mean square + pause durations (with standard error bars) for high- and
low-attached triangle tokens, by situational ambiguity level. Number of tokens for
each mean are indicated.
Figure 2. Percentages of correct classifications of tokens as high- or low-attached
sentences, by level of situational ambiguity. The percentages are averages of the
values obtained for 19 subjects listening to 13 speakers. The numbers of tokens
heard in each condition are indicated.
Figure 3. Percentage of correct classifications of tokens as high- or low-attached
PP sentences, by sequence within high- or low-attached utterances in the game
discourse. The percentages are averages of the values obtained for 19 subjects
listening to 13 speakers. Sequence positions were assigned separately for the two
attachment conditions; the figure shows the average of the high- and low-attached
mean for each position.

20

Schafer, Speer, & Warren

Strongest boundary at the end of:
square
square and some other word(s)
some other word than square
Total number of tokens

Gameboard configuration
Ambiguous
Biased
14 (52%)
12 (48%)
7 (26%)
8 (32%)
6 (22%)
5 (20%)
27
25

Unambiguous
19 (70%)
4 (15%)
4 (15%)
27

1200

Ambiguous
Biased

Mean duration (ms)

1000

Unambiguous

800

600

400

200

27

25

27

4

12

85

0

High attachment

Low attachment

Ambiguous
Biased

100

Percentage correct

Unambiguous
80

60

40

20

27

25

27

4

12

85

0

High attachment

Low attachment

100
90

Percentage correct

80
75

72

70

69

68

67

60
50
40
30
20
10
0

1

2

3

4

Sequence in game discourse

Last

SEGMENTATION OF CONTINUOUS SPEECH USING ACOUSTIC-PHONETIC
PARAMETERS AND STATISTICAL LEARNING
Amit Juneja and Carol Espy-Wilson
ECE Department, University of Maryland, College Park, MD 20742, USA
http://www.ece.umd.edu/~juneja

ABSTRACT
In this paper, we present a methodology for combining
acoustic-phonetic knowledge with statistical learning for
automatic segmentation and classification of continuous
speech. At present we focus on the recognition of broad
classes - vowel, stop, fricative, sonorant consonant and
silence. Judicious use is made of 13 knowledge-based
acoustic parameters (APs) and support vector machines
(SVMs). It has been shown earlier that SVMs perform
comparable to hidden Markov models (HMMs) for
detection of stop consonants. We achieve performance
on segmentation of continuous speech better than the
HMM based approach that uses 39 cepstrum-based
speech parameters.

1. INTRODUCTION
There is strong evidence that human speech recognition
(HSR) starts with a bottom-up analysis [1], and then
later context is integrated into the recognition process.
Present state-of-the-art automatic speech recognition
(ASR) systems are top-down [2,3]. That is, the process
starts by taking a dictionary of words and constituent
phonemes. Each entry in the dictionary is a word with
one or more sequences (pronunciations) of constituent
phonemes. Hidden Markov models (HMMs) are built for
each phone (monophone model) or triphone (triphone
models). For the purpose of recognition, the best path
through a lattice of words is found and the
corresponding sequence of words is chosen as the most
likely sequence. The front ends of ASR usually consist
of mel-frequency cepstral coefficients (MFCCs) or
perceptual linear predictive coefficients (PLPs).

statistical pattern recognition approaches primarily
because (1) acoustic-phonetic approaches have used hard
coded decision rules that are not easy to adapt and (2)
mapping of phonemes to sentences is a difficult task. On
the other hand, since an acoustic-phonetic approach to
recognition involves the explicit extraction of linguistic
information that is combined for recognition, it is
relatively straightforward to pinpoint the cause of
recognition errors. This diagnosis is typically difficult in
HMM-based systems where it is hard to determine if
errors are due to failure of the pattern matcher or illrepresented speech information.
Our goal in this paper is to develop a system that
combines the strengths of an acoustic-phonetic approach
and statistical pattern matching. In particular, we have
developed an adaptable and modular system where it is
easy to assess the full system as well as the components
for errors.
Phonetic feature theory provides a
hierarchical framework [6] and support vector machines
(SVMs) provide the methodology for combining the
speech knowledge. The success of SVMs has been
demonstrated for the problem of detection of stop
consonants [5]. We concentrate on the intensive use of
knowledge-based parameters with SVMs for automatic
segmentation of speech.

2. DATABASE
The TIMIT database [4] was used as a corpus of labeled
speech data. Phonetically rich ‘sx’ and ‘si’ sentences
from all the eight dialect regions in the training set were
used for training and development, and the ‘si’ sentences
from all the dialect regions in the test set (spoken by an
independent set of speakers) were used for testing.

3. SUPPORT VECTOR MACHINES
We are developing an acoustic-phonetic approach to
speech recognition in which speech is first segmented
into broad classes (vowel, stop, fricative, sonorant
consonant and silence). These manner based segments
are then analyzed for place of articulation to decide upon
the constituent phonemes. Acoustic-phonetic approaches
are bottom-up, but they have been overpowered by

SVM [15,16] is a statistical learning method for
regression and pattern classification. While learning
from data, SVM performs structural risk minimization
(SRM) unlike the classical adaptation methods that
minimize training error in a specific norm. For the twoclass pattern classification problem, SVM finds a

decision hypersurface d ( x) , where the vector x
belongs to the space of samples, of the following form

Manner
Vowel

l

d ( x) = ∑ yiα i K ( x, xi )
i =1

The support vectors (SVs) {xi }i =1 , and the weights α i
are found by using quadratic optimization methods and
the training data. yi are the class labels of the support
vectors that take the value +1 or –1 depending upon the
class. The kernel K ( x , xi ) is a function of the dot
l

product of the vectors x and xi . The kernel depends on
the type of the hypersurface d ( x) . The kernels used in
this project are shown in Table 1 along with the type of
the hypersurface. For a test vector x , the class is
determined by the sign of d ( x) . The experiments in
this project were carried using the SVM Light toolkit
[8], which provides very fast training of SVMs.
Hypersurface
type
Linear

Kernel

Polynomial of
degree d
Gaussian
Radial
base
function (RBF)

( x.xi + 1)d

d

exp(-γ | x - xi |2)

γ

( x.xi + 1)

Kernel specific
parameter
None

Table 1 : SVM kernels and their corresponding kernelspecific parameters

4. METHOD
Our event-based speech recognition system (EBS) has
four modules – an acoustic-phonetic knowledge based
parameter extraction front-end, a statistical learning
module, multi-class decision module and a language
modeling module. In this paper, we concentrate on the
first three modules for the task of segmentation of
speech into five broad classes (Figure 1). The front end
generates 13 APs that are acoustic correlates of the
manner phonetic features [10] – syllabic, sonorant,
noncontinuant, obstruent and silence. Using these
acoustic correlates, speech is segmented into the broad
classes mentioned before. The different phonemes of
English that lie in each of the manner classes are shown
in Table 2. This classification of phonemes is not strict
since the surface realization can be significantly
different from its canonical form due to coarticulatory
effects and weakening processes (lenition).

Vowel followed by
sonorant consonant
Sonorant consonant
Fricative
Stop
followed
fricative
Stop

by

Phoneme
ih, eh, ae, aa, ah, ao, uh,
ah, ax, ih, ax, axr-h, en,
em, eng, el, er
iy, ey, ay, ow, oy, aw
w, l, r, y, m, n, ng, nx, dx,
hv
s, sh, f, th, hh, z, zh, v, dh
jh, ch
b, d, g , p, t, k

Table 2: Broad manner classification of English
phonemes
Speech is analyzed every 5ms with a 10ms Hanning
window (5ms overlap). Knowledge-based parameters are
extracted from both the time waveform and the spectrum
of the signal. A classifier is built for each of the classes –
vowel, sonorant consonant, fricative, stop and silence. In
practice, the classifier for sonorancy is used in place of
classifier of vowel because vowels and sonorant
consonants are both sonorants and they are distinguished
by the classifier for sonorant consonants. Each classifier
operates on a frame of speech and takes the acoustic
parameters for that frame and in some cases, a particular
number of adjoining frames. Not all acoustic parameters
are used by each classifier. The parameters for each
classifier are chosen on the basis of knowledge. The
output of each classifier is mapped to a probability
measure, that is, the a posteriori probability of the
manner class. A very crude form of this mapping is used
in the current set of experiments. The SVM outputs are
clipped in the range [-1,1], then scaled and translated to
the range [0,1].
At each frame, the probability outputs of the classifiers
are compared and the maximum is chosen. A speech
segment is then hypothesized by a region in which the
output of a single classifier remains the maximum. Note
that we have only outlined the system for broad class
segmentation. For phoneme and sentence recognition,
there will be a classifier for each of the 20 phonetic
features that are known to be sufficient to describe the
sounds in all the languages in the world [6]. We now
discuss the design and the parameter selection of the
classifiers.
4.1 APs
Table 3 shows the APs used for the detection of each
manner class. Except for stop detection, parameters only
from the current analysis frame are fed to the SVM.
Stops are characterized by a period of silence (closure)
followed by a sudden release in energy (onset) and then

Manner Class
Sonorant

Speech Utterance

(1)

Extraction of APs

Parameter Selection module.
Pass only relevant parameters to each classifier

Stop
Fricative

(2)
SVM
1

SVM
2

SVM
3

SVM
4

SVM
5

Sonorant
consonant
Silence

(3)
Convert SVM outputs to probabilities and find
the maximum probability at each instant.

Figure 1 : The three modules of EBS – (1) front end, (2)
pattern recognition and (3) multi-class decision. SVM 1
: vowel detection, SVM 2 : sonorant consonant
detection, SVM 3 : fricative detection, SVM 4 : stop
detection, SVM 5 : silence detection
a sudden fall in energy (offset). Therefore, for detection
of stops information not only from one frame but
adjoining frames is required. Especially, there is about
30ms of silence before stop bursts [11], so we use 6
frames preceding the analysis frame and 3 frames
following the analysis frame for the detection of stop
burst.
4.2 SVM kernel selection
We trained three different SVMs – linear, polynomial
and RBF – for the detection of each manner class.
Sonorant frames were trained against all non-sonorant
frames including frication, silence, and stops. 30,000
frames of speech were selected for each class randomly
from the TIMIT training data, from both male and
female utterances. The Xi-Alpha estimates [8,9] of the
error bound provided by the learning process and the
number of support vectors for each machine is shown in
Table 4.
We choose RBF kernel with γ = 0.01 for speech
segmentation experiments because of lowest error bound
estimate of 10.86%. Similar analysis was carried out for
other manner classes. The choice of SVM kernel and
error bound estimate for each class is shown in Table 5.

Parameters
(1) Probability of voicing [7], (2)
ZCR, (3) ratio of spectral peak in
[0,400] to the spectral peak in
[400, SF/2], (4) ZCR of high pass
filtered speech, (5) Ratio of E[0,
F3-1000] to E[F3-1000, SF/2], (6)
E[100,400]
(1) Energy onset (2) Energy
offset (3) E[0,F3] (4) E[F3, SF/2]
Same as sonorant parameters, and
E[F3, SF/2]
(1) E[640, 2800], (2)
E[2000,3000]
(1) E [0,F3], (2) E[F3,SF/2], (3)
ratio of spectral peak in [0,400] to
the spectral peak in [400, SF/2]

Table 3 : APs used for detection for each manner class.
ZCR : zero crossing rate, SF : sampling frequency, F3 :
third formant of the speaker, E[a, b] denotes energy in
the frequency band [aHz, bHz].
Kernel

Linear
Polynomial
Polynomial
RBF
RBF
RBF

Kernelspecific
Parameter
d=2
d=3
γ = 0.05
γ = 0.01
γ = 0.005

Number of
SVs
9874
10133
9727
27474
13902
10458

Error
estimate
(%)
16.35
16.66
16.10
16.79
10.86
11.47

Table 4 : Training record of sonorancy SVM. Not all
values of γ are shown.
The error bound estimate in detection of sonorant
consonants is high because boundaries between vowels
and sonorant consonants are not well defined and there
is a lot of overlap in the training data. This does not
harm so much because even if only the central regions of
the sonorants consonants are detected that would suffice
for the purpose of phoneme recognition. This may,
though, cause insertions of sonorant consonants in the
vowel regions with weak energies but that problem can
be solved by using temporal parameters for sonorant
consonants [14].
4.3 Analysis of SVM outputs
Figure 2 shows two of the parameters for sonorancy
detection – ZCR and ratio of E[0, F3-1000] to E[F31000, SF/2] – plotted against a speech spectrum. Also
shown is the output of sonorancy SVM converted to
probability estimate. Sonorant regions have low ZCR
and large designated ratio of energies. High values of

Manner
Class

Kernel

KernelParameter

Sonorant
Stop
Fricative
Sonorant
consonant
Silence

RBF
RBF
RBF
Linear

ã =0.01
ã=0.001
ã = 0.008
none

Errorbound
estimate
(%)
10.86
7.41
14.31
49.70

RBF

ã = 0.001

14.92

Table 5 : SVM kernel selection for different manner
classes.
probability are obtained in the sonorant regions and low
values are obtained in the non-sonorant regions as per
expectations. Figure 2 illustrates the ease in which fault
can be found with the system. The oval region in the
spectrum is a /t/ and is not a sonorant region but as
shown by the arrow, we get a high probability of
sonorancy in that region.
This error can be easily explained by presence of low
ZCR (compared to fricatives) and high E[0,3000Hz]
which is characteristic of sonorants. That is, the problem
lies in the parameters. It can be fixed by checking if the
high energy in the low frequency band is periodic or
aperiodic [14], that is, by modulating the low frequency
energy by the periodicity in the low frequency bands. If
the speech is degraded, similar plots can be obtained to
see if it is the parameters that are not behaving in line
with their physical significance. However, if in degraded
speech, the parameters are behaving well but the
recognition is not good, outputs of different SVMs can
be plotted with the spectrogram to find which SVMs are
going wrong.
4.4 HMM experiments
HMM experiments [17] were carried out using HTK [3].
39-parameter set consisting of 12 MFCCs and energy
with their delta and acceleration coefficients were used
in the HMM broad classifier. All the manner class
models were context-independent 3-state (excluding
entry and exit states) left-to-right HMMs with diagonal
covariance matrices and 8-mixture observation densities
for each state. A skip transition was allowed from the
first state to the third state in each model.

5. RESULTS AND DISCUSSION
A manner class segmentation system may not separate
out two consecutive phonemes having the same manner
representation. Therefore, for the purpose of scoring, the
reference phoneme labels from the TIMIT database were
mapped to manner class labels with the mappings listed

in Table 1, and the consecutive identical manner labels
were collapsed into one. The resulting manner class
labels were used as the reference labels for scoring EBS
as well as the HMM broad classifier. EBS with 13 APs
showed performance better than HMMs with 39
cepstrum-based parameters. The results are shown in
Table 6.

Parameters
Number of parameters
% Correct
% Accuracy

HMM
MFCCs
39
69.6
64.9

EBS
APs
13
82.4
68.3

Table 6 : Results of broad classification
The wide gap in the correctness and accuracy of EBS is
because of a higher number of insertions, primarily, of
sonorant consonants and stops. Stop insertions normally
occur at the onset of vowels and strong fricatives
following a period of silence. Sonorant consonant
insertions occur at the weak beginning and end of
vowels. These insertions may be corrected by using
temporal parameters [14] as well as designing more
discriminative parameters.

6. CONCLUSION AND FUTURE WORK
We have seen that statistical learning can be applied
successfully with the knowledge of acoustic-phonetics
for segmentation of speech with performance
comparable to HMM systems. The recognition method
makes it easy to find the source of error in the system.
The system can be easily retrained for any new set of
parameters or for recognition of other languages. The
work will be extended to complete phoneme recognition
in the future. Neural networks that perform equally well
may replace SVMs where the number of support vectors
is too high for real-time operation of EBS. Better
methods of conversion of SVM outputs to probabilities
[13] will be applied. These parameters will be used and
tested with EBS in noise robust conditions. At present
the learning in EBS is supervised, that is, the system
requires time-aligned labeled data for training. In the
future we will explore the possibility of unsupervised
learning for the system.

7. REFERENCES
[1] J. B. Allen, “From Lord Rayleigh to Shannon:
How do humans decode speech?”,
http://auditorymodels.org/jba/PAPERS/ICASSP/
[2] L. Rabiner, B. Juang, “Fundamentals of Speech
Recognition”, Prentice Hall, 1993
[3] HTK documentation, http://htk.eng.cam.ac.uk/

[4] ``TIMIT Acoustic-Phonetic Continuous Speech
Corpus'', National Institute of Standards and Technology
Speech Disc 1-1.1, NTIS Order No. PB91-5050651996,
October 1990
[5] P. Niyogi, “Distinctive Feature Detection Using
Support Vector Machines'', pp 425-428, ICASSP 1998.
[6] M. Halle and G. N. Clements, “Problem Book in
Phonology”, Cambridge, MA, MIT Press, 1983.
[7] ESPS (Entropic Signal Processing System 5.3.1),
Entropic Research Laboratory, http://www.entropic.com
[8] T. Joachims, “Making large-Scale SVM Learning
Practical”, LS8-Report 24, Universität Dortmund, LS
VIII-Report, 1998.
[9] T. Joachims, “Estimating the Generalization
Performance of a SVM Efficiently”, Proceedings of the
International Conference on Machine Learning, Morgan
Kaufman, 2000.
[10] N. Bitar, “Acoustic Analysis and Modelling of
Speech Based on Phonetic Features”, PhD thesis, Boston
University, 1997

[11] K. Stevens, “Acoustic-Phonetics”, MIT Press,
ISBN: 026219404X, 1999
[12] T. Briscoe, “Lexical access in connected speech
recognition”, P98-1011, Computational Linguistics,
Association for Computational Linguistics, 1989.
[13] .T. Kwok, “Moderating the outputs of support
vector machine classiers”. IEEE Transactions on Neural
Networks, 10:1018-1031, 1999.
[14] A. Saloman, and C. Espy-Wilson, “Automatic
Detection of Manner Events for a Knowledge-Based
Speech Signal Representation”, Proc.of Eurospeech,
Sept. 1999, Budapest Hungary, pp. 2797-2800.
[15] V. Vapnik, “The Nature of Statistical Learning
Theory”, SpringerVerlag, 1995.
[16] V. Kecman, “Learning and Soft Computing –
Support Vector Machines, Neural Networks and Fuzzy
Logic models”, The MIT Press, Cambridge, MA, 2001
[17] HMM experiments carried out at Speech
Communication Lab by Om Deshmukh,
http://www.ece.umd.edu/iconip2002.html

(a)

(b)

(c)

(d)

(e)

Figure 2 : Spectrogram of the utterance “Iguanas and alligators are tropical reptiles”. (a) Spectrogram, (b)
SVM a posteriori probability of sonorancy, (c) Ratio of E[0,F3-1000] to E[F3-1000,SF/2], (d) Zero crossing
rate (e) Phoneme labels from TIMIT database. The phoneme in the oval region is /t/

SEGMENTATION OF CONTINUOUS SPEECH USING ACOUSTIC-PHONETIC
PARAMETERS AND STATISTICAL LEARNING
Amit Juneja and Carol Espy-Wilson
ECE Department, University of Maryland, College Park, MD 20742, USA
http://www.ece.umd.edu/~juneja

ABSTRACT
In this paper, we present a methodology for combining
acoustic-phonetic knowledge with statistical learning for
automatic segmentation and classification of continuous
speech. At present we focus on the recognition of broad
classes - vowel, stop, fricative, sonorant consonant and
silence. Judicious use is made of 13 knowledge-based
acoustic parameters (APs) and support vector machines
(SVMs). It has been shown earlier that SVMs perform
comparable to hidden Markov models (HMMs) for
detection of stop consonants. We achieve performance
on segmentation of continuous speech better than the
HMM based approach that uses 39 cepstrum-based
speech parameters.

1. INTRODUCTION
There is strong evidence that human speech recognition
(HSR) starts with a bottom-up analysis [1], and then
later context is integrated into the recognition process.
Present state-of-the-art automatic speech recognition
(ASR) systems are top-down [2,3]. That is, the process
starts by taking a dictionary of words and constituent
phonemes. Each entry in the dictionary is a word with
one or more sequences (pronunciations) of constituent
phonemes. Hidden Markov models (HMMs) are built for
each phone (monophone model) or triphone (triphone
models). For the purpose of recognition, the best path
through a lattice of words is found and the
corresponding sequence of words is chosen as the most
likely sequence. The front ends of ASR usually consist
of mel-frequency cepstral coefficients (MFCCs) or
perceptual linear predictive coefficients (PLPs).

statistical pattern recognition approaches primarily
because (1) acoustic-phonetic approaches have used hard
coded decision rules that are not easy to adapt and (2)
mapping of phonemes to sentences is a difficult task. On
the other hand, since an acoustic-phonetic approach to
recognition involves the explicit extraction of linguistic
information that is combined for recognition, it is
relatively straightforward to pinpoint the cause of
recognition errors. This diagnosis is typically difficult in
HMM-based systems where it is hard to determine if
errors are due to failure of the pattern matcher or illrepresented speech information.
Our goal in this paper is to develop a system that
combines the strengths of an acoustic-phonetic approach
and statistical pattern matching. In particular, we have
developed an adaptable and modular system where it is
easy to assess the full system as well as the components
for errors.
Phonetic feature theory provides a
hierarchical framework [6] and support vector machines
(SVMs) provide the methodology for combining the
speech knowledge. The success of SVMs has been
demonstrated for the problem of detection of stop
consonants [5]. We concentrate on the intensive use of
knowledge-based parameters with SVMs for automatic
segmentation of speech.

2. DATABASE
The TIMIT database [4] was used as a corpus of labeled
speech data. Phonetically rich ‘sx’ and ‘si’ sentences
from all the eight dialect regions in the training set were
used for training and development, and the ‘si’ sentences
from all the dialect regions in the test set (spoken by an
independent set of speakers) were used for testing.

3. SUPPORT VECTOR MACHINES
We are developing an acoustic-phonetic approach to
speech recognition in which speech is first segmented
into broad classes (vowel, stop, fricative, sonorant
consonant and silence). These manner based segments
are then analyzed for place of articulation to decide upon
the constituent phonemes. Acoustic-phonetic approaches
are bottom-up, but they have been overpowered by

SVM [15,16] is a statistical learning method for
regression and pattern classification. While learning
from data, SVM performs structural risk minimization
(SRM) unlike the classical adaptation methods that
minimize training error in a specific norm. For the twoclass pattern classification problem, SVM finds a

decision hypersurface d ( x) , where the vector x
belongs to the space of samples, of the following form

Manner
Vowel

l

d ( x) = ∑ yiα i K ( x, xi )
i =1

The support vectors (SVs) {xi }i =1 , and the weights α i
are found by using quadratic optimization methods and
the training data. yi are the class labels of the support
vectors that take the value +1 or –1 depending upon the
class. The kernel K ( x , xi ) is a function of the dot
l

product of the vectors x and xi . The kernel depends on
the type of the hypersurface d ( x) . The kernels used in
this project are shown in Table 1 along with the type of
the hypersurface. For a test vector x , the class is
determined by the sign of d ( x) . The experiments in
this project were carried using the SVM Light toolkit
[8], which provides very fast training of SVMs.
Hypersurface
type
Linear

Kernel

Polynomial of
degree d
Gaussian
Radial
base
function (RBF)

( x.xi + 1)d

d

exp(-γ | x - xi |2)

γ

( x.xi + 1)

Kernel specific
parameter
None

Table 1 : SVM kernels and their corresponding kernelspecific parameters

4. METHOD
Our event-based speech recognition system (EBS) has
four modules – an acoustic-phonetic knowledge based
parameter extraction front-end, a statistical learning
module, multi-class decision module and a language
modeling module. In this paper, we concentrate on the
first three modules for the task of segmentation of
speech into five broad classes (Figure 1). The front end
generates 13 APs that are acoustic correlates of the
manner phonetic features [10] – syllabic, sonorant,
noncontinuant, obstruent and silence. Using these
acoustic correlates, speech is segmented into the broad
classes mentioned before. The different phonemes of
English that lie in each of the manner classes are shown
in Table 2. This classification of phonemes is not strict
since the surface realization can be significantly
different from its canonical form due to coarticulatory
effects and weakening processes (lenition).

Vowel followed by
sonorant consonant
Sonorant consonant
Fricative
Stop
followed
fricative
Stop

by

Phoneme
ih, eh, ae, aa, ah, ao, uh,
ah, ax, ih, ax, axr-h, en,
em, eng, el, er
iy, ey, ay, ow, oy, aw
w, l, r, y, m, n, ng, nx, dx,
hv
s, sh, f, th, hh, z, zh, v, dh
jh, ch
b, d, g , p, t, k

Table 2: Broad manner classification of English
phonemes
Speech is analyzed every 5ms with a 10ms Hanning
window (5ms overlap). Knowledge-based parameters are
extracted from both the time waveform and the spectrum
of the signal. A classifier is built for each of the classes –
vowel, sonorant consonant, fricative, stop and silence. In
practice, the classifier for sonorancy is used in place of
classifier of vowel because vowels and sonorant
consonants are both sonorants and they are distinguished
by the classifier for sonorant consonants. Each classifier
operates on a frame of speech and takes the acoustic
parameters for that frame and in some cases, a particular
number of adjoining frames. Not all acoustic parameters
are used by each classifier. The parameters for each
classifier are chosen on the basis of knowledge. The
output of each classifier is mapped to a probability
measure, that is, the a posteriori probability of the
manner class. A very crude form of this mapping is used
in the current set of experiments. The SVM outputs are
clipped in the range [-1,1], then scaled and translated to
the range [0,1].
At each frame, the probability outputs of the classifiers
are compared and the maximum is chosen. A speech
segment is then hypothesized by a region in which the
output of a single classifier remains the maximum. Note
that we have only outlined the system for broad class
segmentation. For phoneme and sentence recognition,
there will be a classifier for each of the 20 phonetic
features that are known to be sufficient to describe the
sounds in all the languages in the world [6]. We now
discuss the design and the parameter selection of the
classifiers.
4.1 APs
Table 3 shows the APs used for the detection of each
manner class. Except for stop detection, parameters only
from the current analysis frame are fed to the SVM.
Stops are characterized by a period of silence (closure)
followed by a sudden release in energy (onset) and then

Manner Class
Sonorant

Speech Utterance

(1)

Extraction of APs

Parameter Selection module.
Pass only relevant parameters to each classifier

Stop
Fricative

(2)
SVM
1

SVM
2

SVM
3

SVM
4

SVM
5

Sonorant
consonant
Silence

(3)
Convert SVM outputs to probabilities and find
the maximum probability at each instant.

Figure 1 : The three modules of EBS – (1) front end, (2)
pattern recognition and (3) multi-class decision. SVM 1
: vowel detection, SVM 2 : sonorant consonant
detection, SVM 3 : fricative detection, SVM 4 : stop
detection, SVM 5 : silence detection
a sudden fall in energy (offset). Therefore, for detection
of stops information not only from one frame but
adjoining frames is required. Especially, there is about
30ms of silence before stop bursts [11], so we use 6
frames preceding the analysis frame and 3 frames
following the analysis frame for the detection of stop
burst.
4.2 SVM kernel selection
We trained three different SVMs – linear, polynomial
and RBF – for the detection of each manner class.
Sonorant frames were trained against all non-sonorant
frames including frication, silence, and stops. 30,000
frames of speech were selected for each class randomly
from the TIMIT training data, from both male and
female utterances. The Xi-Alpha estimates [8,9] of the
error bound provided by the learning process and the
number of support vectors for each machine is shown in
Table 4.
We choose RBF kernel with γ = 0.01 for speech
segmentation experiments because of lowest error bound
estimate of 10.86%. Similar analysis was carried out for
other manner classes. The choice of SVM kernel and
error bound estimate for each class is shown in Table 5.

Parameters
(1) Probability of voicing [7], (2)
ZCR, (3) ratio of spectral peak in
[0,400] to the spectral peak in
[400, SF/2], (4) ZCR of high pass
filtered speech, (5) Ratio of E[0,
F3-1000] to E[F3-1000, SF/2], (6)
E[100,400]
(1) Energy onset (2) Energy
offset (3) E[0,F3] (4) E[F3, SF/2]
Same as sonorant parameters, and
E[F3, SF/2]
(1) E[640, 2800], (2)
E[2000,3000]
(1) E [0,F3], (2) E[F3,SF/2], (3)
ratio of spectral peak in [0,400] to
the spectral peak in [400, SF/2]

Table 3 : APs used for detection for each manner class.
ZCR : zero crossing rate, SF : sampling frequency, F3 :
third formant of the speaker, E[a, b] denotes energy in
the frequency band [aHz, bHz].
Kernel

Linear
Polynomial
Polynomial
RBF
RBF
RBF

Kernelspecific
Parameter
d=2
d=3
γ = 0.05
γ = 0.01
γ = 0.005

Number of
SVs
9874
10133
9727
27474
13902
10458

Error
estimate
(%)
16.35
16.66
16.10
16.79
10.86
11.47

Table 4 : Training record of sonorancy SVM. Not all
values of γ are shown.
The error bound estimate in detection of sonorant
consonants is high because boundaries between vowels
and sonorant consonants are not well defined and there
is a lot of overlap in the training data. This does not
harm so much because even if only the central regions of
the sonorants consonants are detected that would suffice
for the purpose of phoneme recognition. This may,
though, cause insertions of sonorant consonants in the
vowel regions with weak energies but that problem can
be solved by using temporal parameters for sonorant
consonants [14].
4.3 Analysis of SVM outputs
Figure 2 shows two of the parameters for sonorancy
detection – ZCR and ratio of E[0, F3-1000] to E[F31000, SF/2] – plotted against a speech spectrum. Also
shown is the output of sonorancy SVM converted to
probability estimate. Sonorant regions have low ZCR
and large designated ratio of energies. High values of

Manner
Class

Kernel

KernelParameter

Sonorant
Stop
Fricative
Sonorant
consonant
Silence

RBF
RBF
RBF
Linear

ã =0.01
ã=0.001
ã = 0.008
none

Errorbound
estimate
(%)
10.86
7.41
14.31
49.70

RBF

ã = 0.001

14.92

Table 5 : SVM kernel selection for different manner
classes.
probability are obtained in the sonorant regions and low
values are obtained in the non-sonorant regions as per
expectations. Figure 2 illustrates the ease in which fault
can be found with the system. The oval region in the
spectrum is a /t/ and is not a sonorant region but as
shown by the arrow, we get a high probability of
sonorancy in that region.
This error can be easily explained by presence of low
ZCR (compared to fricatives) and high E[0,3000Hz]
which is characteristic of sonorants. That is, the problem
lies in the parameters. It can be fixed by checking if the
high energy in the low frequency band is periodic or
aperiodic [14], that is, by modulating the low frequency
energy by the periodicity in the low frequency bands. If
the speech is degraded, similar plots can be obtained to
see if it is the parameters that are not behaving in line
with their physical significance. However, if in degraded
speech, the parameters are behaving well but the
recognition is not good, outputs of different SVMs can
be plotted with the spectrogram to find which SVMs are
going wrong.
4.4 HMM experiments
HMM experiments [17] were carried out using HTK [3].
39-parameter set consisting of 12 MFCCs and energy
with their delta and acceleration coefficients were used
in the HMM broad classifier. All the manner class
models were context-independent 3-state (excluding
entry and exit states) left-to-right HMMs with diagonal
covariance matrices and 8-mixture observation densities
for each state. A skip transition was allowed from the
first state to the third state in each model.

5. RESULTS AND DISCUSSION
A manner class segmentation system may not separate
out two consecutive phonemes having the same manner
representation. Therefore, for the purpose of scoring, the
reference phoneme labels from the TIMIT database were
mapped to manner class labels with the mappings listed

in Table 1, and the consecutive identical manner labels
were collapsed into one. The resulting manner class
labels were used as the reference labels for scoring EBS
as well as the HMM broad classifier. EBS with 13 APs
showed performance better than HMMs with 39
cepstrum-based parameters. The results are shown in
Table 6.

Parameters
Number of parameters
% Correct
% Accuracy

HMM
MFCCs
39
69.6
64.9

EBS
APs
13
82.4
68.3

Table 6 : Results of broad classification
The wide gap in the correctness and accuracy of EBS is
because of a higher number of insertions, primarily, of
sonorant consonants and stops. Stop insertions normally
occur at the onset of vowels and strong fricatives
following a period of silence. Sonorant consonant
insertions occur at the weak beginning and end of
vowels. These insertions may be corrected by using
temporal parameters [14] as well as designing more
discriminative parameters.

6. CONCLUSION AND FUTURE WORK
We have seen that statistical learning can be applied
successfully with the knowledge of acoustic-phonetics
for segmentation of speech with performance
comparable to HMM systems. The recognition method
makes it easy to find the source of error in the system.
The system can be easily retrained for any new set of
parameters or for recognition of other languages. The
work will be extended to complete phoneme recognition
in the future. Neural networks that perform equally well
may replace SVMs where the number of support vectors
is too high for real-time operation of EBS. Better
methods of conversion of SVM outputs to probabilities
[13] will be applied. These parameters will be used and
tested with EBS in noise robust conditions. At present
the learning in EBS is supervised, that is, the system
requires time-aligned labeled data for training. In the
future we will explore the possibility of unsupervised
learning for the system.

7. REFERENCES
[1] J. B. Allen, “From Lord Rayleigh to Shannon:
How do humans decode speech?”,
http://auditorymodels.org/jba/PAPERS/ICASSP/
[2] L. Rabiner, B. Juang, “Fundamentals of Speech
Recognition”, Prentice Hall, 1993
[3] HTK documentation, http://htk.eng.cam.ac.uk/

[4] ``TIMIT Acoustic-Phonetic Continuous Speech
Corpus'', National Institute of Standards and Technology
Speech Disc 1-1.1, NTIS Order No. PB91-5050651996,
October 1990
[5] P. Niyogi, “Distinctive Feature Detection Using
Support Vector Machines'', pp 425-428, ICASSP 1998.
[6] M. Halle and G. N. Clements, “Problem Book in
Phonology”, Cambridge, MA, MIT Press, 1983.
[7] ESPS (Entropic Signal Processing System 5.3.1),
Entropic Research Laboratory, http://www.entropic.com
[8] T. Joachims, “Making large-Scale SVM Learning
Practical”, LS8-Report 24, Universität Dortmund, LS
VIII-Report, 1998.
[9] T. Joachims, “Estimating the Generalization
Performance of a SVM Efficiently”, Proceedings of the
International Conference on Machine Learning, Morgan
Kaufman, 2000.
[10] N. Bitar, “Acoustic Analysis and Modelling of
Speech Based on Phonetic Features”, PhD thesis, Boston
University, 1997

[11] K. Stevens, “Acoustic-Phonetics”, MIT Press,
ISBN: 026219404X, 1999
[12] T. Briscoe, “Lexical access in connected speech
recognition”, P98-1011, Computational Linguistics,
Association for Computational Linguistics, 1989.
[13] .T. Kwok, “Moderating the outputs of support
vector machine classiers”. IEEE Transactions on Neural
Networks, 10:1018-1031, 1999.
[14] A. Saloman, and C. Espy-Wilson, “Automatic
Detection of Manner Events for a Knowledge-Based
Speech Signal Representation”, Proc.of Eurospeech,
Sept. 1999, Budapest Hungary, pp. 2797-2800.
[15] V. Vapnik, “The Nature of Statistical Learning
Theory”, SpringerVerlag, 1995.
[16] V. Kecman, “Learning and Soft Computing –
Support Vector Machines, Neural Networks and Fuzzy
Logic models”, The MIT Press, Cambridge, MA, 2001
[17] HMM experiments carried out at Speech
Communication Lab by Om Deshmukh,
http://www.ece.umd.edu/iconip2002.html

(a)

(b)

(c)

(d)

(e)

Figure 2 : Spectrogram of the utterance “Iguanas and alligators are tropical reptiles”. (a) Spectrogram, (b)
SVM a posteriori probability of sonorancy, (c) Ratio of E[0,F3-1000] to E[F3-1000,SF/2], (d) Zero crossing
rate (e) Phoneme labels from TIMIT database. The phoneme in the oval region is /t/

LangUE 2010 Proceedings

Information structure markers as prosodic affixes
Dragana Šurkalović
CASTL, University of Tromsø, Norway

Abstract
Most literature on focus and topic marking assumes that they are privative features (F, T) on
syntactic nodes. Büring (2007) for English and Yamato (2007) for Japanese introduce a third
category of Contrastive Topic (CT). Apart from syntactic movement and morpheme markers,
F, T and CT are marked by prosodic phrasing (Truckenbrodt 1999 for Chichewa) and pitch
accent and intonational contour (Ladd 1996 and Büring 2007 for English). In OT Prosodic
Phonology, constraints see these syntactic features (Truckenbrodt’s (1999) Align-F in
Chichewa, Samek-Lodovici’s (2005) Stress-Focus). These constraints are undesirable if
modularity is to be maintained, and they fail to connect specific tones or contours to different
information structure being marked. This paper reanalyzes this data utilizing the recent view
in syntax (Starke 2009 inter alia) by which all features are merged into the syntactic tree as
individual terminals. Lexical entries consist of phonological information paired with a
syntactic structure they can spell out, this being either terminal or phrasal nodes. A
suprasegmental affix pairing a H* tone with F feature is parallel to segmental affixes
marking focus and topic e.g. in Japanese (Yamato 2007) or Kîîtharaka (Abels and Muriungi
2006). Lexical entries for F and CT features in English would be </H*/, F >, < /L+H*LH%/, CT >, just as the past suffix is < /id/, Past >. The lexical entry for focus in Chichewa
spells out the F feature as a Prosodic Phrase. This allows us to capture the syntax-phonology
interface without sacrificing the idea of modularity.
1. Introduction
In recent years syntactic theory has been experiencing a proliferation of functional elements
in the syntactic structure. A number of „syntax-all-the-way-down‟ approaches have appeared
(e.g. Distributed Morphology, Nanosyntax), erasing the traditional distinction between
morphology and syntax. This paper explores the effects of this change on the syntaxphonology interface, addressing a problem for language modularity and offering the Lexicon
as the locus of communication between the two modules. The term „modularity‟ refers to the
notion that language consists of three independent modules, (morpho)syntax, phonology and
semantics. This model originated in Chomsky (1965) and has been the basis for generative
theories of grammar ever since. These modules are considered to be independent of one
another, operating on domain-specific primitives and not understanding the „vocabulary‟ of
the other modules. We cannot „see sounds‟, and in the same way phonology cannot
understand or operate on syntactic primitives. Furthermore, the view here is derivational, in
the sense that phonology follows syntax, and output of the syntactic computation serves as
input to the phonological computation. The term „interface‟ refers to the translation of
information from one module to another. In the case of the syntax-phonology interface,
„spell-out‟ is used to refer to the process of linearising the syntactic tree structure and
performing lexical insertion, which provides phonology with a linear input consisting of
underlying forms of lexical items.

1

LangUE 2010 Proceedings

However, certain interactions between the modules do seem to exist, as we will see in section
2, and this has been a problem for current theories of the syntax-phonology mapping. As a
result, they have been unable to maintain full modularity. The goal of the work presented
here is to account for the interaction of syntax and phonology in a modular view of language.
The questions I will be answering are: How can we derive the effects of information structure
on prosody without referring to that structure in the phonological computation? What is the
nature of input to phonology? Where and how is the connection between intonational
contours and their meaning encoded?
Section 2 presents an overview of current theories of prosodic marking of information
structure and shows how they violate modularity. Section 3 gives a brief introduction to
Nanosyntax, the model of syntax assumed by this paper, focusing on aspects relevant to
phonology. Section 4 addresses the issues arising from combining our views on phonology
and its interface with syntax with the current advancements in syntactic research, whereas
section 5 offers a way of formally capturing the proposed solution in Optimality Theory.
Section 6 gives some concluding remarks and offers directions for future research.
2. Prosodic marking of Information Structure
Prosodic Phonology is the part of phonological theory dedicated to modelling the mapping
from syntax to phonology (e.g. Selkirk 1981, 1986, 1995; Nespor and Vogel 1986; Hayes
1989; Truckenbrodt 1995 et seq). Since, in the modular view of grammar, syntactic
representations are not phonological objects and phonology cannot access syntax directly, it
does so indirectly via prosodic structure. Prosodic constituents mediate between syntactic
structure and phonological rules/constraints. In Prosodic Phonology this is known as The
Indirect Reference Hypothesis. Suprasegmental representations are organized into a Prosodic
Hierarchy of domains (PH), consisting of Syllable, Foot, Prosodic Word, Prosodic Phrase,
Intonation Phrase and Utterance levels 1 . The original motivation for proposing it and
evidence for the various prosodic domains comes from a number of segmental processes that
seem to be sensitive to them. Since then, the PH has assumed an increasingly important role
in the syntax-phonology interface. Computationally, when accounting for the mapping from
the output of the syntactic component to a phonological representation, current work in
Prosodic Phonology uses constraints and constraint interaction as defined within Optimality
Theory (Prince and Smolensky 1993; McCarthy and Prince 1993, 1995).
Despite modular underpinnings of the Indirect Reference Hypothesis, even without referring
to specific syntactic categories, labels, syntactic relations or the rest of the syntactic
information present in the tree, current theory assumes that prosody still sees certain syntactic
information, such as edges of syntactic constituents, the distinction between lexical words,
i.e. nouns, verbs, adjectives, and function words, i.e. determiners, prepositions, auxiliaries,
complementizers etc. (cf. Selkirk 1995, Truckenbrodt 1999 inter alia) and information
structure (IS) features, such as Focus and Topic. This paper will be focusing on the
modularity violations as a result of prosodic marking of IS features. Accepting that
information structure receives interpretation both at PF and LF (Chomsky 1995), and that in
the model of language assumed here (the inverted Y model of Chomsky & Lasnik 1977) the
only way for phonology and semantics (i.e language realization and interpretation) to
communicate is through syntax, it is necessary to postulate elements of information structure
1

More detailed versions of PH exist in various work, I list here the most general view, as it will suffice for the
discussion at hand.

2

LangUE 2010 Proceedings

such as Topic and Focus to be part of syntax in some way, present at spell-out and visible to
both post-syntactic modules. Following Jackendoff (1972) most literature on focus and topic
marking assumes that they are represented as privative features (F, T) on syntactic nodes.
Since Rizzi (1997) both are considered to project their own phrases, FocP and TopP, in the
left periphery of a clause. A third category of Contrastive Topic (CT) has been argued for by
Büring (2007) for English and Yamato (2007) for Japanese. In addition to syntactic
movement in e.g. Polish (Szczegielniak 2005), Hungarian (Kiss 1998), Serbian (Migdalski
2006) and morpheme markers in e.g. Japanese (Yamato 2007), Kîîtharaka (Abels and
Muriungi 2006), F, T and CT are marked in phonology by prosodic phrasing (Chichewa:
Kanerva 1990, Truckenbrodt 1999) and pitch accent and intonational contour (English: Ladd
1996 and Büring 2007 in (1) below).
(1) a) A: Well, what about FRED? What did HE eat?
L+H* L- H%

B: FREDCT

H*

L- L%

ate the BEANSF.

b) A: Well, what about the BEANS? Who ate THEM?
H*

L+H* L- H%

B: FREDF ate the BEANSCT.
In literature on Focus there is often a distinction made between broad, novelty or information
focus on the one hand and narrow, contrastive or identificational focus on the other (Ladd
1996, Kiss 1998, Fery and Samek-Lodovici 2006, to name but a few). An important
distinction between the two types is presented in Kiss (1998), in that only the second type is
associated with focus movement to Spec,FocP, and thus with the F-feature, whereas the
neutral „information‟ focus shows no F-marking properties. Also, FocP of Rizzi (1997) is
associated with contrastive focus. Thus, in this paper the term „Focus‟ will refer to
narrow/contrastive focus.
The other category, Topic, is often defined as that which is already present in the discourse. It
has been argued (Lee 2003, Büring 2007 and references cited therein) that instead of the
focus/background and topic/comment distinction, there is a tripartition, which allows, apart
from focused and non-focused, optionally, for a non-focused part to be specially marked as
link, or Contrastive Topic (CT). In English, for example, Topic is marked by topicalisation
movement whereas CT is marked by a specific intonational contour, which would suggest
that there are two features present, both T and CT (existence of separate categories of topic
and contrast, in addition to focus, has also been argued for e.g. Japanese, cf. Yamato 2007).
The example in (1) above illustrates the difference in meaning between (contrastive) Focus
and CT. In (1b) Focus introduces a set of alternatives (“people at the dinner”) and provides an
exhaustive answer (“Fred and nobody else ate the beans”). In (1a) CT creates the set of
alternatives by expanding on the given topic (“Fred” becomes associated with the superset
“people at the dinner”) and provides a non-exhaustive answer (“As for the people at the
dinner, Fred ate the beans, and I don‟t know/care what other people ate”).
In OT work within Prosodic Phonology it is assumed that phonology sees these syntactic
features. Truckenbrodt (1999) introduces the constraint Align-F, aligning the right edge of a
focused constituent with a prosodic phrase to capture the effects of focus in Chichewa,
Samek-Lodovici (2005) and Fery & Samek-Lodovici (2006) use Stress-Focus and StressTopic to assign highest prominence to the focused/topicalised constituent:

3

LangUE 2010 Proceedings

(2)
AlignF
align the right edge of an F constituent with a prosodic phrase
StressFocus
focused phrase has the highest prosodic prominence in its focus domain.
StressTopic
topic phrase has the highest prosodic prominence in its domain.
However, these constraints are undesirable in a fully modular system. Since prosody is not a
separate module, but is for all intents and purposes part of the phonological computation,
these constraints are part of phonology and they contain reference to both phonological and
syntactic primitives. Thus, the separation of the syntactic and phonological module is not
achieved. For full modularity to exist we would need a „No Reference Hypothesis‟ 2 (cf. also
Scheer 2010), which is what this paper is arguing for. Crucially, the Prosodic Hierarchy itself,
being a model of phonological representation of suprasegmental structure, is not a violator of
modularity. It is the mapping constraints used to compute prosodic phrasing for individual
utterances that are non-modular. This paper maintains reference to the PH, but attempts to
derive the particular phrasings modularly. The mapping algorithm used in section 5 is based
on the relevant (morpho)syntactic information being encoded in the Lexicon, via subset
indices, which are then referred to by indexed constraints (e.g.Pater 2009). The approach will,
for lack of space, be briefly outlined in section 5, and the reader is kindly referred to
Šurkalović (to appear) for details.
Furthermore, none of these constraints make the connection between specific tones or tone
contours and different information structure being marked, i.e. the fact that e.g. in English H*
Pitch Accent, and not L*, marks Focus whereas L+H*L-H% tonal contour, and not some
other, marks Contrastive Topic. Section 3 below gives an overview of one of the current
syntactic theories, and shows how it offers a solution to the issue of modular mapping.
3. Nanosyntax
Nanosyntax is an approach to (morpho)syntax currently being developed at the Centre for
Advanced Studies in Theoretical Linguistics in Tromso (Starke 2009, Caha 2009, Lundquist
2008, Ramchand 2008 inter alia). It is most akin to Distributed Morphology (e.g. Harley and
Noyer 1999), in that it subscribes to the idea that what is traditionally considered two
modules, morphology (word-syntax) and (phrasal) syntax, is actually one computational
module governed by syntactic rules and operations, as well as to the idea of post-syntactic
lexical insertion. The crucial difference between Nanosyntax and Distributed Morphology is
in that the latter allows spell-out of only terminal nodes, whereas in Nanosyntax lexical
insertion can target any node in the tree, including phrasal nodes. The building blocks of
syntax are features, not lexical items or feature bundles. Each terminal is a single feature.
Thus, for example, the 3rd person singular present tense „-s‟ in English lexicalizes the stretch
of three terminal nodes, [3rd [Sing [Pres]]]. In some cases a single lexical item can spell-out a
stretch of functional hierarchy for which two items are required in other cases, as in English
„went‟ vs „walk-ed‟. As far as spell-out is concerned, all nodes are equal, be they terminals or
2

I use the term Direct Reference to signal phonology having direct access to syntax, and term No Reference to
refer to phonology only processing phonological information and not referring to syntactic notions. Scheer
(2010) uses the term Direct Reference in the opposite sense than it is used here.

4

LangUE 2010 Proceedings

phrasal nodes.
Lexical items, schematized in (3), consist of three pieces of information: phonological gesture
(the underlying form, input into the phonological module), syntactic configuration (the piece
of syntactic tree that a particular item can spell out) and conceptual information
(encyclopedic knowledge). The conceptual information is limited to the kind that
distinguishes „cat’ from „dog’, whereas the formal semantic interpretation is computed from
the syntactic features being lexicalized. As such, the Lexicon only stores those structures that
syntax has built, and there is no syntactic computation done in the Lexicon.
(3)

</gesture/,

,concept

>

Once two features are chosen, they Merge, within the module in charge of syntactic
computation, following the strict order in a functional hierarchy (f-seq). After each merge the
created tree is sent to spell-out where it is matched against the trees stored in the lexicon, to
check if there is a lexical item corresponding to the tree created at this point (for purposes of
lexical insertion, not for checking whether the created syntactic structure is licit). If one is not
found, the computation continues and lexical matching is attempted at the next merge. If one
is found, it is inserted, and the computation cycles back to do the merging of the next feature
in the f-seq. At the next merge, the whole tree is sent to spell-out again, and either the new
feature is spelled out by a second lexical entry and the lexical entry from the previous cycle is
kept (e.g. creating a regular plural form of a noun, e.g. „book-s‟) or a lexical entry is found
that spells out the whole tree in one lexical item, in which case it overrides the previous spellout (e.g. creating an irregular plural form of a noun, in which case e.g. „mice‟ overrides the
„mouse‟ spell-out of the previous cycle). For a more detailed account of lexical insertion in
Nanosyntax the reader is referred to Starke (2009) and Caha (2009). For the purpose of this
paper the relevant aspect is that each feature in a syntactic tree is an individual terminal and
can correspond to a lexical item spelling out that particular feature.
Section 4 will show how this view of syntactic features and lexical items solves the
modularity problem of prosodically marking information structure by allowing us to
formalize prosodic markers of Focus and Topic as lexical items (morphemes; affixes) that
spell out syntactic features and have no segmental but only suprasegmental phonological
content.
4. Lexicon as the interface: suprasegmental affixes
If we are to argue for the idea of modularity, the only place in the system where syntactic and
phonological information are in contact is the Lexicon. It translates syntactic information to
phonological information that serves as input to phonological computation. A natural avenue
to pursue is to attempt to use the lexical entries as translators of IS features in (morpho)syntax
into phonological information (see also Scheer 2010, and Bye and Svenonius to appear). As
we have seen in section 3, in Nanosyntax all features are merged into the syntactic tree as
individual terminals. By default, then, information structure features are also individual
terminals in a syntactic tree. In some languages those features drive movement while in
others they correspond to lexical items. These lexical items pair a feature with its
phonological realization, which is in some languages a segmental morpheme (e.g. Japanese
Topic marker „-wa‟) and in some a prosodic morpheme. Thus, prosodic markers of Focus
5

LangUE 2010 Proceedings

and Contrastive Topic in English are lexical items (morphemes) with no segmental but only
suprasegmental phonological content that spells out certain syntactic material, much as e.g.
the English „-ed’ suffix spells out Tense/Past. The idea that the Pitch Accent marking Focus
is assigned to a focused element in the (morpho)syntactic structure and is thus already present
in the input to phonology is mentioned in Selkirk (2005:448) when discussing the AssocPA
constraint given in (6) below. However, the source of the PA and the nature of its encoding in
(morpho)syntax is not made explicit, and the idea is lost in subsequent work.
The view of prosodic markers of information structure as suprasegmental affixes, although
absent in the literature on Prosodic Phonology, fits well with what we currently know about
the system. Lexical entries consisting of only segmental phonological information as well as
those consisting of segmental and suprasegmental information (in lexical tone languages)
exist, so the existence of lexical entries consisting of solely suprasegmental information
naturally follows. Furthermore, lexical entries consisting of suprasegmental information that
spell out morpho-syntactic categories such as number or gender are already attested in many
African languages3, so it is possible for suprasegmental affixes to spell out IS-related parts of
the functional sequence. Finally, if discourse-related parts of the f-seq can be marked by
segmental affixes, why could those features not be marked by suprasegmental affixes as well.
Lexical entries for F and CT features in English in (4) would be as in (5), just as the lexical
entry for the past suffix would be < /id/, Past >.
(4)

a)

A: Well, what about FRED? What did HE eat?
L+H* L- H%

B: FREDCT
b)

H* L- L%

ate the BEANSF.

A: Well, what about the BEANS? Who ate THEM?
H*

L+H* L- H%

B: FREDF ate the BEANSCT.
(5)

< /H*/, F >,
< /L+H*L-H%/, CT >

Encoding tones and tunes in the Lexicon and not in the phonology also allows for capturing
the arbitrariness and cross-linguistic variation in their association to different meanings. As a
reviewer points out, not all languages have a one-to-one relation between focus and tones
(e.g. Cressels 1996 on Tswana). There are two possibilities, either it is a case of allophonic
variation at a suprasegmental level, or we have not fully understood the pattern in question.
In any case, as long as there is some sound (segmental or suprasegmental alike) connected to
a meaning, even if we do not yet fully understand the specific meaning or the connection
between the two, the only way to encode the connection is in the Lexicon. Furthermore, this
approach is also applicable to cases of purely intonational marking of questions (e.g. English
Y/N Questions), assuming that the intonational contour is a spell-out of a Q/Interrog feature
in syntax, and tonal marking of various grammatical features such as is found in Bantu
languages.
The merit of this approach in view of modularity is that, after lexical insertion is done, what
reaches phonology is pure phonological information, and the same type of constraints that
3

As an anonymous reviewer points out, although it is not a widespread approach in generative literature,
treating tones as lexical entries spelling out (morpho)syntactic features is standard in literature on Bantu (e.g.
Kula 2007)

6

LangUE 2010 Proceedings

place segmental affixes in their rightful place are used to place suprasegmental affixes in
theirs. The segmental and prosodic affixes are treated equally by phonology. The Lexicon
provides the tonal contour, the spell-out (linearization) provides the domain of realization,
and phonology places the tones within that domain with Prosodic Well-formedness
Constraints, which make sure that the suprasegmental affix is properly placed on an
appropriate Tone Bearing Unit (TBU) within its domain, e.g. that the H* tone marking Focus
in English is realized on the main stress unit of the focused constituent. A Focus feature in
syntax triggers the insertion of a H* morpheme, whose presence in the phonological input
triggers the adjustment of stress, i.e. because something is focused in syntax it receives the
H* which in turn causes it being „focused‟ in phonology by being stressed.
5. OT computation
Constraints currently used in OT Prosodic Phonology are given below:
(6)
AlignF
align the right edge of an F constituent with a prosodic phrase
(Truckenbrodt 1999)
StressFocus
focused phrase has the highest prosodic prominence in its focus domain.
StressTopic
topic phrase has the highest prosodic prominence in its domain.
(Fery and Samek-Lodovici 2006:9)
AssocPA
a Pitch Accent associates to (aligns with) a stressed syllable (head of a Ft)
(Selkirk 1995)
The StressFocus constraint suggests that Focus requires highest stress prominence, which
attracts the H* tone. The focus marker, i.e. the pitch accent, is assigned to the most prominent
segment. Taking it one step further, Fery and Samek-Lodovici (2006) argue against the
relation between pitch accents and F-marking, and that instead their distribution follows from
the interaction between the constraints governing the prosodic organization of the clause, like
AssocPA, on the one hand and the constraints like Stress-Focus and StressTopic governing
the prosodic expression of discourse status on the other. In her recent work, Selkirk (Kratzer
and Selkirk 2007) also adopts this view and uses these constraints.
An example tableau of the current approach is given in (7) below, using function words as an
example of a clear distinction in prosodification dependent on IS status, and the constraint
ranking from Selkirk (1995).
(7)
Throw it [to]F the dog
(not at it)
a. (( tʊ) (ðə (dɔg )))
b. ( tə ( ðə ( dɔg )) )

Stress
Focus

AlignR AlignR AlignL/R
(LexP; PPh;
(Lex;
PPh)
PWd)
PWd)

AlignL/R
(PWd;
HP
Lex)
**

*!

7

*

LangUE 2010 Proceedings

AlignL/R (Lex,; PWd)
Left/right edge of a Lexical Word coincides with the Left/right edge of a Prosodic Word
AlignL/R (PWd; Lex)
Left/right edge of a Prosodic Word coincides with the Left/right edge of a Lexical Word
AlignR (Lexmax; PPh)
The right edge of a maximal phrase projected from a lexical head coincides with the right
edge of a PPh
AlignR (PPh; PWd)
the right edge of a PPh coincides with the right edge of a PWd
HP
Align the right boundary of every P-phrase with its head(s). (Fery and Samek-Lodovici 2006)
We see from the tableau how requirements of Focus force function words to assume PWd
status in order to be able to bear PA, and the otherwise optimal candidate (7b), with the
reduced function word and the right-aligned head of the Prosodic Phrase, yields to (7a).
In the account presented here it is argued that it is not the prominence that drives tone
placement, but the other way around. Focus is spelled out by a H* tone4, which then attracts
the main prominence of the sentence due to prosodic well-formedness constraints requiring
pitch accents to be realized on the head of the intonational domain. Using the Lexicon
subcategorisation approach of Šurkalović (to appear), we can state that different affixes and
function words form lexicon subsets. „prefix‟, „suffix‟ and „fnc‟ (function word) are
shorthand for a phonological input consisting of a string of segments with a specific index
indicating its membership in a Lexicon subset, while alignment constraints listed in (8) below
specify their position. In (8) below we see a tableau parallel to (7) where it is shown that, if
we assume that the H* is present in the input as a suprasegmental affix, and specified as e.g. a
suffix, the presence of this Focus-marking Pitch Accent requires the presence of prosodic
structure that satisfies AssocPA, and the optimal candidate in (8a) has the stressed/strong
form of the pitch-accented function word (boldface indicates location of main stress). It is
assumed that some form of Realize Morpheme or Contiguity prevents the relocation of H*
onto „dog’, by any formal means that militate against relocation of segmental affixes.
(8)
Throw it tofnc-H*Suff thefnc dogR
(not at it)
a.
H*
(( tʊ) (ðə (dɔg )))
b. H*
( tə ( ðə ( dɔg )) )

AlignR
Assoc
(suffix;
PA
PWd)

Align
(fnc,R;
PWd, L)

AlignL/R AlignL/R
(Root;
(PWd;
HP
PWd)
Root)
**

*!

*

Align (suffix, R; PWd, R)
right edge of a suffix coincides with the right edge of a Prosodic Word
Align (fnc, R; PWd, L)
right edge of a fnc coincides with the left edge of a Prosodic Word
AlignL/R (Root; PWd)
Left/right edge of a Root coincides with the Left/right edge of a Prosodic Word
AlignL/R (PWd; Root)
Left/right edge of a Prosodic Word coincides with the Left/right edge of a Root
4

Or L+H*, if we follow Selkirk (2002), distinguishing it from the default clausal prominence marker H*.

8

*

LangUE 2010 Proceedings

Büring (2007) argues that, in English, CTs are characteristically marked by a fall-rise
contour, what Jackendoff (1972) calls the B-accent (whereas focus is A-accent), and what has
been described as an H* or L+H* followed by a L-H% boundary sequence.
A further example from Büring (2007:16) illustrates the non-exhaustive meaning of CT:
(9)

(What did the pop stars wear?)
L+H*

L- H%

H*

L- L%

The FEMALECT pop stars wore CAFTANSF.
Here, female pop stars are contrasted with male pop stars, suggesting that the male stars wore
something else, thus giving a non-exhaustive answer with respect to „all pop stars‟. In the
account presented here, the input to phonology is /fi:meil L+H* L-H%/. The prosodic wellformedness constraints I propose be used are the AssocPA constraint and the AssocBT
constraint given in (10) below:
(10)
/fi:meil L+H* L-H%/ AssocPA AssocBT FtForm
a

L+H*
L-H%
[ fi:
meil ]
b L+H* L-H%
[ fi:
meil ]
c
L+H*L-H%
[ fi:
meil ]
d L+H*
L-H%
[ fi:
meil]
e
L+H* L-H%
[ fi:
meil ]

*!
*!
*!

*
*!

AssocBT-R/L
A right/left Boundary Tone associates to (aligns with) a right/left edge of a constituent it
associates to
FtForm(Trochaic) 5
The head of a Ft is aligned with the Left edge of a Ft
In candidate (10a) the PA from the suprasegmental affix is associated with the initial syllable
and the BT is associated with the right boundary, resulting in a well-formed structure.
Candidates (10b, c, d) are not optimal due to the misalignment of the two components of the
contour, whereas candidate (10e), in an attempt to not split up the contour, violates FtFormTrochaic.
As we see from the examples above, if we assume that there are no IS features present in
phonology, but that IS marking is present in the input in the form of suprasegmental affixes,
there is no need for modularity-violating constraints, and with slight modifications in form of
introducing the AssocBT constraint, the current system of prosodic well-formedness
constraints is equipped to account for the realization of those prosodic markers.
5

This constraint is used as shorthand for whatever formal way of achieving trochaic feet is in English,
abstracting away from different stress-assignment theories.

9

LangUE 2010 Proceedings

6. Conclusion
This paper has argued that modularity can be maintained, unlike in the current theories of the
syntax-phonology interface, if we utilise Nanosyntax and assume the Lexicon to be the only
means of communication between syntax and phonology and the only source of information
used in phonological computation. We can derive the effects of information structure on
prosody without referring to that structure in the phonological computation by using lexical
entries to translate syntactic structure into phonological material. Input to phonology is purely
phonological information, with no reference to syntactic or information structure categories
or features. It is a linearized string of phonological underlying forms of lexical items.
Phonology operates only on phonological primitives, not syntactic F, T, CT features in the
constraints. The connection between intonational contours and their meaning is encoded in
the Lexicon as the only means of communication between syntax and phonology.
However, this approach presents certain challenges to both syntactic and phonological theory.
If all features are terminals and information structure markers are encoded as lexical
items/prosodic affixes, and we know that e.g. in English any word can be focused, what is the
position of the information structure features in the f-seq? Do they freely adjoin at any point
or is there a fixed functional hierarchy? Furthermore, there has been resistance in the
literature so far to encoding prosodic markers of IS in the lexicon because the exact
correlations between prosody and the various meanings has not been fully explored, and there
is much variation present in the prosody. On the phonological side, thus, the challenge is to
strive for a better understanding of the correlation between prosody and the variation in IS
meanings it encodes, as well as to explore the extent to which prosodic information is
encoded in the lexicon.

References:
Abels, K., & Muriungi, P. (2006). The focus particle in Kîîtharaka. ZAS Papers in Linguistics
46,1-20.
Bye, P. & Svenonius P. (to appear). Extended exponence and non-concatenative
morphology. In J. Trommer, (Ed.) The Morphology and Phonology of Exponence. Oxford:
OUP
Büring, D. (2007). Semantics, intonation and information structure In G. Ramchand & C.
Reiss (Eds.) The Oxford Handbook of Linguistic Interfaces. Oxford: OUP.
Caha, P. (2009). The Nanosyntax of Case. PhD Dissertation, University of Tromsø, Tromsø.
Chomsky, N. (1965). Aspects of the theory of syntax. Cambridge: The MIT Press
Chomsky, N. (1995). The Minimalist Program. Cambridge, MA: MIT Press.
Chomsky, N., & Lasnik, H. (1977). Filters and control. Linguistic Inquiry 8, 425-504.

10

LangUE 2010 Proceedings

Féry, C., & Samek-Lodovici, V. (2006). Focus projection and prosodic prominence in Nested
Foci. Language 8, 2.1.
Harley, H., & Noyer, R. (1999). Distributed morphology. Glot International 4, 4:3-9.
Hayes, B. (1989). The prosodic hierarchy in meter. In P. Kiparsky & G. Youmans (Eds.),
Phonetics and phonology. Rhythm and meter (pp. 201-260). New York: Academic Press.
Jackendoff, R. (1972). Semantics in Generative Grammar. Cambridge, MA: MIT Press
Kanerva, J. (1990) Focus and phrasing in Chichewa phonology. PhD Dissertation, Stanford
University.
Kiss, K. É. (1998). Identificational Focus versus Information Focus. Language 74, 245-273.
Kratzer, A. & Selkirk E. (2007). Phase theory and prosodic spellout: the case of verbs. The
Linguistic Review 24, 93–1.
Kula, N. C. (2007). Effects of phonological phrasing on syntactic structure. The Linguistic
Review 24, 201-231.
Ladd, D. R. (1996). Intonational phonology. Cambridge: CUP.
Lee, C. (2003). Contrastive Topic and/or Contrastive Focus. In B. McClure (Ed.)
Japanese/Korean Linguistics 12. CSLI, Stanford.
Lundquist, B. (2008). Nominalizations and Participles in Swedish. PhD Dissertation,
University of Tromso.
McCarthy, J & Prince A. (1993). Generalized Alignment. Yearbook of Morphology, 79–153.
McCarthy, J., and Prince A. (1995). Faithfulness and reduplicative identity. In J. Beckman, S.
Urbanczyk & L. Walsh Dickey (Eds.) University of Massachusetts Occasional Papers in
Linguistics 18: Papers in Optimality Theory (Pp. 249–384). Amherst, GLSA, UMass
Migdalski, K.(2006). The syntax of compound tenses in Slavic. LOT Dissertation Series 130.
Nespor, M. & I. Vogel. (1986). Prosodic Phonology. Foris, Dordrecht.
Pater, J.. (2009). Morpheme-specific phonology: Constraint indexation and inconsistency
resolution. In S. Parked (ed.) Phonological Argumentation: Essays on evidence and
motivation. London: Equinox. 123-154
Prince, A, & Smolensky P. (1993). Optimality theory: Constraint interaction in generative
grammar. Ms. Brunswick, New Jersey, and Boulder, Colorado: Rutgers University and
University of Colorado, Boulder. [http://roa.rutgers.edu/files/537-0802/537-0802-PRINCE-00.PDF].
Ramchand, G. (2008). Verb meaning and the lexicon: A first phase syntax. Cambridge: CUP.

11

LangUE 2010 Proceedings

Rizzi, L. (1997). The fine structure of the left periphery. In L. Haegeman (Ed.) Elements of
Grammar (281-337). Dordrecht: Kluwer.
Samek-Lodovici, V. (2005). Prosody-syntax interaction in the expression of focus. Natural
Language and Linguistic Theory 23, 687-755.
Scheer, T. (2010). A Guide to Morphosyntax-Phonology Interface Theories: How ExtraPhonological Information is treated in Phonology since Trubetzkoys Grenzsignale. Mouton
de Gruyter, Berlin.
Selkirk, E. (1981). On prosodic structure and its relation to syntactic structure. In T. Fretheim
(Ed.) Nordic Prosody II (pp.111-140). Trondheim: TAPIR.
Selkirk, E. (1986). On derived domains in sentence phonology. Phonology 3, 371-405.
Selkirk, E. (1995). The prosodic structure of function words. In J. Morgan & K. Demuth
(Eds.) Signal to syntax: bootstrapping from syntax to grammar in early acquisition (pp.187213). Mahwah, NJ: Erlbaum.
Starke, M. (2009). Nanosyntax: A short primer to a new approach to language. Tromsø
Working Papers in Language and Linguistics: Nordlyd 36.1, 1-6
Szczegielniak, A. (2005). Clitic positions within the left periphery: Evidence for a
phonological buffer. In L Heggie & P. Ordoñez (Eds.) Clitic and affix combinations:
Theoretical perspectives (pp. 283–299). Amsterdam: Benjamins.
Šurkalović, D. (to appear). Lexical and functional decomposition in Syntax: A view from
Phonology. Poznan Studies in Contemporary Linguistics.
Šurkalović, D. (in preparation). No Reference Hypothesis: A modular account of the syntaxphonology interface. PhD Dissertation, University of Tromsø
Truckenbrodt, H. (1995). Phonological phrases: Their relation to syntax, focus and
prominence (Unpublished doctoral thesis). MIT.
Truckenbrodt, H. (1999). On the relation between syntactic phrases and phonological
phrases. Linguistic Inquiry 30(2), 219-255.
Yamato, N. (2007). A comparative study of embedded V2 in Northern Norwegian and
embedded topic-marking in Japanese. MA Thesis, University of Tromsø.
Dragana Šurkalović
CASTL, University of Tromsø, Norway
4.563 Teorifagbygget, hus 4, plan 5
Universitetet i Tromsø
N-9037 Tromsø
Norge
dragana.surkalovic@uit.no

12

Developing Language Processing
Components with GATE
Version 6 (a User Guide)
For GATE version 6.1-snapshot (development builds)
(built April 24, 2011)

Hamish Cunningham
Diana Maynard
Kalina Bontcheva
Valentin Tablan
Niraj Aswani
Ian Roberts
Genevieve Gorrell
Adam Funk
Angus Roberts
Danica Damljanovic
Thomas Heitz
Mark A. Greenwood
Horacio Saggion
Johann Petrak
Yaoyong Li
Wim Peters
et al
The University of Sheﬃeld, Department of Computer Science 2001-2011
http://gate.ac.uk/
This user manual is free, but please consider making a donation.
HTML version: http://gate.ac.uk/userguide

Work on GATE has been partly supported by EPSRC grants GR/K25267 (Large-Scale
Information Extraction), GR/M31699 (GATE 2), RA007940 (EMILLE), GR/N15764/01 (AKT)
and GR/R85150/01 (MIAKT), AHRB grant APN16396 (ETCSL/GATE), Matrixware, the
Information Retrieval Facility and several EU-funded projects: (SEKT, TAO, NeOn,
MediaCampaign, Musing, KnowledgeWeb, PrestoSpace, h-TechSight, and enIRaF).

Developing Language Processing Components with GATE Version 6.0
2011 The University of Sheﬃeld, Department of Computer Science
The University of Sheﬃeld, Department of Computer Science
Regent Court
211 Portobello
Sheﬃeld
S1 4DP
United Kingdom
http://gate.ac.uk
This work is licenced under the Creative Commons Attribution-No Derivative Licence. You are
free to copy, distribute, display, and perform the work under the following conditions:
Attribution You must give the original author credit.
No Derivative Works You may not alter, transform, or build upon this work.
With the understanding that:
Waiver Any of the above conditions can be waived if you get permission from the copyright
holder.
Other Rights In no way are any of the following rights aﬀected by the license: your fair
dealing or fair use rights; the author’s moral rights; rights other persons may have either in
the work itself or in how the work is used, such as publicity or privacy rights.
Notice For any reuse or distribution, you must make clear to others the licence terms of
this work.
For more information about the Creative Commons Attribution-No Derivative License, please visit
this web address: http://creativecommons.org/licenses/by-nd/2.0/uk/

Brief Contents
I

GATE Basics

3

1 Introduction

5

2 Installing and Running GATE

29

3 Using GATE Developer

41

4 CREOLE: the GATE Component Model

71

5 Language Resources: Corpora, Documents and Annotations

91

6 ANNIE: a Nearly-New Information Extraction System

II

GATE for Advanced Users

115

133

7 GATE Embedded

135

8 JAPE: Regular Expressions over Annotations

183

9 ANNIC: ANNotations-In-Context

219

10 Performance Evaluation of Language Analysers

229

11 Proﬁling Processing Resources

259

12 Developing GATE

267

III

CREOLE Plugins

279

13 Gazetteers

281

14 Working with Ontologies

303

15 Non-English Language Support

341

16 Parsers

347

17 Machine Learning

361

18 Tools for Alignment Tasks

411
iii

iv

Contents

19 Combining GATE and UIMA

427

20 More (CREOLE) Plugins

439

IV

The GATE Family: Cloud, MIMIR, Teamware

21 GATE Cloud

499
501

22 GATE Teamware: A Web-based Collaborative Corpus Annotation Tool 511
23 GATE M´
ımir

525

Appendices

525

A Change Log

527

B Version 5.1 Plugins Name Map

557

C Design Notes

559

D JAPE: Implementation

567

E Ant Tasks for GATE

577

F Named-Entity State Machine Patterns

585

G Part-of-Speech Tags used in the Hepple Tagger

593

References

594

Contents
I

GATE Basics

3

1 Introduction
1.1 How to Use this Text . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 Developing and Deploying Language Processing Facilities
1.3.2 Built-In Components . . . . . . . . . . . . . . . . . . . .
1.3.3 Additional Facilities in GATE Developer/Embedded . .
1.3.4 An Example . . . . . . . . . . . . . . . . . . . . . . . . .
1.4 Some Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Recent Changes . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.1 April 2011 . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.2 March 2011 . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.3 February 2011 . . . . . . . . . . . . . . . . . . . . . . . .
1.5.4 January 2011 . . . . . . . . . . . . . . . . . . . . . . . .
1.5.5 December 2010 . . . . . . . . . . . . . . . . . . . . . . .
1.5.6 Version 6.0 (November 2010) . . . . . . . . . . . . . . . .
1.6 Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Installing and Running GATE
2.1 Downloading GATE . . . . . . . . . . . . . . . . . . . . . .
2.2 Installing and Running GATE . . . . . . . . . . . . . . . . .
2.2.1 The Easy Way . . . . . . . . . . . . . . . . . . . . .
2.2.2 The Hard Way (1) . . . . . . . . . . . . . . . . . . .
2.2.3 The Hard Way (2): Subversion . . . . . . . . . . . .
2.2.4 Running GATE Developer on Unix/Linux . . . . . .
2.3 Using System Properties with GATE . . . . . . . . . . . . .
2.4 Conﬁguring GATE . . . . . . . . . . . . . . . . . . . . . . .
2.5 Building GATE . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.1 Using GATE with Maven/Ivy . . . . . . . . . . . . .
2.6 Uninstalling GATE . . . . . . . . . . . . . . . . . . . . . . .
2.7 Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . .
2.7.1 I don’t see the Java console messages under Windows
2.7.2 When I execute GATE, nothing happens . . . . . . .
v

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

5
8
8
9
9
11
12
12
14
15
15
16
16
17
18
18
20

.
.
.
.
.
.
.
.
.
.
.
.
.
.

31
31
31
31
32
33
33
34
36
37
38
38
39
39
39

vi

Contents

2.7.3
2.7.4
2.7.5
2.7.6

On Ubuntu, GATE is very slow or doesn’t start . . . . . . . . . . . .
How to use GATE on a 64 bit system? . . . . . . . . . . . . . . . . .
I got the error: Could not reserve enough space for object heap . . .
From Eclipse, I got the error: java.lang.OutOfMemoryError: Java
heap space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7.7 On MacOS, I got the error: java.lang.OutOfMemoryError: Java heap
space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7.8 I got the error: log4j:WARN No appenders could be found for logger...
2.7.9 Text is incorrectly refreshed after scrolling and become unreadable . .
2.7.10 An error occurred when running the TreeTagger plugin . . . . . . . .
2.7.11 I got the error: HighlightData cannot be cast to ...HighlightInfo . . .
3 Using GATE Developer
3.1 The GATE Developer Main Window . . . . . . . . . . . . . . . . . . .
3.2 Loading and Viewing Documents . . . . . . . . . . . . . . . . . . . . .
3.3 Creating and Viewing Corpora . . . . . . . . . . . . . . . . . . . . . . .
3.4 Working with Annotations . . . . . . . . . . . . . . . . . . . . . . . . .
3.4.1 The Annotation Sets View . . . . . . . . . . . . . . . . . . . . .
3.4.2 The Annotations List View . . . . . . . . . . . . . . . . . . . .
3.4.3 The Annotations Stack View . . . . . . . . . . . . . . . . . . . .
3.4.4 The Co-reference Editor . . . . . . . . . . . . . . . . . . . . . .
3.4.5 Creating and Editing Annotations . . . . . . . . . . . . . . . . .
3.4.6 Schema-Driven Editing . . . . . . . . . . . . . . . . . . . . . . .
3.4.7 Printing Text with Annotations . . . . . . . . . . . . . . . . . .
3.5 Using CREOLE Plugins . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Loading and Using Processing Resources . . . . . . . . . . . . . . . . .
3.7 Creating and Running an Application . . . . . . . . . . . . . . . . . . .
3.7.1 Running an Application on a Datastore . . . . . . . . . . . . . .
3.7.2 Running PRs Conditionally on Document Features . . . . . . .
3.7.3 Doing Information Extraction with ANNIE . . . . . . . . . . . .
3.7.4 Modifying ANNIE . . . . . . . . . . . . . . . . . . . . . . . . .
3.8 Saving Applications and Language Resources . . . . . . . . . . . . . . .
3.8.1 Saving Documents to File . . . . . . . . . . . . . . . . . . . . .
3.8.2 Saving and Restoring LRs in Datastores . . . . . . . . . . . . .
3.8.3 Saving Application States to a File . . . . . . . . . . . . . . . .
3.8.4 Saving an Application with its Resources (e.g. GATECloud.net)
3.9 Keyboard Shortcuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.10 Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.10.1 Stopping GATE from Restoring Developer Sessions/Options . .
3.10.2 Working with Unicode . . . . . . . . . . . . . . . . . . . . . . .

39
40
40
41
41
41
41
42
42

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

43
44
46
49
51
51
52
52
53
54
56
58
59
61
61
63
63
64
65
65
65
66
67
68
69
71
71
71

4 CREOLE: the GATE Component Model
4.1 The Web and CREOLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 The GATE Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73
74
75

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

Contents

4.3
4.4
4.5
4.6
4.7

4.8

The Lifecycle of a CREOLE Resource . . . . . .
Processing Resources and Applications . . . . .
Language Resources and Datastores . . . . . . .
Built-in CREOLE Resources . . . . . . . . . . .
CREOLE Resource Conﬁguration . . . . . . . .
4.7.1 Conﬁguration with XML . . . . . . . . .
4.7.2 Conﬁguring Resources using Annotations
4.7.3 Mixing the Conﬁguration Styles . . . . .
Tools: How to Add Utilities to GATE Developer
4.8.1 Putting your tools in a sub-menu . . . .

vii

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

75
76
77
78
78
78
84
89
91
91

5 Language Resources: Corpora, Documents and Annotations
5.1 Features: Simple Attribute/Value Data . . . . . . . . . . . . . .
5.2 Corpora: Sets of Documents plus Features . . . . . . . . . . . .
5.3 Documents: Content plus Annotations plus Features . . . . . .
5.4 Annotations: Directed Acyclic Graphs . . . . . . . . . . . . . .
5.4.1 Annotation Schemas . . . . . . . . . . . . . . . . . . . .
5.4.2 Examples of Annotated Documents . . . . . . . . . . . .
5.4.3 Creating, Viewing and Editing Diverse Annotation Types
5.5 Document Formats . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.1 Detecting the Right Reader . . . . . . . . . . . . . . . .
5.5.2 XML . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.3 HTML . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.4 SGML . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.5 Plain text . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.6 RTF . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.7 Email . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5.8 PDF Files and Oﬃce Documents . . . . . . . . . . . . .
5.6 XML Input/Output . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

93
93
94
94
94
94
96
99
99
101
102
110
111
111
112
113
114
114

6 ANNIE: a Nearly-New Information Extraction
6.1 Document Reset . . . . . . . . . . . . . . . . . .
6.2 Tokeniser . . . . . . . . . . . . . . . . . . . . .
6.2.1 Tokeniser Rules . . . . . . . . . . . . . .
6.2.2 Token Types . . . . . . . . . . . . . . .
6.2.3 English Tokeniser . . . . . . . . . . . . .
6.3 Gazetteer . . . . . . . . . . . . . . . . . . . . .
6.4 Sentence Splitter . . . . . . . . . . . . . . . . .
6.5 RegEx Sentence Splitter . . . . . . . . . . . . .
6.6 Part of Speech Tagger . . . . . . . . . . . . . .
6.7 Semantic Tagger . . . . . . . . . . . . . . . . .
6.8 Orthographic Coreference (OrthoMatcher) . . .
6.8.1 GATE Interface . . . . . . . . . . . . . .
6.8.2 Resources . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

117
118
119
119
120
121
121
123
124
125
126
126
126
127

System
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.

viii

Contents

6.8.3 Processing . . . . . . . . . . . . . . . .
6.9 Pronominal Coreference . . . . . . . . . . . .
6.9.1 Quoted Speech Submodule . . . . . . .
6.9.2 Pleonastic It Submodule . . . . . . . .
6.9.3 Pronominal Resolution Submodule . .
6.9.4 Detailed Description of the Algorithm .
6.10 A Walk-Through Example . . . . . . . . . . .
6.10.1 Step 1 - Tokenisation . . . . . . . . . .
6.10.2 Step 2 - List Lookup . . . . . . . . . .
6.10.3 Step 3 - Grammar Rules . . . . . . . .

II

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

GATE for Advanced Users

7 GATE Embedded
7.1 Quick Start with GATE Embedded . . . . . . . . . . . . .
7.2 Resource Management in GATE Embedded . . . . . . . .
7.3 Using CREOLE Plugins . . . . . . . . . . . . . . . . . . .
7.4 Language Resources . . . . . . . . . . . . . . . . . . . . .
7.4.1 GATE Documents . . . . . . . . . . . . . . . . . .
7.4.2 Feature Maps . . . . . . . . . . . . . . . . . . . . .
7.4.3 Annotation Sets . . . . . . . . . . . . . . . . . . . .
7.4.4 Annotations . . . . . . . . . . . . . . . . . . . . . .
7.4.5 GATE Corpora . . . . . . . . . . . . . . . . . . . .
7.5 Processing Resources . . . . . . . . . . . . . . . . . . . . .
7.6 Controllers . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.7 Duplicating a Resource . . . . . . . . . . . . . . . . . . . .
7.8 Persistent Applications . . . . . . . . . . . . . . . . . . . .
7.9 Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.10 Creating a New Annotation Schema . . . . . . . . . . . . .
7.11 Creating a New CREOLE Resource . . . . . . . . . . . . .
7.12 Adding Support for a New Document Format . . . . . . .
7.13 Using GATE Embedded in a Multithreaded Environment .
7.14 Using GATE Embedded within a Spring Application . . .
7.14.1 Duplication in Spring . . . . . . . . . . . . . . . . .
7.14.2 Spring pooling . . . . . . . . . . . . . . . . . . . . .
7.14.3 Further reading . . . . . . . . . . . . . . . . . . . .
7.15 Using GATE Embedded within a Tomcat Web Application
7.15.1 Recommended Directory Structure . . . . . . . . .
7.15.2 Conﬁguration Files . . . . . . . . . . . . . . . . . .
7.15.3 Initialization Code . . . . . . . . . . . . . . . . . .
7.16 Groovy for GATE . . . . . . . . . . . . . . . . . . . . . . .
7.16.1 Groovy Scripting Console for GATE . . . . . . . .
7.16.2 Groovy scripting PR . . . . . . . . . . . . . . . . .

127
127
128
128
128
129
133
133
134
134

135
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

137
137
138
141
143
143
143
145
146
148
150
150
153
154
155
156
157
160
162
163
166
167
168
169
169
170
170
171
172
173

Contents

7.16.3 The Scriptable Controller . .
7.16.4 Utility methods . . . . . . . .
7.17 Saving Conﬁg Data to gate.xml . . .
7.18 Annotation merging through the API

ix

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

176
182
183
183

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

185
187
187
188
189
190
190
193
193
195
196
197
199
199
199
200
203
203
203
204
205
208
208
210
212
212
214
215
218
218
219
219
219
219

9 ANNIC: ANNotations-In-Context
9.1 Instantiating SSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 Search GUI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

221
222
223
223

8 JAPE: Regular Expressions over Annotations
8.1 The Left-Hand Side . . . . . . . . . . . . . . . . . . . . .
8.1.1 Matching a Simple Text String . . . . . . . . . .
8.1.2 Matching Entire Annotation Types . . . . . . . .
8.1.3 Using Attributes and Values . . . . . . . . . . . .
8.1.4 Using Meta-Properties . . . . . . . . . . . . . . .
8.1.5 Using Templates . . . . . . . . . . . . . . . . . .
8.1.6 Multiple Pattern/Action Pairs . . . . . . . . . . .
8.1.7 LHS Macros . . . . . . . . . . . . . . . . . . . . .
8.1.8 Using Context . . . . . . . . . . . . . . . . . . . .
8.1.9 Multi-Constraint Statements . . . . . . . . . . . .
8.1.10 Negation . . . . . . . . . . . . . . . . . . . . . . .
8.1.11 Escaping Special Characters . . . . . . . . . . . .
8.2 LHS Operators in Detail . . . . . . . . . . . . . . . . . .
8.2.1 Compositional Operators . . . . . . . . . . . . . .
8.2.2 Matching Operators . . . . . . . . . . . . . . . .
8.3 The Right-Hand Side . . . . . . . . . . . . . . . . . . . .
8.3.1 A Simple Example . . . . . . . . . . . . . . . . .
8.3.2 Copying Feature Values from the LHS to the RHS
8.3.3 RHS Macros . . . . . . . . . . . . . . . . . . . . .
8.4 Use of Priority . . . . . . . . . . . . . . . . . . . . . . .
8.5 Using Phases Sequentially . . . . . . . . . . . . . . . . .
8.6 Using Java Code on the RHS . . . . . . . . . . . . . . .
8.6.1 A More Complex Example . . . . . . . . . . . . .
8.6.2 Adding a Feature to the Document . . . . . . . .
8.6.3 Finding the Tokens of a Matched Annotation . .
8.6.4 Using Named Blocks . . . . . . . . . . . . . . . .
8.6.5 Java RHS Overview . . . . . . . . . . . . . . . . .
8.7 Optimising for Speed . . . . . . . . . . . . . . . . . . . .
8.8 Ontology Aware Grammar Transduction . . . . . . . . .
8.9 Serializing JAPE Transducer . . . . . . . . . . . . . . . .
8.9.1 How to Serialize? . . . . . . . . . . . . . . . . . .
8.9.2 How to Use the Serialized Grammar File? . . . .
8.10 Notes for Montreal Transducer Users . . . . . . . . . . .

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

x

Contents

9.3

9.2.2
9.2.3
9.2.4
9.2.5
Using
9.3.1
9.3.2

Syntax of Queries . . . . . . . . . . . . .
Top Section . . . . . . . . . . . . . . . .
Central Section . . . . . . . . . . . . . .
Bottom Section . . . . . . . . . . . . . .
SSD from GATE Embedded . . . . . . .
How to instantiate a searchabledatastore
How to search in this datastore . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

224
225
226
227
227
227
228

10 Performance Evaluation of Language Analysers
10.1 Metrics for Evaluation in Information Extraction . . . . . . . .
10.1.1 Annotation Relations . . . . . . . . . . . . . . . . . . . .
10.1.2 Cohen’s Kappa . . . . . . . . . . . . . . . . . . . . . . .
10.1.3 Precision, Recall, F-Measure . . . . . . . . . . . . . . . .
10.1.4 Macro and Micro Averaging . . . . . . . . . . . . . . . .
10.2 The Annotation Diﬀ Tool . . . . . . . . . . . . . . . . . . . . .
10.2.1 Performing Evaluation with the Annotation Diﬀ Tool . .
10.2.2 Creating a Gold Standard with the Annotation Diﬀ Tool
10.3 Corpus Quality Assurance . . . . . . . . . . . . . . . . . . . . .
10.3.1 Description of the interface . . . . . . . . . . . . . . . . .
10.3.2 Step by step usage . . . . . . . . . . . . . . . . . . . . .
10.3.3 Details of the Corpus statistics table . . . . . . . . . . .
10.3.4 Details of the Document statistics table . . . . . . . . . .
10.3.5 GATE Embedded API for the measures . . . . . . . . .
10.3.6 sec:eval:qapr . . . . . . . . . . . . . . . . . . . . . . . . .
10.4 Corpus Benchmark Tool . . . . . . . . . . . . . . . . . . . . . .
10.4.1 Preparing the Corpora for Use . . . . . . . . . . . . . . .
10.4.2 Deﬁning Properties . . . . . . . . . . . . . . . . . . . . .
10.4.3 Running the Tool . . . . . . . . . . . . . . . . . . . . . .
10.4.4 The Results . . . . . . . . . . . . . . . . . . . . . . . . .
10.5 A Plugin Computing Inter-Annotator Agreement (IAA) . . . . .
10.5.1 IAA for Classiﬁcation . . . . . . . . . . . . . . . . . . . .
10.5.2 IAA For Named Entity Annotation . . . . . . . . . . . .
10.5.3 The BDM-Based IAA Scores . . . . . . . . . . . . . . . .
10.6 A Plugin Computing the BDM Scores for an Ontology . . . . .
10.7 Quality Assurance Summariser for Teamware . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

231
232
232
233
236
237
238
238
240
242
242
242
243
244
244
247
248
248
249
250
251
252
254
255
256
257
258

11 Proﬁling Processing Resources
11.1 Overview . . . . . . . . . . . . . . .
11.1.1 Features . . . . . . . . . . .
11.1.2 Limitations . . . . . . . . .
11.2 Graphical User Interface . . . . . .
11.3 Command Line Interface . . . . . .
11.4 Application Programming Interface
11.4.1 Log4j.properties . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

261
261
262
262
262
263
264
264

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

Contents

xi

11.4.2 Benchmark log format . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.4.3 Enabling proﬁling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
11.4.4 Reporting tool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
12 Developing GATE
12.1 Reporting Bugs and Requesting Features . . . . . . . .
12.2 Contributing Patches . . . . . . . . . . . . . . . . . . .
12.3 Creating New Plugins . . . . . . . . . . . . . . . . . . .
12.3.1 Where to Keep Plugins in the GATE Hierarchy
12.3.2 What to Call your Plugin . . . . . . . . . . . .
12.3.3 Writing a New PR . . . . . . . . . . . . . . . .
12.3.4 Writing a New VR . . . . . . . . . . . . . . . .
12.3.5 Adding Plugins to the Nightly Build . . . . . .
12.4 Updating this User Guide . . . . . . . . . . . . . . . .
12.4.1 Building the User Guide . . . . . . . . . . . . .
12.4.2 Making Changes to the User Guide . . . . . . .

III

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

CREOLE Plugins

13 Gazetteers
13.1 Introduction to Gazetteers . . . . . . . . . . . . . . .
13.2 ANNIE Gazetteer . . . . . . . . . . . . . . . . . . . .
13.2.1 Creating and Modifying Gazetteer Lists . . .
13.2.2 ANNIE Gazetteer Editor . . . . . . . . . . . .
13.3 Gazetteer Visual Resource - GAZE . . . . . . . . . .
13.3.1 Display Modes . . . . . . . . . . . . . . . . .
13.3.2 Linear Deﬁnition Pane . . . . . . . . . . . . .
13.3.3 Linear Deﬁnition Toolbar . . . . . . . . . . .
13.3.4 Operations on Linear Deﬁnition Nodes . . . .
13.3.5 Gazetteer List Pane . . . . . . . . . . . . . . .
13.3.6 Mapping Deﬁnition Pane . . . . . . . . . . . .
13.4 OntoGazetteer . . . . . . . . . . . . . . . . . . . . . .
13.5 Gaze Ontology Gazetteer Editor . . . . . . . . . . . .
13.5.1 The Gaze Gazetteer List and Mapping Editor
13.5.2 The Gaze Ontology Editor . . . . . . . . . . .
13.6 Hash Gazetteer . . . . . . . . . . . . . . . . . . . . .
13.6.1 Prerequisites . . . . . . . . . . . . . . . . . . .
13.6.2 Parameters . . . . . . . . . . . . . . . . . . .
13.7 Flexible Gazetteer . . . . . . . . . . . . . . . . . . . .
13.8 Gazetteer List Collector . . . . . . . . . . . . . . . .
13.9 OntoRoot Gazetteer . . . . . . . . . . . . . . . . . .
13.9.1 How Does it Work? . . . . . . . . . . . . . . .
13.9.2 Initialisation of OntoRoot Gazetteer . . . . .
13.9.3 Simple steps to run OntoRoot Gazetteer . . .

269
269
269
270
270
270
271
275
277
277
278
278

281
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

283
283
284
285
285
287
287
288
288
288
288
289
289
289
289
290
290
291
292
292
293
294
295
296
297

xii

Contents

13.10Large KB Gazetteer . . . . . . . . . . . . . . . . .
13.10.1 Quick usage overview . . . . . . . . . . . . .
13.10.2 Dictionary setup . . . . . . . . . . . . . . .
13.10.3 Additional dictionary conﬁguration . . . . .
13.10.4 Processing Resource Conﬁguration . . . . .
13.10.5 Runtime conﬁguration . . . . . . . . . . . .
13.10.6 Semantic Enrichment PR . . . . . . . . . . .
13.11The Shared Gazetteer for multithreaded processing

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

14 Working with Ontologies
14.1 Data Model for Ontologies . . . . . . . . . . . . . . . . . . . . . . . . .
14.1.1 Hierarchies of Classes and Restrictions . . . . . . . . . . . . . .
14.1.2 Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.1.3 Hierarchies of Properties . . . . . . . . . . . . . . . . . . . . . .
14.1.4 URIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.2 Ontology Event Model . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.2.1 What Happens when a Resource is Deleted? . . . . . . . . . . .
14.3 The Ontology Plugin: Current Implementation . . . . . . . . . . . . . .
14.3.1 The OWLIMOntology Language Resource . . . . . . . . . . . .
14.3.2 The ConnectSesameOntology Language Resource . . . . . . . .
14.3.3 The CreateSesameOntology Language Resource . . . . . . . . .
14.3.4 The OWLIM2 Backwards-Compatible Language Resource . . .
14.3.5 Using Ontology Import Mappings . . . . . . . . . . . . . . . . .
14.3.6 Using BigOWLIM . . . . . . . . . . . . . . . . . . . . . . . . . .
14.4 The Ontology OWLIM2 plugin: backwards-compatible implementation
14.4.1 The OWLIMOntologyLR Language Resource . . . . . . . . . .
14.5 GATE Ontology Editor . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.6 Ontology Annotation Tool . . . . . . . . . . . . . . . . . . . . . . . . .
14.6.1 Viewing Annotated Text . . . . . . . . . . . . . . . . . . . . . .
14.6.2 Editing Existing Annotations . . . . . . . . . . . . . . . . . . .
14.6.3 Adding New Annotations . . . . . . . . . . . . . . . . . . . . .
14.6.4 Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.7 Relation Annotation Tool . . . . . . . . . . . . . . . . . . . . . . . . .
14.7.1 Description of the two views . . . . . . . . . . . . . . . . . . . .
14.7.2 Create new annotation and instance from text selection . . . . .
14.7.3 Create new annotation and add label to existing instance from
selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.7.4 Create and set properties for annotation relation . . . . . . . . .
14.7.5 Delete instance, label or property . . . . . . . . . . . . . . . . .
14.7.6 Diﬀerences with OAT and Ontology Editor . . . . . . . . . . . .
14.8 Using the ontology API . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.9 Using the ontology API (old version) . . . . . . . . . . . . . . . . . . .
14.10Ontology-Aware JAPE Transducer . . . . . . . . . . . . . . . . . . . .
14.11Annotating Text with Ontological Information . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
text
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .

300
300
300
302
302
302
303
303
305
306
306
307
308
310
310
312
313
314
316
317
317
318
319
319
319
322
326
327
327
329
330
331
331
332
332
333
333
333
334
335
337
338

Contents

xiii

14.12Populating Ontologies . . . . . . . . . . . . . . . . . .
14.13Ontology API and Implementation Changes . . . . . .
14.13.1 Diﬀerences between the implementation plugins
14.13.2 Changes in the Ontology API . . . . . . . . . .
15 Non-English Language Support
15.1 French Plugin . . . . . . . . . . . .
15.2 German Plugin . . . . . . . . . . .
15.3 Romanian Plugin . . . . . . . . . .
15.4 Arabic Plugin . . . . . . . . . . . .
15.5 Chinese Plugin . . . . . . . . . . .
15.5.1 Chinese Word Segmentation
15.6 Hindi Plugin . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

16 Parsers
16.1 MiniPar Parser . . . . . . . . . . . . . . . . .
16.1.1 Platform Supported . . . . . . . . . . .
16.1.2 Resources . . . . . . . . . . . . . . . .
16.1.3 Parameters . . . . . . . . . . . . . . .
16.1.4 Prerequisites . . . . . . . . . . . . . . .
16.1.5 Grammatical Relationships . . . . . . .
16.2 RASP Parser . . . . . . . . . . . . . . . . . .
16.3 SUPPLE Parser . . . . . . . . . . . . . . . . .
16.3.1 Requirements . . . . . . . . . . . . . .
16.3.2 Building SUPPLE . . . . . . . . . . .
16.3.3 Running the Parser in GATE . . . . .
16.3.4 Viewing the Parse Tree . . . . . . . . .
16.3.5 System Properties . . . . . . . . . . . .
16.3.6 Conﬁguration Files . . . . . . . . . . .
16.3.7 Parser and Grammar . . . . . . . . . .
16.3.8 Mapping Named Entities . . . . . . . .
16.3.9 Upgrading from BuChart to SUPPLE .
16.4 Stanford Parser . . . . . . . . . . . . . . . . .
16.4.1 Input Requirements . . . . . . . . . . .
16.4.2 Initialization Parameters . . . . . . . .
16.4.3 Runtime Parameters . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

17 Machine Learning
17.1 ML Generalities . . . . . . . . . . . . . . . . . . . . . . . . . . .
17.1.1 Some Deﬁnitions . . . . . . . . . . . . . . . . . . . . . .
17.1.2 GATE-Speciﬁc Interpretation of the Above Deﬁnitions .
17.2 Batch Learning PR . . . . . . . . . . . . . . . . . . . . . . . . .
17.2.1 Batch Learning PR Conﬁguration File Settings . . . . .
17.2.2 Case Studies for the Three Learning Types . . . . . . . .
17.2.3 How to Use the Batch Learning PR in GATE Developer

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

339
341
341
342

.
.
.
.
.
.
.

343
344
344
344
345
345
345
347

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

349
349
350
351
351
351
352
352
354
355
355
355
356
356
357
358
359
359
360
360
361
361

.
.
.
.
.
.
.

363
364
365
365
365
366
380
387

xiv

Contents

17.2.4 Output of the Batch Learning PR . . . . . .
17.2.5 Using the Batch Learning PR from the API
17.3 Machine Learning PR . . . . . . . . . . . . . . . . .
17.3.1 The DATASET Element . . . . . . . . . . .
17.3.2 The ENGINE Element . . . . . . . . . . . .
17.3.3 The WEKA Wrapper . . . . . . . . . . . . .
17.3.4 The MAXENT Wrapper . . . . . . . . . . .
17.3.5 The SVM Light Wrapper . . . . . . . . . . .
17.3.6 Example Conﬁguration File . . . . . . . . .

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.

389
395
396
397
398
399
400
401
404

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

413
413
413
414
416
416
417
418
419
419
419
426
427

19 Combining GATE and UIMA
19.1 Embedding a UIMA AE in GATE . . . . . . . . . . .
19.1.1 Mapping File Format . . . . . . . . . . . . . .
19.1.2 The UIMA Component Descriptor . . . . . .
19.1.3 Using the AnalysisEnginePR . . . . . . . . .
19.2 Embedding a GATE CorpusController in UIMA . .
19.2.1 Mapping File Format . . . . . . . . . . . . . .
19.2.2 The GATE Application Deﬁnition . . . . . . .
19.2.3 Conﬁguring the GATEApplicationAnnotator .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

429
430
430
434
435
436
436
437
438

.
.
.
.
.
.
.
.
.
.

441
442
442
442
442
443
446
448
448
448
449

18 Tools for Alignment Tasks
18.1 Introduction . . . . . . . . . . . . . . .
18.2 The Tools . . . . . . . . . . . . . . . .
18.2.1 Compound Document . . . . .
18.2.2 CompoundDocumentFromXml .
18.2.3 Compound Document Editor .
18.2.4 Composite Document . . . . . .
18.2.5 DeleteMembersPR . . . . . . .
18.2.6 SwitchMembersPR . . . . . . .
18.2.7 Saving as XML . . . . . . . . .
18.2.8 Alignment Editor . . . . . . . .
18.2.9 Saving Files and Alignments . .
18.2.10 Section-by-Section Processing .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

20 More (CREOLE) Plugins
20.1 Verb Group Chunker . . . . . . . . . . . . . .
20.2 Noun Phrase Chunker . . . . . . . . . . . . .
20.2.1 Diﬀerences from the Original . . . . .
20.2.2 Using the Chunker . . . . . . . . . . .
20.3 TaggerFramework . . . . . . . . . . . . . . . .
20.3.1 TreeTagger - Multilingual POS Tagger
20.4 Chemistry Tagger . . . . . . . . . . . . . . . .
20.4.1 Using the Tagger . . . . . . . . . . . .
20.5 Annotating Numbers . . . . . . . . . . . . . .
20.5.1 Numbers in Words and Numbers . . .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

Contents

20.5.2 Roman Numerals . . . . . . . . . . . . . .
20.6 Annotating Measurements . . . . . . . . . . . . .
20.7 Annotating and Normalizing Dates . . . . . . . .
20.8 ABNER - A Biomedical Named Entity Recogniser
20.9 Snowball Based Stemmers . . . . . . . . . . . . .
20.9.1 Algorithms . . . . . . . . . . . . . . . . .
20.10GATE Morphological Analyzer . . . . . . . . . .
20.10.1 Rule File . . . . . . . . . . . . . . . . . . .
20.11Flexible Exporter . . . . . . . . . . . . . . . . . .
20.12Annotation Set Transfer . . . . . . . . . . . . . .
20.13Schema Enforcer . . . . . . . . . . . . . . . . . .
20.14Information Retrieval in GATE . . . . . . . . . .
20.14.1 Using the IR Functionality in GATE . . .
20.14.2 Using the IR API . . . . . . . . . . . . . .
20.15Websphinx Web Crawler . . . . . . . . . . . . . .
20.15.1 Using the Crawler PR . . . . . . . . . . .
20.16Google Plugin . . . . . . . . . . . . . . . . . . . .
20.17Yahoo Plugin . . . . . . . . . . . . . . . . . . . .
20.17.1 Using the YahooPR . . . . . . . . . . . . .
20.18Google Translator PR . . . . . . . . . . . . . . .
20.19WordNet in GATE . . . . . . . . . . . . . . . . .
20.19.1 The WordNet API . . . . . . . . . . . . .
20.20Kea - Automatic Keyphrase Detection . . . . . .
20.20.1 Using the ‘KEA Keyphrase Extractor’ PR
20.20.2 Using Kea Corpora . . . . . . . . . . . . .
20.21Ontotext JapeC Compiler . . . . . . . . . . . . .
20.22Annotation Merging Plugin . . . . . . . . . . . .
20.23Copying Annotations between Documents . . . .
20.24OpenCalais Plugin . . . . . . . . . . . . . . . . .
20.25LingPipe Plugin . . . . . . . . . . . . . . . . . . .
20.25.1 LingPipe Tokenizer PR . . . . . . . . . . .
20.25.2 LingPipe Sentence Splitter PR . . . . . . .
20.25.3 LingPipe POS Tagger PR . . . . . . . . .
20.25.4 LingPipe NER PR . . . . . . . . . . . . .
20.25.5 LingPipe Language Identiﬁer PR . . . . .
20.26OpenNLP Plugin . . . . . . . . . . . . . . . . . .
20.26.1 Parameters common to all PRs . . . . . .
20.26.2 OpenNLP PRs . . . . . . . . . . . . . . .
20.26.3 Training new models . . . . . . . . . . . .
20.27Tagger MetaMap Plugin . . . . . . . . . . . . . .
20.27.1 Run-time parameters . . . . . . . . . . . .
20.28Content Detection Using Boilerpipe . . . . . . . .
20.29Inter Annotator Agreement . . . . . . . . . . . .
20.30Schema Annotation Editor . . . . . . . . . . . . .

xv

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

452
452
455
457
458
459
459
460
462
463
465
466
468
470
472
472
474
474
475
475
476
480
482
482
484
485
486
487
488
489
490
490
490
491
491
492
493
493
496
496
496
498
499
499

xvi

IV

Contents

The GATE Family: Cloud, MIMIR, Teamware

21 GATE Cloud
21.1 GATE Cloud services: an overview . . . . . . . . . . .
21.2 Comparison with other systems . . . . . . . . . . . . .
21.3 How to buy services . . . . . . . . . . . . . . . . . . . .
21.4 Pricing and discounts . . . . . . . . . . . . . . . . . . .
21.5 Annotation Jobs on GATECloud.net . . . . . . . . . .
21.5.1 The Annotation Service Charges Explained . . .
21.5.2 Annotation Job Execution in Detail . . . . . . .
21.6 Running Custom Annotation Jobs on GATECloud.net
21.6.1 Preparing Your Application: The Basics . . . .
21.6.2 The GATECloud.net environment . . . . . . . .

501
.
.
.
.
.
.
.
.
.
.

503
504
504
505
506
507
507
508
508
509
509

22 GATE Teamware: A Web-based Collaborative Corpus Annotation Tool
22.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22.2 Requirements for Multi-Role Collaborative Annotation Environments . . . .
22.2.1 Typical Division of Labour . . . . . . . . . . . . . . . . . . . . . . . .
22.2.2 Remote, Scalable Data Storage . . . . . . . . . . . . . . . . . . . . .
22.2.3 Automatic annotation services . . . . . . . . . . . . . . . . . . . . . .
22.2.4 Workﬂow Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22.3 Teamware: Architecture, Implementation, and Examples . . . . . . . . . . .
22.3.1 Data Storage Service . . . . . . . . . . . . . . . . . . . . . . . . . . .
22.3.2 Annotation Services . . . . . . . . . . . . . . . . . . . . . . . . . . .
22.3.3 The Executive Layer . . . . . . . . . . . . . . . . . . . . . . . . . . .
22.3.4 The User Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22.4 Practical Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

513
513
515
515
517
517
518
518
519
520
521
522
524

23 GATE M´
ımir

527

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

Appendices
A Change Log
A.1 April 2011 . . . . . . . . . . . . . . . .
A.2 March 2011 . . . . . . . . . . . . . . .
A.3 February 2011 . . . . . . . . . . . . . .
A.4 January 2011 . . . . . . . . . . . . . .
A.5 December 2010 . . . . . . . . . . . . .
A.6 Version 6.0 (November 2010) . . . . . .
A.6.1 Major new features . . . . . . .
A.6.2 Breaking changes . . . . . . . .
A.6.3 Other new features and bugﬁxes
A.7 Version 5.2.1 (May 2010) . . . . . . . .
A.8 Version 5.2 (April 2010) . . . . . . . .

527
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.

529
529
530
530
531
531
532
532
532
533
534
535

Contents

xvii

A.8.1 JAPE and JAPE-related . . . . . . . .
A.8.2 Other Changes . . . . . . . . . . . . .
A.9 Version 5.1 (December 2009) . . . . . . . . . .
A.9.1 New Features . . . . . . . . . . . . . .
A.9.2 JAPE improvements . . . . . . . . . .
A.9.3 Other improvements and bug ﬁxes . .
A.10 Version 5.0 (May 2009) . . . . . . . . . . . . .
A.10.1 Major New Features . . . . . . . . . .
A.10.2 Other New Features and Improvements
A.10.3 Speciﬁc Bug Fixes . . . . . . . . . . .
A.11 Version 4.0 (July 2007) . . . . . . . . . . . . .
A.11.1 Major New Features . . . . . . . . . .
A.11.2 Other New Features and Improvements
A.11.3 Bug Fixes and Optimizations . . . . .
A.12 Version 3.1 (April 2006) . . . . . . . . . . . .
A.12.1 Major New Features . . . . . . . . . .
A.12.2 Other New Features and Improvements
A.12.3 Bug Fixes . . . . . . . . . . . . . . . .
A.13 January 2005 . . . . . . . . . . . . . . . . . .
A.14 December 2004 . . . . . . . . . . . . . . . . .
A.15 September 2004 . . . . . . . . . . . . . . . . .
A.16 Version 3 Beta 1 (August 2004) . . . . . . . .
A.17 July 2004 . . . . . . . . . . . . . . . . . . . .
A.18 June 2004 . . . . . . . . . . . . . . . . . . . .
A.19 April 2004 . . . . . . . . . . . . . . . . . . . .
A.20 March 2004 . . . . . . . . . . . . . . . . . . .
A.21 Version 2.2 – August 2003 . . . . . . . . . . .
A.22 Version 2.1 – February 2003 . . . . . . . . . .
A.23 June 2002 . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

B Version 5.1 Plugins Name Map
C Design Notes
C.1 Patterns . . . . . . . . . . . .
C.1.1 Components . . . . . .
C.1.2 Model, view, controller
C.1.3 Interfaces . . . . . . .
C.2 Exception Handling . . . . . .

535
536
536
537
539
540
540
541
543
544
544
544
546
548
549
549
549
551
552
552
553
553
554
554
555
555
555
556
556
559

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

D JAPE: Implementation
D.1 Formal Description of the JAPE Grammar
D.2 Relation to CPSL . . . . . . . . . . . . . .
D.3 Initialisation of a JAPE Grammar . . . . .
D.4 Execution of JAPE Grammars . . . . . . .
D.5 Using a Diﬀerent Java Compiler . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

561
561
562
564
565
565

.
.
.
.
.

569
570
572
573
575
577

Contents

1

E Ant Tasks for GATE
579
E.1 Declaring the Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
E.2 The packagegapp task - bundling an application with its dependencies . . . 579
E.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
E.2.2 Basic Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
E.2.3 Handling Non-Plugin Resources . . . . . . . . . . . . . . . . . . . . . 581
E.2.4 Streamlining your Plugins . . . . . . . . . . . . . . . . . . . . . . . . 584
E.2.5 Bundling Extra Resources . . . . . . . . . . . . . . . . . . . . . . . . 584
E.3 The expandcreoles Task - Merging Annotation-Driven Conﬁg into creole.xml 585
F Named-Entity State Machine Patterns
F.1 Main.jape . . . . . . . . . . . . . . . .
F.2 ﬁrst.jape . . . . . . . . . . . . . . . . .
F.3 ﬁrstname.jape . . . . . . . . . . . . . .
F.4 name.jape . . . . . . . . . . . . . . . .
F.4.1 Person . . . . . . . . . . . . . .
F.4.2 Location . . . . . . . . . . . . .
F.4.3 Organization . . . . . . . . . .
F.4.4 Ambiguities . . . . . . . . . . .
F.4.5 Contextual information . . . . .
F.5 name post.jape . . . . . . . . . . . . .
F.6 date pre.jape . . . . . . . . . . . . . .
F.7 date.jape . . . . . . . . . . . . . . . . .
F.8 reldate.jape . . . . . . . . . . . . . . .
F.9 number.jape . . . . . . . . . . . . . . .
F.10 address.jape . . . . . . . . . . . . . . .
F.11 url.jape . . . . . . . . . . . . . . . . .
F.12 identiﬁer.jape . . . . . . . . . . . . . .
F.13 jobtitle.jape . . . . . . . . . . . . . . .
F.14 ﬁnal.jape . . . . . . . . . . . . . . . . .
F.15 unknown.jape . . . . . . . . . . . . . .
F.16 name context.jape . . . . . . . . . . .
F.17 org context.jape . . . . . . . . . . . . .
F.18 loc context.jape . . . . . . . . . . . . .
F.19 clean.jape . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

587
587
588
589
589
589
589
590
590
590
590
591
591
591
591
592
592
592
592
592
593
593
593
594
594

G Part-of-Speech Tags used in the Hepple Tagger

595

References

596

2

Contents

Part I
GATE Basics

3

Chapter 1
Introduction
Software documentation is like sex: when it is good, it is very, very good; and
when it is bad, it is better than nothing. (Anonymous.)
There are two ways of constructing a software design: one way is to make it so
simple that there are obviously no deﬁciencies; the other way is to make it so
complicated that there are no obvious deﬁciencies. (C.A.R. Hoare)
A computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about
methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute. (The Structure and Interpretation of Computer
Programs, H. Abelson, G. Sussman and J. Sussman, 1985.)
If you try to make something beautiful, it is often ugly. If you try to make
something useful, it is often beautiful. (Oscar Wilde)1
GATE2 is an infrastructure for developing and deploying software components that process
human language. It is nearly 15 years old and is in active use for all types of computational
task involving human language. GATE excels at text analysis of all shapes and sizes. From
large corporations to small startups, from multi-million research consortia to undergraduate
projects, our user community is the largest and most diverse of any system of this type, and
is spread across all but one of the continents3 .
GATE is open source free software; users can obtain free support from the user and developer
community via GATE.ac.uk or on a commercial basis from our industrial partners. We
are the biggest open source language processing project with a development team more
than double the size of the largest comparable projects (many of which are integrated with
1

These were, at least, our ideals; of course we didn’t completely live up to them. . .
If you’ve read the overview at http://gate.ac.uk/overview.html, you may prefer to skip to Section 1.1.
3
Rumours that we’re planning to send several of the development team to Antarctica on one-way tickets
are false, libellous and wishful thinking.
2

5

6

Introduction

GATE4 ). More than 5 million has been invested in GATE development5 ; our objective is
to make sure that this continues to be money well spent for all GATE’s users.
The GATE family of tools has grown over the years to include a desktop client for developers,
a workﬂow-based web application, a Java library, an architecture and a process. GATE is:
an IDE, GATE Developer: an integrated development environment6 for language
processing components bundled with a very widely used Information Extraction system
and a comprehensive set of other plugins
a cloud computing solution for hosted large-scale text processing, GATE Cloud
(http://gatecloud.net/). See also Chapter 21.
a web app, GATE Teamware: a collaborative annotation environment for factorystyle semantic annotation projects built around a workﬂow engine and a heavilyoptimised backend service infrastructure. See also Chapter 22.
a multi-paradigm search repository, GATE M´
ımir, which can be used to index and
search over text, annotations, semantic schemas (ontologies), and semantic meta-data
(instance data). It allows queries that arbitrarily mix full-text, structural, linguistic
and semantic queries and that can scale to terabytes of text. See also Chapter 23.
a framework, GATE Embedded: an object library optimised for inclusion in diverse
applications giving access to all the services used by GATE Developer and more.
an architecture: a high-level organisational picture of how language processing software
composition.
a process for the creation of robust and maintainable services.
We also develop:
a wiki/CMS, GATE Wiki (http://gatewiki.sf.net/), mainly to host our own websites
and as a testbed for some of our experiments
For more information on the GATE family see http://gate.ac.uk/family/ and also Part IV
of this book.
One of our original motivations was to remove the necessity for solving common engineering
problems before doing useful research, or re-engineering before deploying research results
into applications. Core functions of GATE take care of the lion’s share of the engineering:
4
Our philosophy is reuse not reinvention, so we integrate and interoperate with other systems e.g.:
LingPipe, OpenNLP, UIMA, and many more speciﬁc tools.
5
This is the ﬁgure for direct Sheﬃeld-based investment only and therefore an underestimate.
6
GATE Developer and GATE Embedded are bundled, and in older distributions were referred to just as
‘GATE’.

Introduction

7

modelling and persistence of specialised data structures
measurement, evaluation, benchmarking (never believe a computing researcher who
hasn’t measured their results in a repeatable and open setting!)
visualisation and editing of annotations, ontologies, parse trees, etc.
a ﬁnite state transduction language for rapid prototyping and eﬃcient implementation
of shallow analysis methods (JAPE)
extraction of training instances for machine learning
pluggable machine learning implementations (Weka, SVM Light, ...)
On top of the core functions GATE includes components for diverse language processing
tasks, e.g. parsers, morphology, tagging, Information Retrieval tools, Information Extraction
components for various languages, and many others. GATE Developer and Embedded are
supplied with an Information Extraction system (ANNIE) which has been adapted and
evaluated very widely (numerous industrial systems, research systems evaluated in MUC,
TREC, ACE, DUC, Pascal, NTCIR, etc.). ANNIE is often used to create RDF or OWL
(metadata) for unstructured content (semantic annotation).
GATE version 1 was written in the mid-1990s; at the turn of the new millennium we completely rewrote the system in Java; version 5 was released in June 2009; and version 6 — in
November 2010. We believe that GATE is the leading system of its type, but as scientists
we have to advise you not to take our word for it; that’s why we’ve measured our software
in many of the competitive evaluations over the last decade-and-a-half (MUC, TREC, ACE,
DUC and more; see Section 1.4 for details). We invite you to give it a try, to get involved
with the GATE community, and to contribute to human language science, engineering and
development.
This book describes how to use GATE to develop language processing components, test their
performance and deploy them as parts of other applications. In the rest of this chapter:
Section 1.1 describes the best way to use this book;
Section 1.2 brieﬂy notes that the context of GATE is applied language processing, or
Language Engineering;
Section 1.3 gives an overview of developing using GATE;
Section 1.4 lists publications describing GATE performance in evaluations;
Section 1.5 outlines what is new in the current version of GATE;
Section 1.6 lists other publications about GATE.

8

Introduction

Note: if you don’t see the component you need in this document, or if we mention a component that you can’t see in the software, contact gate-users@lists.sourceforge.net7 –
various components are developed by our collaborators, who we will be happy to put you
in contact with. (Often the process of getting a new component is as simple as typing the
URL into GATE Developer; the system will do the rest.)

1.1

How to Use this Text

The material presented in this book ranges from the conceptual (e.g. ‘what is software
architecture?’) to practical instructions for programmers (e.g. how to deal with GATE
exceptions) and linguists (e.g. how to write a pattern grammar). Furthermore, GATE’s
highly extensible nature means that new functionality is constantly being added in the form
of new plugins. Important functionality is as likely to be located in a plugin as it is to
be integrated into the GATE core. This presents something of an organisational challenge.
Our (no doubt imperfect) solution is to divide this book into three parts. Part I covers
installation, using the GATE Developer GUI and using ANNIE, as well as providing some
background and theory. We recommend the new user to begin with Part I. Part II covers
the more advanced of the core GATE functionality; the GATE Embedded API and JAPE
pattern language among other things. Part III provides a reference for the numerous plugins
that have been created for GATE. Although ANNIE provides a good starting point, the
user will soon wish to explore other resources, and so will need to consult this part of the
text. We recommend that Part III be used as a reference, to be dipped into as necessary. In
Part III, plugins are grouped into broad areas of functionality.

1.2

Context

GATE can be thought of as a Software Architecture for Language Engineering
[Cunningham 00].
‘Software Architecture’ is used rather loosely here to mean computer infrastructure for software development, including development environments and frameworks, as well as the more
usual use of the term to denote a macro-level organisational structure for software systems
[Shaw & Garlan 96].
Language Engineering (LE) may be deﬁned as:
. . . the discipline or act of engineering software systems that perform tasks involving processing human language. Both the construction process and its outputs
7

Follow the ‘support’ link from http://gate.ac.uk/ to subscribe to the mailing list.

Introduction

9

are measurable and predictable. The literature of the ﬁeld relates to both application of relevant scientiﬁc results and a body of practice. [Cunningham 99a]
The relevant scientiﬁc results in this case are the outputs of Computational Linguistics, Natural Language Processing and Artiﬁcial Intelligence in general. Unlike these other disciplines,
LE, as an engineering discipline, entails predictability, both of the process of constructing LEbased software and of the performance of that software after its completion and deployment
in applications.
Some working deﬁnitions:
1. Computational Linguistics (CL): science of language that uses computation as an
investigative tool.
2. Natural Language Processing (NLP): science of computation whose subject matter is data structures and algorithms for computer processing of human language.
3. Language Engineering (LE): building NLP systems whose cost and outputs are
measurable and predictable.
4. Software Architecture: macro-level organisational principles for families of systems.
In this context is also used as infrastructure.
5. Software Architecture for Language Engineering (SALE): software infrastructure, architecture and development tools for applied CL, NLP and LE.
(Of course the practice of these ﬁelds is broader and more complex than these deﬁnitions.)
In the scientiﬁc endeavours of NLP and CL, GATE’s role is to support experimentation. In
this context GATE’s signiﬁcant features include support for automated measurement (see
Chapter 10), providing a ‘level playing ﬁeld’ where results can easily be repeated across
diﬀerent sites and environments, and reducing research overheads in various ways.

1.3
1.3.1

Overview
Developing and Deploying Language Processing Facilities

GATE as an architecture suggests that the elements of software systems that process natural
language can usefully be broken down into various types of component, known as resources8 .
8
The terms ‘resource’ and ‘component’ are synonymous in this context. ‘Resource’ is used instead of just
‘component’ because it is a common term in the literature of the ﬁeld: cf. the Language Resources and
Evaluation conference series [LREC-1 98, LREC-2 00].

10

Introduction

Components are reusable software chunks with well-deﬁned interfaces, and are a popular
architectural form, used in Sun’s Java Beans and Microsoft’s .Net, for example. GATE
components are specialised types of Java Bean, and come in three ﬂavours:

LanguageResources (LRs) represent entities such as lexicons, corpora or ontologies;
ProcessingResources (PRs) represent entities that are primarily algorithmic, such as
parsers, generators or ngram modellers;
VisualResources (VRs) represent visualisation and editing components that participate
in GUIs.

These deﬁnitions can be blurred in practice as necessary.
Collectively, the set of resources integrated with GATE is known as CREOLE: a Collection
of REusable Objects for Language Engineering. All the resources are packaged as Java
Archive (or ‘JAR’) ﬁles, plus some XML conﬁguration data. The JAR and XML ﬁles are
made available to GATE by putting them on a web server, or simply placing them in the
local ﬁle space. Section 1.3.2 introduces GATE’s built-in resource set.
When using GATE to develop language processing functionality for an application, the
developer uses GATE Developer and GATE Embedded to construct resources of the three
types. This may involve programming, or the development of Language Resources such as
grammars that are used by existing Processing Resources, or a mixture of both. GATE
Developer is used for visualisation of the data structures produced and consumed during
processing, and for debugging, performance measurement and so on. For example, ﬁgure 1.1
is a screenshot of one of the visualisation tools.
GATE Developer is analogous to systems like Mathematica for Mathematicians, or JBuilder
for Java programmers: it provides a convenient graphical environment for research and
development of language processing software.
When an appropriate set of resources have been developed, they can then be embedded in
the target client application using GATE Embedded. GATE Embedded is supplied as a
series of JAR ﬁles.9 To embed GATE-based language processing facilities in an application,
these JAR ﬁles are all that is needed, along with JAR ﬁles and XML conﬁguration ﬁles for
the various resources that make up the new facilities.
9
The main JAR ﬁle (gate.jar) supplies the framework. Built-in resources and various 3rd-party libraries
are supplied as separate JARs; for example (guk.jar, the GATE Unicode Kit.) contains Unicode support
(e.g. additional input methods for languages not currently supported by the JDK). They are separate because
the latter has to be a Java extension with a privileged security proﬁle.

Introduction

11

Figure 1.1: One of GATE’s visual resources

1.3.2

Built-In Components

GATE includes resources for common LE data structures and algorithms, including documents, corpora and various annotation types, a set of language analysis components for
Information Extraction and a range of data visualisation and editing components.
GATE supports documents in a variety of formats including XML, RTF, email, HTML,
SGML and plain text. In all cases the format is analysed and converted into a single uniﬁed model of annotation. The annotation format is a modiﬁed form of the TIPSTER format [Grishman 97] which has been made largely compatible with the Atlas format
[Bird & Liberman 99], and uses the now standard mechanism of ‘stand-oﬀ markup’. GATE
documents, corpora and annotations are stored in databases of various sorts, visualised via
the development environment, and accessed at code level via the framework. See Chapter 5
for more details of corpora etc.
A family of Processing Resources for language analysis is included in the shape of ANNIE,
A Nearly-New Information Extraction system. These components use ﬁnite state techniques
to implement various tasks from tokenisation to semantic tagging or verb phrase chunking.
All ANNIE components communicate exclusively via GATE’s document and annotation
resources. See Chapter 6 for more details. Other CREOLE resources are described in
Part III.

12

Introduction

1.3.3

Additional Facilities in GATE Developer/Embedded

Three other facilities in GATE deserve special mention:
JAPE, a Java Annotation Patterns Engine, provides regular-expression based pattern/action rules over annotations – see Chapter 8.
The ‘annotation diﬀ’ tool in the development environment implements performance
metrics such as precision and recall for comparing annotations. Typically a language
analysis component developer will mark up some documents by hand and then use these
along with the diﬀ tool to automatically measure the performance of the components.
See Chapter 10.
GUK, the GATE Unicode Kit, ﬁlls in some of the gaps in the JDK’s10 support for
Unicode, e.g. by adding input methods for various languages from Urdu to Chinese.
See Section 3.10.2 for more details.

1.3.4

An Example

This section gives a very brief example of a typical use of GATE to develop and deploy
language processing capabilities in an application, and to generate quantitative results for
scientiﬁc publication.
Let’s imagine that a developer called Fatima is building an email client11 for Cyberdyne
Systems’ large corporate Intranet. In this application she would like to have a language
processing system that automatically spots the names of people in the corporation and
transforms them into mailto hyperlinks.
A little investigation shows that GATE’s existing components can be tailored to this purpose.
Fatima starts up GATE Developer, and creates a new document containing some example
emails. She then loads some processing resources that will do named-entity recognition (a
tokeniser, gazetteer and semantic tagger), and creates an application to run these components
on the document in sequence. Having processed the emails, she can see the results in one of
several viewers for annotations.
The GATE components are a decent start, but they need to be altered to deal specially
with people from Cyberdyne’s personnel database. Therefore Fatima creates new ‘cyber-’
versions of the gazetteer and semantic tagger resources, using the ‘bootstrap’ tool. This tool
creates a directory structure on disk that has some Java stub code, a Makeﬁle and an XML
10
JDK: Java Development Kit, Sun Microsystem’s Java implementation. Unicode support is being actively
improved by Sun, but at the time of writing many languages are still unsupported. In fact, Unicode itself
doesn’t support all languages, e.g. Sylheti; hopefully this will change in time.
11
Perhaps because Outlook Express trashed her mail folder again, or because she got tired of Microsoftspeciﬁc viruses and hadn’t heard of Gmail or Thunderbird.

Introduction

13

conﬁguration ﬁle. After several hours struggling with badly written documentation, Fatima
manages to compile the stubs and create a JAR ﬁle containing the new resources. She tells
GATE Developer the URL of these ﬁles12 , and the system then allows her to load them in
the same way that she loaded the built-in resources earlier on.
Fatima then creates a second copy of the email document, and uses the annotation editing
facilities to mark up the results that she would like to see her system producing. She saves
this and the version that she ran GATE on into her serial datastore. From now on she can
follow this routine:
1. Run her application on the email test corpus.
2. Check the performance of the system by running the ‘annotation diﬀ’ tool to compare
her manual results with the system’s results. This gives her both percentage accuracy
ﬁgures and a graphical display of the diﬀerences between the machine and human
outputs.
3. Make edits to the code, pattern grammars or gazetteer lists in her resources, and
recompile where necessary.
4. Tell GATE Developer to re-initialise the resources.
5. Go to 1.
To make the alterations that she requires, Fatima re-implements the ANNIE gazetteer so that
it regenerates itself from the local personnel data. She then alters the pattern grammar in the
semantic tagger to prioritise recognition of names from that source. This latter job involves
learning the JAPE language (see Chapter 8), but as this is based on regular expressions it
isn’t too diﬃcult.
Eventually the system is running nicely, and her accuracy is 93% (there are still some problem cases, e.g. when people use nicknames, but the performance is good enough for production use). Now Fatima stops using GATE Developer and works instead on embedding
the new components in her email application using GATE Embedded. This application is
written in Java, so embedding is very easy13 : the GATE JAR ﬁles are added to the project
CLASSPATH, the new components are placed on a web server, and with a little code to do
initialisation, loading of components and so on, the job is ﬁnished in half a day – the code
to talk to GATE takes up only around 150 lines of the eventual application, most of which
is just copied from the example in the sheffield.examples.StandAloneAnnie class.
Because Fatima is worried about Cyberdyne’s unethical policy of developing Skynet to help
the large corporates of the West strengthen their strangle-hold over the World, she wants
to get a job as an academic instead (so that her conscience will only have to cope with the
12

While developing, she uses a file:/... URL; for deployment she can put them on a web server.
Languages other than Java require an additional interface layer, such as JNI, the Java Native Interface,
which is in C.
13

14

Introduction

torture of students, as opposed to humanity). She takes the accuracy measures that she
has attained for her system and writes a paper for the Journal of Nasturtium Logarithm
Incitement describing the approach used and the results obtained. Because she used GATE
for development, she can cite the repeatability of her experiments and oﬀer access to example
binary versions of her software by putting them on an external web server.
And everybody lived happily ever after.

1.4

Some Evaluations

This section contains an incomplete list of publications describing systems that used GATE in
competitive quantitative evaluation programmes. These programmes have had a signiﬁcant
impact on the language processing ﬁeld and the widespread presence of GATE is some
measure of the maturity of the system and of our understanding of its likely performance on
diverse text processing tasks.
[Li et al. 07d] describes the performance of an SVM-based learning system in the NTCIR-6
Patent Retrieval Task. The system achieved the best result on two of three measures
used in the task evaluation, namely the R-Precision and F-measure. The system obtained close to the best result on the remaining measure (A-Precision).
[Saggion 07] describes a cross-source coreference resolution system based on semantic clustering. It uses GATE for information extraction and the SUMMA system to create summaries and semantic representations of documents. One system conﬁguration
ranked 4th in the Web People Search 2007 evaluation.
[Saggion 06] describes a cross-lingual summarization system which uses SUMMA components and the Arabic plugin available in GATE to produce summaries in English from
a mixture of English and Arabic documents.
Open-Domain Question Answering: The University of Sheﬃeld has a long history
of research into open-domain question answering. GATE has formed the basis of much of this research resulting in systems which have ranked highly during independent evaluations since 1999. The ﬁrst successful question answering
system developed at the University of Sheﬃeld was evaluated as part of TREC
8 and used the LaSIE information extraction system (the forerunner of ANNIE)
which was distributed with GATE [Humphreys et al. 99]. Further research was
reported in [Scott & Gaizauskas. 00], [Greenwood et al. 02], [Gaizauskas et al. 03],
[Gaizauskas et al. 04] and [Gaizauskas et al. 05]. In 2004 the system was ranked 9th
out of 28 participating groups.
[Saggion 04] describes techniques for answering deﬁnition questions. The system uses definition patterns manually implemented in GATE as well as learned JAPE patterns

Introduction

15

induced from a corpus. In 2004, the system was ranked 4th in the TREC/QA evaluations.
[Saggion & Gaizauskas 04b] describes a multidocument summarization system implemented using summarization components compatible with GATE (the SUMMA system). The system was ranked 2nd in the Document Understanding Evaluation programmes.
[Maynard et al. 03e] and [Maynard et al. 03d] describe participation in the TIDES
surprise language program. ANNIE was adapted to Cebuano with four person days of
eﬀort, and achieved an F-measure of 77.5%. Unfortunately, ours was the only system
participating!
[Maynard et al. 02b] and [Maynard et al. 03b] describe results obtained on systems
designed for the ACE task (Automatic Content Extraction). Although a comparison to other participating systems cannot be revealed due to the stipulations of ACE,
results show 82%-86% precision and recall.
[Humphreys et al. 98] describes the LaSIE-II system used in MUC-7.
[Gaizauskas et al. 95] describes the LaSIE-II system used in MUC-6.

1.5

Recent Changes

This section details recent changes made to GATE. Appendix A provides a complete change
log.

1.5.1

April 2011

Three new optional attributes can be speciﬁed in <GATECONFIG> element of gate.xml or
local conﬁguration ﬁle:

addNamespaceFeatures - set to ”true” to deserialize namespace preﬁx and URI
information as features.
namespaceURI - The feature name to use that will hold the namespace URI of the
element, e.g. ”namespace”
namespacePreﬁx - The feature name to use that will hold the namespace preﬁx of
the element, e.g. ”preﬁx”

16

Introduction

Setting these attributes will alter GATE’s default namespace deserialization behaviour to
remove the namespace preﬁx and add it as a feature, along with the namespace URI. This
allows namespace-preﬁxed elements in the Original markups annotation set to be matched
with JAPE expressions, and also allows namespace scope to be added to new annotations
when serialized to XML. See 5.5.2 for details.
Searchable Serial Datastores (Lucene-based) are now portable and can be moved across
diﬀerent systems. Also, several GUI improvements have been made to ease the creation of
Lucene datastores. See 9 for details.
The Websphinx Crawler PR (section 20.15) has new runtime parameters for controlling the
maximum page size and spooﬁng the user-agent.

1.5.2

March 2011

A new creole repository, Teamware Tools, contains a new PR called QA Summariser for
Teamware. When documents are annotated using Teamware, this PR can be used for generating a summary of agreements among annotators. It does this by pairing individual
annotators. It also compares each individual annotator’s annotations with those available in
the consensus annotation set in the respective documents. See Section 10.7 for full details.
A new creole repository, Tagger Measurements, contains a new PR for annotating and normalizing measurements within a document. See Section 20.6 for full details.
A new creole repository, Tagger DateNormalizer, contains a new PR for annotating and
normalizing dates within a document. See Section 20.7 for full details.
The populate method that allowed populating corpus from a trecweb ﬁle has been made
more generic to accept a tag. The method extracts content between the start and end of
this tag to create new documents. In GATE Developer, right-clicking on an instance of the
Corpus and choosing the option “Populate from Single Concatenated File” allows users to
populate the corpus using this functionality. See Section 7.4.5 for more details.

1.5.3

February 2011

GATE now requires Java 6 or above.
Fixed a regression in the JAPE parser that prevented the use of RHS macros that refer to
a LHS label (named blocks :label { ... } and assignments :label.Type = {}
A new creole repository, Tagger Numbers, containing a number of PRs for annotating numbers with their numeric value. See Section 20.5 for full details.

Introduction

17

A new creole repository, Tagger Boilerpipe, which contains a boilerpipe14 based PR for
performing content detection. See Section 20.28 for full details.
The Tagger MetaMap plugin has been rewritten to make use of the new MetaMap Java API
features. There are numerous performance enhancements and a bug ﬁx where changes to
the metaMapOptions run-time parameter were previously not enacted. The previous version
of the plugin has been moved to plugins/Obsolete.
Please note that the updated Tagger MetaMap plugin is not parameter-compatible with
the previous version, so your application pipelines will need to be updated. Specifically, the following parameters have been removed: mmServerHost , mmServerPort ,
mmServerTimeout , excludeSemanticTypes, restrictSemanticTypes, scoreThreshold.
These can now be speciﬁed using the --metamap server host, --metamap server port,
--metamap server timeout, -k, -J and -r options in the metaMapOptions run-time parameter string.
outputASType has been made a run-time parameter.
useNegEx has been renamed to annotateNegEx
The following changes have been made to outputMode
MappingsOnly has been renamed to AllMappings
CandidatesOnly has been renamed to AllCandidates
CandidatesAndMappings has been renamed to AllCandidatesAndMappings
In addition, new parameters have been added. See Section 20.27 for full details.

1.5.4

January 2011

Added a new Schema Enforcer PR that can be used to create a ‘clean’ output annotation
set based on a set of annotation schemas. See Section 20.13 for full details.
Enhanced the Groovy scriptable controller with some features inspired by the realtime controller, in particular the ability to ignore exceptions thrown by PRs and the ability to limit
the running time of certain PRs. See section 7.16.3 for details.
A few bug ﬁxes and improvements to the “recover” logic of the packagegapp Ant task (see
section E.2).
14

http://code.google.com/p/boilerpipe/

18

1.5.5

Introduction

December 2010

Added support for handling controller events to JAPE by making it possible to deﬁne
ControllerStarted, ControllerFinished, and ControllerAborted code blocks in a JAPE
ﬁle (see section 8.6.5).
JAPE Java right-hand-side code can now access an ActionContext object through the predeﬁned ﬁeld ctx which allows access to the corpus LR and the transducer PR and their
features (see section 8.6.5).

1.5.6

Version 6.0 (November 2010)

Major new features
Added an annotation tool for the document editor: the Relation Annotation Tool (RAT). It
is designed to annotate a document with ontology instances and to create relations between
annotations with ontology object properties. It is close and compatible with the Ontology
Annotation Tool (OAT) but focus on relations between annotations. See section 14.7 for
details.
Added a new scriptable controller to the Groovy plugin, whose execution strategy is controlled by a simple Groovy DSL. This supports more powerful conditional execution than
is possible with the standard conditional controllers (for example, based on the presence or
absence of a particular annotation, or a combination of several document feature values),
rich ﬂow control using Groovy loops, etc. See section 7.16.3 for details.
A new version of Alignment Editor has been added to the GATE distribution. It consists
of several new features such as the new alignment viewer, ability to create alignment tasks
and store in xml ﬁles, three diﬀerent views to align the text (links view and matrix view
- suitable for character, word and phrase alignments, parallel view - suitable for sentence
or long text alignment), an alignment exporter and many more. See chapter 18 for more
information.
MetaMap, from the National Library of Medicine (NLM), maps biomedical text to the
UMLS Metathesaurus and allows Metathesaurus concepts to be discovered in a text corpus. The Tagger MetaMap plugin for GATE wraps the MetaMap Java API client to allow
GATE to communicate with a remote (or local) MetaMap PrologBeans mmserver and
MetaMap distribution. This allows the content of speciﬁed annotations (or the entire document content) to be processed by MetaMap and the results converted to GATE annotations
and features. See section 20.27 for details.
A new plugin called Web Translate Google has been added with a PR called Google Translator PR in it. It allows users to translate text using the Google translation services. See
section 20.18 for more information.

Introduction

19

New Gazetteer Editor for ANNIE Gazetteer that can be used instead of Gaze. It uses tables
instead of text area to display the gazetteer deﬁnition and lists, allows sorting on any column,
ﬁltering of the lists, reloading a list, etc. See section 13.2.2.

Breaking changes
This release contains a few small changes that are not backwards-compatible:
Changed the semantics of the ontology-aware matching mode in JAPE to take account of the default namespace in an ontology. Now class feature values that are
not complete URIs will be treated as naming classes within the default namespace of
the target ontology only, and not (as previously) any class whose URI ends with the
speciﬁed name. This is more consistent with the way OWL normally works, as well as
being much more eﬃcient to execute. See section 14.10 for more details.
Updated the WordNet plugin to support more recent releases of WordNet than 1.6.
The format of the conﬁguration ﬁle has changed, if you are using the previous WordNet
1.6 support you will need to update your conﬁguration. See section 20.19 for details.
The deprecated Tagger TreeTagger plugin has been removed, applications that used it
will need to be updated to use the Tagger Framework plugin instead. See section 20.3
for details of how to do this.

Other new features and bugﬁxes
The concept of templates has been introduced to JAPE. This is a way to declare named
“variables” in a JAPE grammar that can contain placeholders that are ﬁlled in when the
template is referenced. See section 8.1.5 for full details.
Added a JAPE operator to get the string covered by a left-hand-side label and assign it to
a feature of a new annotation on the right hand side (see section 8.1.4).
Added a new API to the CREOLE registry to permit plugins that live entirely on the
classpath. CreoleRegister.registerComponent instructs the registry to scan a single java
Class for annotations, adding it to the set of registered plugins. See section 7.3 for details.
Maven artifacts for GATE are now published to the central Maven repository. See section 2.5.1 for details.
Bugﬁx: DocumentImpl no longer changes its stringContent parameter value whenever the
document’s content changes. Among other things, this means that saved application states
will no longer contain the full text of the documents in their corpus, and documents containing XML or HTML tags that were originally created from string content (rather than a

20

Introduction

URL) can now safely be stored in saved application states and the GATE Developer saved
session.
A processing resource called Quality Assurance PR has been added in the Tools plugin. The
PR wraps the functionality of the Quality Assurance Tool (section 10.3).
A new section for using the Corpus Quality Assurance from GATE Embedded has been
written. See section 10.3.
The Generic Tagger PR (in the Tagger Framework plugin) now allows more ﬂexible speciﬁcation of the input to the tagger, and is no longer limited to passing just the “string” feature
from the input annotations. See section 20.3 for details.
Added new parameters and options to the LingPipe Language Identiﬁer PR. (section 20.25.5), and corrected the documentation for the LingPipe POS Tagger (section 20.25.3).
In the document editor, ﬁxed several exceptions to make editing text with annotations
highlighted working. So you should now be able to edit the text and the annotations should
behave correctly that is to say move, expand or disappear according to the text insertions
and deletions.
Options for document editor: read-only and insert append/prepend have been moved from
the options dialogue to the document editor toolbar at the top right on the triangle icon
that display a menu with the options. See section 3.2.
Added new parameters and options to the Crawl PR and document features to its output;
see section 20.15 for details.
Fixed a bug where ontology-aware JAPE rules worked correctly when the target annotation’s
class was a subclass of the class speciﬁed in the rule, but failed when the two class names
matched exactly.
Improved support for conditional pipelines containing non-LanguageAnalyser processing resources.
Added the current Corpus to the script binding for the Groovy Script PR, allowing a Groovy
script to access and set corpus-level features. Also added callbacks that a Groovy script
can implement to do additional pre- or post-processing before the ﬁrst and after the last
document in a corpus. See section 7.16 for details.

1.6

Further Reading

Lots of documentation lives on the GATE web site, including:

Introduction

21

GATE online tutorials;
the main system documentation tree;
JavaDoc API documentation;
HTML of the source code;
comprehensive list of GATE plugins.
For more details about Sheﬃeld University’s work in human language processing see the NLP
group pages or A Deﬁnition and Short History of Language Engineering ([Cunningham 99a]).
For more details about Information Extraction see IE, a User Guide or the GATE IE pages.
A list of publications on GATE and projects that use it (some of which are available on-line
from http://gate.ac.uk/gate/doc/papers.html):
2010
[Bontcheva et al. 10] describes the Teamware web-based collaborative annotation environment, emphasising the diﬀerent roles that users play in the corpus annotation process.
[Damljanovic 10] presents the use of GATE in the development of controlled natural language interfaces. There is other related work by Damljanovic, Agatonovic, and Cunningham on using GATE to build natural language interfaces for quering ontologies.
[Aswani & Gaizauskas 10] discusses the use of GATE to process South Asian languages
(Hindi and Gujarati).
2009
[Saggion & Funk 09] focuses in detail on the use of GATE for mining opinions and facts
for business intelligence gathering from web content.
[Aswani & Gaizauskas 09] presents in more detail the text alignment component of
GATE.
[Bontcheva et al. 09] is the ‘Human Language Technologies’ chapter of ‘Semantic Knowledge Management’ (John Davies, Marko Grobelnik and Dunja Mladeni eds.)
[Damljanovic et al. 09] discusses the use of semantic annotation for software engineering,
as part of the TAO research project.
[Laclavik & Maynard 09] reviews the current state of the art in email processing and
communication research, focusing on the roles played by email in information management, and commercial and research eﬀorts to integrate a semantic-based approach to
email.

22

Introduction

[Li et al. 09] investigates two techniques for making SVMs more suitable for language learning tasks. Firstly, an SVM with uneven margins (SVMUM) is proposed to deal with
the problem of imbalanced training data. Secondly, SVM active learning is employed
in order to alleviate the diﬃculty in obtaining labelled training data. The algorithms
are presented and evaluated on several Information Extraction (IE) tasks.
2008
[Agatonovic et al. 08] presents our approach to automatic patent enrichment, tested in
large-scale, parallel experiments on USPTO and EPO documents.
[Damljanovic et al. 08] presents Question-based Interface to Ontologies (QuestIO) - a tool
for querying ontologies using unconstrained language-based queries.
[Damljanovic & Bontcheva 08] presents a semantic-based prototype that is made for
an open-source software engineering project with the goal of exploring methods for
assisting open-source developers and software users to learn and maintain the system
without major eﬀort.
[Della Valle et al. 08] presents ServiceFinder.
[Li & Cunningham 08] describes our SVM-based system and several techniques we developed successfully to adapt SVM for the speciﬁc features of the F-term patent classiﬁcation task.
[Li & Bontcheva 08] reviews the recent developments in applying geometric and quantum
mechanics methods for information retrieval and natural language processing.
[Maynard 08] investigates the state of the art in automatic textual annotation tools, and
examines the extent to which they are ready for use in the real world.
[Maynard et al. 08a] discusses methods of measuring the performance of ontology-based
information extraction systems, focusing particularly on the Balanced Distance Metric
(BDM), a new metric we have proposed which aims to take into account the more
ﬂexible nature of ontologically-based applications.
[Maynard et al. 08b] investigates NLP techniques for ontology population, using a combination of rule-based approaches and machine learning.
[Tablan et al. 08] presents the QuestIO system a natural language interface for accessing
structured information, that is domain independent and easy to use without training.
2007
[Funk et al. 07a] describes an ontologically based approach to multi-source, multilingual
information extraction.

Introduction

23

[Funk et al. 07b] presents a controlled language for ontology editing and a software implementation, based partly on standard NLP tools, for processing that language and
manipulating an ontology.
[Maynard et al. 07a] proposes a methodology to capture (1) the evolution of metadata
induced by changes to the ontologies, and (2) the evolution of the ontology induced by
changes to the underlying metadata.
[Maynard et al. 07b] describes the development of a system for content mining using domain ontologies, which enables the extraction of relevant information to be fed into
models for analysis of ﬁnancial and operational risk and other business intelligence
applications such as company intelligence, by means of the XBRL standard.
[Saggion 07] describes experiments for the cross-document coreference task in SemEval
2007. Our cross-document coreference system uses an in-house agglomerative clustering
implementation to group documents referring to the same entity.
[Saggion et al. 07] describes the application of ontology-based extraction and merging in
the context of a practical e-business application for the EU MUSING Project where the
goal is to gather international company intelligence and country/region information.
[Li et al. 07a] introduces a hierarchical learning approach for IE, which uses the target
ontology as an essential part of the extraction process, by taking into account the
relations between concepts.
[Li et al. 07b] proposes some new evaluation measures based on relations among classiﬁcation labels, which can be seen as the label relation sensitive version of important
measures such as averaged precision and F-measure, and presents the results of applying the new evaluation measures to all submitted runs for the NTCIR-6 F-term patent
classiﬁcation task.
[Li et al. 07c] describes the algorithms and linguistic features used in our participating
system for the opinion analysis pilot task at NTCIR-6.
[Li et al. 07d] describes our SVM-based system and the techniques we used to adapt the
approach for the speciﬁcs of the F-term patent classiﬁcation subtask at NTCIR-6
Patent Retrieval Task.
[Li & Shawe-Taylor 07] studies Japanese-English cross-language patent retrieval using
Kernel Canonical Correlation Analysis (KCCA), a method of correlating linear relationships between two variables in kernel deﬁned feature spaces.
2006
[Aswani et al. 06] (Proceedings of the 5th International Semantic Web Conference
(ISWC2006)) In this paper the problem of disambiguating author instances in ontology is addressed. We describe a web-based approach that uses various features such
as publication titles, abstract, initials and co-authorship information.

24

Introduction

[Bontcheva et al. 06a] ‘Semantic Annotation and Human Language Technology’, contribution to ‘Semantic Web Technology: Trends and Research’ (Davies, Studer and Warren, eds.)
[Bontcheva et al. 06b] ‘Semantic Information Access’, contribution to ‘Semantic Web
Technology: Trends and Research’ (Davies, Studer and Warren, eds.)
[Bontcheva & Sabou 06] presents an ontology learning approach that 1) exploits a range
of information sources associated with software projects and 2) relies on techniques
that are portable across application domains.
[Davis et al. 06] describes work in progress concerning the application of Controlled Language Information Extraction - CLIE to a Personal Semantic Wiki - Semper- Wiki,
the goal being to permit users who have no specialist knowledge in ontology tools or
languages to semi-automatically annotate their respective personal Wiki pages.
[Li & Shawe-Taylor 06] studies a machine learning algorithm based on KCCA for crosslanguage information retrieval. The algorithm is applied to Japanese-English crosslanguage information retrieval.
[Maynard et al. 06] discusses existing evaluation metrics, and proposes a new method for
evaluating the ontology population task, which is general enough to be used in a variety
of situation, yet more precise than many current metrics.
[Tablan et al. 06b] describes an approach that allows users to create and edit ontologies
simply by using a restricted version of the English language. The controlled language
described is based on an open vocabulary and a restricted set of grammatical constructs.
[Tablan et al. 06a] describes the creation of linguistic analysis and corpus search tools for
Sumerian, as part of the development of the ETCSL.
[Wang et al. 06] proposes an SVM based approach to hierarchical relation extraction, using
features derived automatically from a number of GATE-based open-source language
processing tools.
2005
[Aswani et al. 05] (Proceedings of Fifth International Conference on Recent Advances in
Natural Language Processing (RANLP2005)) It is a full-featured annotation indexing
and search engine, developed as a part of the GATE. It is powered with Apache Lucene
technology and indexes a variety of documents supported by the GATE.
[Bontcheva 05] presents the ONTOSUM system which uses Natural Language Generation
(NLG) techniques to produce textual summaries from Semantic Web ontologies.

Introduction

25

[Cunningham 05] is an overview of the ﬁeld of Information Extraction for the 2nd Edition
of the Encyclopaedia of Language and Linguistics.
[Cunningham & Bontcheva 05] is an overview of the ﬁeld of Software Architecture for
Language Engineering for the 2nd Edition of the Encyclopaedia of Language and Linguistics.
[Dowman et al. 05a] (Euro Interactive Television Conference Paper) A system which can
use material from the Internet to augment television news broadcasts.
[Dowman et al. 05b] (World Wide Web Conference Paper) The Web is used to assist the
annotation and indexing of broadcast news.
[Dowman et al. 05c] (Second European Semantic Web Conference Paper) A system that
semantically annotates television news broadcasts using news websites as a resource to
aid in the annotation process.
[Li et al. 05a] (Proceedings of Sheﬃeld Machine Learning Workshop) describe an SVM
based IE system which uses the SVM with uneven margins as learning component and
the GATE as NLP processing module.
[Li et al. 05b] (Proceedings of Ninth Conference on Computational Natural Language
Learning (CoNLL-2005)) uses the uneven margins versions of two popular learning
algorithms SVM and Perceptron for IE to deal with the imbalanced classiﬁcation problems derived from IE.
[Li et al. 05c] (Proceedings of Fourth SIGHAN Workshop on Chinese Language processing
(Sighan-05)) a system for Chinese word segmentation based on Perceptron learning, a
simple, fast and eﬀective learning algorithm.
[Polajnar et al. 05] (University of Sheﬃeld-Research Memorandum CS-05-10) UserFriendly Ontology Authoring Using a Controlled Language.
[Saggion & Gaizauskas 05] describes experiments on content selection for producing biographical summaries from multiple documents.
[Ursu et al. 05] (Proceedings of the 2nd European Workshop on the Integration of Knowledge, Semantic and Digital Media Technologies (EWIMT 2005))Digital Media Preservation and Access through Semantically Enhanced Web-Annotation.
[Wang et al. 05] (Proceedings of the 2005 IEEE/WIC/ACM International Conference on
Web Intelligence (WI 2005)) Extracting a Domain Ontology from Linguistic Resource
Based on Relatedness Measurements.
2004
[Bontcheva 04] (LREC 2004) describes lexical and ontological resources in GATE used for
Natural Language Generation.

26

Introduction

[Bontcheva et al. 04] (JNLE) discusses developments in GATE in the early naughties.
[Cunningham & Scott 04a] (JNLE) is the introduction to the above collection.
[Cunningham & Scott 04b] (JNLE) is a collection of papers covering many important
areas of Software Architecture for Language Engineering.
[Dimitrov et al. 04] (Anaphora Processing) gives a lightweight method for named entity
coreference resolution.
[Li et al. 04] (Machine Learning Workshop 2004) describes an SVM based learning algorithm for IE using GATE.
[Maynard et al. 04a] (LREC 2004) presents algorithms for the automatic induction of
gazetteer lists from multi-language data.
[Maynard et al. 04b] (ESWS 2004) discusses ontology-based IE in the hTechSight project.
[Maynard et al. 04c] (AIMSA 2004) presents automatic creation and monitoring of semantic metadata in a dynamic knowledge portal.
[Saggion & Gaizauskas 04a] describes an approach to mining deﬁnitions.
[Saggion & Gaizauskas 04b] describes a sentence extraction system that produces two
sorts of multi-document summaries; a general-purpose summary of a cluster of related
documents and an entity-based summary of documents related to a particular person.
[Wood et al. 04] (NLDB 2004) looks at ontology-based IE from parallel texts.
2003
[Bontcheva et al. 03] (NLPXML-2003) looks at GATE for the semantic web.
[Cunningham et al. 03] (Corpus Linguistics 2003) describes GATE as a tool for collaborative corpus annotation.
[Kiryakov 03] (Technical Report) discusses semantic web technology in the context of multimedia indexing and search.
[Manov et al. 03] (HLT-NAACL 2003) describes experiments with geographic knowledge
for IE.
[Maynard et al. 03a] (EACL 2003) looks at the distinction between information and content extraction.
[Maynard et al. 03c] (Recent Advances in Natural Language Processing 2003) looks at
semantics and named-entity extraction.

Introduction

27

[Maynard et al. 03e] (ACL Workshop 2003) describes NE extraction without training
data on a language you don’t speak (!).
[Saggion et al. 03a] (EACL 2003) discusses robust, generic and query-based summarisation.
[Saggion et al. 03b] (Data and Knowledge Engineering) discusses multimedia indexing
and search from multisource multilingual data.
[Saggion et al. 03c] (EACL 2003) discusses event co-reference in the MUMIS project.
[Tablan et al. 03] (HLT-NAACL 2003) presents the OLLIE on-line learning for IE system.
[Wood et al. 03] (Recent Advances in Natural Language Processing 2003) discusses using
parallel texts to improve IE recall.
2002
[Baker et al. 02] (LREC 2002) report results from the EMILLE Indic languages corpus
collection and processing project.
[Bontcheva et al. 02a] (ACl 2002 Workshop) describes how GATE can be used as an environment for teaching NLP, with examples of and ideas for future student projects
developed within GATE.
[Bontcheva et al. 02b] (NLIS 2002) discusses how GATE can be used to create HLT modules for use in information systems.
[Bontcheva et al. 02c], [Dimitrov 02a] and [Dimitrov 02b] (TALN 2002, DAARC
2002, MSc thesis) describe the shallow named entity coreference modules in GATE:
the orthomatcher which resolves pronominal coreference, and the pronoun resolution
module.
[Cunningham 02] (Computers and the Humanities) describes the philosophy and motivation behind the system, describes GATE version 1 and how well it lived up to its
design brief.
[Cunningham et al. 02] (ACL 2002) describes the GATE framework and graphical development environment as a tool for robust NLP applications.
[Dimitrov 02a, Dimitrov et al. 02] (DAARC 2002, MSc thesis) discuss lightweight coreference methods.
[Lal 02] (Master Thesis) looks at text summarisation using GATE.
[Lal & Ruger 02] (ACL 2002) looks at text summarisation using GATE.

28

Introduction

[Maynard et al. 02a] (ACL 2002 Summarisation Workshop) describes using GATE to
build a portable IE-based summarisation system in the domain of health and safety.
[Maynard et al. 02c] (AIMSA 2002) describes the adaptation of the core ANNIE modules
within GATE to the ACE (Automatic Content Extraction) tasks.
[Maynard et al. 02d] (Nordic Language Technology) describes various Named Entity
recognition projects developed at Sheﬃeld using GATE.
[Maynard et al. 02e] (JNLE) describes robustness and predictability in LE systems, and
presents GATE as an example of a system which contributes to robustness and to low
overhead systems development.
[Pastra et al. 02] (LREC 2002) discusses the feasibility of grammar reuse in applications
using ANNIE modules.
[Saggion et al. 02b] and [Saggion et al. 02a] (LREC 2002, SPLPT 2002) describes how
ANNIE modules have been adapted to extract information for indexing multimedia
material.
[Tablan et al. 02] (LREC 2002) describes GATE’s enhanced Unicode support.
Older than 2002
[Maynard et al. 01] (RANLP 2001) discusses a project using ANNIE for named-entity
recognition across wide varieties of text type and genre.
[Bontcheva et al. 00] and [Brugman et al. 99] (COLING 2000, technical report) describe a prototype of GATE version 2 that integrated with the EUDICO multimedia
markup tool from the Max Planck Institute.
[Cunningham 00] (PhD thesis) deﬁnes the ﬁeld of Software Architecture for Language
Engineering, reviews previous work in the area, presents a requirements analysis for
such systems (which was used as the basis for designing GATE versions 2 and 3), and
evaluates the strengths and weaknesses of GATE version 1.
[Cunningham et al. 00a], [Cunningham et al. 98a] and [Peters et al. 98] (OntoLex
2000, LREC 1998) presents GATE’s model of Language Resources, their access and
distribution.
[Cunningham et al. 00b] (LREC 2000) taxonomises Language Engineering components
and discusses the requirements analysis for GATE version 2.
[Cunningham et al. 00c] and [Cunningham et al. 99] (COLING 2000, AISB 1999)
summarise experiences with GATE version 1.
[Cunningham et al. 00d] and [Cunningham 99b] (technical reports) document early
versions of JAPE (superseded by the present document).

Introduction

29

[Gamb¨ck & Olsson 00] (LREC 2000) discusses experiences in the Svensk project, which
a
used GATE version 1 to develop a reusable toolbox of Swedish language processing
components.
[Maynard et al. 00] (technical report) surveys users of GATE up to mid-2000.
[McEnery et al. 00] (Vivek) presents the EMILLE project in the context of which GATE’s
Unicode support for Indic languages has been developed.
[Cunningham 99a] (JNLE) reviewed and synthesised deﬁnitions of Language Engineering.
[Stevenson et al. 98] and [Cunningham et al. 98b] (ECAI 1998, NeMLaP 1998) report work on implementing a word sense tagger in GATE version 1.
[Cunningham et al. 97b] (ANLP 1997) presents motivation for GATE and GATE-like
infrastructural systems for Language Engineering.
[Cunningham et al. 96a] (manual) was the guide to developing CREOLE components for
GATE version 1.
[Cunningham et al. 96b] (TIPSTER) discusses a selection of projects in Sheﬃeld using
GATE version 1 and the TIPSTER architecture it implemented.
[Cunningham et al. 96c, Cunningham et al. 96d, Cunningham et al. 95] (COLING
1996, AISB Workshop 1996, technical report) report early work on GATE version 1.
[Gaizauskas et al. 96a] (manual) was the user guide for GATE version 1.
[Gaizauskas et al. 96b, Cunningham et al. 97a, Cunningham et al. 96e] (ICTAI
1996, TIPSTER 1997, NeMLaP 1996) report work on GATE version 1.
[Humphreys et al. 96] (manual) describes the language processing components distributed with GATE version 1.
[Cunningham 94, Cunningham et al. 94] (NeMLaP 1994, technical report) argue that
software engineering issues such as reuse, and framework construction, are important
for language processing R&D.

30

Introduction

Chapter 2
Installing and Running GATE
2.1

Downloading GATE

To download GATE point your web browser at http://gate.ac.uk/download/.

2.2

Installing and Running GATE

GATE will run anywhere that supports Java 6 or later, including Solaris, Linux, Mac OS
X and Windows platforms. We don’t run tests on other platforms, but have had reports of
successful installs elsewhere.

2.2.1

The Easy Way

The easy way to install is to use one of the platform-speciﬁc installers (created using the
excellent IzPack). Download a ‘platform-speciﬁc installer’ and follow the instructions it
gives you. Once the installation is complete, you can start GATE Developer using gate.exe
(Windows) or GATE.app (Mac) in the top-level installation directory, on Linux and other
platforms use gate.sh in the bin directory (see section 2.2.4).
Note for Mac users: on 64-bit-capable systems, GATE.app will run as a 64-bit application.
It will use the ﬁrst listed 64-bit JVM in your Java Preferences, even if your highest priority
JVM is a 32-bit one.
31

32

Installing and Running GATE

2.2.2

The Hard Way (1)

Download the Java-only release package or the binary build snapshot, and follow the instructions below.
Prerequisites:
A conforming Java 2 environment,
– version 1.4.2 or above for GATE 3.1
– version 5.0 for GATE 4.0 beta 1 or later.
– version 6.0 for GATE 6.1 or later.
available free from Sun Microsystems or from your UNIX supplier. (We test on various
Sun JDKs on Solaris, Linux and Windows XP.)
Binaries from the GATE distribution you downloaded: gate.jar, lib/ext/guk.jar
(Unicode editing support) and a suitable script to start Ant, e.g. ant.sh or ant.bat.
These are held in a directory called bin like this:
.../bin/
gate.jar
ant.sh
ant.bat
You will also need the lib directory, containing various libraries that GATE depends
on.
An open mind and a sense of humour.
Using the binary distribution:
Unpack the distribution, creating a directory containing jar ﬁles and scripts.
To run GATE Developer: on Windows, start a Command Prompt window, change
to the directory where you unpacked the GATE distribution and run ‘bin/ant.bat
run’; on UNIX/Linux or Mac open a terminal window and run ‘bin/ant run’. On
UNIX/Linux system you can alternately also use the bin/gate.sh script (see section 2.2.4).
To embed GATE as a library (GATE Embedded), put gate.jar and all the libraries
in the lib directory in your CLASSPATH and tell Java that guk.jar is an extension
(-Djava.ext.dirs=path-to-guk.jar).

Installing and Running GATE

33

The Ant scripts that start GATE Developer (ant.bat or ant) require you to set the
JAVA HOME environment variable to point to the top level directory of your JAVA installation. The value of GATE CONFIG is passed to the system by the scripts using either a -i
command-line option, or the Java property gate.config.

2.2.3

The Hard Way (2): Subversion

The GATE code is maintained in a Subversion repository. You can use a Subversion client
to check out the source code – the most up-to-date version of GATE is the trunk:
svn checkout https://gate.svn.sourceforge.net/svnroot/gate/gate/trunk gate
Once you have checked out the code you can build GATE using Ant (see Section 2.5)
You can browse the complete Subversion repository online at
http://gate.svn.sourceforge.net/.

2.2.4

Running GATE Developer on Unix/Linux

The script gate.sh in the directory bin of your installation can be used to start GATE
Developer. You can run this script by entering its full path in a terminal or by adding the
bin directory to your binary path. In addition you can also add a symbolic link to this script
in any directory that already is in your binary path.
If gate.sh is invoked without parameters, GATE Developer will use the ﬁles ~/.gate.xml
and ~/.gate.session to store session and conﬁguration data. Alternately you can run
gate.sh with the following parameters:
-h show usage information
-ld create or use the ﬁles .gate.session and .gate.xml in the current directory as the
session and conﬁguration ﬁles.
-ln name create or use name.session and name.xml in the current directory as the session
and conﬁguration ﬁles.
-ll if the current directory contains a ﬁle named log4j.properties then use it instead of the
default (GATE_HOME/bin/log4j.properties) to conﬁgure logging. Alternately, you
can specify any log4j conﬁguration ﬁle by setting the log4j.configuration property
explicitly (see below).
all other parameters are passed on to the ant command. This can be used to
e.g. set properties using the ant option -D. For example to set the maximum
amount of heap memory to be used when running GATE to 6000M, you can add

34

Installing and Running GATE

-Druntime.max.memory=6000m as a parameter. The name of any property that should
get passed on to GATE should be prepended with ’run.’. In order to change the
default encoding used by GATE to UTF-8 add -Drun.file.encoding=utf-8 as a parameter. To specify a log4j conﬁguration ﬁle add something like
-Drun.log4j.configuration=file:///home/myuser/log4jconfig.properties.
Running GATE Developer with either the -ld or the -ln option from diﬀerent directories is
useful to keep several projects separate and can be used to run multiple instances of GATE
Developer (or even diﬀerent versions of GATE Developer) in succession or even simultanously
without the conﬁguration ﬁles getting mixed up between them.

2.3

Using System Properties with GATE

During initialisation, GATE reads several Java system properties in order to decide where
to ﬁnd its conﬁguration ﬁles.
Here is a list of the properties used, their default values and their meanings:
gate.home sets the location of the GATE install directory. This should point to the top
level directory of your GATE installation. This is the only property that is required.
If this is not set, the system will display an error message and them it will attempt to
guess the correct value.
gate.plugins.home points to the location of the directory containing installed plugins (a.k.a. CREOLE directories). If this is not set then the default value of
{gate.home}/plugins is used.
gate.site.conﬁg points to the location of the conﬁguration ﬁle containing the site-wide
options. If not set this will default to {gate.home}/gate.xml. The site conﬁguration
ﬁle must exist!
gate.user.conﬁg points to the ﬁle containing the user’s options. If not speciﬁed, or if the
speciﬁed ﬁle does not exist at startup time, the default value of gate.xml (.gate.xml on
Unix platforms) in the user’s home directory is used.
gate.user.session points to the ﬁle containing the user’s saved session. If not speciﬁed,
the default value of gate.session (.gate.session on Unix) in the user’s home directory
is used. When starting up GATE Developer, the session is reloaded from this ﬁle if it
exists, and when exiting GATE Developer the session is saved to this ﬁle (unless the
user has disabled ‘save session on exit’ in the conﬁguration dialog). The session is not
used when using GATE Embedded.
gate.user.ﬁlechooser.defaultdir sets the default directory to be shown in the ﬁle chooser
of GATE Developer to the speciﬁed directory instead of the user’s operating-system
speciﬁc default directory.

Installing and Running GATE

35

load.plugin.path is a path-like structure, i.e. a list of URLs separated by ‘;’. All directories
listed here will be loaded as CREOLE plugins during initialisation. This has similar
functionality with the the -d command line option.

gate.builtin.creole.dir is a URL pointing to the location of GATE’s built-in CREOLE
directory. This is the location of the creole.xml ﬁle that deﬁnes the fundamental
GATE resource types, such as documents, document format handlers, controllers and
the basic visual resources that make up GATE. The default points to a location inside
gate.jar and should not generally need to be overridden.

When using GATE Embedded, you can set the values for these properties before you call
Gate.init(). Alternatively, you can set the values programmatically using the static
methods setGateHome(), setPluginsHome(), setSiteConfigFile(), etc. before calling
Gate.init(). See the Javadoc documentation for details. If you want to set these values
from the command line you can use the following syntax for setting gate.home for example:
java -Dgate.home=/my/new/gate/home/directory -cp...

gate.Main

To set these values when running GATE Developer from the command line via ant, of via
the gate.sh script, use the following syntax, prepending ‘run.’ to the property name, for
example to set gate.user.config:
./bin/ant run -Drun.gate.user.config=/my/path/to/user/config.file
or
./bin/gate.sh -Drun.gate.user.config=/my/path/to/user/config.file
Note that gate.home and some other properties are automatically set by build.xml and
cannot be overwritten on the command line when using either ant run or gate.sh.
When running GATE Developer, you can set the properties by creating a ﬁle
build.properties in the top level GATE directory. In this ﬁle, any system properties
which are preﬁxed with ‘run.’ will be passed to GATE. For example, to set an alternative
user conﬁg ﬁle, put the following line in build.properties1 :
run.gate.user.config= {user.home}/alternative-gate.xml
This facility is not limited to the GATE-speciﬁc properties listed above, for example the
following line changes the default temporary directory for GATE (note the use of forward
slashes, even on Windows platforms):
run.java.io.tmpdir=d:/bigtmp
1
In this speciﬁc case, the alternative conﬁg ﬁle must already exist when GATE starts up, so you should
copy your standard gate.xml ﬁle to the new location.

36

2.4

Installing and Running GATE

Conﬁguring GATE

When GATE Developer is started, or when Gate.init() is called from GATE Embedded,
GATE loads various sorts of conﬁguration data stored as XML in ﬁles generally called
something like gate.xml or .gate.xml. This data holds information such as:
whether to save settings on exit;
whether to save session on exit;
what fonts GATE Developer should use;
plugins to load at start;
colours of the annotations;
locations of ﬁles for the ﬁle chooser;
and a lot of other GUI related options;
This type of data is stored at two levels (in order from general to speciﬁc):
the site-wide level, which by default is located the gate.xml ﬁle in top level directory
of the GATE installation (i.e. the GATE home. This location can be overridden by the
Java system property gate.site.config;
the user level, which lives in the user’s HOME directory on UNIX or their proﬁle
directory on Windows (note that parts of this ﬁle are overwritten when saving user
settings). The default location for this ﬁle can be overridden by the Java system
property gate.user.config.
Where conﬁguration data appears on several diﬀerent levels, the more speciﬁc ones overwrite
the more general. This means that you can set defaults for all GATE users on your system,
for example, and allow individual users to override those defaults without interfering with
others.
Conﬁguration data can be set from the GATE Developer GUI via the ‘Options’ menu then
‘Conﬁguration’. The user can change the appearance of the GUI in the ‘Appearance’ tab,
which includes the options of font and the ‘look and feel’. The ‘Advanced’ tab enables the
user to include annotation features when saving the document and preserving its format, to
save the selected Options automatically on exit, and to save the session automatically on
exit. The ‘Input Methods’ submenu from the ‘Options’ menu enables the user to change the
default language for input. These options are all stored in the user’s .gate.xml ﬁle.
When using GATE Embedded, you can also set the site conﬁg location using
Gate.setSiteConfigFile(File) prior to calling Gate.init().

Installing and Running GATE

2.5

37

Building GATE

Note that you don’t need to build GATE unless you’re doing development on the system
itself.
Prerequisites:
A conforming Java environment as above.
A copy of the GATE sources and the build scripts – either the SRC distribution package
from the nightly snapshots or a copy of the code obtained through Subversion (see
Section 2.2.3).
An appreciation of natural beauty.
GATE now includes a copy of the ANT build tool which can be accessed through the scripts
included in the bin directory (use ant.bat for Windows 98 or ME, ant.cmd for Windows
NT, 2000 or XP, and ant.sh for Unix platforms).
To build gate, cd to gate and:
1. Type:
bin/ant
2. [optional] To test the system:
bin/ant test
3. [optional] To make the Javadoc documentation:
bin/ant doc
4. You can also run GATE Developer using Ant, by typing:
bin/ant run
5. To see a full list of options type: bin/ant help

(The details of the build process are all speciﬁed by the build.xml ﬁle in the gate directory.)
You can also use a development environment like Borland JBuilder (click on the gate.jpx
ﬁle), but note that it’s still advisable to use ant to generate documentation, the jar ﬁle and so
on. Also note that the run conﬁgurations have the location of a gate.xml site conﬁguration
ﬁle hard-coded into them, so you may need to change these for your site.

38

2.5.1

Installing and Running GATE

Using GATE with Maven/Ivy

This section is based on contributions by Marin Nozhchev (Ontotext) and Benson Margulies
(Basis Technology Corp).
Stable releases of GATE (since 5.2.1) are available in the standard central Maven repository,
with group ID “uk.ac.gate” and artifact ID “gate-core”. To use GATE in a Maven-based
project you can simply add a dependency:

<dependency>
<groupId>uk.ac.gate</groupId>
<artifactId>gate-core</artifactId>
<version>6.0</version>
</dependency>

Similarly, with a project that uses Ivy for dependency management:

<dependency org="uk.ac.gate" name="gate-core" rev="6.0"/>

The “gate-core” dependency brings in just gate.jar and its minimal dependencies, suﬃcient
to initialize GATE, load and save documents and corpora as XML, etc. The POM lists many
other dependencies which are marked as optional, so you can pick and choose which parts
of GATE you wish to depend on. In addition you will require the matching versions of any
GATE plugins you wish to use in your application – these are not managed by Maven or
Ivy, but can be obtained from the standard GATE release download.

2.6

Uninstalling GATE

If you have used the installer, run:

java -jar uninstaller.jar

or just delete the whole of the installation directory (the one containing bin, lib, Uninstaller,
etc.). The installer doesn’t install anything outside this directory, but for completeness you
might also want to delete the settings ﬁles GATE creates in your home directory (.gate.xml
and .gate.session).

Installing and Running GATE

2.7
2.7.1

39

Troubleshooting
I don’t see the Java console messages under Windows

Note that the gate.bat script uses javaw.exe to run GATE which means that you will see
no console for the java process. If you have problems starting GATE and you would like to
be able to see the console to check for messages then you should edit the gate.bat script
and replace javaw.exe with java.exe in the deﬁnition of the JAVA environment variable.

2.7.2

When I execute GATE, nothing happens

You might get some clues if you start GATE from the command line, using:
bin/ant -Druntime.spawn=false run

which will allow you to see all error messages GATE generates.

2.7.3

On Ubuntu, GATE is very slow or doesn’t start

GATE and many other Java applications are known not to work with GCJ, the open-source
Java SDK or others non SUN Java SDK.
Make sure you have the oﬃcial version of Java installed. Provided by Sun, the package is
named ‘sun-java6-jdk’ in Synaptic.
To install it, run in a terminal:
sudo apt-get install sun-java6-jdk

Make sure that your default Java version is the one from SUN. You can do this by running:
sudo update-java-alternatives -l

This will list the installed Java VMs. You should see ‘java-6-sun’ as one of the options.
Then you should run :
sudo update-java-alternatives -s java-6-sun

to set the ‘java-6-sun’ as your default.
Finally, try GATE again.

40

2.7.4

Installing and Running GATE

How to use GATE on a 64 bit system?

32-bit vs. 64-bit is a matter of the JVM rather than the build of GATE For example, on Mac OS X, either use Applications/Utilities/Java Preferences and put one
of the 64-bit options at the top of the list, or run GATE from the terminal using Java 1.6.0
(which is 64-bit only on Mac OS):
export JAVA_HOME=
/System/Library/Frameworks/JavaVM.framework/Versions/1.6.0/Home
bin/ant run

2.7.5

I got the error: Could not reserve enough space for object
heap

GATE doesn’t use the JAVA OPTS variable. The default memory allocations are deﬁned in
the gate/build.xml ﬁle but you can override them by creating a ﬁle called build.properties
in the same directory containing
runtime.start.memory=256m
runtime.max.memory=1048m

If you start GATE by running bin/ant run you can also overwrite these properties by using
the ant command line option -D, e.g.:
./bin/ant run -Druntime.start.memory=256m -Druntime.max.memory=1048m

On Linux, when using the bin/gate.sh command or a link to it to start GATE (see 2.2.4),
you can add these parameters to the command line arguments using the -D option, e.g.:
gate.sh -Druntime.start.memory=256m -Druntime.max.memory=1048m

If you don’t use ant to start GATE but your own application directly with the ‘java’ executable then you must use something like:
java -Xmx512m -classpath <your classpath here> <yourClassName>

Installing and Running GATE

2.7.6

41

From Eclipse, I got the error: java.lang.OutOfMemoryError:
Java heap space

Conﬁguring xms and xmx parameters in eclipse.ini ﬁle just adds memory to your Eclipse
process. If you start a Java application from within Eclipse, that will run in a diﬀerent
process.
To give more memory to your application, as opposed to just to Eclipse, you need to add
those values in the ‘VM Arguments’ section of the run application dialog: lower pane, in the
second tab of ‘Run Conﬁgurations’ dialog.

2.7.7

On MacOS, I got the error: java.lang.OutOfMemoryError:
Java heap space

You can try to set the environment variable ANT OPTS to allow for more memory as follows:
export ANT_OPTS=-Xmx1024m

2.7.8

I got the error: log4j:WARN No appenders could be found
for logger...

This may occur if you start GATE Developer or an application using GATE Embedded in a way so that the directory ’GATE_HOME/bin’ is not in the Java classpath
and you did not specify a valid URL for the system property log4j.configuration
either. The log4j logging component uses either the ﬁle whose URL is speciﬁed
with the property log4j.configuration or a ﬁle named log4j.properties from the
classpath for conﬁguration. A quick ﬁx for this problem is usually to copy the ﬁle
’GATE_HOME/bin/log4j.properties’ ﬁle to a directory which is part of the classpath of
your project.

2.7.9

Text is incorrectly refreshed after scrolling and become unreadable

Change the look and feel used in GATE with menu ‘Options’ then ‘Conﬁguration’. Restart
GATE and try again. We use mainly ‘Metal’ and ‘Nimbus’ without problem.
Change the video driver you use.
Update Java.

42

2.7.10

Installing and Running GATE

An error occurred when running the TreeTagger plugin

The TreeTagger plugin isn’t supported anymore. However, the TaggerFramework plugin
provides support for TreeTagger. Try using that plugin instead. See section 20.3.

2.7.11

I got the error: HighlightData cannot be cast to ...HighlightInfo

That’s a recurring problem when editing a document with annotation highlights showing
and it usually involves inserting/deleting some text close or belonging to an annotation in
the ﬁrst place.
The current solutions are to hide the annotation highlights before to edit the text or use
document read-only mode so you can only edit annotations or hide then show again the
document after the error.

Chapter 3
Using GATE Developer
‘The law of evolution is that the strongest survives!’
‘Yes; and the strongest, in the existence of any social species, are those who are
most social. In human terms, most ethical. . . . There is no strength to be gained
from hurting one another. Only weakness.’
The Dispossessed [p.183], Ursula K. le Guin, 1974.
This chapter introduces GATE Developer, which is the GATE graphical user interface. It is
analogous to systems like Mathematica for mathematicians, or Eclipse for Java programmers,
providing a convenient graphical environment for research and development of language
processing software. As well as being a powerful research tool in its own right, it is also very
useful in conjunction with GATE Embedded (the GATE API by which GATE functionality
can be included in your own applications); for example, GATE Developer can be used to
create applications that can then be embedded via the API. This chapter describes how
to complete common tasks using GATE Developer. It is intended to provide a good entry
point to GATE functionality, and so explanations are given assuming only basic knowledge
of GATE. However, probably the best way to learn how to use GATE Developer is to use
this chapter in conjunction with the demonstrations and tutorials movies. There are speciﬁc
links to them throughout the chapter. There is also a complete new set of video tutorials
here.
The basic business of GATE is annotating documents, and all the functionality we will
introduce relates to that. Core concepts are;
the documents to be annotated,
corpora comprising sets of documents, grouping documents for the purpose of running
uniform processes across them,
annotations that are created on documents,
43

44

Using GATE Developer

annotation types such as ‘Name’ or ‘Date’,
annotation sets comprising groups of annotations,
processing resources that manipulate and create annotations on documents, and
applications, comprising sequences of processing resources, that can be applied to a
document or corpus.
What is considered to be the end result of the process varies depending on the task, but
for the purposes of this chapter, output takes the form of the annotated document/corpus.
Researchers might be more interested in ﬁgures demonstrating how successfully their application compares to a ‘gold standard’ annotation set; Chapter 10 in Part II will cover ways of
comparing annotation sets to each other and obtaining measures such as F1. Implementers
might be more interested in using the annotations programmatically; Chapter 7, also in Part
II, talks about working with annotations from GATE Embedded. For the purposes of this
chapter, however, we will focus only on creating the annotated documents themselves, and
creating GATE applications for future use.
GATE includes a complete information extraction system that you are free to use, called
ANNIE (a Nearly-New Information Extraction System). Many users ﬁnd this is a good
starting point for their own application, and so we will cover it in this chapter. Chapter 6
talks in a lot more detail about the inner workings of ANNIE, but we aim to get you started
using ANNIE from inside of GATE Developer in this chapter.
We start the chapter with an exploration of the GATE Developer GUI, in Section 3.1. We
describe how to create documents (Section 3.2) and corpora (Section 3.3). We talk about
viewing and manually creating annotations (Section 3.4).
We then talk about loading the plugins that contain the processing resources you will use
to construct your application, in Section 3.5. We then talk about instantiating processing
resources (Section 3.6). Section 3.7 covers applications, including using ANNIE (Section
3.7.3). Saving applications and language resources (documents and corpora) is covered in
Section 3.8. We conclude with a few assorted topics that might be useful to the GATE
Developer user, in Section 3.10.

3.1

The GATE Developer Main Window

Figure 3.1 shows the main window of GATE Developer, as you will see it when you ﬁrst run
it. There are ﬁve main areas:
1. at the top, the menus bar and tools bar with menus ‘File’, ‘Options’, ‘Tools’, ‘Help’
and icons for the most frequently used actions;

Using GATE Developer

45

Figure 3.1: Main Window of GATE Developer

2. on the left side, a tree starting from ‘GATE’ and containing ‘Applications’, ‘Language
Resources’ etc. – this is the resources tree;
3. in the bottom left corner, a rectangle, which is the small resource viewer;
4. in the center, containing tabs with ‘Messages’ or the name of a resource from the
resources tree, the main resource viewer;
5. at the bottom, the messages bar.
The menu and the messages bar do the usual things. Longer messages are displayed in the
messages tab in the main resource viewer area.
The resource tree and resource viewer areas work together to allow the system to display
diverse resources in various ways. The many resources integrated with GATE can have either
a small view, a large view, or both.
At any time, the main viewer can also be used to display other information, such as messages,
by clicking on the appropriate tab at the top of the main window. If an error occurs in
processing, the messages tab will ﬂash red, and an additional popup error message may also
occur.

46

Using GATE Developer

In the options dialogue from the Options menu you can choose if you want to link the
selection in the resources tree and the selected main view.

3.2

Loading and Viewing Documents

Figure 3.2: Making a New Document
If you right-click on ‘Language Resources’ in the resources pane, select “New’ then ‘GATE
Document’, the window ‘Parameters for the new GATE Document’ will appear as shown in
ﬁgure 3.2. Here, you can specify the GATE document to be created. Required parameters
are indicated with a tick. The name of the document will be created for you if you do not
specify it. Enter the URL of your document or use the ﬁle browser to indicate the ﬁle you
wish to use for your document source. For example, you might use ‘http://gate.ac.uk’, or
browse to a text or XML ﬁle you have on disk. Click on ‘OK’ and a GATE document will
be created from the source you speciﬁed.
See also the movie for creating documents.
The document editor is contained in the central tabbed pane in GATE Developer. Doubleclick on your document in the resources pane to view the document editor. The document
editor consists of a top panel with buttons and icons that control the display of diﬀerent
views and the search box. Initially, you will see just the text of your document, as shown in
ﬁgure 3.3. Click on ‘Annotation Sets’ and ‘Annotations List’ to view the annotation sets to
the right and the annotations list at the bottom. You will see a view similar to ﬁgure 3.4.
In place of the annotations list, you can also choose to see the annotations stack. In place of
the annotation sets, you can also choose to view the co-reference editor. More information
about this functionality is given in Section 3.4.
Several options can be set from the triangle icon at the top right corner.

Using GATE Developer

47

Figure 3.3: The Document Editor

With ‘Save Current Layout’ you store the way the diﬀerent views are shown and the annotation types highlighted in the document. Then if you set ‘Restore Layout Automatically’
you will get the same views and annotation types each time you open a document.
Another setting make the document editor ‘Read-only’. If enabled, you won’t be able to edit
the text but you will still be able to edit annotations. It is useful to avoid to involuntarily
modify the original text.
Finally you can choose between ‘Insert Append’ and ‘Insert Prepend’. That setting is only
relevant when you’re inserting text at the very border of an annotation.
If you place the cursor at the start of an annotation, in one case the newly entered text will
become part of the annotation, in the other case it will stay outside. If you place the cursor
at the end of an annotation, the opposite will happen.
Let use this sentence: ‘This is an [annotation].’ with the square brackets [] denoting the
boundaries of the annotation. If we insert a ‘x’ just before the ‘a’ or just after the ‘n’ of
‘annotation’, here’s what we get:
Append

48

Using GATE Developer

This is an x[annotation].
This is an [annotationx].
Prepend
This is an [xannotation].
This is an [annotation]x.

Figure 3.4: The Document Editor with Annotation Sets and Annotations List
Text in a loaded document can be edited in the document viewer. The usual platform speciﬁc
cut, copy and paste keyboard shortcuts should also work, depending on your operating
system (e.g. CTRL-C, CTRL-V for Windows). The last icon, a magnifying glass, at the
top of the document editor is for searching in the document. To prevent the new annotation
windows popping up when a piece of text is selected, hold down the CTRL key. Alternatively,
you can hide the annotation sets view by clicking on its button at the top of the document
view; this will also cause the highlighted portions of the text to become un-highlighted.
See also Section 18.2.3 for the compound document editor.

Using GATE Developer

3.3

49

Creating and Viewing Corpora

You can create a new corpus in a similar manner to creating a new document; simply rightclick on ‘Language Resources’ in the resources pane, select ‘New’ then ‘GATE corpus’. A
brief dialogue box will appear in which you can optionally give a name for your corpus (if
you leave this blank, a corpus name will be created for you) and optionally add documents
to the corpus from those already loaded into GATE.
There are three ways of adding documents to a corpus:
1. When creating the corpus, clicking on the icon next to the “documentsList” input ﬁeld
brings up a popup window with a list of the documents already loaded into GATE
Developer. This enables the user to add any documents to the corpus.
2. Alternatively, the corpus can be loaded ﬁrst, and documents added later by double
clicking on the corpus and using the + and - icons to add or remove documents to the
corpus. Note that the documents must have been loaded into GATE Developer before
they can be added to the corpus.
3. Once loaded, the corpus can be populated by right clicking on the corpus and selecting
‘Populate’. With this method, documents do not have to have been previously loaded
into GATE Developer, as they will be loaded during the population process. If you
right-click on your corpus in the resources pane, you will see that you have the option
to ‘Populate’ the corpus. If you select this option, you will see a dialogue box in which
you can specify a directory in which GATE will search for documents. You can specify
the extensions allowable; for example, XML or TXT. This will restrict the corpus
population to only those documents with the extensions you wish to load. You can
choose whether to recurse through the directories contained within the target directory
or restrict the population to those documents contained in the top level directory. Click
on ‘OK’ to populate your corpus. This option provides a quick way to create a GATE
Corpus from a directory of documents.
Additionally, right-clicking on a loaded document in the tree and selecting the ‘New corpus
with this document’ option creates a new transient corpus named Corpus for document
name containing just this document.
See also the movie for creating and populating corpora.
Double click on your corpus in the resources pane to see the corpus editor, shown in ﬁgure 3.5.
You will see a list of the documents contained within the corpus.
In the top left of the corpus editor, plus and minus buttons allow you to add documents to
the corpus from those already loaded into GATE and remove documents from the corpus
(note that removing a document from a corpus does not remove it from GATE).

50

Using GATE Developer

Figure 3.5: Corpus Editor

Up and down arrows at the top of the view allow you to reorder the documents in the corpus.
The rightmost button in the view opens the currently selected document in a document
editor.
At the bottom, you will see that tabs entitled ‘Initialisation Parameters’ and ‘Corpus Quality
Assurance’ are also available in addition to the corpus editor tab you are currently looking at.
Clicking on the ‘Initialisation Parameters’ tab allows you to view the initialisation parameters
for the corpus. The ‘Corpus Quality Assurance’ tab allows you to calculate agreement
measures between the annotations in your corpus. Agreement measures are discussed in
depth in Chapter 10. The use of corpus quality assurance is discussed in Section 10.3.

Using GATE Developer

3.4

51

Working with Annotations

In this section, we will talk in more detail about viewing annotations, as well as creating and
editing them manually. As discussed in at the start of the chapter, the main purpose of GATE
is annotating documents. Whilst applications can be used to annotate the documents entirely
automatically, annotation can also be done manually, e.g. by the user, or semi-automatically,
by running an application over the corpus and then correcting/adding new annotations
manually. Section 3.4.5 focuses on manual annotation. In Section 3.6 we talk about running
processing resources on our documents. We begin by outlining the functionality around
viewing annotations, organised by the GUI area to which the functionality pertains.

3.4.1

The Annotation Sets View

To view the annotation sets, click on the ‘Annotation Sets’ button at the top of the document
editor, or use the F3 key (see Section 3.9 for more keyboard shortcuts). This will bring up the
annotation sets viewer, which displays the annotation sets available and their corresponding
annotation types.
The annotation sets view is displayed on the left part of the document editor. It’s a tree-like
view with a root for each annotation set. The ﬁrst annotation set in the list is always a
nameless set. This is the default annotation set. You can see in ﬁgure 3.4 that there is a
drop-down arrow with no name beside it. Other annotation sets on the document shown in
ﬁgure 3.4 are ‘Key’ and ‘Original markups’. Because the document is an XML document,
the original XML markup is retained in the form of an annotation set. This annotation set
is expanded, and you can see that there are annotations for ‘TEXT’, ‘body’, ‘font’, ‘html’,
‘p’, ‘table’, ‘td’ and ‘tr’.
To display all the annotations of one type, tick its checkbox or use the space key. The text
segments corresponding to these annotations will be highlighted in the main text window.
To delete an annotation type, use the delete key. To change the color, use the enter key.
There is a context menu for all these actions that you can display by right-clicking on one
annotation type, a selection or an annotation set.
If you keep shift key pressed when you open the annotation sets view, GATE Developer will
try to select any annotations that were selected in the previous document viewed (if any);
otherwise no annotation will be selected.
Having selected an annotation type in the annotation sets view, hovering over an annotation
in the main resource viewer or right-clicking on it will bring up a popup box containing a
list of the annotations associated with it, from which one can select an annotation to view
in the annotation editor, or if there is only one, the annotation editor for that annotation.
Figure 3.6 shows the annotation editor.

52

Using GATE Developer

Figure 3.6: The Annotation Editor

3.4.2

The Annotations List View

To view the list of annotations and their features, click on the ‘Annotations list’ button
at the top of the main window or use F4 key. The annotation list view will appear below
the main text. It will only contain the annotations selected from the annotation sets view.
These lists can be sorted in ascending and descending order for any column, by clicking on
the corresponding column heading. Moreover you can hide a column by using the context
menu by right-clicking on the column headings. Selecting rows in the table will blink the
respective annotations in the document. Right-click on a row or selection in this view to
delete or edit an annotation. Delete key is a shortcut to delete selected annotations.

3.4.3

The Annotations Stack View

Figure 3.7: Annotations stack view centred on the document caret.
This view is similar to the ANNIC view described in section 9.2. It displays annotations
at the document caret position with some context before and after. The annotations are
stacked from top to bottom, which gives a clear view when they are overlapping.

Using GATE Developer

53

As the view is centred on the document caret, you can use the conventional keypresses to
move it and update the view: notably the keys left and right to skip one letter; control +
left/right to skip one word; up and down to go one line up or down; and use the document
scrollbar then click in the document to move further. There are also two buttons at the top
of the view that centre the view on the closest previous/next annotation boundary among
all displayed. This is useful when you want to skip a region without annotation or when you
want to reach the beginning or end of a very long annotation.
The annotation types displayed correspond to those selected in the annotation sets view. You
can display feature values for an annotation rectangle by hovering the mouse on it or select
only one feature to display by double-clicking on the annotation type in the ﬁrst column.
Right-clicking on an annotation in the annotations stack view gives the option to edit that
annotation.

3.4.4

The Co-reference Editor

Figure 3.8: Co-reference editor inside a document editor. The popup window in the document under the word ‘EPSRC’ is used to add highlighted annotations to a co-reference
chain. Here the annotation type ‘Organization’ of the annotation set ‘Default’ is highlighted
and also the co-references ‘EC’ and ‘GATE’.
The co-reference editor allows co-reference chains (see Section 6.9) to be displayed and edited
in GATE Developer. To display the co-reference editor, ﬁrst open a document in GATE
Developer, and then click on the Co-reference Editor button in the document viewer.
The combo box at the top of the co-reference editor allows you to choose which annotation

54

Using GATE Developer

set to display co-references for. If an annotation set contains no co-reference data, then the
tree below the combo box will just show ‘Coreference Data’ and the name of the annotation
set. However, when co-reference data does exist, a list of all the co-reference chains that are
based on annotations in the currently selected set is displayed. The name of each co-reference
chain in this list is the same as the text of whichever element in the chain is the longest. It
is possible to highlight all the member annotations of any chain by selecting it in the list.
When a co-reference chain is selected, if the mouse is placed over one of its member annotations, then a pop-up box appears, giving the user the option of deleting the item from the
chain. If the only item in a chain is deleted, then the chain itself will cease to exist, and it
will be removed from the list of chains. If the name of the chain was derived from the item
that was deleted, then the chain will be given a new name based on the next longest item
in the chain.
A combo box near the top of the co-reference editor allows the user to select an annotation
type from the current set. When the Show button is selected all the annotations of the
selected type will be highlighted. Now when the mouse pointer is placed over one of those
annotations, a pop-up box will appear giving the user the option of adding the annotation
to a co-reference chain. The annotation can be added to an existing chain by typing the
name of the chain (as shown in the list on the right) in the pop-up box. Alternatively, if
the user presses the down cursor key, a list of all the existing annotations appears, together
with the option [New Chain]. Selecting the [New Chain] option will cause a new chain to
be created containing the selected annotation as its only element.
Each annotation can only be added to a single chain, but annotations of diﬀerent types can
be added to the same chain, and the same text can appear in more than one chain if it is
referenced by two or more annotations.
The movie for inspecting results is also useful for learning about viewing annotations.

3.4.5

Creating and Editing Annotations

To create annotations manually, select the text you want to annotate and hover the mouse
on the selection or use control+E keys. A popup will appear, allowing you to create an
annotation, as shown in ﬁgure 3.9

Figure 3.9: Creating a New Annotation

Using GATE Developer

55

The type of the annotation, by default, will be the same as the last annotation you created,
unless there is none, in which case it will be ‘ New ’. You can enter any annotation type name
you wish in the text box, unless you are using schema-driven annotation (see Section 3.4.6).
You can add or change features and their values in the table below.
To delete an annotation, click on the red X icon at the top of the popup window. To
grow/shrink the span of the annotation at its start use the two arrow icons on the left or
right and left keys. Use the two arrow icons next on the right to change the annotation end
or alt+right and alt+left keys. Add shift and control+shift keys to make the span increment
bigger. The red X icon is for removing the annotation.
The pin icon is to pin the window so that it remains where it is. If you drag and drop the
window, this automatically pins it too. Pinning it means that even if you select another
annotation (by hovering over it in the main resource viewer) it will still stay in the same
position.
The popup menu only contains annotation types present in the Annotation Schema and
those already listed in the relevant Annotation Set. To create a new Annotation Schema,
see Section 3.4.6. The popup menu can be edited to add a new annotation type, however.
The new annotation created will automatically be placed in the annotation set that has been
selected (highlighted) by the user. To create a new annotation set, type the name of the new
set to be created in the box below the list of annotation sets, and click on ‘New’.
Figure 3.10 demonstrates adding a ‘Organization’ annotation for the string ‘EPSRC’ (highlighted in green) to the default annotation set (blank name in the annotation set view on
the right) and a feature name ‘type’ with a value about to be added.
To add a second annotation to a selected piece of text, or to add an overlapping annotation to
an existing one, press the CTRL key to avoid the existing annotation popup appearing, and
then select the text and create the new annotation. Again by default the last annotation type
to have been used will be displayed; change this to the new annotation type. When a piece
of text has more than one annotation associated with it, on mouseover all the annotations
will be displayed. Selecting one of them will bring up the relevant annotation popup.
To search and annotate the document automatically, use the search and annotate function
as shown in ﬁgure 3.11:
Create and/or select an annotation to be used as a model to annotate.
Open the panel at the bottom of the annotation editor window.
Change the expression to search if necessary.
Use the [First] button or Enter key to select the ﬁrst expression to annotate.
Use the [Annotate] button if the selection is correct otherwise the [Next] button. After
a few cycles of [Annotate] and [Next], Use the [Ann. all next] button.

56

Using GATE Developer

Figure 3.10: Adding an Organization annotation to the Default Annotation Set

Note that after using the [First] button you can move the caret in the document and use the
[Next] button to avoid continuing the search from the beginning of the document. The [?]
button at the end of the search text ﬁeld will help you to build powerful regular expressions
to search.

3.4.6

Schema-Driven Editing

Annotation schemas allow annotation types and features to be pre-speciﬁed, so that during
manual annotation, the relevant options appear on the drop-down lists in the annotation
editor. You can see some example annotation schemas in Section 5.4.1. Annotation schemas
provide a means to deﬁne types of annotations in GATE Developer. Basically this means
that GATE Developer ‘knows about’ annotations deﬁned in a schema.

Using GATE Developer

57

Figure 3.11: Search and Annotate Function of the Annotation Editor.

Annotation schemas are supported by the ‘Annotation schema’ language resource in ANNIE,
so to use them you must ﬁrst ensure that the ‘ANNIE’ plugin is loaded (see Section 3.5).
This will load a set of default schemas, as well as allowing you to load schemas of your own.
The default annotation schemas contain common named entities such as Person, Organisation, Location, etc. You can modify the existing schema or create a new one, in order to tell
GATE Developer about other kinds of annotations you frequently use. You can still create
annotations in GATE Developer without having speciﬁed them in an annotation schema,
but you may then need to tell GATE Developer about the properties of that annotation
type each time you create an annotation for it.
To load a schema of your own, right-click on ‘Language Resources’ in the resources pane.
Select ‘New’ then ‘Annotation schema’. A popup box will appear in which you can browse
to your annotation schema XML ﬁle.
An alternative annotation editor component is available which constrains the available annotation types and features much more tightly, based on the annotation schemas that are
currently loaded. This is particularly useful when annotating large quantities of data or for
use by less skilled users.
To use this, you must load the Schema_Annotation_Editor plugin. With this plugin loaded,
the annotation editor will only oﬀer the annotation types permitted by the currently loaded
set of schemas, and when you select an annotation type only the features permitted by the
schema are available to edit1 . Where a feature is declared as having an enumerated type the
available enumeration values are presented as an array of buttons, making it easy to select
the required value quickly.

1
existing features outwith the schema, e.g. those created by previously-run processing resources, are not
editable but not modiﬁed or removed by the editor.

58

3.4.7

Using GATE Developer

Printing Text with Annotations

We suggest you to use your browser to print a document as GATE don’t propose a printing
facility for the moment.
First save your document by right clicking on the document in the left resources tree then
choose ‘Save Preserving Format’. You will get an XML ﬁle with all the annotations highlighted as XML tags plus the ‘Original markups’ annotations set.
It’s possible that the output will not have an XML header and footer because the document
was created from a plain text document. In that case you can use the XHTML example
below.
Then add a stylesheet processing instruction at the beginning of the XML ﬁle, the second
line in the following minimalist XHTML document:
<?xml version="1.0" encoding="UTF-8" ?>
<?xml-stylesheet type="text/css" href="gate.css"?>
<!DOCTYPE html
PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Virtual Library</title>
</head>
<body>
<p>Content of the document</p>
...
</body>
</html>

And create a ﬁle ‘gate.css’ in the same directory:
BODY, body { margin: 2em } /* or any other first level tag */
P, p { display: block } /* or any other paragraph tag */
/* ANNIE tags but you can use whatever tags you want */
/* be careful that XML tags are case sensitive */
Date
{ background-color: rgb(230, 150, 150) }
FirstPerson { background-color: rgb(150, 230, 150) }
Identifier
{ background-color: rgb(150, 150, 230) }
JobTitle
{ background-color: rgb(150, 230, 230) }
Location
{ background-color: rgb(230, 150, 230) }
Money
{ background-color: rgb(230, 230, 150) }
Organization { background-color: rgb(230, 200, 200) }
Percent
{ background-color: rgb(200, 230, 200) }

Using GATE Developer

59

Person
{ background-color: rgb(200, 200, 230) }
Title
{ background-color: rgb(200, 230, 230) }
Unknown
{ background-color: rgb(230, 200, 230) }
Etc
{ background-color: rgb(230, 230, 200) }
/* The next block is an example for having a small tag
with the name of the annotation type after each annotation */
Date:after {
content: "Date";
font-size: 50%;
vertical-align: sub;
color: rgb(100, 100, 100);
}

Finally open the XML ﬁle in your browser and print it.
Note that overlapping annotations, cannot be expressed correctly with inline XML tags and
thus won’t be displayed correctly.

3.5

Using CREOLE Plugins

In GATE, processing resources are used to automatically create and manipulate annotations
on documents. We will talk about processing resources in the next section. However, we
must ﬁrst introduce CREOLE plugins. In most cases, in order to use a particular processing
resource (and certain language resources) you must ﬁrst load the CREOLE plugin that
contains it. This section talks about using CREOLE plugins. Then, in Section 3.6, we will
talk about creating and using processing resources.
The deﬁnitions of CREOLE resources (e.g. processing resources such as taggers and parsers,
see Chapter 4) are stored in CREOLE directories (directories containing an XML ﬁle describing the resources, the Java archive with the compiled executable code and whatever
libraries are required by the resources).
Starting with version 3, CREOLE directories are called ‘CREOLE plugins’ or simply ‘plugins’. In previous versions, the CREOLE resources distributed with GATE used to be included
in the monolithic gate.jar archive. Version 3 includes them as separate directories under
the plugins directory of the distribution. This allows easy access to the linguistic resources
used without the requirement to unpack the gate.jar ﬁle.
Plugins can have one or more of the following states in relation with GATE:
known plugins are those plugins that the system knows about. These include all the plugins
in the plugins directory of the GATE installation (the so–called installed plugins) as
well all the plugins that were manually loaded from the user interface.

60

Using GATE Developer

loaded plugins are the plugins currently loaded in the system. All CREOLE resource types
from the loaded plugins are available for use. All known plugins can easily be loaded
and unloaded using the user interface.
auto-loadable plugins are the list of plugins that the system loads automatically during
initialisation.
The default location for installed plugins can be modiﬁed using the gate.plugins.home system property while the list of auto-loadable plugins can be set using the load.plugin.path
property, see Section 2.3 above.
The CREOLE plugins can be managed through the graphical user interface which can be
activated by selecting ‘Manage CREOLE Plugins’ from the ‘File’ menu. This will bring up
a window listing all the known plugins. For each plugin there are two check-boxes – one
labelled ‘Load now’, which will load the plugin, and the other labelled ‘Load always’ which
will add the plugin to the list of auto-loadable plugins. A ‘Delete’ button is also provided –
which will remove the plugin from the list of known plugins. This operation does not delete
the actual plugin directory. Installed plugins are found automatically when GATE is started;
if an installed plugin is deleted from the list, it will re-appear next time GATE is launched.

Figure 3.12: Plugin Management Console
If you select a plugin, you will see in the pane on the right the list of resources that plugin
contains. For example, in ﬁgure 3.12, the ‘Alignment’ plugin is selected, and you can see that
it contains ten processing resources; ‘Compound Document’, ‘Compound Document From
Xml’, ‘Compound Document Editor’, ‘GATE Composite document’ etc. If you wish to use
a particular resource you will have to ascertain which plugin contains it. This list can be

Using GATE Developer

61

useful for that. Alternatively, the GATE website provides a directory of plugins and their
processing resources.
Having loaded the plugins you need, the resources they deﬁne will be available for use.
Typically, to the GATE Developer user, this means that they will appear on the ‘New’ menu
when you right-click on ‘Processing Resources’ in the resources pane, although some special
plugins have diﬀerent eﬀects; for example, the Schema Annotation Editor (see Section 3.4.6).

3.6

Loading and Using Processing Resources

This section describes how to load and run CREOLE resources not present in ANNIE. To load
ANNIE, see Section 3.7.3. For technical descriptions of these resources, see the appropriate
chapter in Part III (e.g. Chapter 20). First ensure that the necessary plugins have been
loaded (see Section 3.5). If the resource you require does not appear in the list of Processing
Resources, then you probably do not have the necessary plugin loaded. Processing resources
are loaded by selecting them from the set of Processing Resources: right click on Processing
Resources or select ‘New Processing Resource’ from the File menu.
For example, use the Plugin Console Manager to load the ‘Tools’ plugin. When you right
click on ‘Processing Resources’ in the resources pane and select ‘New’ you have the option
to create any of the processing resources that plugin provides. You may choose to create a
‘GATE Morphological Analyser’, with the default parameters. Having done this, an instance
of the GATE Morphological Analyser appears under ‘Processing Resources’. This processing
resource, or PR, is now available to use. Double-clicking on it in the resources pane reveals
its initialisation parameters, see ﬁgure 3.13.
This processing resource is now available to be added to applications. It must be added to
an application before it can be applied to documents. You may create as many of a particular processing resource as you wish, for example with diﬀerent initialisation parameters.
Section 3.7 talks about creating and running applications.
See also the movie for loading processing resources.

3.7

Creating and Running an Application

Once all the resources you need have been loaded, an application can be created from them,
and run on your corpus. Right click on ‘Applications’ and select ‘New’ and then either
‘Corpus Pipeline’ or ‘Pipeline’. A pipeline application can only be run over a single document,
while a corpus pipeline can be run over a whole corpus.
To build the pipeline, double click on it, and select the resources needed to run the application
(you may not necessarily wish to use all those which have been loaded).

62

Using GATE Developer

Figure 3.13: GATE Morphological Analyser Initialisation Parameters

Transfer the necessary components from the set of ‘loaded components’ displayed on the left
hand side of the main window to the set of ‘selected components’ on the right, by selecting
each component and clicking on the left and right arrows, or by double-clicking on each
component.
Ensure that the components selected are listed in the correct order for processing (starting
from the top). If not, select a component and move it up or down the list using the up/down
arrows at the left side of the pane.
Ensure that any parameters necessary are set for each processing resource (by clicking on
the resource from the list of selected resources and checking the relevant parameters from
the pane below). For example, if you wish to use annotation sets other than the Default one,
these must be deﬁned for each processing resource.
Note that if a corpus pipeline is used, the corpus needs only to be set once, using the dropdown menu beside the ‘corpus’ box. If a pipeline is used, the document must be selected for

Using GATE Developer

63

each processing resource used.
Finally, click on ‘Run’ to run the application on the document or corpus.
See also the movie for loading and running processing resources.
For how to use the conditional versions of the pipelines see Section 3.7.2 and for saving/restoring the conﬁguration of an application see Section 3.8.3.

3.7.1

Running an Application on a Datastore

To avoid loading all your documents at the same time you can run an application on a
datastore corpus.
To do this you need to load your datastore, see section 3.8.2, and to load the corpus from
the datastore by double clicking on it in the datastore viewer.
Then, in the application viewer, you need to select this corpus in the drop down list of
corpora.
When you run the application on the corpus datastore, each document will be loaded, processed, saved then unloaded. So at any time there will be only one document from the
datastore corpus loaded. This prevent memory shortage but is also a little bit slower than
if all your documents were already loaded.
The processed documents are automatically saved back to the datastore so you may want
to use a copy of the datastore to experiment.
Be very careful that if you have some documents from the datastore corpus already loaded
before running the application then they will not be unloaded nor saved. To save such
document you have to right click on it in the resources tree view and save it to the datastore.

3.7.2

Running PRs Conditionally on Document Features

The ‘Conditional Pipeline’ and ‘Conditional Corpus Pipeline’ application types are conditional versions of the pipelines mentioned in Section 3.7 and allow processing resources to
be run or not according to the value of a feature on the document. In terms of graphical
interface, the only addition brought by the conditional versions of the applications is a box
situated underneath the lists of available and selected resources which allows the user to
choose whether the currently selected processing resource will run always, never or only on
the documents that have a particular value for a named feature.
If the Yes option is selected then the corresponding resource will be run on all the documents
processed by the application as in the case of non-conditional applications. If the No option

64

Using GATE Developer

is selected then the corresponding resource will never be run; the application will simply
ignore its presence. This option can be used to temporarily and quickly disable an application
component, for debugging purposes for example.
The If value of feature option permits running speciﬁc application components conditionally
on document features. When selected, this option enables two text input ﬁelds that are used
to enter the name of a feature and the value of that feature for which the corresponding
processing resource will be run. When a conditional application is run over a document, for
each component that has an associated condition, the value of the named feature is checked
on the document and the component will only be used if the value entered by the user
matches the one contained in the document features.
At ﬁrst sight the conditional behaviour available with these controller may seem limited, but
in fact it is very powerful when used in conjunction with JAPE grammars (see chapter 8).
Complex conditions can be encoded in JAPE rules which set the appropriate feature values
on the document for use by the conditional controllers. Alternatively, the Groovy plugin
provides a scriptable controller (see section 7.16.3) in which the execution strategy is deﬁned
by a Groovy script, allowing much richer conditional behaviour to be encoded directly in the
controller’s conﬁguration.

3.7.3

Doing Information Extraction with ANNIE

This section describes how to load and run ANNIE (see Chapter 6) from GATE Developer. ANNIE is a good place to start because it provides a complete information extraction
application, that you can run on any corpus. You can then view the eﬀects.
From the File menu, select ‘Load ANNIE System’. To run it in its default state, choose
‘with Defaults’. This will automatically load all the ANNIE resources, and create a corpus
pipeline called ANNIE with the correct resources selected in the right order, and the default
input and output annotation sets.
If ‘without Defaults’ is selected, the same processing resources will be loaded, but a popup
window will appear for each resource, which enables the user to specify a name, location
and other parameters for the resource. This is exactly the same procedure as for loading a
processing resource individually, the diﬀerence being that the system automatically selects
those resources contained within ANNIE. When the resources have been loaded, a corpus
pipeline called ANNIE will be created as before.
The next step is to add a corpus (see Section 3.3), and select this corpus from the dropdown corpus menu in the Serial Application editor. Finally click on ‘Run’ from the Serial
Application editor, or by right clicking on the application name in the resources pane and
selecting ‘Run’. (Many people prefer to switch to the messages tab, then run their application
by right-clicking on it in the resources pane, because then it is possible to monitor any
messages that appear whilst the application is running.)

Using GATE Developer

65

To view the results, double click on one of the document contained in the corpus processed in
the left hand tree view. No annotation sets nor annotations will be shown until annotations
are selected in the annotation sets; the ‘Default’ set is indicated only with an unlabelled
right-arrowhead which must be selected in order to make visible the available annotations.
Open the default annotation set and select some of the annotations to see what the ANNIE
application has done.
See also the movie for loading and running ANNIE.

3.7.4

Modifying ANNIE

You will ﬁnd the ANNIE resources in gate/plugins/ANNIE/resources. Simply locate the
existing resources you want to modify, make a copy with a new name, edit them, and load
the new resources into GATE as new Processing Resources (see Section 3.6).

3.8

Saving Applications and Language Resources

In this section, we will describe how applications and language resources can be saved for
use outside of GATE and for use with GATE at a later time. Section 3.8.1 talks about
saving documents to ﬁle. Section 3.8.2 outlines how to use datastores. Section 3.8.3 talks
about saving application states (resource parameter states), and Section 3.8.4 talks about
exporting applications together with referenced ﬁles and resources to a ZIP ﬁle.

3.8.1

Saving Documents to File

There are three main ways to save annotated documents:
1. preserving the original markup, with optional added annotations;
2. in GATE’s own XML serialisation format (including all the annotations on the document);
3. by writing your own dump algorithm as a processing resource.
This section describes how to use the ﬁrst two options.
Both types of data export are available in the popup menu triggered by right-clicking on a
document in the resources tree (see Section 3.1): type 1 is called ‘Save Preserving Format’
and type 2 is called ‘Save as XML’. In addition, all documents in a corpus can be saved as

66

Using GATE Developer

individual XML ﬁles into a directory by right-clicking on the corpus in the resources tree
and choosing the option ‘Save as XML‘.
Selecting the save as XML option leads to a ﬁle open dialogue; give the name of the ﬁle you
want to create, and the whole document and all its data will be exported to that ﬁle. If you
later create a document from that ﬁle, the state will be restored. (Note: because GATE’s
annotation model is richer than that of XML, and because our XML dump implementation
sometimes cuts corners2 , the state may not be identical after restoration. If your intention
is to store the state for later use, use a DataStore instead.)
The ‘Save Preserving Format’ option also leads to a ﬁle dialogue; give a name and the data
you require will be dumped into the ﬁle. The action can be used for documents that were
created from ﬁles using the XML or HTML format. It will save all the original tags as well
as the document annotations that are currently displayed in the ‘Annotations List’ view.
This option is useful for selectively saving only some annotation types.
The annotations are saved as normal document tags, using the annotation type as the tag
name. If the advanced option ‘Include annotation features for “Save Preserving Format”’ (see
Section 2.4) is set to true, then the annotation features will also be saved as tag attributes.
Using this operation for GATE documents that were not created from an HTML or XML
ﬁle results in a plain text ﬁle, with in-line tags for the saved annotations.
Note that GATE’s model of annotation allows graph structures, which are diﬃcult to represent in XML (XML is a tree-structured representation format). During the dump process,
annotations that cross each other in ways that cannot be represented in legal XML will be
discarded, and a warning message printed.

3.8.2

Saving and Restoring LRs in Datastores

Where corpora are large, the memory available may not be suﬃcient to have all documents
open simultaneously. The datastore functionality provides the option to save documents to
disk and open them only one at a time for processing. This means that much larger corpora
can be used. A datastore can also be useful for saving documents in an eﬃcient and lossless
way.
To save a text in a datastore, a new datastore must ﬁrst be created if one does not already
exist. Create a datastore by right clicking on Datastore in the left hand pane, and select the
option ‘Create Datastore’. Select the data store type you wish to use. Create a directory to
be used as the datastore (note that the datastore is a directory and not a ﬁle).
You can either save a whole corpus to the datastore (in which case the structure of the corpus
2
Gorey details: features of annotations and documents in GATE may be any virtually any Java object;
serialising arbitrary binary data to XML is not simple; instead we serialise them as strings, and therefore
they will be re-loaded as strings.

Using GATE Developer

67

will be preserved) or you can save individual documents. The recommended method is to
save the whole corpus. To save a corpus, right click on the corpus name and select the ‘Save
to...’ option (giving the name of the datastore created earlier). To save individual documents
to the datastore, right clicking on each document name and follow the same procedure.
To load a document from a datastore, do not try to load it as a language resource. Instead,
open the datastore by right clicking on Datastore in the left hand pane, select ‘Open Datastore’ and choose the datastore to open. The datastore tree will appear in the main window.
Double click on a corpus or document in this tree to open it. To save a corpus and document
back to the same datastore, simply select the ‘Save’ option.
See also the movie for creating a datastore and the movie for loading corpus and documents
from a datastore.

3.8.3

Saving Application States to a File

Resources, and applications that are made up of them, are created based on the settings of
their parameters (see Section 3.6). It is possible to save the data used to create an application
to a ﬁle and re-load it later. To save the application to a ﬁle, right click on it in the resources
tree and select ‘Save application state’, which will give you a ﬁle creation dialogue. Choose
a ﬁle name that ends in gapp as this ﬁle dialog and the one for loading application states age
displays all ﬁles which have a name ending in gapp. A common convention is to use .gapp
as a ﬁle extension.
To restore the application later, select ‘Restore application from ﬁle’ from the ‘File’ menu.
Note that the data that is saved represents how to recreate an application – not the resources
that make up the application itself. So, for example, if your application has a resource that
initialises itself from some ﬁle (e.g. a grammar, a document) then that ﬁle must still exist
when you restore the application.
In case you don’t want to save the corpus conﬁguration associated with the application then
you must select ‘<none>’ in the corpus list of the application before saving the application.
The ﬁle resulting from saving the application state contains the values of the initialisation
and runtime parameters for all the processing resources contained by the stored application
as well as the values of the initialisation parameters for all the language resources referenced
by those processing resources. Note that if you reference a document that has been created
with an empty URL and empty string content parameter and subsequently been manually
edited to add content, that content will not be saved. In order for document content to be
preserved, load the document from an URL, specify the content as for the string content
parameter or use a document from a datastore.
For the parameters of type URL (which are typically used to select external resources such
as grammars or rules ﬁles) a transformation is applied so that the paths are stored relative

68

Using GATE Developer

to the location of the ﬁle used to store the state. This means that the resource ﬁles which
are not part of GATE and used by an application do not need to be in the same location
as when the application was initially created but rather in the same location relative to the
location of the application ﬁle. It also means that the resource ﬁles which are part of GATE
should be found correctly no matter where GATE is installed. This allows the creation and
deployment of portable applications by keeping the application ﬁle and the resource ﬁles
used by the application together.
If you want to save your application along with all the resources it requires you can use the
‘Export for GATECloud.net’ option (see Section 3.8.4).
See also the movie for saving and restoring applications.

3.8.4

Saving an Application with its Resources (e.g.
Cloud.net)

GATE-

When you save an application using the ‘Save application state’ option (see Section 3.8.3),
the saved ﬁle contains references to the plugins that were loaded when the application was
saved, and to any resource ﬁles required by the application. To be able to reload the ﬁle,
these plugins and other dependencies must exist at the same locations (relative to the saved
state ﬁle). While this is ﬁne for saving and loading applications on a single machine it
means that if you want to package your application to run it elsewhere (e.g. deploy it to
GATECloud.net) then you need to be careful to include all the resource ﬁles and plugins
at the right locations in your package. The ‘Export for GATECloud.net’ option on the
right-click menu for an application helps to automate this process.
When you export an application in this way, GATE Developer produces a ZIP ﬁle containing
the saved application state (in the same format as ‘Save application state’). Any plugins and
resource ﬁles that the application refers to are also included in the zip ﬁle, and the relative
paths in the saved state are rewritten to point to the correct locations within the package.
The resulting package is therefore self-contained and can be copied to another machine and
unpacked there, or passed to GATECloud.net for deployment.
As well as selecting the location where you want to save the package, the ‘Export for GATECloud.net’ option will also prompt you to select the annotation sets that your application
uses for input and output. For example, if your application makes use of the unpacked XML
markup in source documents and creates annotations in the default set then you would select ‘Original markups’ as an input set and the ‘<Default annotation set>’ as an output
set. GATE Developer will try to make an educated guess at the correct sets but you should
check and amend the lists as necessary.
There are a few important points to note about the export process:
The complete contents of all the plugin directories that are loaded when you perform

Using GATE Developer

69

the export will be included in the resulting package. Use the plugin manager to unload
any plugins your application is not using before you export it.
If your application refers to a resource ﬁle in a directory that is not under one of the
loaded plugins, the entire contents of this directory will be recursively included in the
package. If you have a number of unrelated resources in a single directory (e.g. many
sets of large gazetteer lists) you may want to separate them into separate directories
so that only the relevant ones are included in the package.
The packager only knows about resources that your application refers to directly in its
parameters. For example, if your application includes a multi-phase JAPE grammar
the packager will only consider the main grammar ﬁle, not any of its sub-phases. If
the sub-phases are not contained in the same directory as the main grammar you may
ﬁnd they are not included. If indirect references of this kind are all to ﬁles under the
same directory as the ‘master’ ﬁle it will work OK.
If you require more ﬂexibility than this option provides you should read Section E.2, which
describes the underlying Ant task that the exporter uses.

3.9

Keyboard Shortcuts

You can use various keyboard shortcuts for common tasks in GATE Developer. These are
listed in this section.
General (Section 3.1):
F1 Display a help page for the selected component
Alt+F4 Exit the application without conﬁrmation
Tab Put the focus on the next component or frame
Shift+Tab Put the focus on the previous component or frame
F6 Put the focus on the next frame
Shift+F6 Put the focus on the previous frame
Alt+F Show the File menu
Alt+O Show the Options menu
Alt+T Show the Tools menu
Alt+H Show the Help menu

70

Using GATE Developer

F10 Show the ﬁrst menu
Resources tree (Section 3.1):
Enter Show the selected resources
Ctrl+H Hide the selected resource
Ctrl+Shift+H Hide all the resources
F2 Rename the selected resource
Ctrl+F4 Close the selected resource
Document editor (Section 3.2):
Ctrl+F Show the search dialog for the document
Ctrl+S Save the document in a ﬁle
F3 Show/Hide the annotation sets
Shift+F3 Show the annotation sets with preselection
F4 Show/Hide the annotations list
F5 Show/Hide the coreference editor
F7 Show/Hide the text
Annotation editor (Section 3.4):
Right/Left Grow/Shrink the annotation span at its start
Alt+Right/Alt+Left Grow/Shrink the annotation span at its end
+Shift/+Ctrl+Shift Use a span increment of 5/10 characters
Alt+Delete Delete the currently edited annotation
Annic/Lucene datastore (Chapter 9):
Alt+Enter Search the expression in the datastore
Alt+Backspace Delete the search expression

Using GATE Developer

71

Alt+Right Display the next page of results
Alt+Left Display the row manager
Alt+E Export the results to a ﬁle
Annic/Lucene query text ﬁeld (Chapter 9):
Ctrl+Enter Insert a new line
Enter Search the expression
Alt+Top Select the previous result
Alt+Bottom Select the next result

3.10

Miscellaneous

3.10.1

Stopping GATE from Restoring Developer Sessions/Options

GATE can remember Developer options and the state of the resource tree when it exits. The
options are saved by default; the session state is not saved by default. This default behaviour
can be changed from the ‘Advanced’ tab of the ‘Conﬁguration’ choice on the ‘Options’ menu.
If a problem occurs and the saved data prevents GATE Developer from starting, you can
ﬁx this by deleting the conﬁguration and session data ﬁles. These are stored in your home
directory, and are called gate.xml and gate.sesssion or .gate.xml and .gate.sesssion
depending on platform. On Windows your home is:
95, 98, NT: Windows Directory/proﬁles/username
2000, XP: Windows Drive/Documents and Settings/username

3.10.2

Working with Unicode

GATE provides various facilities for working with Unicode beyond those that come as default
with Java3 :
3
Implemented by Valentin Tablan, Mark Leisher and Markus Kramer. Initial version developed by Mark
Leisher.

72

Using GATE Developer

1. a Unicode editor with input methods for many languages;
2. use of the input methods in all places where text is edited in the GUI;
3. a development kit for implementing input methods;
4. ability to read diverse character encodings.
1 using the editor:
In GATE Developer, select ‘Unicode editor’ from the ‘Tools’ menu. This will display an
editor window, and, when a language with a custom input method is selected for input (see
next section), a virtual keyboard window with the characters of the language assigned to the
keys on the keyboard. You can enter data either by typing as normal, or with mouse clicks
on the virtual keyboard.
2 conﬁguring input methods:
In the editor and in GATE Developer’s main window, the ‘Options’ menu has an ‘Input
methods’ choice. All supported input languages (a superset of the JDK languages) are
available here. Note that you need to use a font capable of displaying the language you
select. By default GATE Developer will choose a Unicode font if it can ﬁnd one on the
platform you’re running on. Otherwise, select a font manually from the ‘Options’ menu
‘Conﬁguration’ choice.
3 using the development kit:
GUK, the GATE Unicode Kit, is documented at:
http://gate.ac.uk/gate/doc/javadoc/guk/package-summary.html.
4 reading diﬀerent character encodings:
When you create a document from a URL pointing to textual data in GATE, you have to
tell the system what character encoding the text is stored in. By default, GATE will set this
parameter to be the empty string. This tells Java to use the default encoding for whatever
platform it is running on at the time – e.g. on Western versions of Windows this will be ISO8859-1, and Eastern ones ISO-8859-9. On Linux systems, the default encoding is inﬂuenced
by the LANG environment variable, e.g. when this variable is set to en US.utf-8 the default
encoding used will be UTF-8. When GATE is started using the bin/ant run command or
(on Linux) through the gate.sh script or a link to it, you can change the default encoding
used by GATE to UTF-8 by adding -Drun.file.encoding=utf-8 as a parameter.
A popular way to store Unicode documents is in UTF-8, which is a superset of ASCII (but
can still store all Unicode data); if you get an error message about document I/O during
reading, try setting the encoding to UTF-8, or some other locally popular encoding. (To see
a list of available encodings, try opening a document in GATE’s unicode editor – you will
be prompted to select an encoding.)

Chapter 4
CREOLE: the GATE Component
Model
. . . Noam Chomsky’s answer in Secrets, Lies and Democracy (David Barsamian
1994; Odonian) to ‘What do you think about the Internet?’
‘I think that there are good things about it, but there are also aspects of it that
concern and worry me. This is an intuitive response – I can’t prove it – but my
feeling is that, since people aren’t Martians or robots, direct face-to-face contact
is an extremely important part of human life. It helps develop self-understanding
and the growth of a healthy personality.
‘You just have a diﬀerent relationship to somebody when you’re looking at them
than you do when you’re punching away at a keyboard and some symbols come
back. I suspect that extending that form of abstract and remote relationship,
instead of direct, personal contact, is going to have unpleasant eﬀects on what
people are like. It will diminish their humanity, I think.’
Chomsky, quoted at http://philip.greenspun.com/wtr/dead-trees/53015.
The GATE architecture is based on components: reusable chunks of software with welldeﬁned interfaces that may be deployed in a variety of contexts. The design of GATE is
based on an analysis of previous work on infrastructure for LE, and of the typical types
of software entities found in the ﬁelds of NLP and CL (see in particular chapters 4–6 of
[Cunningham 00]). Our research suggested that a proﬁtable way to support LE software
development was an architecture that breaks down such programs into components of various
types. Because LE practice varies very widely (it is, after all, predominantly a research ﬁeld),
the architecture must avoid restricting the sorts of components that developers can plug into
the infrastructure. The GATE framework accomplishes this via an adapted version of the
Java Beans component framework from Sun, as described in section 4.2.
GATE components may be implemented by a variety of programming languages and
databases, but in each case they are represented to the system as a Java class. This class
73

74

CREOLE: the GATE Component Model

may do nothing other than call the underlying program, or provide an access layer to a
database; on the other hand it may implement the whole component.
GATE components are one of three types:
LanguageResources (LRs) represent entities such as lexicons, corpora or ontologies;
ProcessingResources (PRs) represent entities that are primarily algorithmic, such as
parsers, generators or ngram modellers;
VisualResources (VRs) represent visualisation and editing components that participate
in GUIs.
The distinction between language resources and processing resources is explored more fully in
section C.1.1. Collectively, the set of resources integrated with GATE is known as CREOLE:
a Collection of REusable Objects for Language Engineering.
In the rest of this chapter:
Section 4.3 describes the lifecycle of GATE components;
Section 4.4 describes how Processing Resources can be grouped into applications;
Section 4.5 describes the relationship between Language Resources and their datastores;
Section 4.6 summarises GATE’s set of built-in components;
Section 4.7 describes how conﬁguration data for Resource types is supplied to GATE.

4.1

The Web and CREOLE

GATE allows resource implementations and Language Resource persistent data to be distributed over the Web, and uses Java annotations and XML for conﬁguration of resources
(and GATE itself).
Resource implementations are grouped together as ‘plugins’, stored at a URL (when the
resources are in the local ﬁle system this can be a file:/ URL). When a plugin is loaded
into GATE it looks for a conﬁguration ﬁle called creole.xml relative to the plugin URL and
uses the contents of this ﬁle to determine what resources this plugin declares and where to ﬁnd
the classes that implement the resource types (typically these classes are stored in a JAR ﬁle
in the plugin directory). Conﬁguration data for the resources may be stored directly in the
creole.xml ﬁle, or it may be stored as Java annotations on the resource classes themselves; in
either case GATE retrieves this conﬁguration information and adds the resource deﬁnitions

CREOLE: the GATE Component Model

75

to the CREOLE register. When a user requests an instantiation of a resource, GATE creates
an instance of the resource class in the virtual machine.
Language resource data can be stored in binary serialised form in the local ﬁle system.

4.2

The GATE Framework

We can think of the GATE framework as a backplane into which users can plug CREOLE
components. The user gives the system a list of URLs to search when it starts up, and
components at those locations are loaded by the system.
The backplane performs these functions:
component discovery, bootstrapping, loading and reloading;
management and visualisation of native data structures for common information types;
generalised data storage and process execution.
A set of components plus the framework is a deployment unit which can be embedded in
another application.
At their most basic, all GATE resources are Java Beans, the Java platform’s model of
software components. Beans are simply Java classes that obey certain interface conventions:
beans must have no-argument constructors.
beans have properties, deﬁned by pairs of methods named by the convention setProp
and getProp .
GATE uses Java Beans conventions to construct and conﬁgure resources at runtime, and
deﬁnes interfaces that diﬀerent component types must implement.

4.3

The Lifecycle of a CREOLE Resource

CREOLE resources exhibit a variety of forms depending on the perspective they are viewed
from. Their implementation is as a Java class plus an XML metadata ﬁle living at the
same URL. When using GATE Developer, resources can be loaded and viewed via the
resources tree (left pane) and the ‘create resource’ mechanism. When programming with
GATE Embedded, they are Java objects that are obtained by making calls to GATE’s
Factory class. These various incarnations are the phases of a CREOLE resource’s ‘lifecycle’.

76

CREOLE: the GATE Component Model

Depending on what sort of task you are using GATE for, you may use resources in any or
all of these phases. For example, you may only be interested in getting a graphical view of
what GATE’s ANNIE Information Extraction system (see Chapter 6) does; in this case you
will use GATE Developer to load the ANNIE resources, and load a document, and create
an ANNIE application and run it on the document. If, on the other hand, you want to
create your own resources, or modify the Java code of an existing resource (as opposed to
just modifying its grammar, for example), you will need to deal with all the lifecycle phases.
The various phases may be summarised as:
Creating a new resource from scratch (bootstrapping). To create the binary image
of a resource (a Java class in a JAR ﬁle), and the XML ﬁle that describes the resource
to GATE, you need to create the appropriate .java ﬁle(s), compile them and package
them as a .jar. GATE provides a bootstrap tool to start this process – see Section
7.11. Alternatively you can simply copy code from an existing resource.
Instantiating a resource in GATE Embedded. To create a resource in your own Java
code, use GATE’s Factory class (this takes care of parameterising the resource, restoring it from a database where appropriate, etc. etc.). Section 7.2 describes how to do
this.
Loading a resource into GATE Developer. To load a resource into GATE Developer,
use the various ‘New ... resource’ options from the File menu and elsewhere. See
Section 3.1.
Resource conﬁguration and implementation. GATE’s bootstrap tool will create an
empty resource that does nothing. In order to achieve the behaviour you require,
you’ll need to change the conﬁguration of the resource (by editing the creole.xml
ﬁle) and/or change the Java code that implements the resource. See section 4.7.

4.4

Processing Resources and Applications

PRs can be combined into applications. Applications model a control strategy for the execution of PRs. In GATE, applications are called ‘controllers’ accordingly.
Currently only sequential, or pipeline, execution is supported. There are two main types of
pipeline:
Simple pipelines simply group a set of PRs together in order and execute them in turn.
The implementing class is called SerialController.

CREOLE: the GATE Component Model

77

Corpus pipelines are speciﬁc for LanguageAnalysers – PRs that are applied to documents
and corpora. A corpus pipeline opens each document in the corpus in turn, sets that
document as a runtime parameter on each PR, runs all the PRs on the corpus, then
closes the document. The implementing class is called SerialAnalyserController.
Conditional versions of these controllers are also available. These allow processing resources
to be run conditionally on document features. See Section 3.7.2 for how to use these. If more
ﬂexibility is required, the Groovy plugin provides a scriptable controller (see section 7.16.3)
whose execution strategy is speciﬁed using the Groovy programming language.
Controllers are themselves PRs – in particular a simple pipeline is a standard PR and a
corpus pipeline is a LanguageAnalyser – so one pipeline can be nested in another. This is
particularly useful with conditional controllers to group together a set of PRs that can all
be turned on or oﬀ as a group.
There is also a real-time version of the corpus pipeline. When creating such a controller,
a timeout parameter needs to be set which determines the maximum amount of time (in
milliseconds) allowed for the processing of a document. Documents that take longer to
process, are simply ignored and the execution moves to the next document after the timeout
interval has lapsed.
All controllers have special handling for processing resources that implement the interface
gate.creole.ControllerAwarePR. This interface provides methods that are called by the
controller at the start and end of the whole application’s execution – for a corpus pipeline,
this means before any document has been processed and after all documents in the corpus
have been processed, which is useful for PRs that need to share data structures across the
whole corpus, build aggregate statistics, etc. For full details, see the JavaDoc documentation
for ControllerAwarePR.

4.5

Language Resources and Datastores

Language Resources can be stored in Datastores. Datastores are an abstract model of diskbased persistence, which can be implemented by various types of storage mechanism. Here
are the types implemented:
Serial Datastores are based on Java’s serialisation system, and store data directly into
ﬁles and directories.
Lucene Datastores is a full-featured annotation indexing and retrieval system. It is provided as part of an extension of the Serial Datastores. See Section 9 for more details.

78

CREOLE: the GATE Component Model

4.6

Built-in CREOLE Resources

GATE comes with various built-in components:
Language Resources modelling Documents and Corpora, and various types of Annotation Schema – see Chapter 5.
Processing Resources that are part of the ANNIE system – see Chapter 6.
Gazetteers – see Chapter 13.
Ontologies – see Chapter 14.
Machine Learning resources – see Chapter 17.
Alignment tools – see Chapter 18.
Parsers and taggers – see Chapter 16.
Other miscellaneous resources – see Chapter 20.

4.7

CREOLE Resource Conﬁguration

This section describes how to supply GATE with the conﬁguration data it needs about a
resource, such as what its parameters are, how to display it if it has a visualisation, etc.
Several GATE resources can be grouped into a single plugin, which is a directory containing
an XML conﬁguration ﬁle called creole.xml. Conﬁguration data for the plugin’s resources
can be given in the creole.xml ﬁle or directly in the Java source ﬁle using Java annotations.
A creole.xml ﬁle has a root element <CREOLE-DIRECTORY>, but the further contents of this
element depend on the conﬁguration style. The following three sections discuss the diﬀerent
styles – all-XML, all-annotations and a mixture of the two.

4.7.1

Conﬁguration with XML

To conﬁgure your resources in the creole.xml ﬁle, the <CREOLE-DIRECTORY> element should
contain one <RESOURCE> element for each resource type in the plugin. The <RESOURCE> elements may optionally be contained within a <CREOLE> element (to allow a single creole.xml
ﬁle to be built up by concatenating multiple separate ﬁles). For example:

CREOLE: the GATE Component Model

<CREOLE-DIRECTORY>
<CREOLE>
<RESOURCE>
<NAME>Minipar Wrapper</NAME>
<JAR>MiniparWrapper.jar</JAR>
<CLASS>minipar.Minipar</CLASS>
<COMMENT>MiniPar is a shallow parser. It determines the
dependency relationships between the words of a sentence.</COMMENT>
<HELPURL>http://gate.ac.uk/cgi-bin/userguide/sec:parsers:minipar</HELPURL>
<PARAMETER NAME="document"
RUNTIME="true"
COMMENT="document to process">gate.Document</PARAMETER>
<PARAMETER NAME="miniparDataDir"
RUNTIME="true"
COMMENT="location of the Minipar data directory">
java.net.URL
</PARAMETER>
<PARAMETER NAME="miniparBinary"
RUNTIME="true"
COMMENT="Name of the Minipar command file">
java.net.URL
</PARAMETER>
<PARAMETER NAME="annotationInputSetName"
RUNTIME="true"
OPTIONAL="true"
COMMENT="Name of the input Source">
java.lang.String
</PARAMETER>
<PARAMETER NAME="annotationOutputSetName"
RUNTIME="true"
OPTIONAL="true"
COMMENT="Name of the output AnnotationSetName">
java.lang.String
</PARAMETER>
<PARAMETER NAME="annotationTypeName"
RUNTIME="false"
DEFAULT="DepTreeNode"
COMMENT="Annotations to store with this type">
java.lang.String
</PARAMETER>
</RESOURCE>
</CREOLE>
</CREOLE-DIRECTORY>

79

80

CREOLE: the GATE Component Model

Basic Resource-Level Data
Each resource must give a name, a Java class and the JAR ﬁle that it can be loaded from.
The above example is taken from the Parser Minipar plugin, and deﬁnes a single resource
with a number of parameters.
The full list of valid elements under <RESOURCE> is as follows:
NAME the name of the resource, as it will appear in the ‘New’ menu in GATE Developer.
If omitted, defaults to the bare name of the resource class (without a package name).
CLASS the fully qualiﬁed name of the Java class that implements this resource.
JAR names JAR ﬁles required by this resource (paths are relative to the location of
creole.xml). Typically this will be the JAR ﬁle containing the class named by the
<CLASS> element, but additional <JAR> elements can be used to name third-party JAR
ﬁles that the resource depends on.
COMMENT a descriptive comment about the resource, which will appear as the tooltip
when hovering over an instance of this resource in the resources tree in GATE Developer. If omitted, no comment is used.
HELPURL a URL to a help document on the web for this resource. It is used in the help
browser inside GATE Developer.
INTERFACE the interface type implemented by this resource, for example new types of
document would specify <INTERFACE>gate.Document</INTERFACE>.
ICON the icon used to represent this resource in GATE Developer. This is a path inside
the plugin’s JAR ﬁle, for example <ICON>/some/package/icon.png</ICON>. If the
path speciﬁed does not start with a forward slash, it is assumed to name an icon from
the GATE default set, which is located in gate.jar at gate/resources/img. If no icon
is speciﬁed, a generic language resource or processing resource icon (as appropriate) is
used.
PRIVATE if present, this resource type is hidden in the GATE Developer GUI, i.e. it is
not shown in the ‘New’ menus. This is useful for resource types that are intended to be
created internally by other resources, or for resources that have parameters of a type
that cannot be set in the GUI. <PRIVATE/> resources can still be created in Java code
using the Factory.
AUTOINSTANCE (and HIDDEN-AUTOINSTANCE) tells GATE to automatically create instances of this resource when the plugin is loaded. Any number of auto
instances may be deﬁned, GATE will create them all. Each <AUTOINSTANCE> element
may optionally contain <PARAM NAME="..." VALUE="..." /> elements giving parameter values to use when creating the instance. Any parameters not speciﬁed explicitly

CREOLE: the GATE Component Model

81

will take their default values. Use <HIDDEN-AUTOINSTANCE> if you want the auto instances not to show up in GATE Developer – this is useful for things like document
formats where there should only ever be a single instance in GATE and that instance
should not be deleted.
TOOL if present, this resource type is considered to be a “tool”. Tools can contribute items
to the Tools menu in GATE Developer.
For visual resources, a <GUI> element should also be provided. This takes a TYPE attribute,
which can have the value LARGE or SMALL. LARGE means that the visual resource is a large
viewer and should appear in the main part of the GATE Developer window on the right
hand side, SMALL means the VR is a small viewer which appears in the space below the
resources tree in the bottom left. The <GUI> element supports the following sub-elements:
RESOURCE DISPLAYED the type of GATE resource this VR can display. Any resource whose type is assignable to this type will be displayed with this viewer, so for
example a VR that can display all types of document would specify gate.Document,
whereas a VR that can only display the default GATE document implementation would
specify gate.corpora.DocumentImpl.
MAIN VIEWER if present, GATE will consider this VR to be the ‘most important’
viewer for the given resource type, and will ensure that if several diﬀerent viewers are
all applicable to this resource, this viewer will be the one that is initially visible.
For annotation viewers, you should specify an <ANNOTATION_TYPE_DISPLAYED> element giving the annotation type that the viewer can display (e.g. Sentence).
Resource Parameters
Resources may also have parameters of various types. These resources, from the GATE
distribution, illustrate the various types of parameters:
<RESOURCE>
<NAME>GATE document</NAME>
<CLASS>gate.corpora.DocumentImpl</CLASS>
<INTERFACE>gate.Document</INTERFACE>
<COMMENT>GATE transient document</COMMENT>
<OR>
<PARAMETER NAME="sourceUrl"
SUFFIXES="txt;text;xml;xhtm;xhtml;html;htm;sgml;sgm;mail;email;eml;rtf"
COMMENT="Source URL">java.net.URL</PARAMETER>
<PARAMETER NAME="stringContent"
COMMENT="The content of the document">java.lang.String</PARAMETER>

82

CREOLE: the GATE Component Model

</OR>
<PARAMETER
COMMENT="Should the document read the original markup"
NAME="markupAware" DEFAULT="true">java.lang.Boolean</PARAMETER>
<PARAMETER NAME="encoding" OPTIONAL="true"
COMMENT="Encoding" DEFAULT="">java.lang.String</PARAMETER>
<PARAMETER NAME="sourceUrlStartOffset"
COMMENT="Start offset for documents based on ranges"
OPTIONAL="true">java.lang.Long</PARAMETER>
<PARAMETER NAME="sourceUrlEndOffset"
COMMENT="End offset for documents based on ranges"
OPTIONAL="true">java.lang.Long</PARAMETER>
<PARAMETER NAME="preserveOriginalContent"
COMMENT="Should the document preserve the original content"
DEFAULT="false">java.lang.Boolean</PARAMETER>
<PARAMETER NAME="collectRepositioningInfo"
COMMENT="Should the document collect repositioning information"
DEFAULT="false">java.lang.Boolean</PARAMETER>
<ICON>lr.gif</ICON>
</RESOURCE>
<RESOURCE>
<NAME>Document Reset PR</NAME>
<CLASS>gate.creole.annotdelete.AnnotationDeletePR</CLASS>
<COMMENT>Document cleaner</COMMENT>
<PARAMETER NAME="document" RUNTIME="true">gate.Document</PARAMETER>
<PARAMETER NAME="annotationTypes" RUNTIME="true"
OPTIONAL="true">java.util.ArrayList</PARAMETER>
</RESOURCE>

Parameters may be optional, and may have default values (and may have comments to
describe their purpose, which is displayed by GATE Developer during interactive parameter
setting).
Some PR parameters are execution time (RUNTIME), some are initialisation time. E.g. at
execution time a doc is supplied to a language analyser; at initialisation time a grammar
may be supplied to a language analyser.
The <PARAMETER> tag takes the following attributes:
NAME: name of the JavaBean property that the parameter refers to, i.e. for a parameter
named ‘someParam’ the class must have setSomeParam and getSomeParam methods.1
1
The JavaBeans spec allows is instead of get for properties of the primitive type boolean, but GATE
does not support parameters with primitive types. Parameters of type java.lang.Boolean (the wrapper
class) are permitted, but these have get accessors anyway.

CREOLE: the GATE Component Model

83

DEFAULT: default value (see below).
RUNTIME: doesn’t need setting at initialisation time, but must be set before calling
execute(). Only meaningful for PRs
OPTIONAL: not required
COMMENT: for display purposes
ITEM CLASS NAME: (only applies to parameters whose type is java.util.Collection
or a type that implements or extends this) this speciﬁes the type of elements the collection contains, so GATE can use the right type when parameters are set. If omitted,
GATE will pass in the elements as Strings.
SUFFIXES: (only applies to parameters of type java.net.URL) a semicolon-separated list
of ﬁle suﬃxes that this parameter typically accepts, used as a ﬁlter in the ﬁle chooser
provided by GATE Developer to select a local ﬁle as the parameter value.
It is possible for two or more parameters to be mutually exclusive (i.e. a user must specify
one or the other but not both). In this case the <PARAMETER> elements should be grouped
together under an <OR> element.
The type of the parameter is speciﬁed as the text of the <PARAMETER> element, and the type
supplied must match the return type of the parameter’s get method. Any reference type
(class, interface or enum) may be used as the parameter type, including other resource types –
in this case GATE Developer will oﬀer a list of the loaded instances of that resource as options
for the parameter value. Primitive types (char, boolean, . . . ) are not supported, instead you
should use the corresponding wrapper type (java.lang.Character, java.lang.Boolean,
. . . ). If the getter returns a parameterized type (e.g. List<Integer>) you should just specify
the raw type (java.util.List) here2 .
The DEFAULT string is converted to the appropriate type for the parameter java.lang.String parameters use the value directly, primitive wrapper types e.g.
java.lang.Integer use their respective valueOf methods, and other built-in Java types
can have defaults speciﬁed provided they have a constructor taking a String.
The type java.net.URL is treated specially: if the default string is not an absolute URL (e.g.
http://gate.ac.uk/) then it is treated as a path relative to the location of the creole.xml ﬁle.
Thus a DEFAULT of ‘resources/main.jape’ in the ﬁle file:/opt/MyPlugin/creole.xml
is treated as the absolute URL file:/opt/MyPlugin/resources/main.jape.
For Collection-valued parameters multiple values may be speciﬁed, separated by semicolons, e.g. ‘foo;bar;baz’; if the parameter’s type is an interface – Collection or one of
its sub-interfaces (e.g. List) – a suitable concrete class (e.g. ArrayList, HashSet) will be
chosen automatically for the default value.
2
In this particular case, as the type is a collection, you would specify java.lang.Integer as the
ITEM CLASS NAME.

84

CREOLE: the GATE Component Model

For parameters of type gate.FeatureMap multiple name=value pairs can be speciﬁed, e.g.
‘kind=word;orth=upperInitial’. For enum-valued parameters the default string is taken
as the name of the enum constant to use. Finally, if no DEFAULT attribute is speciﬁed, the
default value is null.

4.7.2

Conﬁguring Resources using Annotations

As an alternative to the XML conﬁguration style, GATE provides Java annotation types
to embed the conﬁguration data directly in the Java source code. @CreoleResource is
used to mark a class as a GATE resource, and parameter information is provided through
annotations on the JavaBean set methods. At runtime these annotations are read and
mapped into the equivalent entries in creole.xml before parsing. The metadata annotation
types are all marked @Documented so the CREOLE conﬁguration data will be visible in the
generated JavaDoc documentation.
For more detailed information, see the JavaDoc documentation for gate.creole.metadata.
To use annotation-driven conﬁguration for a plugin a creole.xml ﬁle is still required but it
need only contain the following:
<CREOLE-DIRECTORY>
<JAR SCAN="true">myPlugin.jar</JAR>
<JAR>lib/thirdPartyLib.jar</JAR>
</CREOLE-DIRECTORY>

This tells GATE to load myPlugin.jar and scan its contents looking for resource classes
annotated with @CreoleResource. Other JAR ﬁles required by the plugin can be speciﬁed
using other <JAR> elements without SCAN="true".
In a GATE Embedded application it is possible to register a single @CreoleResource annotated class without using a creole.xml ﬁle by calling
Gate.getCreoleRegister().registerComponent(MyResource.class);

GATE will extract the conﬁguration from the annotations on the class and make it available
for use as if it had been deﬁned in a plugin.

Basic Resource-Level Data
To mark a class as a CREOLE resource, simply use the @CreoleResource annotation (in
the gate.creole.metadata package), for example:

CREOLE: the GATE Component Model

1
2

85

import gate . creole . A bs t r a ct L a n g u a g e A n a l y s e r ;
import gate . creole . metadata .*;

3
4
5
6
7

@CreoleResource ( name = " GATE Tokeniser " ,
comment = " Splits text into tokens and spaces " )
public class Tokeniser extends A b s t r a c t L a n g u a g e A n a l y s e r {
...

The @CreoleResource annotation provides slots for all the values that can be speciﬁed under
<RESOURCE> in creole.xml, except <CLASS> (inferred from the name of the annotated class)
and <JAR> (taken to be the JAR containing the class):
name (String) the name of the resource, as it will appear in the ‘New’ menu in GATE
Developer. If omitted, defaults to the bare name of the resource class (without a
package name). (XML equivalent <NAME>)
comment (String) a descriptive comment about the resource, which will appear as the
tooltip when hovering over an instance of this resource in the resources tree in GATE
Developer. If omitted, no comment is used. (XML equivalent <COMMENT>)
helpURL (String) a URL to a help document on the web for this resource. It is used in
the help browser inside GATE Developer. (XML equivalent <HELPURL>)
isPrivate (boolean) should this resource type be hidden from the GATE Developer GUI, so
it does not appear in the ‘New’ menus? If omitted, defaults to false (i.e. not hidden).
(XML equivalent <PRIVATE/>)
icon (String) the icon to use to represent the resource in GATE Developer. If omitted, a
generic language resource or processing resource icon is used. (XML equivalent <ICON>,
see the description above for details)
interfaceName (String) the interface type implemented by this resource, for example
a new type of document would specify "gate.Document" here. (XML equivalent
<INTERFACE>)
autoInstances (array of @AutoInstance annotations) deﬁnitions for any instances of this
resource that should be created automatically when the plugin is loaded. If omitted, no
auto-instances are created by default. (XML equivalent, one or more <AUTOINSTANCE>
and/or <HIDDEN-AUTOINSTANCE> elements, see the description above for details)
tool (boolean) is this resource type a tool?
For visual resources only, the following elements are also available:
guiType (GuiType enum) the type of GUI this resource deﬁnes.
<GUI TYPE="LARGE|SMALL">)

(XML equivalent

86

CREOLE: the GATE Component Model

resourceDisplayed (String) the class name of the resource type that this VR displays, e.g.
"gate.Corpus". (XML equivalent <RESOURCE_DISPLAYED>)
mainViewer (boolean) is this VR the ‘most important’ viewer for its displayed resource
type? (XML equivalent <MAIN_VIEWER/>, see above for details)
For annotation viewers, you should specify an annotationTypeDisplayed element giving
the annotation type that the viewer can display (e.g. Sentence).

Resource Parameters
Parameters are declared by placing annotations on their JavaBean set methods. To mark
a setter method as a parameter, use the @CreoleParameter annotation, for example:
@CreoleParameter(comment = "The location of the list of abbreviations")
public void setAbbrListUrl(URL listUrl) {
...

GATE will infer the parameter’s name from the name of the JavaBean property in the usual
way (i.e. strip oﬀ the leading set and convert the following character to lower case, so in
this example the name is abbrListUrl). The parameter name is not taken from the name
of the method parameter. The parameter’s type is inferred from the type of the method
parameter (java.net.URL in this case).
The annotation elements of @CreoleParameter correspond to the attributes of the
<PARAMETER> tag in the XML conﬁguration style:
comment (String) an optional descriptive comment about the parameter. (XML equivalent
COMMENT)
defaultValue (String) the optional default value for this parameter. The value is speciﬁed
as a string but is converted to the relevant type by GATE according to the conversions
described in the previous section. Note that relative path default values for URL-valued
parameters are still relative to the location of the creole.xml ﬁle, not the annotated
class3 . (XML equivalent DEFAULT)
suﬃxes (String) for URL-valued parameters, a semicolon-separated list of default ﬁle sufﬁxes that this parameter accepts. (XML equivalent SUFFIXES)
3
When registering a class using CreoleRegister.registerComponent the base URL against which defaults for URL parameters are resolved is not speciﬁed. In such a resource it may be better to use
Class.getResource to construct the default URLs if no value is supplied for the parameter by the user.

CREOLE: the GATE Component Model

87

collectionElementType (Class) for Collection-valued parameters, the type of the elements in the collection. This can usually be inferred from the generic type information, for example public void setIndices(List<Integer> indices), but must be
speciﬁed if the set method’s parameter has a raw (non-parameterized) type. (XML
equivalent ITEM_CLASS_NAME)
Mutually-exclusive parameters (such as would be grouped in an <OR> in creole.xml) are
handled by adding a disjunction="label" and priority=n to the @CreoleParameter annotation – all parameters that share the same label are grouped in the same disjunction,
and will be oﬀered in order of priority. The parameter with the smallest priority value will
be the one listed ﬁrst, and thus the one that is oﬀered initially when creating a resource
of this type in GATE Developer. For example, the following is a simpliﬁed extract from
gate.corpora.DocumentImpl:
1
2

@CreoleParameter ( disjunction = " src " , priority =1)
public void setSourceUrl ( URL src ) { /
/ }

3
4
5

@CreoleParameter ( disjunction = " src " , priority =2)
public void setStringContent ( String content ) { /

/ }

This declares the parameters “stringContent” and “sourceUrl” as mutually-exclusive, and
when creating an instance of this resource in GATE Developer the parameter that will be
shown initially is sourceUrl. To set stringContent instead the user must select it from the
drop-down list. Parameters with the same declared priority value will appear next to each
other in the list, but their relative ordering is not speciﬁed. Parameters with no explicit
priority are always listed after those that do specify a priority.
Optional and runtime parameters are marked using extra annotations, for example:
1
2
3
4
5

@Optional
@RunTime
@CreoleParameter
public void s e t A n n o t a t i o n S e t N a m e ( String asName ) {
...

Inheritance
Unlike with pure XML conﬁguration, when using annotations a resource will inherit any
conﬁguration data that was not explicitly speciﬁed from annotations on its parent class
and on any interfaces it implements. Speciﬁcally, if you do not specify a comment, interfaceName, icon, annotationTypeDisplayed or the GUI-related elements (guiType and resourceDisplayed) on your @CreoleResource annotation then GATE will look up the class
tree for other @CreoleResource annotations, ﬁrst on the superclass, its superclass, etc.,
then at any implemented interfaces, and use the ﬁrst value it ﬁnds. This is useful if you are
deﬁning a family of related resources that inherit from a common base class.

88

CREOLE: the GATE Component Model

The resource name and the isPrivate and mainViewer ﬂags are not inherited.
Parameter deﬁnitions are inherited in a similar way. This is one of the big advantages of
annotation conﬁguration over pure XML – if one resource class extends another then with
pure XML conﬁguration all the parent class’s parameter deﬁnitions must be duplicated in
the subclass’s creole.xml deﬁnition. With annotations, parameters are inherited from the
parent class (and its parent, etc.) as well as from any interfaces implemented. For example, the gate.LanguageAnalyser interface provides two parameter deﬁnitions via annotated
set methods, for the corpus and document parameters. Any @CreoleResource annotated
class that implements LanguageAnalyser, directly or indirectly, will get these parameters
automatically.
Of course, there are some cases where this behaviour is not desirable, for example if a subclass
calculates a value for a superclass parameter rather than having the user set it directly. In
this case you can hide the parameter by overriding the set method in the subclass and using
a marker annotation:
1
2
3
4

@H id de n C r e o l e P a r a m e t e r
public void setSomeParam ( String someParam ) {
super . setSomeParam ( someParam );
}

The overriding method will typically just call the superclass one, as its only purpose is to
provide a place to put the @HiddenCreoleParameter annotation.
Alternatively, you may want to override some of the conﬁguration for a parameter but inherit
the rest from the superclass. Again, this is handled by trivially overriding the set method
and re-annotating it:
1
2
3
4
5
6

// superclass
@CreoleParameter ( comment = " Location of the grammar file " ,
suffixes = " jape " )
public void setGrammarUrl ( URL grammarLocation ) {
...
}

7
8
9
10
11
12
13

1
2

@Optional
@RunTime
@CreoleParameter ( comment = " Feature to set on success " )
public void setSuccessFeature ( String name ) {
...
}
//———————————–
// subclass

3
4
5
6
7
8

// override the default value, inherit everything else
@CreoleParameter ( defaultValue = " resources / defaultGrammar . jape " )
public void setGrammarUrl ( URL url ) {
super . setGrammarUrl ( url );
}

CREOLE: the GATE Component Model

89

9
10
11
12
13
14
15

// we want the parameter to be required in the subclass
@Optional ( false )
@CreoleParameter
public void setSuccessFeat ure ( String name ) {
super . setSu ccessF eature ( name );
}

Note that for backwards compatibility, data is only inherited from superclass annotations
if the subclass is itself annotated with @CreoleResource. If the subclass is not annotated
then GATE assumes that all its conﬁguration is contained in creole.xml in the usual way.

4.7.3

Mixing the Conﬁguration Styles

It is possible and often useful to mix and match the XML and annotation-driven conﬁguration styles. The rule is always that anything speciﬁed in the XML takes priority over the
annotations. The following examples show what this allows.
Overriding Conﬁguration for a Third-Party Resource
Suppose you have a plugin from some third party that uses annotation-driven conﬁguration.
You don’t have the source code but you would like to override the default value for one of
the parameters of one of the plugin’s resources. You can do this in the creole.xml:
<CREOLE-DIRECTORY>
<JAR SCAN="true">acmePlugin-1.0.jar</JAR>
<!-- Add the following to override the annotations -->
<RESOURCE>
<CLASS>com.acme.plugin.UsefulPR</CLASS>
<PARAMETER NAME="listUrl"
DEFAULT="resources/myList.txt">java.net.URL</PARAMETER>
</RESOURCE>
</CREOLE-DIRECTORY>

The default value for the listUrl parameter in the annotated class will be replaced by your
value.
External AUTOINSTANCEs
For resources like document formats, where there should always and only be one instance in GATE at any time, it makes sense to put the auto-instance deﬁnitions in the

90

CREOLE: the GATE Component Model

@CreoleResource annotation. But if the automatically created instances are a convenience
rather than a necessity it may be better to deﬁne them in XML so other users can disable
them without re-compiling the class:
<CREOLE-DIRECTORY>
<JAR SCAN="true">myPlugin.jar</JAR>
<RESOURCE>
<CLASS>com.acme.AutoPR</CLASS>
<AUTOINSTANCE>
<PARAM NAME="type" VALUE="Sentence" />
</AUTOINSTANCE>
<AUTOINSTANCE>
<PARAM NAME="type" VALUE="Paragraph" />
</AUTOINSTANCE>
</RESOURCE>
</CREOLE-DIRECTORY>

Inheriting Parameters
If you would prefer to use XML conﬁguration for your own resources, but would like to beneﬁt
from the parameter inheritance features of the annotation-driven approach, you can write a
normal creole.xml ﬁle with all your conﬁguration and just add a blank @CreoleResource
annotation to your class. For example:
1
2
3

package com . acme ;
import gate .*;
import gate . creole . metadata . CreoleResource ;

4
5
6
7
8

@CreoleResource
public class MyPR implements LanguageAnalyser {
...
}

<!-- creole.xml -->
<CREOLE-DIRECTORY>
<CREOLE>
<RESOURCE>
<NAME>My Processing Resource</NAME>
<CLASS>com.acme.MyPR</CLASS>
<COMMENT>...</COMMENT>
<PARAMETER NAME="annotationSetName"
RUNTIME="true" OPTIONAL="true">java.lang.String</PARAMETER>
<!-don’t need to declare document and corpus parameters, they
are inherited from LanguageAnalyser

CREOLE: the GATE Component Model

91

-->
</RESOURCE>
</CREOLE>
</CREOLE-DIRECTORY>

N.B. Without the @CreoleResource the parameters would not be inherited.

4.8

Tools: How to Add Utilities to GATE Developer

Visual Resources allow a developer to provide a GUI to interact with a particular resource
type (PR or LR), but sometimes it is useful to provide general utilities for use in the GATE
Developer GUI that are not tied to any speciﬁc resource type. Examples include the annotation diﬀ tool and the Groovy console (provided by the Groovy plugin), both of which
are self-contained tools that display in their own top-level window. To support this, the
CREOLE model has the concept of a tool.
A resource type is marked as a tool by using the <TOOL/> element in its creole.xml
deﬁnition, or by setting tool = true if using the @CreoleResource annotation conﬁguration style. If a resource is declared to be a tool, and written to implement the
gate.gui.ActionsPublisher interface, then whenever an instance of the resource is created
its published actions will be added to the “Tools” menu in GATE Developer.
Since the published actions of every instance of the resource will be added to the tools menu,
it is best not to use this mechanism on resource types that can be instantiated by the user.
The “tool” marker is best used in combination with the “private” ﬂag (to hide the resource
from the list of available types in the GUI) and one or more hidden autoinstance deﬁnitions
to create a limited number of instances of the resource when its deﬁning plugin is loaded.
See the GroovySupport resource in the Groovy plugin for an example of this.

4.8.1

Putting your tools in a sub-menu

If your plugin provides a number of tools (or a number of actions from the same tool) you
may wish to organise your actions into one or more sub-menus, rather than placing them
all on the single top-level tools menu. To do this, you need to put a special value into the
actions returned by the tool’s getActions() method:
1
2

action . putValue ( GateConstants . MENU_PATH_KEY ,
new String [] { " Acme toolkit " , " Statistics " });

The key must be GateConstants.MENU_PATH_KEY and the value must be an array of strings. Each
string in the array represents the name of one level of sub-menus. Thus in the example above
the action would be placed under “Tools → Acme toolkit → Statistics”. If no MENU_PATH_KEY
value is provided the action will be placed directly on the Tools menu.

92

CREOLE: the GATE Component Model

Chapter 5
Language Resources: Corpora,
Documents and Annotations
Sometimes in life you’ve got to dance like nobody’s watching.
...
I think they should introduce ‘sleeping’ to the Olympics. It would be an excellent
ﬁeld event, in which the ‘athletes’ (for want of a better word) all lay down in
beds, just beyond where the javelins land, and the ﬁrst one to fall asleep and
not wake up for three hours would win gold. I, for one, would be interested
in seeing what kind of personality would be suited to sleeping in a competitive
environment.
...
Life is a mystery to be lived, not a problem to be solved.
Round Ireland with a Fridge, Tony Hawks, 1998 (pp. 119, 147, 179).
This chapter documents GATE’s model of corpora, documents and annotations on documents. Section 5.1 describes the simple attribute/value data model that corpora, documents
and annotations all share. Section 5.2, Section 5.3 and Section 5.4 describe corpora, documents and annotations on documents respectively. Section 5.5 describes GATE’s support
for diverse document formats, and Section 5.5.2 describes facilities for XML input/output.

5.1

Features: Simple Attribute/Value Data

GATE has a single model for information that describes documents, collections of documents
(corpora), and annotations on documents, based on attribute/value pairs. Attribute names
are strings; values can be any Java object. The API for accessing this feature data is Java’s
Map interface (part of the Collections API).
93

94

5.2

Language Resources: Corpora, Documents and Annotations

Corpora: Sets of Documents plus Features

A Corpus in GATE is a Java Set whose members are Documents. Both Corpora and Documents are types of LanguageResource (LR); all LRs have a FeatureMap (a Java Map) associated with them that stored attribute/value information about the resource. FeatureMaps
are also used to associate arbitrary information with ranges of documents (e.g. pieces of
text) via the annotation model (see below).
Documents have a DocumentContent which is a text at present (future versions may add
support for audiovisual content) and one or more AnnotationSets which are Java Sets.

5.3

Documents: Content plus Annotations plus Features

Documents are modelled as content plus annotations (see Section 5.4) plus features (see
Section 5.1). The content of a document can be any subclass of DocumentContent.

5.4

Annotations: Directed Acyclic Graphs

Annotations are organised in graphs, which are modelled as Java sets of Annotation. Annotations may be considered as the arcs in the graph; they have a start Node and an end
Node, an ID, a type and a FeatureMap. Nodes have pointers into the sources document, e.g.
character oﬀsets.

5.4.1

Annotation Schemas

Annotation schemas provide a means to deﬁne types of annotations in GATE.
GATE uses the XML Schema language supported by W3C for these deﬁnitions.
When using GATE Developer to create/edit annotations, a component is available
(gate.gui.SchemaAnnotationEditor) which is driven by an annotation schema ﬁle. This
component will constrain the data entry process to ensure that only annotations that correspond to a particular schema are created. (Another component allows unrestricted annotations to be created.)
Schemas are resources just like other GATE components. Below we give some examples of
such schemas. Section 3.4.6 describes how to create new schemas.

Language Resources: Corpora, Documents and Annotations

Date Schema
<?xml version="1.0"?>
<schema
xmlns="http://www.w3.org/2000/10/XMLSchema">
<!-- XSchema deffinition for Date-->
<element name="Date">
<complexType>
<attribute name="kind" use="optional">
<simpleType>
<restriction base="string">
<enumeration value="date"/>
<enumeration value="time"/>
<enumeration value="dateTime"/>
</restriction>
</simpleType>
</attribute>
</complexType>
</element>
</schema>

Person Schema
<?xml version="1.0"?>
<schema
xmlns="http://www.w3.org/2000/10/XMLSchema">
<!-- XSchema definition for Person-->
<element name="Person" />
</schema>

Address Schema
<?xml version="1.0"?> <schema
xmlns="http://www.w3.org/2000/10/XMLSchema">
<!-- XSchema deffinition for Address-->
<element name="Address">
<complexType>
<attribute name="kind" use="optional">
<simpleType>
<restriction base="string">
<enumeration value="email"/>
<enumeration value="url"/>
<enumeration value="phone"/>
<enumeration value="ip"/>

95

96

Language Resources: Corpora, Documents and Annotations

<enumeration
<enumeration
<enumeration
<enumeration
</restriction>
</simpleType>
</attribute>
</complexType>
</element>
</schema>

5.4.2

value="street"/>
value="postcode"/>
value="country"/>
value="complete"/>

Examples of Annotated Documents

This section shows some simple examples of annotated documents.
This material is adapted from [Grishman 97], the TIPSTER Architecture Design document
upon which GATE version 1 was based. Version 2 has a similar model, although annotations
are now graphs, and instead of multiple spans per annotation each annotation now has a single start/end node pair. The current model is largely compatible with [Bird & Liberman 99],
and roughly isomorphic with "stand-oﬀ markup" as latterly adopted by the SGML/XML
community.
Each example is shown in the form of a table. At the top of the table is the document being
annotated; immediately below the line with the document is a ruler showing the position
(byte oﬀset) of each character (see TIPSTER Architecture Design Document).
Underneath this appear the annotations, one annotation per line. For each annotation is
shown its Id, Type, Span (start/end oﬀsets derived from the start/end nodes), and Features.
Integers are used as the annotation Ids. The features are shown in the form name = value.
The ﬁrst example shows a single sentence and the result of three annotation procedures: tokenization with part-of-speech assignment, name recognition, and sentence boundary recognition. Each token has a single feature, its part of speech (pos), using the tag set from the
University of Pennsylvania Tree Bank; each name also has a single feature, indicating the
type of name: person, company, etc.
Annotations will typically be organized to describe a hierarchical decomposition of a text.
A simple illustration would be the decomposition of a sentence into tokens. A more complex
case would be a full syntactic analysis, in which a sentence is decomposed into a noun phrase
and a verb phrase, a verb phrase into a verb and its complement, etc. down to the level of
individual tokens. Such decompositions can be represented by annotations on nested sets
of spans. Both of these are illustrated in the second example, which is an elaboration of
our ﬁrst example to include parse information. Each non-terminal node in the parse tree is
represented by an annotation of type parse.

Language Resources: Corpora, Documents and Annotations

Id
1
2
3
4
5
6
7

Text
Cyndi savored the soup.
^0...^5...^10..^15..^20
Annotations
Type
SpanStart Span End Features
token
0
5
pos=NP
token
6
13
pos=VBD
token
14
17
pos=DT
token
18
22
pos=NN
token
22
23
name
0
5
name type=person
sentence 0
23
Table 5.1: Result of annotation on a single sentence

Id
1
2
3
4
5
6
7

Text
Cyndi savored the soup.
^0...^5...^10..^15..^20
Annotations
Type
SpanStart Span End Features
token
0
5
pos=NP
token
6
13
pos=VBD
token
14
17
pos=DT
token
18
22
pos=NN
token
22
23
name
0
5
name type=person
sentence 0
23
constituents=[1],[2],[3].[4],[5]
Table 5.2: Result of annotations including parse information

97

98

Language Resources: Corpora, Documents and Annotations

Id
1
2
3
4
5
6
7
8

Text
To: All Barnyard Animals
^0...^5...^10..^15..^20.
From: Chicken Little
^25..^30..^35..^40..
Date: November 10,1194
...^50..^55..^60..^65.
Subject: Descending Firmament
.^70..^75..^80..^85..^90..^95
Priority: Urgent
.^100.^105.^110.
The sky is falling. The sky is falling.
....^120.^125.^130.^135.^140.^145.^150.
Annotations
Type
SpanStart Span End Features
Addressee 4
24
Source
31
45
Date
53
69
ddmmyy=101194
Subject
78
98
Priority
109
115
Body
116
155
Sentence
116
135
Sentence
136
155

Table 5.3: Annotation showing overall document structure

In most cases, the hierarchical structure could be recovered from the spans. However, it may
be desirable to record this structure directly through a constituents feature whose value is
a sequence of annotations representing the immediate constituents of the initial annotation.
For the annotations of type parse, the constituents are either non-terminals (other annotations in the parse group) or tokens. For the sentence annotation, the constituents feature
points to the constituent tokens. A reference to another annotation is represented in the
table as "[ Annotation Id]"; for example, "[3]" represents a reference to annotation 3. Where
the value of an feature is a sequence of items, these items are separated by commas. No
special operations are provided in the current architecture for manipulating constituents. At
a less esoteric level, annotations can be used to record the overall structure of documents,
including in particular documents which have structured headers, as is shown in the third
example (Table 5.3).
If the Addressee, Source, ... annotations are recorded when the document is indexed for
retrieval, it will be possible to perform retrieval selectively on information in particular
ﬁelds. Our ﬁnal example (Table 5.4) involves an annotation which eﬀectively modiﬁes the
document. The current architecture does not make any speciﬁc provision for the modiﬁcation

Language Resources: Corpora, Documents and Annotations

Id
1
2
3
4
5

Type
token
token
token
token
token

99

Text
Topster tackles 2 terrorbytes.
^0...^5...^10..^15..^20..^25..
Annotations
SpanStart Span End Features
0
7
pos=NP correction=TIPSTER
8
15
pos=VBZ
16
17
pos=CD
18
29
pos=NNS correction=terabytes
29
30
Table 5.4: Annotation modifying the document

of the original text. However, some allowance must be made for processes such as spelling
correction. This information will be recorded as a correction feature on token annotations
and possibly on name annotations:

5.4.3

Creating, Viewing and Editing Diverse Annotation Types

Note that annotation types should consist of a single word with no spaces. Otherwise they
may not be recognised by other components such as JAPE transducers, and may create
problems when annotations are saved as inline (‘Save Preserving Format’ in the context
menu).
To view and edit annotation types, see Section 3.4. To add annotations of a new type, see
Section 3.4.5. To add a new annotation schema, see Section 3.4.6.

5.5

Document Formats

The following document formats are supported by GATE:
Plain Text
HTML
SGML
XML
RTF
Email

100

Language Resources: Corpora, Documents and Annotations

PDF (some documents)
Microsoft Oﬃce (some formats)
OpenOﬃce (some formats)
By default GATE will try and identify the type of the document, then strip and convert
any markup into GATE’s annotation format. To disable this process, set the markupAware
parameter on the document to false.
When reading a document of one of these types, GATE extracts the text between tags (where
such exist) and create a GATE annotation ﬁlled as follows:
The name of the tag will constitute the annotation’s type, all the tags attributes will materialize in the annotation’s features and the annotation will span over the text covered by the
tag. A few exceptions of this rule apply for the RTF, Email and Plain Text formats, which
will be described later in the input section of these formats.
The text between tags is extracted and appended to the GATE document’s content and all
annotations created from tags will be placed into a GATE annotation set named ‘Original
markups’.
Example:
If the markup is like this:
<aTagName attrib1="value1" attrib2="value2" attrib3="value3"> A
piece of text</aTagName>

then the annotation created by GATE will look like:
annotation.type = "aTagName";
annotation.fm = {attrib1=value1;atrtrib2=value2;attrib3=value3};
annotation.start = startNode;
annotation.end = endNode;

The startNode and endNode are created from oﬀsets referring the beginning and the end of
‘A piece of text’ in the document’s content.
The documents supported by GATE have to be in one of the encodings accepted by Java.
The most popular is the ‘UTF-8’ encoding which is also the most storage eﬃcient one for
UNICODE. If, when loading a document in GATE the encoding parameter is set to ‘’(the
empty string), then the default encoding of the platform will be used.

Language Resources: Corpora, Documents and Annotations

5.5.1

101

Detecting the Right Reader

In order to successfully apply the document creation algorithm described above, GATE
needs to detect the proper reader to use for each document format. If the user knows in
advance what kind of document they are loading then they can specify the MIME type (e.g.
text/html) using the init parameter mimeType, and GATE will respect this. If an explicit type
is not given, GATE attempts to determine the type by other means, taking into consideration
(where possible) the information provided by three sources:
Document’s extension
The web server’s content type
Magic numbers detection
The ﬁrst represents the extension of a ﬁle like (xml,htm,html,txt,sgm,rtf, etc), the second
represents the HTTP information sent by a web server regarding the content type of the
document being send by it (text/html; text/xml, etc), and the third one represents certain
sequences of chars which are ultimately number sequences. GATE is capable of supporting
multimedia documents, if the right reader is added to the framework. Sometimes, multimedia
documents are identiﬁed by a signature consisting in a sequence of numbers. Inside GATE
they are called magic numbers. For textual documents, certain char sequences form such
magic numbers. Examples of magic numbers sequences will be provided in the Input section
of each format supported by GATE.
All those tests are applied to each document read, and after that, a voting mechanism decides
what is the best reader to associate with the document. There is a degree of priority for all
those tests. The document’s extension test has the highest priority. If the system is in doubt
which reader to choose, then the one associated with document’s extension will be selected.
The next higher priority is given to the web server’s content type and the third one is given
to the magic numbers detection. However, any two tests that identify the same mime type,
will have the highest priority in deciding the reader that will be used. The web server test is
not always successful as there might be documents that are loaded from a local ﬁle system,
and the magic number detection test is not always applicable. In the next paragraphs we
will se how those tests are performed and what is the general mechanism behind reader
detection.
The method that detects the proper reader is a static one, and it belongs to the
gate.DocumentFormat class. It uses the information stored in the maps ﬁlled by the init()
method of each reader. This method comes with three signatures:
1
2

static public DocumentFormat ge tDocum entFor mat ( gate . Document
aGateDocument , URL url )

3
4
5

static public DocumentFormat ge tDocum entFor mat ( gate . Document
aGateDocument , String fileSuffix )

102

Language Resources: Corpora, Documents and Annotations

6
7
8

static public DocumentFormat ge tDocum entFor mat ( gate . Document
aGateDocument , MimeType mimeType )

The ﬁrst two methods try to detect the right MimeType for the GATE document, and after
that, they call the third one to return the reader associate with a MimeType. Of course, if an
explicit mimeType parameter was speciﬁed, GATE calls the third form of the method directly,
passing the speciﬁed type. GATE uses the implementation from ‘http://jigsaw.w3.org’ for
mime types.
The magic numbers test is performed using the information form
magic2mimeTypeMap map. Each key from this map, is searched in the ﬁrst buﬀerSize (the
default value is 2048) chars of text. The method that does this is called
runMagicNumbers(InputStreamReader aReader) and it belongs to DocumentFormat class.
More details about it can be found in the GATE API documentation.
In order to activate a reader to perform the unpacking, the creole deﬁnition of a GATE
document deﬁnes a parameter called ‘markupAware’ initialized with a default value of true.
This parameter, forces GATE to detect a proper reader for the document being read. If no
reader is found, the document’s content is load and presented to the user, just like any other
text editor (this for textual documents).
The next subsections investigates particularities for each format and will describe the ﬁle
extensions registered with each document format.

5.5.2

XML

Input
GATE permits the processing of any XML document and oﬀers support for XML namespaces.
It beneﬁts the power of Apache’s Xerces parser and also makes use of Sun’s JAXP layer.
Changing the XML parser in GATE can be achieved by simply replacing the value of a Java
system property (‘javax.xml.parsers.SAXParserFactory’).
GATE will accept any well formed XML document as input. Although it has the possibility
to validate XML documents against DTDs it does not do so because the validating procedure
is time consuming and in many cases it issues messages that are annoying for the user.
There is an open problem with the general approach of reading XML, HTML and SGML
documents in GATE. As we previously said, the text covered by tags/elements is appended
to the GATE document content and a GATE annotation refers to this particular span of
text. When appending, in cases such as ‘end.</P><P>Start’ it might happen that the ending
word of the previous annotation is concatenated with the beginning phrase of the annotation
currently being created, resulting in a garbage input for GATE processing resources that

Language Resources: Corpora, Documents and Annotations

103

operate at the text surface.
Let’s take another example in order to better understand the problem:
<title>This is a title</title><p>This is a paragraph</p><a
href="#link">Here is an useful link</a>

When the markup is transformed to annotations, it is likely that the text from the document’s
content will be as follows:
This is a titleThis is a paragraphHere is an useful link
The annotations created will refer the right parts of the texts but for the GATE’s processing
resources like (tokenizer, gazetteer, etc) which work on this text, this will be a major disaster.
Therefore, in order to prevent this problem from happening, GATE checks if it’s likely to
join words and if this happens then it inserts a space between those words. So, the text will
look like this after loaded in GATE Developer:
This is a title This is a paragraph Here is an useful link
There are cases when these words are meant to be joined, but they are rare. This is why it’s
an open problem.
The extensions associate with the XML reader are:
xml
xhtm
xhtml
The web server content type associate with xml documents is: text/xml.
The magic numbers test searches inside the document for the XML(<?xml version="1.0")
signature. It is also able to detect if the XML document uses the semantics described in the
GATE document format DTD (see 5.5.2 below) or uses other semantics.
Namespace handling
By default, GATE will retain the namespace preﬁx and namespace URIs of XML elements
when creating annotations and features within the Original markups annotation set. For
example, the element
<dc:title xmlns:dc="http://purl.org/dc/elements/1.1/">Document title</dc:title>
will create the following annotation

104

Language Resources: Corpora, Documents and Annotations

dc:title(xmlns:dc=http://purl.org/dc/elements/1.1/)
However, as the colon character ’:’ is a reserved meta-character in JAPE, it is not possible
to write a JAPE rule that will match the dc:title element or its namespace URI.
If you need to match namespace-preﬁxed elements in the Original markups AS, you can alter
the default namespace deserialization behaviour to remove the namespace preﬁx and add it
as a feature (along with the namespace URI), by specifying the following attributes in the
<GATECONFIG> element of gate.xml or local conﬁguration ﬁle:
addNamespaceFeatures - set to ”true” to deserialize namespace preﬁx and uri information as features.
namespaceURI - The feature name to use that will hold the namespace URI of the
element, e.g. ”namespace”
namespacePreﬁx - The feature name to use that will hold the namespace preﬁx of
the element, e.g. ”preﬁx”
i.e.
<GATECONFIG
addNamespaceFeatures="true"
namespaceURI="namespace"
namespacePrefix="prefix" />
For example
<dc:title>Document title</dc:title>
would create in Original markups AS (assuming the xmlns:dc URI has deﬁned in the document root or parent element)
title(prefix=dc, namespace=http://purl.org/dc/elements/1.1/)
If a JAPE rule is written to create a new annotation, e.g.
description(prefix=foo, namespace=http://www.example.org/)
then these would be serialized to
<dc:title xmlns:dc="http://purl.org/dc/elements/1.1/">Document title</dc:title>
<foo:description xmlns:foo="http://www.example.org/">...</foo:description>
when using the ’Save preserving document format’ XML output option (see 5.5.2 below).

Language Resources: Corpora, Documents and Annotations

105

Output
GATE is capable of ensuring persistence for its resources. The types of persistent storage
used for Language Resources are:
Java serialization;
XML serialization.
We describe the latter case here.
XML persistence doesn’t necessarily preserve all the objects belonging to the annotations,
documents or corpora. Their features can be of all kinds of objects, with various layers of
nesting. For example, lists containing lists containing maps, etc. Serializing these arbitrary
data types in XML is not a simple task; GATE does the best it can, and supports native Java
types such as Integers and Booleans, but where complex data types are used, information
may be lost(the types will be converted into Strings). GATE provides a full serialization of
certain types of features such as collections, strings and numbers. It is possible to serialize
only those collections containing strings or numbers. The rest of other features are serialized
using their string representation and when read back, they will be all strings instead of being
the original objects. Consequences of this might be observed when performing evaluations
(see Chapter 10).
When GATE outputs an XML document it may do so in one of two ways:
When the original document that was imported into GATE was an XML document,
GATE can dump that document back into XML (possibly with additional markup
added);
For all document formats, GATE can dump its internal representation of the document
into XML.
In the former case, the XML output will be close to the original document. In the latter
case, the format is a GATE-speciﬁc one which can be read back by the system to recreate
all the information that GATE held internally for the document.
In order to understand why there are two types of XML serialization, one needs to understand
the structure of a GATE document. GATE allows a graph of annotations that refer to
parts of the text. Those annotations are grouped under annotation sets. Because of this
structure, sometimes it is impossible to save a document as XML using tags that surround
the text referred to by the annotation, because tags crossover situations could appear (XML
is essentially a tree-based model of information, whereas GATE uses graphs). Therefore, in
order to preserve all annotations in a GATE document, a custom type of XML document
was developed.

106

Language Resources: Corpora, Documents and Annotations

The problem of crossover tags appears with GATE’s second option (the preserve format
one), which is implemented at the cost of losing certain annotations. The way it is applied
in GATE is that it tries to restore the original markup and where it is possible, to add in
the same manner annotations produced by GATE.
How to Access and Use the Two Forms of XML Serialization
Save as XML Option This option is available in GATE Developer in the pop-up menu
associated with each language resource (document or corpus). Saving a corpus as XML
is done by calling ‘Save as XML’ on each document of the corpus. This option saves all
the annotations of a document together their features(applying the restrictions previously
discussed), using the GateDocument.dtd :
<!ELEMENT GateDocument (GateDocumentFeatures,
TextWithNodes, (AnnotationSet+))>
<!ELEMENT GateDocumentFeatures (Feature+)>
<!ELEMENT Feature (Name, Value)>
<!ELEMENT Name (\#PCDATA)>
<!ELEMENT Value (\#PCDATA)>
<!ELEMENT TextWithNodes (\#PCDATA | Node)*>
<!ELEMENT AnnotationSet (Annotation*)>
<!ATTLIST AnnotationSet Name CDATA \#IMPLIED>
<!ELEMENT Annotation (Feature*)>
<!ATTLIST Annotation Type
CDATA \#REQUIRED
StartNode CDATA \#REQUIRED
EndNode
CDATA \#REQUIRED>
<!ELEMENT Node EMPTY>
<!ATTLIST Node id CDATA \#REQUIRED>

The document is saved under a name chosen by the user and it may have any extension.
However, the recommended extension would be ‘xml’.
Using GATE Embedded, this option is available by calling gate.Document’s toXml()
method. This method returns a string which is the XML representation of the document on
which the method was called.
Note: It is recommended that the string representation to be saved on the ﬁle system using the UTF-8 encoding, as the ﬁrst line of the string is : <?xml version="1.0"
encoding="UTF-8"?>
Example of such a GATE format document:
<?xml version="1.0" encoding="UTF-8" ?>

Language Resources: Corpora, Documents and Annotations

<GateDocument>
<!-- The document’s features-->
<GateDocumentFeatures>
<Feature>
<Name className="java.lang.String">MimeType</Name>
<Value className="java.lang.String">text/plain</Value>
</Feature>
<Feature>
<Name className="java.lang.String">gate.SourceURL</Name>
<Value className="java.lang.String">file:/G:/tmp/example.txt</Value>
</Feature>
</GateDocumentFeatures>
<!-- The document content area with serialized nodes -->
<TextWithNodes>
<Node id="0"/>A TEENAGER <Node
id="11"/>yesterday<Node id="20"/> accused his parents of cruelty
by feeding him a daily diet of chips which sent his weight
ballooning to 22st at the age of l2<Node id="146"/>.<Node
id="147"/>
</TextWithNodes>
<!-- The default annotation set -->
<AnnotationSet>
<Annotation Type="Date" StartNode="11"
EndNode="20">
<Feature>
<Name className="java.lang.String">rule2</Name>
<Value className="java.lang.String">DateOnlyFinal</Value>
</Feature> <Feature>
<Name className="java.lang.String">rule1</Name>
<Value className="java.lang.String">GazDateWords</Value>
</Feature> <Feature>
<Name className="java.lang.String">kind</Name>
<Value className="java.lang.String">date</Value>
</Feature> </Annotation> <Annotation Type="Sentence" StartNode="0"
EndNode="147"> </Annotation> <Annotation Type="Split"
StartNode="146" EndNode="147"> <Feature>
<Name className="java.lang.String">kind</Name>
<Value className="java.lang.String">internal</Value>
</Feature> </Annotation> <Annotation Type="Lookup" StartNode="11"
EndNode="20"> <Feature>
<Name className="java.lang.String">majorType</Name>

107

108

Language Resources: Corpora, Documents and Annotations

<Value className="java.lang.String">date_key</Value>
</Feature> </Annotation>
</AnnotationSet>
<!-- Named annotation set -->
<AnnotationSet Name="Original markups" >
<Annotation
Type="paragraph" StartNode="0" EndNode="147"> </Annotation>
</AnnotationSet>
</GateDocument>

Note: One must know that all features that are not collections containing numbers or strings
or that are not numbers or strings are discarded. With this option, GATE does not preserve
those features it cannot restore back.

The Preserve Format Option This option is available in GATE Developer from the
popup menu of the annotations table. If no annotation in this table is selected, then the
option will restore the document’s original markup. If certain annotations are selected, then
the option will attempt to restore the original markup and insert all the selected ones. When
an annotation violates the crossed over condition, that annotation is discarded and a message
is issued.
This option makes it possible to generate an XML document with tags surrounding the annotation’s referenced text and features saved as attributes. All features which are collections,
strings or numbers are saved, and the others are discarded. However, when read back, only
the attributes under the GATE namespace (see below) are reconstructed back diﬀerently to
the others. That is because GATE does not store in the XML document the information
about the features class and for collections the class of the items. So, when read back, all
features will become strings, except those under the GATE namespace.
One will notice that all generated tags have an attribute called ‘gateId’ under the namespace ‘http://www.gate.ac.uk’. The attribute is used when the document is read back in
GATE, in order to restore the annotation’s old ID. This feature is needed because it works
in close cooperation with another attribute under the same namespace, called ‘matches’.
This attribute indicates annotations/tags that refer the same entity1 . They are under this
namespace because GATE is sensitive to them and treats them diﬀerently to all other elements with their attributes which fall under the general reading algorithm described at the
beginning of this section.
The ‘gateId’ under GATE namespace is used to create an annotation which has as ID the
value indicated by this attribute. The ‘matches’ attribute is used to create an ArrayList in
1

It’s not an XML entity but a information extraction named entity

Language Resources: Corpora, Documents and Annotations

109

which the items will be Integers, representing the ID of annotations that the current one
matches.
Example:
If the text being processed is as follows:

<Person gate:gateId="23">John</Person> and <Person
gate:gateId="25" gate:matches="23;25;30">John Major</Person> are
the same person.

What GATE does when it parses this text is it creates two annotations:

a1.type = "Person"
a1.ID = Integer(23)
a1.start = <the start offset of
John>
a1.end = <the end offset of John>
a1.featureMap = {}
a2.type = "Person"
a2.ID = Integer(25)
a2.start = <the start offset
of John Major>
a2.end = <the end offset of John Major>
a2.featureMap = {matches=[Integer(23); Integer(25); Integer(30)]}

Under GATE Embedded, this option is available by calling gate.Document’s toXml(Set
aSetContainingAnnotations) method. This method returns a string which is the XML
representation of the document on which the method was called. If called with null as
a parameter, then the method will attempt to restore only the original markup. If the
parameter is a set that contains annotations, then each annotation is tested against the
crossover restriction, and for those found to violate it, a warning will be issued and they will
be discarded.
In the next subsections we will show how this option applies to the other formats supported
by GATE.

110

Language Resources: Corpora, Documents and Annotations

5.5.3

HTML

Input
HTML documents are parsed by GATE using the NekoHTML parser. The documents are
read and created in GATE the same way as the XML documents.
The extensions associate with the HTML reader are:

htm

html

The web server content type associate with html documents is: text/html.
The magic numbers test searches inside the document for the HTML(<html) signature.There
are certain HTML documents that do not contain the HTML tag, so the magical numbers
test might not hold.
There is a certain degree of customization for HTML documents in that GATE introduces
new lines into the document’s text content in order to obtain a readable form. The annotations will refer the pieces of text as described in the original document but there will be a
few extra new line characters inserted.
After reading H1, H2, H3, H4, H5, H6, TR, CENTER, LI, BR and DIV tags, GATE will
introduce a new line (NL) char into the text. After a TITLE tag it will introduce two NLs.
With P tags, GATE will introduce one NL at the beginning of the paragraph and one at
the end of the paragraph. All newly added NLs are not considered to be part of the text
contained by the tag.

Output
The ‘Save as XML’ option works exactly the same for all GATE’s documents so there is no
particular observation to be made for the HTML formats.
When attempting to preserve the original markup formatting, GATE will generate the document in xhtml. The html document will look the same with any browser after processed
by GATE but it will be in another syntax.

Language Resources: Corpora, Documents and Annotations

5.5.4

111

SGML

Input
The SGML support in GATE is fairly light as there is no freely available Java SGML parser.
GATE uses a light converter attempting to transform the input SGML ﬁle into a well formed
XML. Because it does not make use of a DTD, the conversion might not be always good.
It is advisable to perform a SGML2XML conversion outside the system(using some other
specialized tools) before using the SGML document inside GATE.
The extensions associate with the SGML reader are:
sgm
sgml
The web server content type associate with xml documents is : text/sgml.
There is no magic numbers test for SGML.

Output
When attempting to preserve the original markup formatting, GATE will generate the document as XML because the real input of a SGML document inside GATE is an XML one.

5.5.5

Plain text

Input
When reading a plain text document, GATE attempts to detect its paragraphs and add
‘paragraph’ annotations to the document’s ‘Original markups’ annotation set. It does that
by detecting two consecutive NLs. The procedure works for both UNIX like or DOS like
text ﬁles.
Example:
If the plain text read is as follows:
Paragraph 1. This text belongs to the first paragraph.
Paragraph 2. This text belongs to the second paragraph

112

Language Resources: Corpora, Documents and Annotations

then two ‘paragraph’ type annotation will be created in the ‘Original markups’ annotation
set (referring the ﬁrst and second paragraphs ) with an empty feature map.
The extensions associate with the plain text reader are:

txt
text

The web server content type associate with plain text documents is: text/plain.
There is no magic numbers test for plain text.

Output
When attempting to preserve the original markup formatting, GATE will dump XML
markup that surrounds the text refereed.
The procedure described above applies both for plain text and RTF documents.

5.5.6

RTF

Input
Accessing RTF documents is performed by using the Java’s RTF editor kit. It only extracts
the document’s text content from the RTF document.
The extension associate with the RTF reader is ‘rtf ’.
The web server content type associate with xml documents is : text/rtf.
The magic numbers test searches for {\\rtf1.

Output
Same as the plain tex output.

Language Resources: Corpora, Documents and Annotations

5.5.7

113

Email

Input
GATE is able to read email messages packed in one document (UNIX mailbox format). It
detects multiple messages inside such documents and for each message it creates annotations
for all the ﬁelds composing an e-mail, like date, from, to, subject, etc. The message’s body
is analyzed and a paragraph detection is performed (just like in the plain text case) . All
annotation created have as type the name of the e-mail’s ﬁelds and they are placed in the
Original markup annotation set.
Example:
From someone@zzz.zzz.zzz Wed Sep

6 10:35:50 2000

Date: Wed, 6 Sep2000 10:35:49 +0100 (BST)
From: forename1 surname2 <someone1@yyy.yyy.xxx>
To: forename2 surname2 <someone2@ddd.dddd.dd.dd>
Subject: A subject
Message-ID: <Pine.SOL.3.91.1000906103251.26010A-100000@servername>
MIME-Version: 1.0
Content-Type: TEXT/PLAIN; charset=US-ASCII
This text belongs to the e-mail body....
This is a paragraph in the body of the e-mail
This is another paragraph.

GATE attempts to detect lines such as ‘From someone@zzz.zzz.zzz Wed Sep 6 10:35:50 2000’
in the e-mail text. Those lines separate e-mail messages contained in one ﬁle. After that,
for each ﬁeld in the e-mail message annotations are created as follows:
The annotation type will be the name of the ﬁeld, the feature map will be empty and the
annotation will span from the end of the ﬁeld until the end of the line containing the e-mail
ﬁeld.
Example:

114

Language Resources: Corpora, Documents and Annotations

a1.type = "date" a1 spans between the two ^ ^. Date:^ Wed,
6Sep2000 10:35:49 +0100 (BST)^
a2.type = "from"; a2 spans between the two ^ ^. From:^ forename1
surname2 <someone1@yyy.yyy.xxx>^

The extensions associated with the email reader are:
eml
email
mail
The web server content type associate with plain text documents is: text/email.
The magic numbers test searches for keywords like Subject:,etc.
Output
Same as plain text output.

5.5.8

PDF Files and Oﬃce Documents

GATE uses the Apache Tika library to provide support for PDF documents and a number of
the document formats from both Microsoft Oﬃce and OpenOﬃce. In essense Tika converts
the document structure into HTML which is then used to create a GATE document. This
means that whilst a PDF or Word document may have been loaded the “Original markups”
set will contain HTML elements. One advantage of this approach is that processing resources
and JAPE grammars designed for use with HTML ﬁles should also work well with PDF and
Oﬃce documents.

5.6

XML Input/Output

Support for input from and output to XML is described in Section 5.5.2. In short:
GATE will read any well-formed XML document (it does not attempt to validate XML
documents). Markup will by default be converted into native GATE format.
GATE will write back into XML in one of two ways:

Language Resources: Corpora, Documents and Annotations

115

1. Preserving the original format and adding selected markup (for example to add
the results of some language analysis process to the document).
2. In GATE’s own XML serialisation format, which encodes all the data in a GATE
Document (as far as this is possible within a tree-structured paradigm – for 100%
non-lossy data storage use GATE’s RDBMS or binary serialisation facilities – see
Section 4.5).
When using GATE Embedded, object representations of XML documents such as DOM or
jDOM, or query and transformation languages such as X-Path or XSLT, may be used in parallel
with GATE’s own Document representation (gate.Document) without conﬂicts.

116

Language Resources: Corpora, Documents and Annotations

Chapter 6
ANNIE: a Nearly-New Information
Extraction System
And so the time had passed predictably and soberly enough in work and routine
chores, and the events of the previous night from ﬁrst to last had faded; and only
now that both their days’ work was over, the child asleep and no further disturbance anticipated, did the shadowy ﬁgures from the masked ball, the melancholy
stranger and the dominoes in red, revive; and those trivial encounters became
magically and painfully interfused with the treacherous illusion of missed opportunities. Innocent yet ominous questions and vague ambiguous answers passed
to and fro between them; and, as neither of them doubted the other’s absolute
candour, both felt the need for mild revenge. They exaggerated the extent to
which their masked partners had attracted them, made fun of the jealous stirrings the other revealed, and lied dismissively about their own. Yet this light
banter about the trivial adventures of the previous night led to more serious discussion of those hidden, scarcely admitted desires which are apt to raise dark and
perilous storms even in the purest, most transparent soul; and they talked about
those secret regions for which they felt hardly any longing, yet towards which the
irrational wings of fate might one day drive them, if only in their dreams. For
however much they might belong to one another heart and soul, they knew last
night was not the ﬁrst time they had been stirred by a whiﬀ of freedom, danger
and adventure.
Dream Story, Arthur Schnitzler, 1926 (pp. 4-5).
GATE was originally developed in the context of Information Extraction (IE) R&D, and IE
systems in many languages and shapes and sizes have been created using GATE with the
IE components that have been distributed with it (see [Maynard et al. 00] for descriptions
of some of these projects).1
1
The principal architects of the IE systems in GATE version 1 were Robert Gaizauskas and Kevin
Humphreys. This work lives on in the LaSIE system. (A derivative of LaSIE was distributed with GATE

117

118

ANNIE: a Nearly-New Information Extraction System

GATE is distributed with an IE system called ANNIE, A Nearly-New IE system (developed by Hamish Cunningham, Valentin Tablan, Diana Maynard, Kalina Bontcheva, Marin
Dimitrov and others). ANNIE relies on ﬁnite state algorithms and the JAPE language (see
Chapter 8).
ANNIE components form a pipeline which appears in ﬁgure 6.1. ANNIE components are

Figure 6.1: ANNIE and LaSIE
included with GATE (though the linguistic resources they rely on are generally more simple
than the ones we use in-house). The rest of this chapter describes these components.

6.1

Document Reset

The document reset resource enables the document to be reset to its original state, by removing all the annotation sets and their contents, apart from the one containing the document
format analysis (Original Markups). An optional parameter, keepOriginalMarkupsAS, allows users to decide whether to keep the Original Markups AS or not while reseting the
document. The parameter annotationTypes can be used to specify a list of annotation
types to remove from all the sets instead of the whole sets.
version 1 under the name VIE, a Vanilla IE system.)

ANNIE: a Nearly-New Information Extraction System

119

Alternatively, if the parameter setsToRemove is not empty, the other parameters except
annotationTypes are ignored and only the annotation sets speciﬁed in this list will be
removed. If annotationTypes is also speciﬁed, only those annotation types in the speciﬁed
sets are removed. In order to specify that you want to reset the default annotation set, just
click the ”Add” button without entering a name – this will add <null> which denotes the
default annotation set. This resource is normally added to the beginning of an application,
so that a document is reset before an application is rerun on that document.

6.2

Tokeniser

The tokeniser splits the text into very simple tokens such as numbers, punctuation and words
of diﬀerent types. For example, we distinguish between words in uppercase and lowercase,
and between certain types of punctuation. The aim is to limit the work of the tokeniser
to maximise eﬃciency, and enable greater ﬂexibility by placing the burden on the grammar
rules, which are more adaptable.

6.2.1

Tokeniser Rules

A rule has a left hand side (LHS) and a right hand side (RHS). The LHS is a regular
expression which has to be matched on the input; the RHS describes the annotations to be
added to the AnnotationSet. The LHS is separated from the RHS by ‘>’. The following
operators can be used on the LHS:
|
*
?
+

(or)
(0 or more occurrences)
(0 or 1 occurrences)
(1 or more occurrences)

The RHS uses ‘;’ as a separator, and has the following format:
{LHS} > {Annotation type};{attribute1}={value1};...;{attribute
n}={value n}

Details about the primitive constructs available are given in the tokeniser ﬁle (DefaultTokeniser.Rules).
The following tokeniser rule is for a word beginning with a single capital letter:
‘UPPERCASE_LETTER’ ‘LOWERCASE_LETTER’* >
Token;orth=upperInitial;kind=word;

120

ANNIE: a Nearly-New Information Extraction System

It states that the sequence must begin with an uppercase letter, followed by zero or more
lowercase letters. This sequence will then be annotated as type ‘Token’. The attribute ‘orth’
(orthography) has the value ‘upperInitial’; the attribute ‘kind’ has the value ‘word’.

6.2.2

Token Types

In the default set of rules, the following kinds of Token and SpaceToken are possible:

Word
A word is deﬁned as any set of contiguous upper or lowercase letters, including a hyphen
(but no other forms of punctuation). A word also has the attribute ‘orth’, for which four
values are deﬁned:
upperInitial - initial letter is uppercase, rest are lowercase
allCaps - all uppercase letters
lowerCase - all lowercase letters
mixedCaps - any mixture of upper and lowercase letters not included in the above
categories

Number
A number is deﬁned as any combination of consecutive digits. There are no subdivisions of
numbers.

Symbol
Two types of symbol are deﬁned: currency symbol (e.g. ‘ ’, ‘ ’) and symbol (e.g. ‘&’, ‘ˆ’).
These are represented by any number of consecutive currency or other symbols (respectively).

Punctuation
Three types of punctuation are deﬁned: start punctuation (e.g. ‘(’), end punctuation (e.g.
‘)’), and other punctuation (e.g. ‘:’). Each punctuation symbol is a separate token.

ANNIE: a Nearly-New Information Extraction System

121

SpaceToken
White spaces are divided into two types of SpaceToken - space and control - according to
whether they are pure space characters or control characters. Any contiguous (and homogeneous) set of space or control characters is deﬁned as a SpaceToken.
The above description applies to the default tokeniser. However, alternative tokenisers can
be created if necessary. The choice of tokeniser is then determined at the time of text
processing.

6.2.3

English Tokeniser

The English Tokeniser is a processing resource that comprises a normal tokeniser and a JAPE
transducer (see Chapter 8). The transducer has the role of adapting the generic output of
the tokeniser to the requirements of the English part-of-speech tagger. One such adaptation
is the joining together in one token of constructs like “ ’30s”, “ ’Cause”, “ ’em”, “ ’N”, “
’S”, “ ’s”, “ ’T”, “ ’d”, “ ’ll”, “ ’m”, “ ’re”, “ ’til”, “ ve”, etc. Another task of the JAPE
transducer is to convert negative constructs like “don’t” from three tokens (“don”, “ ’ “ and
“t”) into two tokens (“do” and “n’t”).
The English Tokeniser should always be used on English texts that need to be processed
afterwards by the POS Tagger.

6.3

Gazetteer

The role of the gazetteer is to identify entity names in the text based on lists. The ANNIE
gazetteer is described here, and also covered in Chapter 13 in Section 13.2.
The gazetteer lists used are plain text ﬁles, with one entry per line. Each list represents a
set of names, such as names of cities, organisations, days of the week, etc.
Below is a small section of the list for units of currency:
Ecu
European Currency Units
FFr
Fr
German mark
German marks
New Taiwan dollar
New Taiwan dollars
NT dollar

122

ANNIE: a Nearly-New Information Extraction System

NT dollars

An index ﬁle (lists.def) is used to access these lists; for each list, a major type is speciﬁed
and, optionally, a minor type 2 . In the example below, the ﬁrst column refers to the list
name, the second column to the major type, and the third to the minor type. These lists are
compiled into ﬁnite state machines. Any text tokens that are matched by these machines
will be annotated with features specifying the major and minor types. Grammar rules then
specify the types to be identiﬁed in particular circumstances. Each gazetteer list should
reside in the same directory as the index ﬁle.
currency_prefix.lst:currency_unit:pre_amount
currency_unit.lst:currency_unit:post_amount
date.lst:date:specific
day.lst:date:day

So, for example, if a speciﬁc day needs to be identiﬁed, the minor type ‘day’ should be
speciﬁed in the grammar, in order to match only information about speciﬁc days; if any kind
of date needs to be identiﬁed,the major type ‘date’ should be speciﬁed, to enable tokens
annotated with any information about dates to be identiﬁed. More information about this
can be found in the following section.
In addition, the gazetteer allows arbitrary feature values to be associated with particular
entries in a single list. ANNIE does not use this capability, but to enable it for your own
gazetteers, set the optional gazetteerFeatureSeparator parameter to a single character
(or an escape sequence such as \t or \uNNNN) when creating a gazetteer. In this mode, each
line in a .lst ﬁle can have feature values speciﬁed, for example, with the following entry in
the index ﬁle:
software_company.lst:company:software

the following software_company.lst:
Red Hat&stockSymbol=RHAT
Apple Computer&abbrev=Apple&stockSymbol=AAPL
Microsoft&abbrev=MS&stockSymbol=MSFT

and gazetteerFeatureSeparator set to &, the gazetteer will annotate Red Hat as a Lookup
with features majorType=company, minorType=software and stockSymbol=RHAT. Note that
you do not have to provide the same features for every line in the ﬁle, in particular it is
possible to provide extra features for some lines in the list but not others.
2
It is also possible to include a language in the same way, where lists for diﬀerent languages are used,
though ANNIE is only concerned with monolingual recognition

ANNIE: a Nearly-New Information Extraction System

123

Here is a full list of the parameters used by the Default Gazetteer:
Init-time parameters
listsURL A URL pointing to the index ﬁle (usually lists.def) that contains the list of pattern
lists.
encoding The character encoding to be used while reading the pattern lists.
gazetteerFeatureSeparator The character used to add arbitrary features to gazetteer
entries. See above for an example.
caseSensitive Should the gazetteer be case sensitive during matching.
Run-time parameters
document The document to be processed.
annotationSetName The name for annotation set where the resulting Lookup annotations
will be created.
wholeWordsOnly Should the gazetteer only match whole words? If set to true, a string
segment in the input document will only be matched if it is bordered by characters
that are not letters, non spacing marks, or combining spacing marks (as identiﬁed by
the Unicode standard).
longestMatchOnly Should the gazetteer only match the longest possible string starting
from any position. This parameter is only relevant when the list of lookups contains
proper preﬁxes of other entries (e.g when both ‘Dell’ and ‘Dell Europe’ are in the lists).
The default behaviour (when this parameter is set to true) is to only match the longest
entry, ‘Dell Europe’ in this example. This is the default GATE gazetteer behaviour
since version 2.0. Setting this parameter to false will cause the gazetteer to match
all possible preﬁxes.

6.4

Sentence Splitter

The sentence splitter is a cascade of ﬁnite-state transducers which segments the text into
sentences. This module is required for the tagger. The splitter uses a gazetteer list of
abbreviations to help distinguish sentence-marking full stops from other kinds.
Each sentence is annotated with the type ‘Sentence’. Each sentence break (such as a full
stop) is also given a ‘Split’ annotation. It has a feature ‘kind’ with two possible values:
‘internal’ for any combination of exclamation and question mark or one to four dots and
‘external’ for a newline.

124

ANNIE: a Nearly-New Information Extraction System

The sentence splitter is domain and application-independent.
There is an alternative ruleset for the Sentence Splitter which considers newlines and carriage
returns diﬀerently. In general this version should be used when a new line on the page
indicates a new sentence). To use this alternative version, simply load the main-singlenl.jape from the default location instead of main.jape (the default ﬁle) when asked to select
the location of the grammar ﬁle to be used.

6.5

RegEx Sentence Splitter

The RegEx sentence splitter is an alternative to the standard ANNIE Sentence Splitter.
Its main aim is to address some performance issues identiﬁed in the JAPE-based splitter,
mainly do to with improving the execution time and robustness, especially when faced with
irregular input.
As its name suggests, the RegEx splitter is based on regular expressions, using the default
Java implementation.
The new splitter is conﬁgured by three ﬁles containing (Java style, see http://
java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html) regular expressions, one regex per line. The three diﬀerent ﬁles encode patterns for:
internal splits sentence splits that are part of the sentence, such as sentence ending punctuation;
external splits sentence splits that are NOT part of the sentence, such as 2 consecutive
new lines;
non splits text fragments that might be seen as splits but they should be ignored (such as
full stops occurring inside abbreviations).
The new splitter comes with an initial set of patterns that try to emulate the behaviour of
the original splitter (apart from the situations where the original one was obviously wrong,
like not allowing sentences to start with a number).
Here is a full list of the parameters used by the RegEx Sentence Splitter:
Init-time parameters
encoding The character encoding to be used while reading the pattern lists.
externalSplitListURL URL for the ﬁle containing the list of external split patterns;
internalSplitListURL URL for the ﬁle containing the list of internal split patterns;

ANNIE: a Nearly-New Information Extraction System

125

nonSplitListURL URL for the ﬁle containing the list of non split patterns;
Run-time parameters
document The document to be processed.
outputASName The name for annotation set where the resulting Split and Sentence
annotations will be created.

6.6

Part of Speech Tagger

The tagger [Hepple 00] is a modiﬁed version of the Brill tagger, which produces a partof-speech tag as an annotation on each word or symbol. The list of tags used is given in
Appendix G. The tagger uses a default lexicon and ruleset (the result of training on a large
corpus taken from the Wall Street Journal). Both of these can be modiﬁed manually if
necessary. Two additional lexicons exist - one for texts in all uppercase (lexicon cap), and
one for texts in all lowercase (lexicon lower). To use these, the default lexicon should be
replaced with the appropriate lexicon at load time. The default ruleset should still be used
in this case.
The ANNIE Part-of-Speech tagger requires the following parameters.
encoding - encoding to be used for reading rules and lexicons (init-time)
lexiconURL - The URL for the lexicon ﬁle (init-time)
rulesURL - The URL for the ruleset ﬁle (init-time)
document - The document to be processed (run-time)
inputASName - The name of the annotation set used for input (run-time)
outputASName - The name of the annotation set used for output (run-time). This is
an optional parameter. If user does not provide any value, new annotations are created
under the default annotation set.
baseTokenAnnotationType - The name of the annotation type that refers to Tokens in
a document (run-time, default = Token)
baseSentenceAnnotationType - The name of the annotation type that refers to Sentences in a document (run-time, default = Sentences)
outputAnnotationType - POS tags are added as category features on the annotations
of type ‘outputAnnotationType’ (run-time, default = Token)

126

ANNIE: a Nearly-New Information Extraction System

failOnMissingInputAnnotations - if set to false, the PR will not fail with an ExecutionException if no input Annotations are found and instead only log a single warning
message per session and a debug message per document that has no input annotations
(run-time, default = true).
If - (inputASName == outputASName) AND (outputAnnotationType == baseTokenAnnotationType)
then - New features are added on existing annotations of type ‘baseTokenAnnotationType’.
otherwise - Tagger searches for the annotation of type ‘outputAnnotationType’ under the
‘outputASName’ annotation set that has the same oﬀsets as that of the annotation with
type ‘baseTokenAnnotationType’. If it succeeds, it adds new feature on a found annotation, and otherwise, it creates a new annotation of type ‘outputAnnotationType’ under the
‘outputASName’ annotation set.

6.7

Semantic Tagger

ANNIE’s semantic tagger is based on the JAPE language – see Chapter 8. It contains rules
which act on annotations assigned in earlier phases, in order to produce outputs of annotated
entities.

6.8

Orthographic Coreference (OrthoMatcher)

(Note: this component was previously known as a ‘NameMatcher’.)
The Orthomatcher module adds identity relations between named entities found by the
semantic tagger, in order to perform coreference. It does not ﬁnd new named entities as
such, but it may assign a type to an unclassiﬁed proper name, using the type of a matching
name.
The matching rules are only invoked if the names being compared are both of the same type,
i.e. both already tagged as (say) organisations, or if one of them is classiﬁed as ‘unknown’.
This prevents a previously classiﬁed name from being recategorised.

6.8.1

GATE Interface

Input – entity annotations, with an id attribute.
Output – matches attributes added to the existing entity annotations.

ANNIE: a Nearly-New Information Extraction System

6.8.2

127

Resources

A lookup table of aliases is used to record non-matching strings which represent the same
entity, e.g. ‘IBM’ and ‘Big Blue’, ‘Coca-Cola’ and ‘Coke’. There is also a table of spurious
matches, i.e. matching strings which do not represent the same entity, e.g. ‘BT Wireless’ and
‘BT Cellnet’ (which are two diﬀerent organizations). The list of tables to be used is a load
time parameter of the orthomatcher: a default list is set but can be changed as necessary.

6.8.3

Processing

The wrapper builds an array of the strings, types and IDs of all name annotations, which is
then passed to a string comparison function for pairwise comparisons of all entries.

6.9

Pronominal Coreference

The pronominal coreference module performs anaphora resolution using the JAPE grammar
formalism. Note that this module is not automatically loaded with the other ANNIE modules, but can be loaded separately as a Processing Resource. The main module consists of
three submodules:
quoted text module
pleonastic it module
pronominal resolution module
The ﬁrst two modules are helper submodules for the pronominal one, because they do not
perform anything related to coreference resolution except the location of quoted fragments
and pleonastic it occurrences in text. They generate temporary annotations which are used
by the pronominal submodule (such temporary annotations are removed later).
The main coreference module can operate successfully only if all ANNIE modules were
already executed. The module depends on the following annotations created from the respective ANNIE modules:
Token (English Tokenizer)
Sentence (Sentence Splitter)
Split (Sentence Splitter)
Location (NE Transducer, OrthoMatcher)

128

ANNIE: a Nearly-New Information Extraction System

Person (NE Transducer, OrthoMatcher)
Organization (NE Transducer, OrthoMatcher)
For each pronoun (anaphor) the coreference module generates an annotation of type ‘Coreference’ containing two features:
antecedent oﬀset - this is the oﬀset of the starting node for the annotation (entity)
which is proposed as the antecedent, or null if no antecedent can be proposed.
matches - this is a list of annotation IDs that comprise the coreference chain comprising
this anaphor/antecedent pair.

6.9.1

Quoted Speech Submodule

The quoted speech submodule identiﬁes quoted fragments in the text being analysed. The
identiﬁed fragments are used by the pronominal coreference submodule for the proper resolution of pronouns such as I, me, my, etc. which appear in quoted speech fragments. The
module produces ‘Quoted Text’ annotations.
The submodule itself is a JAPE transducer which loads a JAPE grammar and builds an
FSM over it. The FSM is intended to match the quoted fragments and generate appropriate
annotations that will be used later by the pronominal module.
The JAPE grammar consists of only four rules, which create temporary annotations for all
punctuation marks that may enclose quoted speech, such as ”, ’, ‘, etc. These rules then
try to identify fragments enclosed by such punctuation. Finally all temporary annotations
generated during the processing, except the ones of type ‘Quoted Text’, are removed (because
no other module will need them later).

6.9.2

Pleonastic It Submodule

The pleonastic it submodule matches pleonastic occurrences of ‘it’. Similar to the quoted
speech submodule, it is a JAPE transducer operating with a grammar containing patterns
that match the most commonly observed pleonastic it constructs.

6.9.3

Pronominal Resolution Submodule

The main functionality of the coreference resolution module is in the pronominal resolution
submodule. This uses the result from the execution of the quoted speech and pleonastic it
submodules. The module works according to the following algorithm:

ANNIE: a Nearly-New Information Extraction System

129

Preprocess the current document. This step locates the annotations that the submodule need (such as Sentence, Token, Person, etc.) and prepares the appropriate data
structures for them.
For each pronoun do the following:
– inspect the proper appropriate context for all candidate antecedents for this kind
of pronoun;
– choose the best antecedent (if any);
Create the coreference chains from the individual anaphor/antecedent pairs and the
coreference information supplied by the OrthoMatcher (this step is performed from the
main coreference module).

6.9.4

Detailed Description of the Algorithm

Full details of the pronominal coreference algorithm are as follows.
Preprocessing
The preprocessing task includes the following subtasks:
Identifying the sentences in the document being processed. The sentences are identiﬁed
with the help of the Sentence annotations generated from the Sentence Splitter. For
each sentence a data structure is prepared that contains three lists. The lists contain
the annotations for the person/organization/location named entities appearing in the
sentence. The named entities in the sentence are identiﬁed with the help of the Person,
Location and Organization annotations that are already generated from the Named
Entity Transducer and the OrthoMatcher.
The gender of each person in the sentence is identiﬁed and stored in a global data
structure. It is possible that the gender information is missing for some entities - for
example if only the person family name is observed then the Named Entity transducer
will be unable to deduce the gender. In such cases the list with the matching entities
generated by the OrhtoMatcher is inspected and if some of the orthographic matches
contains gender information it is assigned to the entity being processed.
The identiﬁed pleonastic it occurrences are stored in a separate list. The ‘Pleonastic
It’ annotations generated from the pleonastic submodule are used for the task.
For each quoted text fragment, identiﬁed by the quoted text submodule, a special
structure is created that contains the persons and the 3rd person singular pronouns
such as ‘he’ and ‘she’ that appear in the sentence containing the quoted text, but not
in the quoted text span (i.e. the ones preceding and succeeding the quote).

130

ANNIE: a Nearly-New Information Extraction System

Pronoun Resolution
This task includes the following subtasks:
Retrieving all the pronouns in the document. Pronouns are represented as annotations of
type ‘Token’ with feature ‘category’ having value ‘PRP ’ or ‘PRP’. The former classiﬁes
possessive adjectives such as my, your, etc. and the latter classiﬁes personal, reﬂexive etc.
pronouns. The two types of pronouns are combined in one list and sorted according to their
oﬀset in the text.
For each pronoun in the list the following actions are performed:
If the pronoun is ‘it’, then the module performs a check to determine if this is a
pleonastic occurrence. If it is, then no further attempt for resolution is made.
The proper context is determined. The context size is expressed in the number of
sentences it will contain. The context always includes the current sentence (the one
containing the pronoun), the preceding sentence and zero or more preceding sentences.
Depending on the type of pronoun, a set of candidate antecedents is proposed. The
candidate set includes the named entities that are compatible with this pronoun. For
example if the current pronoun is she then only the Person annotations with ‘gender’
feature equal to ‘female’ or ‘unknown’ will be considered as candidates.
From all candidates, one is chosen according to evaluation criteria speciﬁc for the
pronoun.

Coreference Chain Generation
This step is actually performed by the main module. After executing each of the submodules
on the current document, the coreference module follows the steps:

Retrieves the anaphor/antecedent pairs generated from them.
For each pair, the orthographic matches (if any) of the antecedent entity is retrieved
and then extended with the anaphor of the pair (i.e. the pronoun). The result is
the coreference chain for the entity. The coreference chain contains the IDs of the
annotations (entities) that co-refer.
A new Coreference annotation is created for each chain. The annotation contains a
single feature ‘matches’ whose value is the coreference chain (the list with IDs). The
annotations are exported in a pre-speciﬁed annotation set.

ANNIE: a Nearly-New Information Extraction System

131

The resolution of she, her, her , he, him, his, herself and himself are similar because an
analysis of a corpus showed that these pronouns are related to their antecedents in a similar
manner. The characteristics of the resolution process are:
Context inspected is not very big - cases where the antecedent is found more than 3
sentences back from the anaphor are rare.
Recency factor is heavily used - the candidate antecedents that appear closer to the
anaphor in the text are scored better.
Anaphora have higher priority than cataphora. If there is an anaphoric candidate and
a cataphoric one, then the anaphoric one is preferred, even if the recency factor scores
the cataphoric candidate better.
The resolution process performs the following steps:
Inspect the context of the anaphor for candidate antecedents. Every Person annotation
is consider to be a candidate. Cases where she/her refers to inanimate entity (ship for
example) are not handled.
For each candidate perform a gender compatibility check - only candidates having
‘gender’ feature equal to ‘unknown’ or compatible with the pronoun are considered for
further evaluation.
Evaluate each candidate with the best candidate so far. If the two candidates are
anaphoric for the pronoun then choose the one that appears closer. The same holds
for the case where the two candidates are cataphoric relative to the pronoun. If one is
anaphoric and the other is cataphoric then choose the former, even if the latter appears
closer to the pronoun.
Resolution of ‘it’, ‘its’, ‘itself ’
This set of pronouns also shares many common characteristics. The resolution process contains certain diﬀerences with the one for the previous set of pronouns. Successful resolution
for it, its, itself is more diﬃcult because of the following factors:
There is no gender compatibility restriction. In the case in which there are several
candidates in the context, the gender compatibility restriction is very useful for rejecting some of the candidates. When no such restriction exists, and with the lack of
any syntactic or ontological information about the entities in the context, the recency
factor plays the major role in choosing the best antecedent.
The number of nominal antecedents (i.e. entities that are not referred by name) is
much higher compared to the number of such antecedents for she, he, etc. In this case
trying to ﬁnd an antecedent only amongst named entities degrades the precision a lot.

132

ANNIE: a Nearly-New Information Extraction System

Resolution of ‘I’, ‘me’, ‘my’, ‘myself ’
Resolution of these pronouns is dependent on the work of the quoted speech submodule.
One important diﬀerence from the resolution process of other pronouns is that the context
is not measured in sentences but depends solely on the quote span. Another diﬀerence is
that the context is not contiguous - the quoted fragment itself is excluded from the context,
because it is unlikely that an antecedent for I, me, etc. appears there. The context itself
consists of:
the part of the sentence where the quoted fragment originates, that is not contained
in the quote - i.e. the text prior to the quote;
the part of the sentence where the quoted fragment ends, that is not contained in the
quote - i.e. the text following the quote;
the part of the sentence preceding the sentence where the quote originates, which is
not included in other quote.
It is worth noting that contrary to other pronouns, the antecedent for I, me, my and myself is
most often cataphoric or if anaphoric it is not in the same sentence with the quoted fragment.
The resolution algorithm consists of the following steps:
Locate the quoted fragment description that contains the pronoun. If the pronoun is
not contained in any fragment then return without proposing an antecedent.
Inspect the context for the quoted fragment (as deﬁned above) for candidate antecedents. Candidates are considered annotations of type Pronoun or annotations
of type Token with features category = ‘PRP’, string = ‘she’ or category = ‘PRP’,
string = ‘he’.
Try to locate a candidate in the text succeeding the quoted fragment (ﬁrst pattern).
If more than one candidate is present, choose the closest to the end of the quote. If a
candidate is found then propose it as antecedent and exit.
Try to locate a candidate in the text preceding the quoted fragment (third pattern).
Choose the closest one to the beginning of the quote. If found then set as antecedent
and exit.
Try to locate antecedents in the unquoted part of the sentence preceding the sentence
where the quote starts (second pattern). Give preference to the one closest to the end
of the quote (if any) in the preceding sentence or closest to the sentence beginning.

ANNIE: a Nearly-New Information Extraction System

6.10

133

A Walk-Through Example

Let us take an example of a 3-stage procedure using the tokeniser, gazetteer and namedentity grammar. Suppose we wish to recognise the phrase ‘800,000 US dollars’ as an entity
of type ‘Number’, with the feature ‘money’.
First of all, we give an example of a grammar rule (and corresponding macros) for money,
which would recognise this type of pattern.

Macro: MILLION_BILLION
({Token.string == "m"}|
{Token.string == "million"}|
{Token.string == "b"}|
{Token.string == "billion"}
)
Macro: AMOUNT_NUMBER
({Token.kind == number}
(({Token.string == ","}|
{Token.string == "."})
{Token.kind == number})*
(({SpaceToken.kind == space})?
(MILLION_BILLION)?)
)
Rule: Money1
// e.g. 30 pounds
(
(AMOUNT_NUMBER)
(SpaceToken.kind == space)?
({Lookup.majorType == currency_unit})
)
:money -->
:money.Number = {kind = "money", rule = "Money1"}

6.10.1

Step 1 - Tokenisation

The tokeniser separates this phrase into the following tokens. In general, a word is comprised
of any number of letters of either case, including a hyphen, but nothing else; a number is
composed of any sequence of digits; punctuation is recognised individually (each character
is a separate token), and any number of consecutive spaces and/or control characters are
recognised as a single spacetoken.

134

ANNIE: a Nearly-New Information Extraction System

Token, string = ‘800’, kind = number, length = 3
Token, string = ‘,’, kind = punctuation, length = 1
Token, string = ‘000’, kind = number, length = 3
SpaceToken, string = ‘ ’, kind = space, length = 1
Token, string = ‘US’, kind = word, length = 2, orth = allCaps
SpaceToken, string = ‘ ’, kind = space, length = 1
Token, string = ‘dollars’, kind = word, length = 7, orth = lowercase

6.10.2

Step 2 - List Lookup

The gazetteer lists are then searched to ﬁnd all occurrences of matching words in the text.
It ﬁnds the following match for the string ‘US dollars’:
Lookup, minorType = post_amount, majorType = currency_unit

6.10.3

Step 3 - Grammar Rules

The grammar rule for money is then invoked. The macro MILLION BILLION recognises
any of the strings ‘m’, ‘million’, ‘b’, ‘billion’. Since none of these exist in the text, it passes
onto the next macro. The AMOUNT NUMBER macro recognises a number, optionally
followed by any number of sequences of the form‘dot or comma plus number’, followed
by an optional space and an optional MILLION BILLION. In this case, ‘800,000’ will be
recognised. Finally, the rule Money1 is invoked. This recognises the string identiﬁed by the
AMOUNT NUMBER macro, followed by an optional space, followed by a unit of currency
(as determined by the gazetteer). In this case, ‘US dollars’ has been identiﬁed as a currency
unit, so the rule Money1 recognises the entire string ‘800,000 US dollars’. Following the rule,
it will be annotated as a Number entity of type Money:
Number, kind = money, rule = Money1

Part II
GATE for Advanced Users

135

Chapter 7
GATE Embedded
7.1

Quick Start with GATE Embedded

Embedding GATE-based language processing in other applications using GATE Embedded
(the GATE API) is straightforward:
add $GATE_HOME/bin/gate.jar and the JAR ﬁles in $GATE_HOME/lib to the Java
CLASSPATH ($GATE_HOME is the GATE root directory)
tell Java that the GATE Unicode Kit is an extension:
-Djava.ext.dirs=$GATE_HOME/lib/ext
N.B. This is only necessary for GUI applications that need to support Unicode text
input; other applications such as command line or web applications don’t generally
need GUK.
initialise GATE with gate.Gate.init();
program to the framework API.
For example, this code will create the ANNIE extraction system:
1
2

// initialise the GATE library
Gate . init ();

3
4
5
6
7
8

// load ANNIE as an application from a gapp ﬁle
S e r i a l A n a l y s e r C o n t r o l l e r controller = ( S e r i a l A n a l y s e r C o n t r o l l e r )
Pe rsistenceManager . load Ob je ct Fr om Fi le ( new File ( new File (
Gate . getPluginsHome () , ANNIEConstants . PLUGIN_DIR ) ,
ANNIEConstants . DEFAULT_FILE ));

137

138

GATE Embedded

If you want to use resources from any plugins, you need to load the plugins before calling
createResource:
Gate . init ();

1
2

// need Tools plugin for the Morphological analyser
Gate . getCreoleRegister (). reg i s te r D ir e c to r i es (
new File ( Gate . getPluginsHome () , " Tools " ). toURL ()
);

3
4
5
6
7

...

8
9

Proces si ng Re so ur ce morpher = ( P ro ces si ng Re so ur ce )
Factory . createResource ( " gate . creole . morph . Morph " );

10
11

Instead of creating your processing resources individually using the Factory, you can create
your application in GATE Developer, save it using the ‘save application state’ option (see
Section 3.8.3), and then load the saved state from your code. This will automatically reload
any plugins that were loaded when the state was saved, you do not need to load them
manually.
Gate . init ();

1
2

CorpusController controller = ( CorpusController )
Persis te nc eM an ag er . l oa dOb je ct Fr om Fi le ( new File ( " savedState . xgapp " ));

3
4
5

// loadObjectFromUrl is also available

6

There are many examples of using GATE Embedded available at:
http://gate.ac.uk/wiki/code-repository/.
See Section 2.3 for details of the system properties GATE uses to ﬁnd its conﬁguration ﬁles.

7.2

Resource Management in GATE Embedded

As outlined earlier, GATE deﬁnes three diﬀerent types of resources:
Language Resources : (LRs) entities that hold linguistic data.
Processing Resources : (PRs) entities that process data.
Visual Resources : (VRs) components used for building graphical interfaces.
These resources are collectively named CREOLE1 resources.
1

CREOLE stands for Collection of REusable Objects for Language Engineering

GATE Embedded

139

All CREOLE resources have some associated meta-data in the form of an entry in a special
XML ﬁle named creole.xml. The most important role of that meta-data is to specify the set
of parameters that a resource understands, which of them are required and which not, if they
have default values and what those are. The valid parameters for a resource are described
in the resource’s section of its creole.xml ﬁle or in Java annotations on the resource class
– see Section 4.7.
All resource types have creation-time parameters that are used during the initialisation
phase. Processing Resources also have run-time parameters that get used during execution
(see Section 7.5 for more details).
Controllers are used to deﬁne GATE applications and have the role of controlling the
execution ﬂow (see Section 7.6 for more details).
This section describes how to create and delete CREOLE resources as objects in a running
Java virtual machine. This process involves using GATE’s Factory class2 , and, in the case
of LRs, may also involve using a DataStore.
CREOLE resources are Java Beans; creation of a resource object involves using a default
constructor, then setting parameters on the bean, then calling an init() method. The
Factory takes care of all this, makes sure that the GATE Developer GUI is told about what
is happening (when GUI components exist at runtime), and also takes care of restoring LRs
from DataStores. A programmer using GATE Embedded should never call the
constructor of a resource: always use the Factory!
Creating a resource involves providing the following information:
fully qualiﬁed class name for the resource. This is the only required value. For
all the rest, defaults will be used if actual values are not provided.
values for the creation time parameters.†
initial values for resource features.† For an explanation on features see Section 7.4.2.
a name for the new resource;
†

Parameters and features need to be provided in the form of a GATE Feature Map which is
essentially a java Map (java.util.Map) implementation, see Section 7.4.2 for more details
on Feature Maps.
Creating a resource via the Factory involves passing values for any create-time parameters
that require setting to the Factory’s createResource method. If no parameters are passed,
the defaults are used. So, for example, the following code creates a default ANNIE part-ofspeech tagger:
2

Fully qualiﬁed name: gate.Factory

140

GATE Embedded

Gate . getCreoleRegister (). regi s te r D ir e c to r i es ( new File (
Gate . getPluginsHome () , ANNIEConstants . PLUGIN_DIR ). toURI (). toURL ());
FeatureMap params = Factory . newFeatureMap (); //empty map:default params
ProcessingResource tagger = ( Pr oc es si ng Re so ur ce )
Factory . createResource ( " gate . creole . POSTagger " , params );

1
2
3
4
5

Note that if the resource created here had any parameters that were both mandatory and
had no default value, the createResource call would throw an exception. In this case, all
the information needed to create a tagger is available in default values given in the tagger’s
XML deﬁnition (in plugins/ANNIE/creole.xml):
<RESOURCE>
<NAME>ANNIE POS Tagger</NAME>
<COMMENT>Mark Hepple’s Brill-style POS tagger</COMMENT>
<CLASS>gate.creole.POSTagger</CLASS>
<PARAMETER NAME="document"
COMMENT="The document to be processed"
RUNTIME="true">gate.Document</PARAMETER>
....
<PARAMETER NAME="rulesURL" DEFAULT="resources/heptag/ruleset"
COMMENT="The URL for the ruleset file"
OPTIONAL="true">java.net.URL</PARAMETER>
</RESOURCE>

Here the two parameters shown are either ‘runtime’ parameters, which are set before a PR is
executed, or have a default value (in this case the default rules ﬁle is distributed with GATE
itself).
When creating a Document, however, the URL of the source for the document must be
provided3 . For example:
URL u = new URL ( " http :// gate . ac . uk / hamish / " );
FeatureMap params = Factory . newFeatureMap ();
params . put ( " sourceUrl " , u );
Document doc = ( Document )
Factory . createResource ( " gate . corpora . DocumentImpl " , params );

1
2
3
4
5

Note that the document created here is transient: when you quit the JVM the document
will no longer exist. If you want the document to be persistent, you need to store it in a
DataStore (see Section 7.4.5).
Apart from createResource() methods with diﬀerent signatures, Factory also provides
some shortcuts for common operations, listed in table 7.1.
GATE maintains various data structures that allow the retrieval of loaded resources. When
a resource is no longer required, it needs to be removed from those structures in order to
3

Alternatively a string giving the document source may be provided.

GATE Embedded

141

Method
newFeatureMap()
newDocument(String content)

newDocument(URL sourceUrl)

newDocument(URL sourceUrl,
String encoding)
newCorpus(String name)

Purpose
Creates a new Feature Map (as used in
the example above).
Creates a new GATE Document starting from a String value that will be used
to generate the document content.
Creates a new GATE Document using
the text pointed by an URL to generate
the document content.
Same as above but allows the speciﬁcation of an encoding to be used while
downloading the document content.
creates a new GATE Corpus with a
speciﬁed name.

Table 7.1: Factory Operations

remove all references to it, thus making it a candidate for garbage collection. This is achieved
using the deleteResource(Resource res) method on Factory.
Simply removing all references to a resource from the user code will NOT be enough to
make the resource collect-able. Not calling Factory.deleteResource() will lead to memory
leaks!

7.3

Using CREOLE Plugins

As shown in the examples above, in order to use a CREOLE resource the relevant CREOLE
plugin must be loaded. Processing Resources, Visual Resources and Language Resources
other than Document, Corpus and DataStore all require that the appropriate plugin is ﬁrst
loaded. When using Document, Corpus or DataStore, you do not need to ﬁrst load a plugin.
The following API calls listed in table 7.2 are relevant to working with CREOLE plugins.
If you are writing a GATE Embedded application and have a single resource class
that will only be used from your embedded code (and so does not need to be distributed as a complete plugin), and all the conﬁguration for that resource is provided
as Java annotations on the class, then it is possible to register the class with the
CreoleRegister at runtime without needing to package it in a JAR and provide a
creole.xml ﬁle. You can pass the Class object representing your resource class to
Gate.getCreoleRegister().registerComponent() method and then create instances of
the resource in the usual way using Factory.createResource. Note that resources cannot
be registered this way in the developer GUI, and cannot be included in saved application
states (see section 7.8 below).

142

GATE Embedded

Class gate.Gate
Method
Purpose
public static void addKnownadds the plugin to the list of known pluPlugin(URL pluginURL)
gins.
public static void removetells the system to ‘forget’ about one
KnownPlugin(URL pluginURL)
previously known directory. If the speciﬁed directory was loaded, it will be unloaded as well - i.e. all the metadata
relating to resources deﬁned by this directory will be removed from memory.
public static void addAutoload- adds a new directory to the list of pluPlugin(URL pluginUrl)
gins that are loaded automatically at
start-up.
public static void removeAutells the system to remove a plugin URL
toloadPlugin(URL pluginURL)
from the list of plugins that are loaded
automatically at system start-up. This
will be reﬂected in the user’s conﬁguration data ﬁle.
Class gate.CreoleRegister
public void registerDirectoloads a new CREOLE directory. The
ries(URL directoryUrl)
new plugin is added to the list of known
plugins if not already there.
public void registerComporegisters a single @CreoleResource annent(Class<? extends Resource>
notated class without the need for a
cls)
creole.xml ﬁle.
public void removeDirectory(URL unloads a loaded CREOLE plugin.
directory)

Table 7.2: Calls Relevant to CREOLE Plugins

GATE Embedded

7.4

143

Language Resources

This section describes the implementation of documents and corpora in GATE.

7.4.1

GATE Documents

Documents are modelled as content plus annotations (see Section 7.4.4) plus features (see
Section 7.4.2).
The
content
of
a
document
can
be
any
implementation
of
the
gate.DocumentContent interface; the features are <attribute, value> pairs stored a Feature
Map. Attributes are String values while the values can be any Java object.
The annotations are grouped in sets (see section 7.4.3). A document has a default (anonymous) annotations set and any number of named annotations sets.
Documents are deﬁned by the gate.Document interface and there is also a provided implementation:
gate.corpora.DocumentImpl : transient document. Can be stored persistently through
Java serialisation.
Main Document functions are presented in table 7.3.

7.4.2

Feature Maps

All CREOLE resources as well as the Controllers and the annotations can have attached
meta-data in the form of Feature Maps.
A Feature Map is a Java Map (i.e. it implements the java.util.Map interface) and holds
<attribute-name, attribute-value> pairs. The attribute names are Strings while the values
can be any Java Objects.
The use of non-Serialisable objects as values is strongly discouraged.
Feature Maps are created using the gate.Factory.newFeatureMap() method.
The
actual
implementation
for
gate.util.SimpleFeatureMapImpl class.

FeatureMaps

is

provided

by

the

Objects that have features in GATE implement the gate.util.FeatureBearer interface which has only the two accessor methods for the object features: FeatureMap
getFeatures() and void setFeatures(FeatureMap features).

144

GATE Embedded

Content Manipulation
Method
Purpose
DocumentContent getContent()
Gets the Document content.
void edit(Long start, Long end,
Modiﬁes the Document content.
DocumentContent replacement)
void setContent(DocumentContent
Replaces the entire content.
newContent)
Annotations Manipulation
Method
Purpose
public AnnotationSet getAnnotaReturns the default annotation set.
tions()
public AnnotationSet getAnnotaReturns a named annotation set.
tions(String name)
public Map getNamedAnnotation- Returns all the named annotation sets.
Sets()
void removeAnnotationRemoves a named annotation set.
Set(String name)
Input Output
String toXml()
Serialises the Document in XML format.
String toXml(Set
Generates XML from a set of annotaaSourceAnnotationSet, boolean
tions only, trying to preserve the origiincludeFeatures)
nal format of the ﬁle used to create the
document.

Table 7.3: gate.Document methods.

GATE Embedded

145

Getting a particular feature from an object
¯
1
2
3
4
5
6
7

Object obj ;
String featureName = " length " ;
if ( obj instanceof FeatureBearer ){
FeatureMap features = (( FeatureBearer ) obj ). getFeatures ();
Object value = ( features == null ) ? null :
features . get ( featureName );
}

7.4.3

Annotation Sets

A GATE document can have one or more annotation layers — an anonymous one, (also
called default), and as many named ones as necessary.
An annotation layer is organised as a Directed Acyclic Graph (DAG) on which the nodes
are particular locations —anchors— in the document content and the arcs are made out of
annotations reaching from the location indicated by the start node to the one pointed by the
end node (see Figure 7.1 for an illustration). Because of the graph metaphor, the annotation
layers are also called annotation graphs. In terms of Java objects, the annotation layers are
represented using the Set paradigm as deﬁned by the collections library and they are hence
named annotation sets. The terms of annotation layer, graph and set are interchangeable
and refer to the same concept when used in this book.

Figure 7.1: The Annotation Graph model.
An annotation set holds a number of annotations and maintains a series of indices in order
to provide fast access to the contained annotations.
The GATE Annotation Sets are deﬁned by the gate.AnnotationSet interface and there is
a default implementation provided:
gate.annotation.AnnotationSetImpl annotation set implementation used by transient
documents.
The annotation sets are created by the document as required. The ﬁrst time a particular
annotation set is requested from a document it will be transparently created if it doesn’t
exist.

146

GATE Embedded
Annotations Manipulation
Method
Purpose
Integer add(Long start, Long
Creates a new annotation between two
end, String type, FeatureMap
oﬀsets, adds it to this set and returns
features)
its id.
Integer add(Node start, Node
Creates a new annotation between two
end, String type, FeatureMap
nodes, adds it to this set and returns its
features)
id.
boolean remove(Object o)
Removes an annotation from this set.
Nodes
Method
Purpose
Node ﬁrstNode()
Gets the node with the smallest oﬀset.
Node lastNode()
Gets the node with the largest oﬀset.
Node nextNode(Node node)
Get the ﬁrst node that is relevant for
this annotation set and which has the
oﬀset larger than the one of the node
provided.
Set implementation
Iterator iterator()
int size()

Table 7.4: gate.AnnotationSet methods (general purpose).

Tables 7.4 and 7.5 list the most used Annotation Set functions.
Iterating from left to right over all annotations of a given type
¯
1
2
3
4
5
6
7
8
9
10
11
12

AnnotationSet annSet = ...;
String type = " Person " ;
//Get all person annotations
AnnotationSet persSet = annSet . get ( type );
//Sort the annotations
List persList = new ArrayList ( persSet );
Collections . sort ( persList , new gate . util . OffsetComparator ());
//Iterate
Iterator persIter = persList . iterator ();
while ( persIter . hasNext ()){
...
}

7.4.4

Annotations

An annotation, is a form of meta-data attached to a particular section of document content.
The connection between the annotation and the content it refers to is made by means of two
pointers that represent the start and end locations of the covered content. An annotation

GATE Embedded

147

Searching
AnnotationSet get(Long offset)
Select annotations by oﬀset. This returns the set of annotations whose start
node is the least such that it is less than
or equal to oﬀset. If a positional index
doesn’t exist it is created. If there are
no nodes at or beyond the oﬀset parameter then it will return null.
AnnotationSet get(Long
Select annotations by oﬀset. This restartOffset, Long endOffset)
turns the set of annotations that overlap
totally or partially with the interval deﬁned by the two provided oﬀsets. The
result will include all the annotations
that either:
start before the start oﬀset and
end strictly after it
start at a position between the
start and the end oﬀsets
AnnotationSet get(String type)
AnnotationSet get(Set types)
AnnotationSet get(String type,
FeatureMap constraints)
Set getAllTypes()

AnnotationSet getContained(Long
startOffset, Long endOffset)
AnnotationSet getCovering(String
neededType, Long startOffset,
Long endOffset)

Returns all annotations of the speciﬁed
type.
Returns all annotations of the speciﬁed
types.
Selects annotations by type and features.
Gets a set of java.lang.String objects
representing all the annotation types
present in this annotation set.
Select annotations contained within an
interval, i.e.
Select annotations of the given type
that completely span the range.

Table 7.5: gate.AnnotationSet methods (searching).

148

GATE Embedded

must also have a type (or a name) which is used to create classes of similar annotations,
usually linked together by their semantics.
An Annotation is deﬁned by:
start node a location in the document content deﬁned by an oﬀset.
end node a location in the document content deﬁned by an oﬀset.
type a String value.
features (see Section 7.4.2).
ID an Integer value. All annotations IDs are unique inside an annotation set.
In GATE Embedded, annotations are deﬁned by the gate.Annotation interface and implemented by the gate.annotation.AnnotationImpl class. Annotations exist only as members
of annotation sets (see Section 7.4.3) and they should not be directly created by means of a
constructor. Their creation should always be delegated to the containing annotation set.

7.4.5

GATE Corpora

A corpus in GATE is a Java List (i.e. an implementation of java.util.List) of documents.
GATE corpora are deﬁned by the gate.Corpus interface and the following implementations
are available:
gate.corpora.CorpusImpl used for transient corpora.
gate.corpora.SerialCorpusImpl used for persistent corpora that are stored in a serial
datastore (i.e. as a directory in a ﬁle system).
Apart from implementation for the standard List methods, a Corpus also implements the
methods in table 7.6.
Creating a corpus from all XML ﬁles in a directory
1
2
3
4
5

Corpus corpus = Factory . newCorpus ( " My XML Files " );
File directory = ...;
Extension F il e F il t e r filter = new E x te n s io n F il e F il t e r ( " XML files " , " xml " );
URL url = directory . toURL ();
corpus . populate ( url , filter , null , false );

Using a DataStore
Assuming that you have a DataStore already open called myDataStore, this code will ask
the datastore to take over persistence of your document, and to synchronise the memory
representation of the document with the disk storage:

GATE Embedded

149

Method
String getDocumentName(int
index)
List getDocumentNames()
void populate(URL directory,
FileFilter filter,
String encoding, boolean
recurseDirectories)

void populate(URL
singleConcatenatedFile,
String documentRootElement,
String encoding, int
numberOfDocumentsToExtract,
String documentNamePrefix,
DocType documentType)

Purpose
Gets the name of a document in this
corpus.
Gets the names of all the documents in
this corpus.
Fills this corpus with documents created on the ﬂy from selected ﬁles
in a directory. Uses a FileFilter
to select which ﬁles will be used
and which will be ignored. A simple ﬁle ﬁlter based on extensions
is provided in the Gate distribution
(gate.util.ExtensionFileFilter).
Fills the provided corpus with documents extracted from the provided
single concatenated ﬁle.
Uses the
content between the start and
end of the element as speciﬁed by
documentRootElement for each document. The parameter documentType
speciﬁes if the resulting ﬁles are html,
xml or of any other type. User can also
restrict the number of documents to
extract by providing the relevant value
for
numberOfDocumentsToExtract
parameter.

Table 7.6: gate.Corpus methods.

150

GATE Embedded

Document persistentDoc = myDataStore.adopt(doc, mySecurity);
myDataStore.sync(persistentDoc);

When you want to restore a document (or other LR) from a datastore, you make the same
createResource call to the Factory as for the creation of a transient resource, but this time
you tell it the datastore the resource came from, and the ID of the resource in that datastore:
1
2
3

URL u = ....; // URL of a serial datastore directory
SerialDataStore sds = new SerialDataStore ( u . toString ());
sds . open ();

4
5
6

// getLrIds returns a list of LR Ids, so we get the ﬁrst one
Object lrId = sds . getLrIds ( " gate . corpora . DocumentImpl " ). get (0);

7
8
9
10
11
12
13

// we need to tell the factory about the LR’s ID in the data
// store, and about which datastore it is in - we do this
// via a feature map:
FeatureMap features = Factory . newFeatureMap ();
features . put ( DataStore . LR_ID_FEATURE_NAME , lrId );
features . put ( DataStore . DATASTORE_FEATURE_NAME , sds );

14
15
16
17

7.5

// read the document back
Document doc = ( Document )
Factory . createResource ( " gate . corpora . DocumentImpl " , features );

Processing Resources

Processing Resources (PRs) represent entities that are primarily algorithmic, such as parsers,
generators or ngram modellers.
They are created using the GATE Factory in manner similar the Language Resources. Besides the creation-time parameters they also have a set of run-time parameters that are set
by the system just before executing them.
Analysers are a particular type of processing resources in the sense that they always have a
document and a corpus among their run-time parameters.
The most used methods for Processing Resources are presented in table 7.7

7.6

Controllers

Controllers are used to create GATE applications. A Controller handles a set of Processing
Resources and can execute them following a particular strategy. GATE provides a series of
serial controllers (i.e. controllers that run their PRs in sequence):

GATE Embedded

151

Method
void setParameterValue(String
paramaterName, Object
parameterValue)
void setParameterValues(FeatureMap parameters)
Object getParameterValue(String
paramaterName)
Resource init()
void reInit()

void execute()
void interrupt()
boolean isInterrupted()

Purpose
Sets the value for a speciﬁed parameter.
method inherited from gate.Resource
Sets the values for more parameters
in one step. method inherited from
gate.Resource
Gets the value of a named parameter
of this resource. method inherited from
gate.Resource
Initialise this resource, and return it.
method inherited from gate.Resource
Reinitialises the processing resource.
After calling this method the resource
should be in the state it is after calling
init. If the resource depends on external
resources (such as rules ﬁles) then the
resource will re-read those resources. If
the data used to create the resource has
changed since the resource has been created then the resource will change too
after calling reInit().
Starts the execution of this Processing
Resource.
Notiﬁes this PR that it should stop its
execution as soon as possible.
Checks whether this PR has been interrupted since the last time its Executable.execute() method was called.

Table 7.7: gate.ProcessingResource methods.

152

GATE Embedded

gate.creole.SerialController: a serial controller that takes any kind of PRs.

gate.creole.SerialAnalyserController: a serial controller that only accepts Language
Analysers as member PRs.

gate.creole.ConditionalSerialController: a serial controller that accepts all types of
PRs and that allows the inclusion or exclusion of member PRs from the execution
chain according to certain run-time conditions (currently features on the document
being processed are used).

gate.creole.ConditionalSerialAnalyserController: a serial controller that only accepts Language Analysers and that allows the conditional run of member PRs.

gate.creole.RealtimeCorpusController: a SerialAnalyserController that allows you
to specify a timeout parameter. If processing for a document takes longer than this
timeout then it will be forcibly terminated and the controller will move on to the next
document. Also if an exception occurs while processing a document this will simply
cause the controller to move on to the next document rather than failing the entire
corpus processing.

Additionally there is a scriptable controller provided by the Groovy plugin. See section 7.16.3
for details.

GATE Embedded

153

Creating an ANNIE application and running it over a corpus
1
2
3

// load the ANNIE plugin
Gate . getCreoleRegister (). reg i s te r D ir e c to r i es ( new File (
Gate . getPluginsHome () , " ANNIE " ). toURI (). toURL ());

4
5
6
7
8
9
10

// create a serial analyser controller to run ANNIE with
S e r i a l A n a l y s e r C o n t r o l l e r annieController =
( S e r i a l A n a l y s e r C o n t r o l l e r ) Factory . createResource (
" gate . creole . S e r i a l A n a l y s e r C o n t r o l l e r " ,
Factory . newFeatureMap () ,
Factory . newFeatureMap () , " ANNIE " );

11
12
13
14
15
16
17
18
19
20
21

// load each PR as deﬁned in ANNIEConstants
for ( int i = 0; i < ANNIEConstants . PR_NAMES . length ; i ++) {
// use default parameters
FeatureMap params = Factory . newFeatureMap ();
Proc es si ng Re so ur ce pr = ( P ro ces si ng Re so ur ce )
Factory . createResource ( ANNIEConstants . PR_NAMES [ i ] ,
params );
// add the PR to the pipeline controller
annieController . add ( pr );
} // for each ANNIE PR

22
23
24
25
26
27

// Tell ANNIE’s controller about the corpus you want to run on
Corpus corpus = ...;
annieController . setCorpus ( corpus );
// Run ANNIE
annieController . execute ();

7.7

Duplicating a Resource

Sometimes, particularly in a multi-threaded application, it is useful to be able to create an
independent copy of an existing PR, controller or LR. The obvious way to do this is to call
createResource again, passing the same class name, parameters, features and name, and
for many resources this will do the right thing. However there are some resources for which
this may be insuﬃcient (e.g. controllers, which also need to duplicate their PRs), unsafe
(if a PR uses temporary ﬁles, for instance), or simply ineﬃcient. For example for a large
gazetteer this would involve loading a second copy of the lists into memory and compiling
them into a second identical state machine representation, but a much more eﬃcient way to
achieve the same behaviour would be to use a SharedDefaultGazetteer (see section 13.11),
which can re-use the existing state machine.
The GATE Factory provides a duplicate method which takes an existing resource instance
and creates and returns an independent copy of the resource. By default it uses the algorithm
described above, extracting the parameter values from the template resource and calling
createResource to create a duplicate. However, if a particular resource type knows of a
better way to duplicate itself it can implement the CustomDuplication interface, and provide
its own duplicate method which the factory will use instead of performing the default

154

GATE Embedded

duplication algorithm. A caller who needs to duplicate an existing resource can simply call
Factory.duplicate to obtain a copy, which will be constructed in the appropriate way
depending on the resource type.
Note that the duplicate object returned by Factory.duplicate will not necessarily be of the
same class as the original object. However the contract of Factory.duplicate speciﬁes that
where the original object implements any of a list of core GATE interfaces, the duplicate
can be assumed to implement the same ones – if you duplicate a DefaultGazetteer the
result may not be an instance of DefaultGazetteer but it is guaranteed to implement the
Gazetteer interface.
Full details of how to implement a custom duplicate method in your own resource type
can be found in the JavaDoc documentation for the CustomDuplication interface and the
Factory.duplicate method.

7.8

Persistent Applications

GATE Embedded allows the persistent storage of applications in a format based on XML
serialisation. This is particularly useful for applications management and distribution. A
developer can save the state of an application when he/she stops working on its design and
continue developing it in a next session. When the application reaches maturity it can be
deployed to the client site using the same method.
When an application (i.e. a Controller) is saved, GATE will actually only save the values for
the parameters used to create the Processing Resources that are contained in the application.
When the application is reloaded, all the PRs will be re-created using the saved parameters.
Many PRs use external resources (ﬁles) to deﬁne their behaviour and, in most cases, these
ﬁles are identiﬁed using URLs. During the saving process, all the URLs are converted relative
URLs based on the location of the application ﬁle. This way, if the resources are packaged
together with the application ﬁle, the entire application can be reliably moved to a diﬀerent
location.
API access to application saving and loading is provided by means of two static methods on
the gate.util.persistence.PersistenceManager class, listed in table 7.8.

GATE Embedded

155

Method
public static void saveObjectToFile(Object obj, File file)

public static Object loadObjectFromFile(File file)

Purpose
Saves the data needed to re-create the
provided GATE object to the speciﬁed ﬁle. The Object provided can be
any type of Language or Processing Resource or a Controller. The procedures
may work for other types of objects as
well (e.g. it supports most Collection
types).
Parses the ﬁle speciﬁed (which needs to
be a ﬁle created by the above method)
and creates the necessary object(s) as
speciﬁed by the data in the ﬁle. Returns
the root of the object tree.

Table 7.8: Application Saving and Loading

Saving and loading a GATE application
¯
1
2
3
4

//Where to save the application?
File file = ...;
//What to save?
Controller theApplication = ...;

5
6
7
8
9
10
11

//save
gate . util . persistence . Persi st en ce Ma na ge r .
saveObjectToFile ( theApplication , file );
//delete the application
Factory . deleteResource ( theApplication );
theApplication = null ;

12
13
14
15
16

[...]
//load the application back
theApplication = gate . util . persistence . Per si st en ce Ma na ge r .
loadObject Fr om Fi le ( file );

7.9

Ontologies

Starting from GATE version 3.1, support for ontologies has been added. Ontologies are
nominally Language Resources but are quite diﬀerent from documents and corpora and are
detailed in chapter 14.
Classes related to ontologies are to be found in the gate.creole.ontology package and its
sub-packages. The top level package deﬁnes an abstract API for working with ontologies
while the sub-packages contain concrete implementations. A client program should only use
the classes and methods deﬁned in the API and never any of the classes or methods from
the implementation packages.

156

GATE Embedded

The entry point to the ontology API is the gate.creole.ontology.Ontology interface
which is the base interface for all concrete implementations. It provides methods for accessing
the class hierarchy, listing the instances and the properties.
Ontology implementations are available through plugins. Before an ontology language resource can be created using the gate.Factory and before any of the classes and methods in
the API can be used, one of the implementing ontology plugins must be loaded. For details
see chapter 14.

7.10

Creating a New Annotation Schema

An annotation schema (see Section 3.4.6) can be brought inside GATE through the creole.xml
ﬁle. By using the AUTOINSTANCE element, one can create instances of resources deﬁned
in creole.xml. The gate.creole.AnnotationSchema (which is the Java representation of an
annotation schema ﬁle) initializes with some predeﬁned annotation deﬁnitions (annotation
schemas) as speciﬁed by the GATE team.
Example from GATE’s internal creole.xml (in src/gate/resources/creole):
<!-- Annotation schema -->
<RESOURCE>
<NAME>Annotation schema</NAME>
<CLASS>gate.creole.AnnotationSchema</CLASS>
<COMMENT>An annotation type and its features</COMMENT>
<PARAMETER NAME="xmlFileUrl" COMMENT="The url to the definition file"
SUFFIXES="xml;xsd">java.net.URL</PARAMETER>
<AUTOINSTANCE>
<PARAM NAME ="xmlFileUrl" VALUE="schema/AddressSchema.xml" />
</AUTOINSTANCE>
<AUTOINSTANCE>
<PARAM NAME ="xmlFileUrl" VALUE="schema/DateSchema.xml" />
</AUTOINSTANCE>
<AUTOINSTANCE>
<PARAM NAME ="xmlFileUrl" VALUE="schema/FacilitySchema.xml" />
</AUTOINSTANCE>
<!-- etc. -->
</RESOURCE>

In order to create a gate.creole.AnnotationSchema object from a schema annotation ﬁle, one
must use the gate.Factory class;
1
2
3
4

FeatureMap params = new FeatureMap ();\\
param . put ( " xmlFileUrl " , annotSchemaFile . toURL ());\\
AnnotationSchema annotSchema = \\
Factory . createResurce ( " gate . creole . AnnotationSchema " , params );

GATE Embedded

157

Note: All the elements and their values must be written in lower case, as XML is deﬁned as
case sensitive and the parser used for XML Schema inside GATE searches is case sensitive.
In order to be able to write XML Schema deﬁnitions, the ones deﬁned in GATE
(resources/creole/schema) can be used as a model, or the user can have a look at
http://www.w3.org/2000/10/XMLSchema for a proper description of the semantics of the
elements used.
Some examples of annotation schemas are given in Section 5.4.1.

7.11

Creating a New CREOLE Resource

To create a new resource you need to:
write a Java class that implements GATE’s beans model;
compile the class, and any others that it uses, into a Java Archive (JAR) ﬁle;
write some XML conﬁguration data for the new resource;
tell GATE the URL of the new JAR and XML ﬁles.
GATE Developer helps you with this process by creating a set of directories and ﬁles that
implement a basic resource, including a Java code ﬁle and a Makeﬁle. This process is called
‘bootstrapping’.
For example, let’s create a new component called GoldFish, which will be a Processing
Resource that looks for all instances of the word ‘ﬁsh’ in a document and adds an annotation
of type ‘GoldFish’.
First start GATE Developer (see Section 2.2). From the ‘Tools’ menu select ‘BootStrap
Wizard’, which will pop up the dialogue in ﬁgure 7.2. The meaning of the data entry ﬁelds:
The ‘resource name’ will be displayed when GATE Developer loads the resource, and
will be the name of the directory the resource lives in. For our example: GoldFish.
‘Resource package’ is the Java package that the class representing the resource will be
created in. For our example: sheffield.creole.example.
‘Resource type’ must be one of Language, Processing or Visual Resource. In this
case we’re going to process documents (and add annotations to them), so we select
ProcessingResource.
‘Implementing class name’ is the name of the Java class that represents the resource.
For our example: GoldFish.

158

GATE Embedded

Figure 7.2: BootStrap Wizard Dialogue

The ‘interfaces implemented’ ﬁeld allows you to add other interfaces (e.g.
gate.creole.ControllerAwarePR4 ) that you would like your new resource to implement. In this case we just leave the default (which is to implement the
gate.ProcessingResource interface).
The last ﬁeld selects the directory that you want the new resource created in. For our
example: z:/tmp.
Now we need to compile the class and package it into a JAR ﬁle. The bootstrap wizard
creates an Ant build ﬁle that makes this very easy – so long as you have Ant set up properly,
you can simply run ant jar
This will compile the Java source code and package the resulting classes into GoldFish.jar.
If you don’t have your own copy of Ant, you can use the one bundled with GATE
- suppose your GATE is installed at /opt/gate-5.0-snapshot, then you can use
/opt/gate-5.0-snapshot/bin/ant jar to build.
You can now load this resource into GATE; see Section 3.6. The default Java code that was
created for our GoldFish resource looks like this:
/

1

GoldFish . java

2
3

You s h o u l d p r o b a b l y p u t a c o p y r i g h t n o t i c e h e r e . Why n o t u s e t h e
GNU l i c e n c e ? ( S e e h t t p : / / www . gnu . o r g / . )

4
5
6

hamish , 2 6 / 9 / 2 0 0 1

7

4

See Section 4.4.

GATE Embedded

159

8

I d : h o w t o . t e x , v 1 . 1 3 0 2 0 0 6 / 1 0 / 2 3 1 2 : 5 6 : 3 7 i a n Exp

9

/

10
11
12

package sheffield . creole . example ;

13
14
15
16
17

import
import
import
import

java . util .*;
gate .*;
gate . creole .*;
gate . util .*;

18
19

/

20

T h i s c l a s s i s t h e i m p l e m e n t a t i o n o f t h e r e s o u r c e GOLDFISH .
/
@CreoleResource ( name = " GoldFish " ,
comment = " Add a descriptive comment about this resource " )
public class GoldFish extends A b s t r a c t P r o c e s s i n g R e s o u r c e
implements P roc es si ng Re so ur ce {

21
22
23
24
25
26
27
28

} // class GoldFish

The default XML conﬁguration for GoldFish looks like this:

<!-- creole.xml GoldFish -->
<!-- hamish, 26/9/2001 -->
<!-- $Id: howto.tex,v 1.130 2006/10/23 12:56:37 ian Exp $ -->
<CREOLE-DIRECTORY>
<JAR SCAN="true">GoldFish.jar</JAR>
</CREOLE-DIRECTORY>

The directory structure containing these ﬁles is shown in ﬁgure 7.3. GoldFish.java lives
in the src/sheffield/creole/example directory. creole.xml and build.xml are in the
top GoldFish directory. The lib directory is for libraries; the classes directory is where
Java class ﬁles are placed; the doc directory is for documentation. These last two, plus
GoldFish.jar are created by Ant.
This process has the advantage that it creates a complete source tree and build structure
for the component, and the disadvantage that it creates a complete source tree and build
structure for the component. If you already have a source tree, you will need to chop out the
bits you need from the new tree (in this case GoldFish.java and creole.xml) and copy it
into your existing one.
See the example code at http://gate.ac.uk/wiki/code-repository/.

160

GATE Embedded

Figure 7.3: BootStrap directory tree

7.12

Adding Support for a New Document Format

In order to add a new document format, one needs to extend the gate.DocumentFormat
class and to implement an abstract method called:
1
2

public void unpackMarkup ( Document doc ) throws
DocumentFormatException

This method is supposed to implement the functionality of each format reader and to create
annotations on the document. Finally the document’s old content will be replaced with a
new one containing only the text between markups.
If one needs to add a new textual reader will extend the gate.corpora.TextualDocumentFormat
and override the unpackMarkup(doc) method.
This class needs to be implemented under the Java bean speciﬁcations because it will be
instantiated by GATE using Factory.createResource() method.
The init() method that one needs to add and implement is very important because in here
the reader deﬁnes its means to be selected successfully by GATE. What one needs to do is
to add some speciﬁc information into certain static maps deﬁned in DocumentFormat class,
that will be used at reader detection time.
After that, a deﬁnition of the reader will be placed into the one’s creole.xml ﬁle and the
reader will be available to GATE.
We present for the rest of the section a complete three step example of adding such a reader.
The reader we describe in here is an XML reader.

GATE Embedded

161

Step 1
Create a new class called XmlDocumentFormat that extends
gate.corpora.TextualDocumentFormat.
Step 2
Implement the unpackMarkup(Document doc) which performs the required functionality for
the reader. Add XML detection means in init() method:
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

public Resource init () throws R e s o u r c e I n s t a n t i a t i o n E x c e p t i o n {
// Register XML mime type
MimeType mime = new MimeType ( " text " ," xml " );
// Register the class handler for this mime type
m i m e S t r i n g 2 C l a s s H a n d l e r M a p . put ( mime . getType ()+ " / " + mime . getSubtype () ,
this );
// Register the mime type with mine string
mi me S t r i n g 2 m i m e T y p e M a p . put ( mime . getType () + " / " + mime . getSubtype () ,
mime );
// Register ﬁle suﬃxes for this mime type
suff i x e s 2 m i m e T y p e M a p . put ( " xml " , mime );
suff i x e s 2 m i m e T y p e M a p . put ( " xhtm " , mime );
suff i x e s 2 m i m e T y p e M a p . put ( " xhtml " , mime );
// Register magic numbers for this mime type
magic2mimeTypeMap . put ( " <? xml " , mime );
// Set the mimeType for this language resource
setMimeType ( mime );
return this ;
} // init()

More details about the information from those maps can be found in Section 5.5.1
Step 3
Add the following creole deﬁnition in the creole.xml document.

<RESOURCE>
<NAME>My XML Document Format</NAME>
<CLASS>mypackage.XmlDocumentFormat</CLASS>
<AUTOINSTANCE/>
<PRIVATE/>
</RESOURCE>

More information on the operation of GATE’s document format analysers may be found in
Section 5.5.

162

GATE Embedded

7.13

Using GATE Embedded in a Multithreaded Environment

GATE Embedded can be used in multithreaded applications, so long as you observe a few
restrictions. First, you must initialise GATE by calling Gate.init() exactly once in your application, typically in the application startup phase before any concurrent processing threads
are started.
Secondly, you must not make calls that aﬀect the global state of GATE (e.g. loading or
unloading plugins) in more than one thread at a time. Again, you would typically load all
the plugins your application requires at initialisation time. It is safe to create instances of
resources in multiple threads concurrently.
Thirdly, it is important to note that individual GATE processing resources, language resources and controllers are by design not thread safe – it is not possible to use a single
instance of a controller/PR/LR in multiple threads at the same time – but for a well written
resource it should be possible to use several diﬀerent instances of the same resource at once,
each in a diﬀerent thread. When writing your own resource classes you should bear the
following in mind, to ensure that your resource will be useable in this way.
Avoid static data. Where possible, you should avoid using static ﬁelds in your class,
and you should try and take all conﬁguration data via the CREOLE parameters you
declare in your creole.xml ﬁle. System properties may be appropriate for truly static
conﬁguration, such as the location of an external executable, but even then it is generally better to stick to CREOLE parameters – a user may wish to use two diﬀerent
instances of your PR, each talking to a diﬀerent executable.
Read parameters at the correct time. Init-time parameters should be read in the init()
(and reInit()) method, and for processing resources runtime parameters should be
read at each execute().
Use temporary ﬁles correctly. If your resource makes use of external temporary ﬁles
you should create them using File.createTempFile() at init or execute time, as
appropriate. Do not use hardcoded ﬁle names for temporary ﬁles.
If there are objects that can be shared between diﬀerent instances of your resource,
make sure these objects are accessed either read-only, or in a thread-safe way. In
particular you must be very careful if your resource can take other resource instances
as init or runtime parameters (e.g. the Flexible Gazetteer, Section 13.7).
Of course, if you are writing a PR that is simply a wrapper around an external library that
imposes these kinds of limitations there is only so much you can do. If your resource cannot
be made safe you should document this fact clearly.

GATE Embedded

163

All the standard ANNIE PRs are safe when independent instances are used in diﬀerent
threads concurrently, as are the standard transient document, transient corpus and controller
classes. A typical pattern of development for a multithreaded GATE-based application is:
Develop your GATE processing pipeline in GATE Developer.
Save your pipeline as a .gapp ﬁle.
In your application’s initialisation phase, load n copies of the pipeline using
PersistenceManager.loadObjectFromFile() (see the Javadoc documentation for details), or load the pipeline once and then make copies of it using Factory.duplicate
as described in section 7.7, and either give one copy to each thread or store them in a
pool (e.g. a LinkedList).
When you need to process a text, get one copy of the pipeline from the pool, and
return it to the pool when you have ﬁnished processing.
Alternatively you can use the Spring Framework as described in the next section to handle
the pooling for you.

7.14

Using GATE Embedded within a Spring Application

GATE Embedded provides helper classes to allow GATE resources to be created and managed by the Spring framework. For Spring 2.0 or later, GATE Embedded provides a custom
namespace handler that makes them extremely easy to use. To use this namespace, put the
following declarations in your bean deﬁnition ﬁle:
<beans xmlns="http://www.springframework.org/schema/beans"
xmlns:gate="http://gate.ac.uk/ns/spring"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="
http://www.springframework.org/schema/beans
http://www.springframework.org/schema/beans/spring-beans.xsd
http://gate.ac.uk/ns/spring
http://gate.ac.uk/ns/spring.xsd">

You can have Spring initialise GATE:
<gate:init gate-home="WEB-INF" user-config-file="WEB-INF/user.xml">
<gate:preload-plugins>
<value>WEB-INF/ANNIE</value>

164

GATE Embedded

<value>http://example.org/gate-plugin</value>
</gate:preload-plugins>
</gate:init>

The gate-home, user-conﬁg-ﬁle, etc. and the <value> elements under <gate:preload-plugins>
are interpreted as Spring “resource” paths. If the value is not an absolute URL then Spring
will resolve the path in an appropriate way for the type of application context — in a web
application they are taken as being relative to the web app root, and you would typically
use locations within WEB-INF as shown in the example above. To use an absolute path
for gate-home it is not suﬃcient to use a leading slash (e.g. /opt/gate), for backwardscompatibility reasons Spring will still resolve this relative to your web application. Instead
you must specify it as a full URL, i.e. file:/opt/gate.
<gate:preload-plugins> speciﬁes CREOLE plugins that should be loaded after GATE
has been initialised. An alternative way to specify extra plugins is to provide separate
<gate:extra-plugin> elements, for example:
<gate:init gate-home="WEB-INF"
user-config-file="WEB-INF/user.xml" />
<gate:extra-plugin>WEB-INF/ANNIE</gate:extra-plugin>

You can freely mix the two styles – nested <gate:preload-plugins> deﬁnitions are processed ﬁrst, followed by all the <gate:extra-plugin> deﬁnitions found in the application
context. This is useful if, for example, you are providing additional conﬁguration as a separate bean deﬁnition ﬁle from the one containing the main <gate:init> deﬁnition and need
to load extra plugins without editing this main deﬁnition.
To create a GATE resource, use the <gate:resource> element.
<gate:resource id="sharedOntology" scope="singleton"
resource-class="gate.creole.ontology.owlim.OWLIMOntologyLR">
<gate:parameters>
<entry key="rdfXmlURL">
<gate:url>WEB-INF/ontology.rdf</gate:url>
</entry>
</gate:parameters>
<gate:features>
<entry key="ontologyVersion" value="0.1.3" />
<entry key="mainOntology">
<value type="java.lang.Boolean">true</value>
</entry>
</gate:features>
</gate:resource>

GATE Embedded

165

The children of <gate:parameters> are Spring <entry/> elements, just as you would write
when conﬁguring a bean property of type Map<String,Object>. <gate:url> provides a
way to construct a java.net.URL from a resource path as discussed above. If it is possible
to resolve the resource path as a file: URL then this form will be preferred, as there are a
number of areas within GATE which work better with file: URLs than with other types
of URL (for example plugins that run external processes, or that use a URL parameter to
point to a directory in which they will create new ﬁles).
A note about types: The <gate:parameters> and <gate:features> elements deﬁne GATE
FeatureMaps. When using the simple <entry key="..." value="..." /> form, the entry
values will be treated as strings; Spring can convert strings into many other types of object
using the standard Java Beans property editor mechanism, but since a FeatureMap can hold
any kind of values you must use an explicit <value type="...">...</value> to tell Spring
what type the value should be.
There is an additional twist for <gate:parameters> – GATE has its own internal logic
to convert strings to other types required for resource parameters (see the discussion of
default parameter values in section 4.7.1). So for parameter values you have a choice, you
can either use an explicit <value type="..."> to make Spring do the conversion, or you
can pass the parameter value as a string and let GATE do the conversion. For resource
parameters whose type is java.net.URL, if you pass a string value that is not an absolute
URL (starting ﬁle:, http:, etc.) then GATE will treat the string as a path relative to
the creole.xml ﬁle of the plugin that deﬁnes the resource type whose parameter you are
setting. If this is not what you intended then you should use <gate:url> to cause Spring to
resolve the path to a URL before passing it to GATE. For example, for a JAPE transducer,
<entry key="grammarURL" value="grammars/main.jape" /> would resolve to something
like file:/path/to/webapp/WEB-INF/plugins/ANNIE/grammars/main.jape, whereas
<entry key="grammarURL">
<gate:url>grammars/main.jape</gate:url>
</entry>

would resolve to file:/path/to/webapp/grammars/main.jape.
You can load a GATE saved application with
<gate:saved-application location="WEB-INF/application.gapp" scope="prototype">
<gate:customisers>
<gate:set-parameter pr-name="custom transducer" name="ontology"
ref="sharedOntology" />
</gate:customisers>
</gate:saved-application>

‘Customisers’ are used to customise the application after it is loaded. In the example above,
we load a singleton copy of an ontology which is then shared between all the separate

166

GATE Embedded

instances of the (prototype) application. The <gate:set-parameter> customiser accepts
all the same ways to provide a value as the standard Spring <property> element (a ”value”
or ”ref” attribute, or a sub-element - <value>, <list>, <bean>, <gate:resource> . . . ).
The <gate:add-pr> customiser provides support for the case where most of the application
is in a saved state, but we want to create one or two extra PRs with Spring (maybe to inject
other Spring beans as init parameters) and add them to the pipeline.
<gate:saved-application ...>
<gate:customisers>
<gate:add-pr add-before="OrthoMatcher" ref="myPr" />
</gate:customisers>
</gate:saved-application>

By default, the <gate:add-pr> customiser adds the target PR at the end of the pipeline,
but an add-before or add-after attribute can be used to specify the name of a PR before
(or after) which this PR should be placed. Alternatively, an index attribute places the PR
at a speciﬁc (0-based) index into the pipeline. The PR to add can be speciﬁed either as a
‘ref’ attribute, or with a nested <bean> or <gate:resource> element.

7.14.1

Duplication in Spring

The above example deﬁnes the <gate:application> as a prototype-scoped bean, which
means the saved application state will be loaded afresh each time the bean is fetched from the
bean factory (either explicitly using getBean or implicitly when it is injected as a dependency
of another bean). However in many cases it is better to load the application once and then
duplicate it as required (as described in section 7.7), as this allows resources to optimise
their memory usage, for example by sharing a single in-memory representation of a large
gazetteer list between several instances of the gazetteer PR. This approach is supported by
the <gate:duplicate> tag.
<gate:duplicate id="theApp">
<gate:saved-application location="/WEB-INF/application.xgapp" />
</gate:duplicate>

The <gate:duplicate> tag acts like a prototype bean deﬁnition, in that each time it is
fetched or injected it will call Factory.duplicate to create a new duplicate of its template
resource (declared as a nested element or referenced by the template-ref attribute). However the tag also keeps track of all the duplicate instances it has returned over its lifetime,
and will ensure they are released (using Factory.deleteResource) when the Spring context
is shut down.
The <gate:duplicate> tag also supports customisers, which will be applied to the newlycreated duplicate resource before it is returned. This is subtly diﬀerent from applying the

GATE Embedded

167

customisers to the template resource itself, which would cause them to be applied once to
the original resource before it is ﬁrst duplicated.
Finally, <gate:duplicate> takes an optional boolean attribute return-template. If set to
false (or omitted, as this is the default behaviour), the tag always returns a duplicate — the
original template resource is used only as a template and is not made available for use. If set
to true, the ﬁrst time the bean deﬁned by the tag is injected or fetched, the original template
resource is returned. Subsequent uses of the tag will return duplicates. Generally speaking,
it is only safe to set return-template="true" when there are no customisers, and when
the duplicates will all be created up-front before any of them are used. If the duplicates will
be created asynchronously (e.g. with a dynamically expanding pool, see below) then it is
possible that, for example, a template application may be duplicated in one thread whilst it
is being executed by another thread, which may lead to unpredictable behaviour.

7.14.2

Spring pooling

In a multithreaded application it is vital that individual GATE resources are not used in
more than one thread at the same time. Because of this, multithreaded applications that use
GATE Embedded often need to use some form of pooling to provided thread-safe access to
GATE components. This can be managed by hand, but the Spring framework has built-in
tools to support transparent pooling of Spring-managed beans. Spring can create a pool of
identical objects, then expose a single “proxy” object (oﬀering the same interface) for use
by clients. Each method call on the proxy object will be routed to an available member of
the pool in such a way as to guarantee that each member of the pool is accessed by no more
than one thread at a time.
Since the pooling is handled at the level of method calls, this approach is not used to create a
pool of GATE resources directly — making use of a GATE PR typically involves a sequence
of method calls (at least setDocument(doc), execute() and setDocument(null)), and creating a pooling proxy for the resource may result in these calls going to diﬀerent members
of the pool. Instead the typical use of this technique is to deﬁne a helper object with a single method that internally calls the GATE API methods in the correct sequence, and then
create a pool of these helpers. The interface gate.util.DocumentProcessor and its associated implementation gate.util.LanguageAnalyserDocumentProcessor are useful for this.
The DocumentProcessor interface deﬁnes a processDocument method that takes a GATE
document and performs some processing on it. LanguageAnalyserDocumentProcessor implements this interface using a GATE LanguageAnalyser (such as a saved “corpus pipeline”
application) to do the processing. A pool of LanguageAnalyserDocumentProcessor instances can be exposed through a proxy which can then be called from several threads.
The machinery to implement this is all built into Spring, but the conﬁguration typically
required to enable it is quite ﬁddly, involving at least three co-operating bean deﬁnitions.
Since the technique is so useful with GATE Embedded, GATE provides a special syntax to
conﬁgure pooling in a simple way. Given the <gate:duplicate id="theApp"> deﬁnition

168

GATE Embedded

from the previous section we can create a DocumentProcessor proxy that can handle up to
ﬁve concurrent requests as follows:
<bean id="processor"
class="gate.util.LanguageAnalyserDocumentProcessor">
<property name="analyser" ref="theApp" />
<gate:pooled-proxy max-size="5" />
</bean>

The <gate:pooled-proxy> element decorates a singleton bean deﬁnition. It converts the
original deﬁnition to prototype scope and replaces it with a singleton proxy delegating to a
pool of instances of the prototype bean. The pool parameters are controlled by attributes
of the <gate:pooled-proxy> element, the most important ones being:
max-size The maximum size of the pool. If more than this number of threads try to call
methods on the proxy at the same time, the others will (by default) block until an
object is returned to the pool.
initial-size The default behaviour of Spring’s pooling tools is to create instances in the
pool on demand (up to the max-size). This attribute instead causes initial-size
instances to be created up-front and added to the pool when it is ﬁrst created.
when-exhausted-action-name What to do when the pool is exhausted (i.e. there are
already max-size concurrent calls in progress and another one arrives). Should be set
to one of WHEN_EXHAUSTED_BLOCK (the default, meaning block the excess requests until
an object becomes free), WHEN_EXHAUSTED_GROW (create a new object anyway, even
though this pushes the pool beyond max-size) or WHEN_EXHAUSTED_FAIL (cause the
excess calls to fail with an exception).
Many more options are available, corresponding to the properties of the Spring CommonsPoolTargetSource class. These allow you, for example, to conﬁgure a pool that dynamically
grows and shrinks as necessary, releasing objects that have been idle for a set amount of time.
See the JavaDoc documentation of CommonsPoolTargetSource (and the documentation for
Apache commons-pool) for full details.
Note that the <gate:pooled-proxy> technique is not tied to GATE in any way, it is simply
an easy way to conﬁgure standard Spring beans and can be used with any bean that needs
to be pooled, not just objects that make use of GATE.

7.14.3

Further reading

These custom elements all deﬁne various factory beans. For full details, see the JavaDocs for
gate.util.spring (the factory beans) and gate.util.spring.xml (the gate: namespace

GATE Embedded

169

handler). The main Spring framework API documentation is the best place to look for more
detail on the pooling facilities provided by Spring AOP.
Note: the former approach using factory methods of the gate.util.spring.SpringFactory
class will still work, but should be considered deprecated in favour of the new factory beans.

7.15

Using GATE Embedded within a Tomcat Web
Application

Embedding GATE in a Tomcat web application involves several steps.
1. Put the necessary JAR ﬁles (gate.jar and all or most of the jars in gate/lib) in your
webapp/WEB-INF/lib.
2. Put the plugins that your application depends on in a suitable location (e.g.
webapp/WEB-INF/plugins).
3. Create suitable gate.xml conﬁguration ﬁles for your environment.
4. Set the appropriate paths in your application before calling Gate.init().
This process is detailed in the following sections.

7.15.1

Recommended Directory Structure

You will need to create a number of other ﬁles in your web application to allow GATE to
work:
Site and user gate.xml conﬁg ﬁles - we highly recommend deﬁning these speciﬁcally for
the web application, rather than relying on the default ﬁles on your application server.
The plugins your application requires.
In this guide, we assume the following layout:
webapp/
WEB-INF/
gate.xml
user-gate.xml
plugins/
ANNIE/
etc.

170

GATE Embedded

7.15.2

Conﬁguration Files

Your gate.xml (the ‘site-wide conﬁguration ﬁle’) should be as simple as possible:
<?xml version="1.0" encoding="UTF-8" ?>
<GATE>
<GATECONFIG Save_options_on_exit="false"
Save_session_on_exit="false" />
</GATE>

Similarly, keep the user-gate.xml (the ‘user conﬁg ﬁle’) simple:
<?xml version="1.0" encoding="UTF-8" ?>
<GATE>
<GATECONFIG Known_plugin_path=";"
Load_plugin_path=";" />
</GATE>

This way, you can control exactly which plugins are loaded in your webapp code.

7.15.3

Initialization Code

Given the directory structure shown above, you can initialize GATE in your web application
like this:
1
2
3
4

// imports
...
public class MyServlet extends HttpServlet {
private static boolean gateInited = false ;

5
6
7
8
9

public void init () throws ServletException {
if (! gateInited ) {
try {
ServletContext ctx = get Servl etCont ext ();

10
11
12

// use /path/to/your/webapp/WEB-INF as gate.home
File gateHome = new File ( ctx . getRealPath ( " / WEB - INF " ));

13
14
15
16

Gate . setGateHome ( gateHome );
// thus webapp/WEB-INF/plugins is the plugins directory, and
// webapp/WEB-INF/gate.xml is the site conﬁg ﬁle.

17
18
19
20
21

// Use webapp/WEB-INF/user-gate.xml as the user conﬁg ﬁle,
// to avoid confusion with your own user conﬁg.
Gate . setUserConfigFile ( new File ( gateHome , " user - gate . xml " ));

GATE Embedded

171

Gate . init ();
// load plugins, for example...
Gate . getCreoleRegis ter (). r eg i s te r D ir e c to r i es (
ctx . getResource ( " / WEB - INF / plugins / ANNIE " ));

22
23
24
25
26

gateInited = true ;
}
catch ( Exception ex ) {
throw new ServletException ( " Exception initialising GATE " ,
ex );
}

27
28
29
30
31
32

}

33

}

34
35

}

Once initialized, you can create GATE resources using the Factory in the usual way (for
example, see Section 7.1 for an example of how to create an ANNIE application). You should
also read Section 7.13 for important notes on using GATE Embedded in a multithreaded
application.
Instead of an initialization servlet you could also consider doing your initialization in a
ServletContextListener, or using Spring (see Section 7.14).

7.16

Groovy for GATE

Groovy is a dynamic programming language based on Java. Groovy is not used in the
core GATE distribution, so to enable the Groovy features in GATE you must ﬁrst load the
Groovy plugin. Loading this plugin:
provides access to the Groovy scripting console (conﬁgured with some extensions for
GATE) from the GATE Developer “Tools” menu.
provides a PR to run a Groovy script over documents.
provides a controller which uses a Groovy DSL to deﬁne its execution strategy.
enhances a number of core GATE classes with additional convenience methods that can
be used from any Groovy code including the console, the script PR, and any Groovy
class that uses the GATE Embedded API.
This section describes these features in detail, but assumes that the reader already
has some knowledge of the Groovy language. If you are not already familiar with
Groovy you should read this section in conjunction with Groovy’s own documentation at
http://groovy.codehaus.org/.

172

GATE Embedded

7.16.1

Groovy Scripting Console for GATE

Loading the Groovy plugin in GATE Developer will provide a “Groovy Console” item in
the Tools/Groovy Tools menu. This menu item opens the standard Groovy console window
(http://groovy.codehaus.org/Groovy+Console).
To help scripting GATE in Groovy, the console is pre-conﬁgured to import all classes from
the gate, gate.annotation, gate.util, gate.jape and gate.creole.ontology packages
of the core GATE API5 . This means you can refer to classes and interfaces such as Factory,
AnnotationSet, Gate, etc. without needing to preﬁx them with a package name. In addition,
the following (read-only) variable bindings are pre-deﬁned in the Groovy Console.

corpora: a list of loaded corpora LRs (Corpus)
docs: a list of all loaded document LRs (DocumentImpl)
prs: a list of all loaded PRs
apps: a list of all loaded Applications (AbstractController)

These variables are automatically updated as resources are created and deleted in GATE.
Here’s an example script. It ﬁnds all documents with a feature “annotator” set to “fred”,
and puts them in a new corpus called “fredsDocs”.
1
2
3
4
5

Factory . newCorpus ( " fredsDocs " ). addAll (
docs . findAll {
it . features . annotator == " fred "
}
)

You can ﬁnd other examples (and add your own) in the Groovy script repository on the
GATE Wiki: http://gate.ac.uk/wiki/groovy-recipes/.

Why won’t the ‘Groovy executing’ dialog go away? Sometimes, when you execute a
Groovy script through the console, a dialog will appear, saying “Groovy is executing. Please
wait”. The dialog fails to go away even when the script has ended, and cannot be closed by
clicking the “Interrupt” button. You can, however, continue to use the Groovy Console, and
the dialog will usually go away next time you run a script. This is not a GATE problem: it
is a Groovy problem.
5
These are the same classes that are imported by default for use in Java code on the right hand side of
JAPE rules.

GATE Embedded

7.16.2

173

Groovy scripting PR

The Groovy scripting PR enables you to load and execute Groovy scripts as part of a GATE
application pipeline. The Groovy scripting PR is made available when you load the Groovy
plugin via the plugin manager.
Parameters
The Groovy scripting PR has a single initialisation parameter
scriptURL: the path to a valid Groovy script
It has three runtime parameters
inputASName: an optional annotation set intended to be used as input by the PR
(but note that the PR has access to all annotation sets)
outputASName: an optional annotation set intended to be used as output by the
PR (but note that the PR has access to all annotation sets)
scriptParams: optional parameters for the script. In a creole.xml ﬁle, these should
be speciﬁed as key=value pairs, each pair separated by a comma. For example:
’name=fred,type=person’ . In the GATE GUI, these are speciﬁed via a dialog.
Script bindings
As with the Groovy console described above, and with JAPE right-hand-side Java code,
Groovy scripts run by the scripting PR implicitly import all classes from the gate,
gate.annotation, gate.util, gate.jape and gate.creole.ontology packages of the core
GATE API. The Groovy scripting PR also makes available the following bindings, which you
can use in your scripts:
doc: the current document (Document)
corpus: the corpus containing the current document
content: the string content of the current document
inputAS: the annotation set speciﬁed by inputASName in the PRs runtime parameters
outputAS: the annotation set speciﬁed by outputASName in the PRs runtime parameters

174

GATE Embedded

Note that inputAS and outputAS are intended to be used as input and output AnnotationSets. This is, however, a convention: there is nothing to stop a script writing to or reading
from any AnnotationSet. Also, although the script has access to the corpus containing the
document it is running over, it is not generally necessary for the script to iterate over the
documents in the corpus itself – the reference is provided to allow the script to access data
stored in the FeatureMap of the corpus. Any other variables assigned to within the script
code will be added to the binding, and values set while processing one document can be used
while processing a later one.
Passing parameters to the script
In addition to the above bindings, one further binding is available to the script:
scriptParams: a FeatureMap with keys and values as speciﬁed by the scriptParams
runtime parameter
For example, if you were to create a scriptParams runtime parameter for your PR, with the
keys and values: ’name=fred,type=person’, then the values could be retrieved in your script
via scriptParams.name and scriptParams.type
Controller callbacks
A Groovy script may wish to do some pre- or post-processing before or after processing
the documents in a corpus, for example if it is collecting statistics about the corpus. To
support this, the script can declare methods beforeCorpus and afterCorpus, taking a
single parameter. If the beforeCorpus method is deﬁned and the script PR is running in
a corpus pipeline application, the method will be called before the pipeline processes the
ﬁrst document. Similarly, if the afterCorpus method is deﬁned it will be called after the
pipeline has completed processing of all the documents in the corpus. In both cases the
corpus will be passed to the method as a parameter. If the pipeline aborts with an exception
the afterCorpus method will not be called, but if the script declares a method aborted(c)
then this will be called instead.
Note that because the script is not processing a particular document when these methods
are called, the usual doc, corpus, inputAS, etc. are not available within the body of the
methods (though the corpus is passed to the method as a parameter). The scriptParams
variable is available.
The following example shows how this technique could be used to build a simple tf/idf
index for a GATE corpus. The example is available in the GATE distribution as
plugins/Groovy/resources/scripts/tfidf.groovy. The script makes use of some of the
utility methods described in section 7.16.4.

GATE Embedded

1
2
3
4
5
6
7
8
9

// reset variables
void beforeCorpus ( c ) {
// list of maps (one for each doc) from term to frequency
frequencies = []
// sorted map from term to docs that contain it
docMap = new TreeMap ()
// index of the current doc in the corpus
docNum = 0
}

10
11
12

// start frequency list for this document
frequencies << [:]

13
14
15
16
17
18
19

// iterate over the requested annotations
inputAS [ scriptParams . annotationType ]. each {
def str = doc . stringFor ( it )
// increment term frequency for this term
frequencies [ docNum ][ str ] =
( frequencies [ docNum ][ str ] ?: 0) + 1

20

// keep track of which documents this term appears in
if (! docMap [ str ]) {
docMap [ str ] = new LinkedHashSet ()
}
docMap [ str ] << docNum

21
22
23
24
25
26

}

27
28
29
30
31
32

// normalize counts by doc length
def docLength = inputAS [ scriptParams . annotationType ]. size ()
frequencies [ docNum ]. each { freq ->
freq . value = (( double ) freq . value ) / docLength
}

33
34
35

// increment the counter for the next document
docNum ++

36
37
38
39
40
41
42
43
44
45
46
47
48

// compute the IDFs and store the table as a corpus feature
void afterCorpus ( c ) {
def tfIdf = [:]
docMap . each { term , docsWithTerm ->
def idf = Math . log (( double ) docNum / docsWithTerm . size ())
tfIdf [ term ] = [:]
docsWithTerm . each { docId ->
tfIdf [ term ][ docId ] = frequencies [ docId ][ term ] * idf
}
}
c . features . freqTable = tfIdf
}

175

176

GATE Embedded

Examples
The plugin directory Groovy/resources/scripts contains some example scripts. Below is the
code for a naive regular expression PR.
1
2
3
4
5
6
7

matcher = content =~ scriptParams . regex
while ( matcher . find ())
outputAS . add ( matcher . start () ,
matcher . end () ,
scriptParams . type ,
Factory . newFeatureMap ())

The script needs to have the runtime parameter scriptParams set with keys and values as
follows:
regex: the Groovy regular expression that you want to match e.g. [^\s]*ing
type: the type of the annotation to create for each regex match, e.g. regexMatch
When the PR is run over a document, the script will ﬁrst make a matcher over the document
content for the regular expression given by the regex parameter. It will iterate over all
matches for this regular expression, adding a new annotation for each, with a type as given
by the type parameter.

7.16.3

The Scriptable Controller

The Groovy plugin’s “Scriptable Controller” is a more ﬂexible alternative to the standard
pipeline (SerialController) and corpus pipeline (SerialAnalyserController) applications and their conditional variants, and also supports the time limiting and robustness
features of the realtime controller. Like the standard controllers, a scriptable controller contains a list of processing resources and can optionally be conﬁgured with a corpus, but unlike
the standard controllers it does not necessarily execute the PRs in a linear order. Instead
the execution strategy is controlled by a script written in a Groovy domain speciﬁc language
(DSL), which is detailed in the following sections.

Running a single PR
To run a single PR from the scriptable controller’s list of PRs, simply use the PR’s name as
a Groovy method call:
1
2

somePr ()
" ANNIE English Tokeniser " ()

GATE Embedded

177

If the PR’s name contains spaces or any other character that is not valid in a Groovy
identiﬁer, or if the name is a reserved word (such as “import”) then you must enclose the
name in single or double quotes. You may prefer to rename the PRs so their names are valid
identiﬁers. Also, if there are several PRs in the controller’s list with the same name, they
will all be run in the order in which they appear in the list.
You can optionally provide a Map of named parameters to the call, and these will override
the corresponding runtime parameter values for the PR (the original values will be restored
after the PR has been executed):
1

myTransducer ( outputASName : " output " )

Iterating over the corpus
If a corpus has been provided to the controller then you can iterate over all the documents
in the corpus using eachDocument:
1
2
3
4
5

eachDocument {
tokeniser ()
sentenceSplitter ()
myTransducer ()
}

The block of code (in fact a Groovy closure) is executed once for each document in the corpus
exactly as a standard corpus pipeline application would operate. The current document is
available to the script in the variable doc and the corpus in the variable corpus, and in
addition any calls to PRs that implement the LanguageAnalyser interface will set the PR’s
document and corpus parameters appropriately.

Running all the PRs in sequence
Calling allPRs() will execute all the controller’s PRs once in the order in which they appear
in the list. This is rarely useful in practice but it serves to deﬁne the default behaviour:
the initial script that is used by default in a newly instantiated scriptable controller is
eachDocument { allPRs() }, which mimics the behaviour of a standard corpus pipeline application.

More advanced scripting
The basic DSL is extremely simple, but because the script is Groovy code you can use all
the other facilities of the Groovy language to do conditional execution, grouping of PRs,
etc. The control script has the same implicit imports as provided by the Groovy Script PR
(section 7.16.2), and additional import statements can be added as required.

178

GATE Embedded

For example, suppose you have a pipeline for multi-lingual document processing, containing
PRs named “englishTokeniser”, “englishGazetteer”, “frenchTokeniser”, “frenchGazetteer”,
“genericTokeniser”, etc., and you need to choose which ones to run based on a document
feature:
1
2
3
4
5

eachDocument {
def lang = doc . features . language ?: ’ generic ’
" { lang } Tokeniser " ()
" { lang } Gazetteer " ()
}

As another example, suppose you have a particular JAPE grammar that you know is slow
on documents that mention a large number of locations, so you only want to run it on
documents with up to 100 Location annotations, and use a faster but less accurate one on
others:
1
2
3
4
5
6
7

// helper method to group several PRs together
void annotateLocations () {
tokeniser ()
splitter ()
gazetteer ()
locationGrammar ()
}

8
9
10
11
12
13
14
15
16
17

eachDocument {
annotateLocations ()
if ( doc . annotations [ " Location " ]. size () <= 100) {
fu ll L o c a t i o n C l a s s i f i e r ()
}
else {
fa st L o c a t i o n C l a s s i f i e r ()
}
}

You can have more than one call to eachDocument, for example a controller that pre-processes
some documents, then collects some corpus-level statistics, then further processes the documents based on those statistics.
As a ﬁnal example, consider a controller to post-process data from a manual annotation
task. Some of the documents have been annotated by one annotator, some by more than
one (the annotations are in sets named “annotator1”, “annotator2”, etc., but the number of
sets varies from document to document).
1
2
3
4
5
6

eachDocument {
// ﬁnd all the annotatorN sets on this document
def annotators =
doc . annotationSetNames . findAll {
it ==~ / annotator \ d +/
}

7
8
9

// run the post-processing JAPE grammar on each one
annotators . each { asName ->

GATE Embedded

179

po s t P r o c e s s i n g G r a m m a r (
inputASName : asName ,
outputASName : asName )

10
11
12

}

13
14

// now merge them to form a consensus set
mergingPR ( annSetsForMerging : annotators . join ( ’; ’ ))

15
16

}

17

Nesting a scriptable controller in another application
Like the standard SerialAnalyserController, the scriptable controller implements the
LanugageAnalyser interface and so can itself be nested as a PR in another pipeline. When
used in this way, eachDocument does not iterate over the corpus but simply calls its closure
once, with the “current document” set to the document that was passed to the controller as
a parameter. This is the same logic as is used by SerialAnalyserController, which runs its
PRs once only rather than once per document in the corpus.

Ignoring errors
By default, if an exception or error occurs while processing (either thrown by a PR or
occurring directly within the controller’s script) then the controller’s execution will terminate
with an exception. If this occurs during an eachDocument then the remaining documents
will not be processed. In some circumstances it may be preferable to ignore the error and
simply continue with the next document. To support this you can use ignoringErrors:
eachDocument {
ignoringErrors {
tokeniser ()
sentenceSplitter ()
myTransducer ()
}
}

1
2
3
4
5
6
7

Any exceptions or errors thrown within the ignoringErrors block will be logged6 but not
rethrown. So in the example above if myTransducer fails with an exception the controller
will continue with the next document. Note that it is important to nest the blocks correctly
– if the nesting were reversed (with the eachDocument inside the ignoringErrors) then
an exception would terminate the whole eachDocument loop and the remaining documents
would not be processed.
6

to the gate.groovy.ScriptableController Log4J logger

180

GATE Embedded

Realtime behaviour
Some GATE processing resources can be very slow when operating on large or complex
documents. In many cases it is possible to use heuristics within your controller’s script to
spot likely “problem” documents and avoid running such PRs over them (see the fast vs.
full location classiﬁer example above), but for situations where this is not possible you can
use the timeLimit method to put a blanket limit on the time that PRs will be allowed to
consume, in a similar way to the real-time controller.
1
2
3
4
5
6
7
8

eachDocument {
ignoringErrors {
annotateLocations ()
timeLimit ( soft :30. seconds , hard :30. seconds ) {
classifyLocations ()
}
}
}

A call to timeLimit will attempt to limit the running time of its associated code block. You
can specify three diﬀerent kinds of limit:
soft if the block is still executing after this time, attempt to interrupt it gently. This
uses Thread.interrupt() and also calls the interrupt() method of the currently
executing PR (if any).
exception if the block is still executing after this time beyond the soft limit, attempt to
induce an exception by setting the corpus and document parameters of the currently
running PR to null. This is useful to deal with PRs that do not properly respect the
interrupt call.
hard if the block is still executing after this time beyond the previous limit, forcibly terminate it using Thread.stop. This is inherently dangerous and prone to memory leakage
but may be the only way to stop particularly stubborn PRs. It should be used with
caution.
Limits can be speciﬁed using Groovy’s TimeCategory notation as shown above (e.g.
10.seconds, 2.minutes, 1.minute+45.seconds), or as simple numbers (of milliseconds).
Each limit starts counting from the end of the last, so in the example above the hard limit
is 30 seconds after the soft limit, or 1 minute after the start of execution. If no hard limit is
speciﬁed the controller will wait indeﬁnitely for the block to complete.
Note also that when a timeLimit block is terminated it will throw an exception. If you do
not wish this exception to terminate the execution of the controller as a whole you will need
to wrap the timeLimit block in an ignoringErrors block.
timeLimit blocks, particularly ones with a hard limit speciﬁed, should be regarded as a last
resort – if there are heuristic methods you can use to avoid running slow PRs in the ﬁrst place

GATE Embedded

181

Figure 7.4: Accessing the script editor for a scriptable controller

it is a good idea to use them as a ﬁrst defence, possibly wrapping them in a timeLimit block
if you need hard guarantees (for example when you are paying per hour for your compute
time in a cloud computing system).

The Scriptable Controller in GATE Developer
When you double-click on a scriptable controller in the resources tree of GATE Developer
you see the same controller editor that is used by the standard controllers. This view allows
you to add PRs to the controller and set their default runtime parameter values, and to
specify the corpus over which the controller should run. A separate view is provided to
allow you to edit the Groovy script, which is accessible via the “Control Script” tab (see
ﬁgure 7.4). This tab provides a text editor which does basic Groovy syntax highlighting (the
same editor used by the Groovy Console).

182

GATE Embedded

7.16.4

Utility methods

Loading the Groovy plugin adds some additional methods to several of the core GATE API
classes and interfaces using the Groovy “mixin” mechanism. Any Groovy code that runs
after the plugin has been loaded can make use of these additional methods, including snippets
run in the Groovy console, scripts run using the Script PR, and any other Groovy code that
uses the GATE Embedded API.
The methods that are injected come from two classes. The gate.Utils class (part of the core
GATE API in gate.jar) deﬁnes a number of static methods that can be used to simplify
common tasks such as getting the string covered by an annotation or annotation set, ﬁnding
the start or end oﬀset of an annotation (or set), etc. These methods do not use any Groovyspeciﬁc types, so they are usable from pure Java code in the usual way as well as being mixed
in for use in Groovy. Additionally, the class gate.groovy.GateGroovyMethods (part of the
Groovy plugin) provides methods that use Groovy types such as closures and ranges.
The added methods include:
Uniﬁed access to the start and end oﬀsets of an Annotation, AnnotationSet or
Document: e.g. someAnnotation.start() or anAnnotationSet.end()
Simple access to the DocumentContent or string covered by an annotation or annotation set: document.stringFor(anAnnotation), document.contentFor(annotationSet)
Simple access to the length of an annotation or document,
(annotation.length()) or a long (annotation.lengthLong()).

either as an int

A method to construct a FeatureMap from any map, to support constructions like def
params = [sourceUrl:’http://gate.ac.uk’, encoding:’UTF-8’].toFeatureMap()
A method to convert an annotation set into a List of annotations in the order they appear
in the document, for iteration in a predictable order: annSet.inDocumentOrder().collect
{ it.type }
The each, eachWithIndex and collect methods for a corpus have been redeﬁned to properly
load and unload documents if the corpus is stored in a datastore.
Various getAt methods to support constructions like annotationSet["Token"] (get all Token annotations from the set), annotationSet[15..20] (get all annotations between oﬀsets
15 and 20), documentContent[0..10] (get the document content between oﬀsets 0 and 10).
A withResource method for any resource, which calls a closure with the resource passed as
a parameter, and ensures that the resource is properly deleted when the closure completes
(analagous to the default Groovy method InputStream.withStream).

For full details, see the source code or javadoc documentation for these two classes.

GATE Embedded

7.17

183

Saving Conﬁg Data to gate.xml

Arbitrary feature/value data items can be saved to the user’s gate.xml ﬁle via the following
API calls:
To get the conﬁg data: Map configData = Gate.getUserConfig().
To add conﬁg data simply put pairs into the map: configData.put("my new config key",
"value");.
To write the conﬁg data back to the XML ﬁle: Gate.writeUserConfig();.
Note that new conﬁg data will simply override old values, where the keys are the same. In
this way defaults can be set up by putting their values in the main gate.xml ﬁle, or the site
gate.xml ﬁle; they can then be overridden by the user’s gate.xml ﬁle.

7.18

Annotation merging through the API

If we have annotations about the same subject on the same document from diﬀerent annotators, we may need to merge those annotations to form a uniﬁed annotation. Two
approaches for merging annotations are implemented in the API, via static methods in the
class gate.util.AnnotationMerging.
The two methods have very similar input and output parameters. Each of the methods
takes an array of annotation sets, which should be the same annotation type on the same
document from diﬀerent annotators, as input. A single feature can also be speciﬁed as a
parameter (or given asnull if no feature is to be speciﬁed).
The output is a map, the key of which is one merged annotation and the value of which
represents the annotators (in terms of the indices of the array of annotation sets) who support the annotation. The methods also have a boolean input parameter to indicate whether
or not the annotations from diﬀerent annotators are based on the same set of instances,
which can be determined by the static method public boolean isSameInstancesForAnnotators(AnnotationSet[] annsA) in the class gate.util.IaaCalculation. One instance corresponds to all the annotations with the same span. If the annotation sets are based on the
same set of instances, the merging methods will ensure that the merged annotations are on
the same set of instances.
The two methods corresponding to those described for the Annotation Merging plugin described in Section 20.22. They are:
The Method public static void mergeAnnotation(AnnotationSet[] annsArr, String
nameFeat, HashMap<Annotation,String>mergeAnns, int numMinK, boolean isTheSameInstances) merges the annotations stored in the array annsArr. The merged

184

GATE Embedded

annotation is put into the map mergeAnns, with a key of the merged annotation and
value of a string containing the indices of elements in the annotation set array annsArr
which contain that annotation. NumMinK speciﬁes the minimal number of the annotators supporting one merged annotation. The boolean parameter isTheSameInstances
indicate if or not those annotation sets for merging are based on the same instances.
Method public static void mergeAnnotationMajority(AnnotationSet[] annsArr, String
nameFeat, HashMap<Annotation, String>mergeAnns, boolean isTheSameInstances)
selects the annotations which the majority of the annotators agree on. The meanings
of parameters are the same as those in the above method.

Chapter 8
JAPE: Regular Expressions over
Annotations
If Osama bin Laden did not exist, it would be necessary to invent him. For the
past four years, his name has been invoked whenever a US president has sought
to increase the defence budget or wriggle out of arms control treaties. He has
been used to justify even President Bush’s missile defence programme, though
neither he nor his associates are known to possess anything approaching ballistic
missile technology. Now he has become the personiﬁcation of evil required to
launch a crusade for good: the face behind the faceless terror.
The closer you look, the weaker the case against Bin Laden becomes. While
the terrorists who inﬂicted Tuesday’s dreadful wound may have been inspired by
him, there is, as yet, no evidence that they were instructed by him. Bin Laden’s
presumed guilt appears to rest on the supposition that he is the sort of man who
would have done it. But his culpability is irrelevant: his usefulness to western
governments lies in his power to terrify. When billions of pounds of military
spending are at stake, rogue states and terrorist warlords become assets precisely
because they are liabilities.
The need for dissent, George Monbiot, The Guardian, Tuesday September 18,
2001.

JAPE is a Java Annotation Patterns Engine. JAPE provides ﬁnite state transduction over
annotations based on regular expressions. JAPE is a version of CPSL – Common Pattern
Speciﬁcation Language1 . This chapter introduces JAPE, and outlines the functionality available. (You can ﬁnd an excellent tutorial here; thanks to Dhaval Thakker, Taha Osmin and
Phil Lakin).
1
A good description of the original version of this language is in Doug Appelt’s TextPro manual. Doug
was a great help to us in implementing JAPE. Thanks Doug!

185

186

JAPE: Regular Expressions over Annotations

JAPE allows you to recognise regular expressions in annotations on documents. Hang on,
there’s something wrong here: a regular language can only describe sets of strings, not graphs,
and GATE’s model of annotations is based on graphs. Hmmm. Another way of saying this:
typically, regular expressions are applied to character strings, a simple linear sequence of
items, but here we are applying them to a much more complex data structure. The result is
that in certain cases the matching process is non-deterministic (i.e. the results are dependent
on random factors like the addresses at which data is stored in the virtual machine): when
there is structure in the graph being matched that requires more than the power of a regular
automaton to recognise, JAPE chooses an alternative arbitrarily. However, this is not the
bad news that it seems to be, as it turns out that in many useful cases the data stored in
annotation graphs in GATE (and other language processing systems) can be regarded as
simple sequences, and matched deterministically with regular expressions.
A JAPE grammar consists of a set of phases, each of which consists of a set of pattern/action rules. The phases run sequentially and constitute a cascade of ﬁnite state transducers
over annotations. The left-hand-side (LHS) of the rules consist of an annotation pattern
description. The right-hand-side (RHS) consists of annotation manipulation statements.
Annotations matched on the LHS of a rule may be referred to on the RHS by means of
labels that are attached to pattern elements. Consider the following example:

Phase: Jobtitle
Input: Lookup
Options: control = appelt debug = true
Rule: Jobtitle1
(
{Lookup.majorType == jobtitle}
(
{Lookup.majorType == jobtitle}
)?
)
:jobtitle
-->
:jobtitle.JobTitle = {rule = "JobTitle1"}

The LHS is the part preceding the ‘-->’ and the RHS is the part following it. The LHS speciﬁes a pattern to be matched to the annotated GATE document, whereas the RHS speciﬁes
what is to be done to the matched text. In this example, we have a rule entitled ‘Jobtitle1’,
which will match text annotated with a ‘Lookup’ annotation with a ‘majorType’ feature of
‘jobtitle’, followed optionally by further text annotated as a ‘Lookup’ with ‘majorType’ of
‘jobtitle’. Once this rule has matched a sequence of text, the entire sequence is allocated a
label by the rule, and in this case, the label is ‘jobtitle’. On the RHS, we refer to this span

JAPE: Regular Expressions over Annotations

187

of text using the label given in the LHS; ‘jobtitle’. We say that this text is to be given an
annotation of type ‘JobTitle’ and a ‘rule’ feature set to ‘JobTitle1’.
We began the JAPE grammar by giving it a phase name, e.g. ‘Phase: Jobtitle’. JAPE
grammars can be cascaded, and so each grammar is considered to be a ‘phase’ (see Section 8.5). The phase name makes up part of the Java class name for the compiled RHS
actions. Because of this, it must contain alphanumeric characters and underscores only, and
cannot start with a number.
We also provide a list of the annotation types we will use in the grammar. In this case,
we say ‘Input: Lookup’ because the only annotation type we use on the LHS are Lookup
annotations. If no annotations are deﬁned, all annotations will be matched.
Then, several options are set:
Control; in this case, ‘appelt’. This deﬁnes the method of rule matching (see Section
8.4)
Debug. When set to true, if the grammar is running in Appelt mode and there is more
than one possible match, the conﬂicts will be displayed on the standard output.
A wide range of functionality can be used with JAPE, making it a very powerful system.
Section 8.1 gives an overview of some common LHS tasks. Section 8.2 talks about the various
operators available for use on the LHS. After that, Section 8.3 outlines RHS functionality.
Section 8.4 talks about priority and Section 8.5 talks about phases. Section 8.6 talks about
using Java code on the RHS, which is the main way of increasing the power of the RHS. We
conclude the chapter with some miscellaneous JAPE-related topics of interest.

8.1

The Left-Hand Side

The LHS of a JAPE grammar aims to match the text span to be annotated, whilst avoiding
undesirable matches. There are various tools available to enable you to do this. This section
outlines how you would approach various common tasks on the LHS of your JAPE grammar.

8.1.1

Matching a Simple Text String

To match a simple text string, you need to refer to a feature on an annotation that contains
the string; for example,
{Token.string == "of"}

188

JAPE: Regular Expressions over Annotations

The following grammar shows a sequence of strings being matched. Bracketing, along with
the ‘or’ operator, is used to deﬁne how the strings should come together:
Phase: UrlPre
Input: Token SpaceToken
Options: control = appelt
Rule: Urlpre
( (({Token.string == "http"} |
{Token.string == "ftp"})
{Token.string == ":"}
{Token.string == "/"}
{Token.string == "/"}
) |
({Token.string == "www"}
{Token.string == "."}
)
):urlpre
-->
:urlpre.UrlPre = {rule = "UrlPre"}

Alternatively you could use the ‘string’ metaproperty. See Section 8.1.4 for an example of
using metaproperties.

8.1.2

Matching Entire Annotation Types

You can specify the presence of an annotation previously assigned from a gazetteer, tokeniser,
or other module. For example, the following will match a Lookup annotation:
{Lookup}

The following will match if there is not a Lookup annotation at this location:
{!Lookup}

The following rule shows several diﬀerent annotation types being matched. We also see a
string being matched, and again, the use of the ‘or’ operator:
Rule: Known
Priority: 100

JAPE: Regular Expressions over Annotations

189

(
{Location}|
{Person}|
{Date}|
{Organization}|
{Address}|
{Money} |
{Percent}|
{Token.string == "Dear"}|
{JobTitle}|
{Lookup}
):known
-->
{}

8.1.3

Using Attributes and Values

You can specify the attributes (and values) of an annotation to be matched. Several operators
are supported; see Section 8.2 for full details:
{Token.kind == "number"}, {Token.length != 4} - equality and inequality.
{Token.string > "aardvark"}, {Token.length < 10} - comparison operators. >=
and <= are also supported.
{Token.string =~ "[Dd]ogs"}, {Token.string !~ "(?i)hello"} - regular expression. ==~ and !=~ are also provided, for whole-string matching.
{X contains Y} and {X within Y} for checking annotations within the context of
other annotations.
In the following rule, the ‘category’ feature of the ‘Token’ annotation is used, along with the
‘equals’ operator:

Rule: Unknown
Priority: 50
(
{Token.category == NNP}
)
:unknown
-->
:unknown.Unknown = {kind = "PN", rule = Unknown}

190

JAPE: Regular Expressions over Annotations

8.1.4

Using Meta-Properties

In addition to referencing annotation features, JAPE allows access to other ‘meta-properties’
of an annotation. This is done by using an ‘@’ symbol rather than a ‘.’ symbol after the
annotation type name. The three meta-properties that are built in are:
length - returns the spanning length of the annotation.
string - returns the string spanned by the annotation in the document.
cleanString - Like string, but with extra white space stripped out. (i.e. ‘\s+’ goes to
a single space and leading or trailing white space is removed).
{X@length > 5}:label-->:label.New = {}

@string is also available in assignments on the right-hand side:
{X@length > 5}:label-->:label.New = {somefeat = :label.X@string }

As well as getting the string covered by a single annotation it is also possible to omit the
annotation type and get the string spanned by all the annotations bound to a label, for
example:
(
{X@length > 5}
({Y}+):ys
):label
-->
:label.New = { somefeat = :ys@string }

If several Y annotations were included in the match, the New annotation’s feature would be
set to the string starting at the beginning of the leftmost Y that was matched and ending
at the end of the rightmost one.
The ‘meta-properties’ @length and @cleanString are not available in the RHS but may be
added in the future.

8.1.5

Using Templates

In cases where a grammar contains many similar or identical strings or other literal values,
JAPE supports the concept of templates. A template is a named value declared in the

JAPE: Regular Expressions over Annotations

191

grammar ﬁle, similar to a variable in Java or other programming languages, which can be
referenced anywhere where a normal string literal, boolean or numeric value could be used,
on the left- or right-hand side of a rule. In the simplest case templates can be constants:
Template: source = "Interesting entity finder"
Template: threshold = 0.6

The templates can be used in rules by providing their names in square brackets:
Rule: InterestingLocation
(
{Location.score >= [threshold]}
):loc
-->
:loc.Entity = { type = Location, source = [source] }

The JAPE grammar parser substitutes the template values for their references when the
grammar is parsed. Thus the example rule is equivalent to
Rule: InterestingLocation
(
{Location.score >= 0.6}
):loc
-->
:loc.Entity = { type = Location,
source = "Interesting entity finder" }

The advantage of using templates is that if there are many rules in the grammar that all
reference the threshold template then it is possible to change the threshold for all rules by
simply changing the template deﬁnition.
The name “template” stems from the fact that templates whose value is a string can contain
parameters, speciﬁed using ${name} notation:
Template: url = "http://gate.ac.uk/${path}"

When a template containing parameters is referenced, values for the parameters may be
speciﬁed:
...
-->
:anchor.Reference = {
page = [url path = "userguide"] }

192

JAPE: Regular Expressions over Annotations

This is equivalent to page = "http://gate.ac.uk/userguide". Multiple parameter value
assignments are separated by commas, for example:

Template: proton =
"http://proton.semanticweb.org/2005/04/proton${mod}#${n}"
...
{Lookup.class == [proton mod="km", n="Mention"]}
// equivalent to
// {Lookup.class ==
//
"http://proton.semanticweb.org/2005/04/protonkm#Mention"}

The parser will report an error if a value is speciﬁed for a parameter that is not declared by
the referenced template, for example [proton module="km"] would not be permitted in the
above example.

Advanced template usage
If a template contains parameters for which values are not provided when the template is
referenced, the parameter placeholders are passed through unchanged. Combined with the
fact that the value for a template deﬁnition can itself be a reference to a previously-deﬁned
template, this allows for idioms like the following:

Template: proton =
"http://proton.semanticweb.org/2005/04/proton${mod}#${n}"
Template: pkm = [proton mod="km"]
Template: ptop = [proton mod="t"]
...
({Lookup.class == [ptop n="Person"]}):look
-->
:look.Mention = { class = [pkm n="Mention"], of = "Person"}

(This example is inspired by the ontology-aware JAPE matching mode described in section 14.10.)
In a multi-phase JAPE grammar, templates deﬁned in earlier phases may be referenced in
later phases. This makes it possible to declare constants (such as the PROTON URIs above)
in one place and reference them throughout a complex grammar.

JAPE: Regular Expressions over Annotations

8.1.6

193

Multiple Pattern/Action Pairs

It is also possible to have more than one pattern and corresponding action, as shown in the
rule below. On the LHS, each pattern is enclosed in a set of round brackets and has a unique
label; on the RHS, each label is associated with an action. In this example, the Lookup
annotation is labelled ‘jobtitle’ and is given the new annotation JobTitle; the TempPerson
annotation is labelled ‘person’ and is given the new annotation ‘Person’.
Rule: PersonJobTitle
Priority: 20
(
{Lookup.majorType == jobtitle}
):jobtitle
(
{TempPerson}
):person
-->
:jobtitle.JobTitle = {rule = "PersonJobTitle"},
:person.Person = {kind = "personName", rule = "PersonJobTitle"}

Similarly, labelled patterns can be nested, as in the example below, where the whole pattern
is annotated as Person, but within the pattern, the jobtitle is annotated as JobTitle.
Rule: PersonJobTitle2
Priority: 20
(
(
{Lookup.majorType == jobtitle}
):jobtitle
{TempPerson}
):person
-->
:jobtitle.JobTitle = {rule = "PersonJobTitle"},
:person.Person = {kind = "personName", rule = "PersonJobTitle"}

8.1.7

LHS Macros

Macros allow you to create a deﬁnition that can then be used multiple times in
your JAPE rules. In the following JAPE grammar, we have a cascade of macros
used. The macro ‘AMOUNT NUMBER’ makes use of the macros ‘MILLION BILLION’
and ‘NUMBER WORDS’, and the rule ‘MoneyCurrencyUnit’ then makes use of
‘AMOUNT NUMBER’:

194

JAPE: Regular Expressions over Annotations

Phase: Number
Input: Token Lookup
Options: control = appelt
Macro: MILLION_BILLION
({Token.string == "m"}|
{Token.string == "million"}|
{Token.string == "b"}|
{Token.string == "billion"}|
{Token.string == "bn"}|
{Token.string == "k"}|
{Token.string == "K"}
)
Macro: NUMBER_WORDS
(
(({Lookup.majorType == number}
({Token.string == "-"})?
)*
{Lookup.majorType == number}
{Token.string == "and"}
)*
({Lookup.majorType == number}
({Token.string == "-"})?
)*
{Lookup.majorType == number}
)
Macro: AMOUNT_NUMBER
(({Token.kind == number}
(({Token.string == ","}|
{Token.string == "."}
)
{Token.kind == number}
)*
|
(NUMBER_WORDS)
)
(MILLION_BILLION)?
)
Rule: MoneyCurrencyUnit
(
(AMOUNT_NUMBER)
({Lookup.majorType == currency_unit})
)
:number -->

JAPE: Regular Expressions over Annotations

195

:number.Money = {kind = "number", rule = "MoneyCurrencyUnit"}

8.1.8

Using Context

Context can be dealt with in the grammar rules in the following way. The pattern to be
annotated is always enclosed by a set of round brackets. If preceding context is to be included
in the rule, this is placed before this set of brackets. This context is described in exactly
the same way as the pattern to be matched. If context following the pattern needs to be
included, it is placed after the label given to the annotation. Context is used where a pattern
should only be recognised if it occurs in a certain situation, but the context itself does not
form part of the pattern to be annotated.
For example, the following rule for Time (assuming an appropriate macro for ‘year’) would
mean that a year would only be recognised if it occurs preceded by the words ‘in’ or ‘by’:
Rule: YearContext1
({Token.string == "in"}|
{Token.string == "by"}
)
(YEAR)
:date -->
:date.Timex = {kind = "date", rule = "YearContext1"}

Similarly, the following rule (assuming an appropriate macro for ‘email’) would mean that
an email address would only be recognised if it occurred inside angled brackets (which would
not themselves form part of the entity):
Rule: Emailaddress1
({Token.string == ‘<’})
(
(EMAIL)
)
:email
({Token.string == ‘>’})
-->
:email.Address= {kind = "email", rule = "Emailaddress1"}

Also, it is possible to specify the constraint that one annotation must start at the same place
as another. For example:
Rule: SurnameStartingWithDe

196

JAPE: Regular Expressions over Annotations

(
{Token.string == "de",
Lookup.majorType == "name",
Lookup.minorType == "surname"}
):de
-->
:de.Surname = {prefix = "de"}

This rule would match anywhere where a Token with string ‘de’ and a Lookup with majorType ‘name’ and minorType ‘surname’ start at the same oﬀset in the text. Both the
Lookup and Token annotations would be included in the :de binding, so the Surname annotation generated would span the longer of the two. Constraints on the same annotation type
must be satisﬁed by a single annotation, so in this example there must be a single Lookup
matching both the major and minor types – the rule would not match if there were two
diﬀerent lookups at the same location, one of them satisfying each constraint.
It is important to remember that context is consumed by the rule, so it cannot be reused in
another rule within the same phase. So, for example, right context cannot be used as left
context for another rule.

8.1.9

Multi-Constraint Statements

In the examples we have seen so far, most statements have contained only one constraint.
For example, in this statement, the ‘category’ of ‘Token’ must equal ‘NNP’:
Rule: Unknown
Priority: 50
(
{Token.category == NNP}
)
:unknown
-->
:unknown.Unknown = {kind = "PN", rule = Unknown}

However, it is equally acceptable to have multiple constraints in a statement. In this example,
the ‘majorType’ of ‘Lookup’ must be ‘name’ and the ‘minorType’ must be ‘surname’:
Rule: Surname
(
{Lookup.majorType == "name",
Lookup.minorType == "surname"}
):surname
-->
:surname.Surname = {}

JAPE: Regular Expressions over Annotations

197

As we saw in Section 8.1.8, the constraints may refer to diﬀerent annotations. In this
example, in addition to the constraints on the ‘majorType’ and ‘minorType’ of ‘Lookup’, we
also have a constraint on the ‘string’ of ‘Token’:
Rule: SurnameStartingWithDe
(
{Token.string == "de",
Lookup.majorType == "name",
Lookup.minorType == "surname"}
):de
-->
:de.Surname = {prefix = "de"}

8.1.10

Negation

All the examples in the preceding sections involve constraints that require the presence of
certain annotations to match. JAPE also supports ‘negative’ constraints which specify the
absence of annotations. A negative constraint is signalled in the grammar by a ‘!’ character.
Negative constraints are generally used in combination with positive ones to constrain the
locations at which the positive constraint can match. For example:
Rule: PossibleName
(
{Token.orth == "upperInitial", !Lookup}
):name
-->
:name.PossibleName = {}

This rule would match any uppercase-initial Token, but only where there is no Lookup annotation starting at the same location. The general rule is that a negative constraint matches
at any location where the corresponding positive constraint would not match. Negative constraints do not contribute any annotations to the bindings - in the example above, the :name
binding would contain only the Token annotation. The exception to this is when a negative
constraint is used alone, without any positive constraints in the combination. In this case
it binds all the annotations at the match position that do not match the constraint. Thus,
{!Lookup} would bind all the annotations starting at this location except Lookups. In most
cases, negative constraints should only be used in combination with positive ones.
Any constraint can be negated, for example:
Rule: SurnameNotStartingWithDe

198

JAPE: Regular Expressions over Annotations

(
{Surname, !Token.string ==~ "[Dd]e"}
):name
-->
:name.NotDe = {}

This would match any Surname annotation that does not start at the same place
as a Token with the string ‘de’ or ‘De’. Note that this is subtly diﬀerent from
{Surname, Token.string !=~ "[Dd]e"}, as the second form requires a Token annotation
to be present, whereas the ﬁrst form (!Token...) will match if there is no Token annotation
at all at this location.2
Although JAPE provides an operator to look for the absence of a single annotation type,
there is no support for a general negative operator to prevent a rule from ﬁring if a particular
sequence of annotations is found. One solution to this is to create a ‘negative rule’ which
has higher priority than the matching ‘positive rule’. The style of matching must be Appelt
for this to work. To create a negative rule, simply state on the LHS of the rule the pattern
that should NOT be matched, and on the RHS do nothing. In this way, the positive rule
cannot be ﬁred if the negative pattern matches, and vice versa, which has the same end
result as using a negative operator. A useful variation for developers is to create a dummy
annotation on the RHS of the negative rule, rather than to do nothing, and to give the
dummy annotation a rule feature. In this way, it is obvious that the negative rule has ﬁred.
Alternatively, use Java code on the RHS to print a message when the rule ﬁres. An example
of a matching negative and positive rule follows. Here, we want a rule which matches a
surname followed by a comma and a set of initials. But we want to specify that the initials
shouldn’t have the POS category PRP (personal pronoun). So we specify a negative rule
that will ﬁre if the PRP category exists, thereby preventing the positive rule from ﬁring.
Rule: NotPersonReverse
Priority: 20
// we don’t want to match ’Jones, I’
(
{Token.category == NNP}
{Token.string == ","}
{Token.category == PRP}
)
:foo
-->
{}
Rule:
PersonReverse
Priority: 5
// we want to match ‘Jones, F.W.’
(
2

In the Montreal transducer, the two forms were equivalent

JAPE: Regular Expressions over Annotations

199

{Token.category == NNP}
{Token.string == ","}
(INITIALS)?
)
:person -->

8.1.11

Escaping Special Characters

To specify a single or double quote as a string, precede it with a backslash, e.g.
{Token.string=="\""}

will match a double quote. For other special characters, such as ‘ ’, enclose it in double
quotes, e.g.
{Token.category == "PRP$"}

8.2

LHS Operators in Detail

This section gives more detail on the behaviour of the matching operators used on the lefthand side of JAPE rules.

8.2.1

Compositional Operators

Compositional operators are used to combine matching constructions in the manner intended.
Union and Kleene operators are available, as is range notation.

Union and Kleene Operators
The following union and Kleene operators are available:
| - or
* - zero or more occurrences
? - zero or one occurrences
+ - one or more occurrences

200

JAPE: Regular Expressions over Annotations

In the following example, you can see the ‘|’ and ‘?’ operators being used:
Rule: LocOrganization
Priority: 50
(
({Lookup.majorType == location} |
{Lookup.majorType == country_adj})
{Lookup.majorType == organization}
({Lookup.majorType == organization})?
)
:orgName -->
:orgName.TempOrganization = {kind = "orgName", rule=LocOrganization}

Range Notation
A range notation can also be added. e.g.
({Token})[1,3]

matches one to three Tokens in a row.
({Token.kind == number})[3]

matches exactly 3 number Tokens in a row.

8.2.2

Matching Operators

Matching operators are used to specify how matching must take place between a speciﬁcation
and an annotation in the document. Equality (‘==’ and ‘!=’) and comparison (‘<’, ‘<=’,
‘>=’ and ‘>’) operators can be used, as can regular expression matching and contextual
operators (‘contains’ and ‘within’).

Equality Operators
The equality operators are ‘==’ and ‘!=’. The basic operator in JAPE is equality.
{Lookup.majorType == "person"} matches a Lookup annotation whose majorType feature has the value ‘person’. Similarly {Lookup.majorType != "person"} would match any
Lookup whose majorType feature does not have the value ‘person’. If a feature is missing

JAPE: Regular Expressions over Annotations

201

it is treated as if it had an empty string as its value, so this would also match a Lookup
annotation that did not have a majorType feature at all.
Certain type coercions are performed:
If the constraint’s attribute is a string, it is compared with the annotation feature value
using string equality (String.equals()).
If the constraint’s attribute is an integer it is treated as a java.lang.Long. If the
annotation feature value is also a Long, or is a string that can be parsed as a Long,
then it is compared using Long.equals().
If the constraint’s attribute is a ﬂoating-point number it is treated as a
java.lang.Double. If the annotation feature value is also a Double, or is a string that
can be parsed as a Double, then it is compared using Double.equals().
If the constraint’s attribute is true or false (without quotes) it is treated as a
java.lang.Boolean. If the annotation feature value is also a Boolean, or is a string
that can be parsed as a Boolean, then it is compared using Boolean.equals().
The != operator matches exactly when == doesn’t.
Comparison Operators
The comparison operators are ‘<’, ‘<=’, ‘>=’ and ‘>’. Comparison operators have their
expected meanings, for example {Token.length > 3} matches a Token annotation whose
length attribute is an integer greater than 3. The behaviour of the operators depends on the
type of the constraint’s attribute:
If the constraint’s attribute is a string it is compared with the annotation feature value
using Unicode-lexicographic order (see String.compareTo()).
If the constraint’s attribute is an integer it is treated as a java.lang.Long. If the
annotation feature value is also a Long, or is a string that can be parsed as a Long,
then it is compared using Long.compareTo().
If the constraint’s attribute is a ﬂoating-point number it is treated as a
java.lang.Double. If the annotation feature value is also a Double, or is a string that
can be parsed as a Double, then it is compared using Double.compareTo().
Regular Expression Operators
The regular expression operators are ‘=∼’, ‘==∼’, ‘!∼’ and ‘!=∼’. These operators match
regular expressions. {Token.string =~ "[Dd]ogs"} matches a Token annotation whose

202

JAPE: Regular Expressions over Annotations

string feature contains a substring that matches the regular expression [Dd]ogs, using
!~ would match if the feature value does not contain a substring that matches the regular expression. The ==~ and !=~ operators are like =~ and !~ respectively, but require
that the whole value match (or not match) the regular expression3 . As with ==, missing features are treated as if they had the empty string as their value, so the constraint
{Identifier.name ==~ "(?i)[aeiou]*"} would match an Identiﬁer annotation which does
not have a name feature, as well as any whose name contains only vowels.
The matching uses the standard Java regular expression library, so full details of the pattern
syntax can be found in the JavaDoc documentation for java.util.regex.Pattern. There are a
few speciﬁc points to note:
To enable ﬂags such as case-insensitive matching you can use the (?ﬂags) notation.
See the Pattern JavaDocs for details.
If you need to include a double quote character in a regular expression you must precede it with a backslash, otherwise JAPE will give a syntax error. Quoted strings in
JAPE grammars also convert the sequences \n, \r and \t to the characters newline
(U+000A), carriage return (U+000D) and tab (U+0009) respectively, but these characters can match literally in regular expressions so it does not make any diﬀerence to
the result in most cases.4

Contextual Operators
The contextual Operators are ‘contains’ and ‘within’. These operators match annotations
within the context of other annotations.
contains - Written as {X contains Y}, returns true if an annotation of type X completely contains an annotation of type Y.
within - Written as {X within Y}, returns true if an annotation of type X is completely
covered by an annotation of type Y.
For either operator, the right-hand value (Y in the above examples) can be a full constraint
itself. For example {X contains {Y.foo==bar}} is also accepted. The operators can be
used in a multi-constraint statement (see Section 8.1.9) just like any of the traditional ones,
so {X.f1 != "something", X contains {Y.foo==bar}} is valid.
3

This syntax will be familiar to Groovy users.
However this does mean that it is not possible to include an n, r or t character after a backslash in a
JAPE quoted string, or to have a backslash as the last character of your regular expression. Workarounds
include placing the backslash in a character class ([\\]—) or enabling the (?x) ﬂag, which allows you to put
whitespace between the backslash and the oﬀending character without changing the meaning of the pattern.
4

JAPE: Regular Expressions over Annotations

203

Custom Operators
It is possible to add additional custom operators without modifying the JAPE language.
There are new init-time parameters to Transducer so that additional annotation ‘metaproperty’ accessors and custom operators can be referenced at runtime. To add a custom
operator, write a class that implements gate.jape.constraint.ConstraintPredicate, make the
class available to GATE (either by putting the class in a JAR ﬁle in the lib directory or
by putting the class in a plugin and loading the plugin), and then list that class name for
the Transducer’s ‘operators’ property. Similarly, to add a custom ‘meta-property’ accessor,
write a class that implements gate.jape.constraint.AnnotationAccessor, and then list that
class name in the Transducer’s ‘annotationAccessors’ property.

8.3

The Right-Hand Side

The RHS of the rule contains information about the annotation to be created/manipulated.
Information about the text span to be annotated is transferred from the LHS of the rule
using the label just described, and annotated with the entity type (which follows it). Finally,
attributes and their corresponding values are added to the annotation. Alternatively, the
RHS of the rule can contain Java code to create or manipulate annotations, see Section 8.6.

8.3.1

A Simple Example

In the simple example below, the pattern described will be awarded an annotation of type
‘Enamex’ (because it is an entity name). This annotation will have the attribute ‘kind’,
with value ‘location’, and the attribute ‘rule’, with value ‘GazLocation’. (The purpose of
the ‘rule’ attribute is simply to ease the process of manual rule validation).

Rule: GazLocation
(
{Lookup.majorType == location}
)
:location -->
:location.Enamex = {kind="location", rule=GazLocation}

8.3.2

Copying Feature Values from the LHS to the RHS

JAPE provides limited support for copying annotation feature values from the left to the
right hand side of a rule, for example:

204

JAPE: Regular Expressions over Annotations

Rule: LocationType
(
{Lookup.majorType == location}
):loc
-->
:loc.Location = {rule = "LocationType", type = :loc.Lookup.minorType}

This will set the ‘type’ feature of the generated location to the value of the ‘minorType’ feature from the ‘Lookup’ annotation bound to the loc label. If the Lookup has no minorType,
the Location will have no ‘type’ feature. The behaviour of newFeat = :bind.Type.oldFeat
is:
Find all the annotations of type Type from the left hand side binding bind.
Find one of them that has a non-null value for its oldFeat feature (if there is more
than one, which one is chosen is up to the JAPE implementation).
If such a value exists, set the newFeat feature of our newly created annotation to this
value.
If no such non-null value exists, do not set the newFeat feature at all.
Notice that the behaviour is deliberately underspeciﬁed if there is more than one Type annotation in bind. If you need more control, or if you want to copy several feature values from
the same left hand side annotation, you should consider using Java code on the right hand
side of your rule (see Section 8.6).

8.3.3

RHS Macros

Macros, ﬁrst introduced in the context of the left-hand side (Section 8.1.7) can also be used
on the RHS of rules. In this case, the label (which matches the label on the LHS of the rule)
should be included in the macro. Below we give an example of using a macro on the RHS:
Macro: UNDERSCORES_OKAY
// separate
:match
// lines
{
AnnotationSet matchedAnns = bindings.get("match");
int begOffset = matchedAnns.firstNode().getOffset().intValue();
int endOffset = matchedAnns.lastNode().getOffset().intValue();
String mydocContent = doc.getContent().toString();
String matchedString = mydocContent.substring(begOffset, endOffset);

JAPE: Regular Expressions over Annotations

205

FeatureMap newFeatures = Factory.newFeatureMap();
if(matchedString.equals("Spanish"))
newFeatures.put("myrule", "Lower");
}
else
{
newFeatures.put("myrule", "Upper");
}

{

newFeatures.put("quality", "1");
annotations.add(matchedAnns.firstNode(), matchedAnns.lastNode(),
"Spanish_mark", newFeatures);
}
Rule: Lower
(
({Token.string == "Spanish"})
:match)-->UNDERSCORES_OKAY
// no label here, only macro name
Rule: Upper
(
({Token.string == "SPANISH"})
:match)-->UNDERSCORES_OKAY
// no label here, only macro name

8.4

Use of Priority

Each grammar has one of 5 possible control styles: ‘brill’, ‘all’, ‘ﬁrst’, ‘once’ and ‘appelt’.
This is speciﬁed at the beginning of the grammar.
The Brill style means that when more than one rule matches the same region of the document,
they are all ﬁred. The result of this is that a segment of text could be allocated more than one
entity type, and that no priority ordering is necessary. Brill will execute all matching rules
starting from a given position and will advance and continue matching from the position in
the document where the longest match ﬁnishes.
The ‘all’ style is similar to Brill, in that it will also execute all matching rules, but the
matching will continue from the next oﬀset to the current one.
For example, where [] are annotations of type Ann
[aaa[bbb]] [ccc[ddd]]

206

JAPE: Regular Expressions over Annotations

then a rule matching {Ann} and creating {Ann-2} for the same spans will generate:
BRILL: [aaabbb] [cccddd]
ALL: [aaa[bbb]] [ccc[ddd]]

With the ‘ﬁrst’ style, a rule ﬁres for the ﬁrst match that’s found. This makes it inappropriate
for rules that end in ‘+’ or ‘?’ or ‘*’. Once a match is found the rule is ﬁred; it does not
attempt to get a longer match (as the other two styles do).
With the ‘once’ style, once a rule has ﬁred, the whole JAPE phase exits after the ﬁrst match.
With the appelt style, only one rule can be ﬁred for the same region of text, according to a
set of priority rules. Priority operates in the following way.
1. From all the rules that match a region of the document starting at some point X, the
one which matches the longest region is ﬁred.
2. If more than one rule matches the same region, the one with the highest priority is
ﬁred
3. If there is more than one rule with the same priority, the one deﬁned earlier in the
grammar is ﬁred.
An optional priority declaration is associated with each rule, which should be a positive integer. The higher the number, the greater the priority. By default (if the priority declaration
is missing) all rules have the priority -1 (i.e. the lowest priority).
For example, the following two rules for location could potentially match the same text.
Rule:
Location1
Priority: 25
(
({Lookup.majorType == loc_key, Lookup.minorType == pre}
{SpaceToken})?
{Lookup.majorType == location}
({SpaceToken}
{Lookup.majorType == loc_key, Lookup.minorType == post})?
)
:locName -->
:locName.Location = {kind = "location", rule = "Location1"}

Rule: GazLocation
Priority: 20

JAPE: Regular Expressions over Annotations

207

(
({Lookup.majorType == location}):location
)
-->
:location.Name = {kind = "location", rule=GazLocation}

Assume we have the text ‘China sea’, that ‘China’ is deﬁned in the gazetteer as ‘location’,
and that sea is deﬁned as a ‘loc key’ of type ‘post’. In this case, rule Location1 would apply,
because it matches a longer region of text starting at the same point (‘China sea’, as opposed
to just ‘China’). Now assume we just have the text ‘China’. In this case, both rules could
be ﬁred, but the priority for Location1 is highest, so it will take precedence. In this case,
since both rules produce the same annotation, so it is not so important which rule is ﬁred,
but this is not always the case.
One important point of which to be aware is that prioritisation only operates within a
single grammar. Although we could make priority global by having all the rules in a single
grammar, this is not ideal due to other considerations. Instead, we currently combine all the
rules for each entity type in a single grammar. An index ﬁle (main.jape) is used to deﬁne
which grammars should be used, and in which order they should be ﬁred.
Note also that depending on the control style, ﬁring a rule may ‘consume’ that part of the
text, making it unavailable to be matched by other rules. This can be a problem for example
if one rule uses context to make it more speciﬁc, and that context is then missed by later
rules, having been consumed due to use of for example the ‘Brill’ control style. ‘All’, on the
other hand, would allow it to be matched.
Using priority to resolve ambiguity
If the Appelt style of matching is selected, rule priority operates in the following way.
1. Length of rule – a rule matching a longer pattern will ﬁre ﬁrst.
2. Explicit priority declaration. Use the optional Priority function to assign a ranking.
The higher the number, the higher the priority. If no priority is stated, the default is
-1.
3. Order of rules. In the case where the above two factors do not distinguish between two
rules, the order in which the rules are stated applies. Rules stated ﬁrst have higher
priority.
Because priority can only operate within a single grammar, this can be a problem for dealing
with ambiguity issues. One solution to this is to create a temporary set of annotations
in initial grammars, and then manipulate this temporary set in one or more later phases
(for example, by converting temporary annotations from diﬀerent phases into permanent
annotations in a single ﬁnal phase). See the default set of grammars for an example of this.

208

JAPE: Regular Expressions over Annotations

If two possible ways of matching are found for the same text string, a conﬂict can arise.
Normally this is handled by the priority mechanism (test length, rule priority and ﬁnally
rule precedence). If all these are equal, Jape will simply choose a match at random and ﬁre
it. This leads ot non-deterministic behaviour, which should be avoided.

8.5

Using Phases Sequentially

A JAPE grammar consists of a set of sequential phases. The list of phases is speciﬁed (in the
order in which they are to be run) in a ﬁle, conventionally named main.jape. When loading
the grammar into GATE, it is only necessary to load this main ﬁle – the phases will then be
loaded automatically. It is, however, possible to omit this main ﬁle, and just load the phases
individually, but this is much more time-consuming. The grammar phases do not need to be
located in the same directory as the main ﬁle, but if they are not, the relative path should
be speciﬁed for each phase.
One of the main reasons for using a sequence of phases is that a pattern can only be used
once in each phase, but it can be reused in a later phase. Combined with the fact that
priority can only operate within a single grammar, this can be exploited to help deal with
ambiguity issues.
The solution currently adopted is to write a grammar phase for each annotation type, or
for each combination of similar annotation types, and to create temporary annotations.
These temporary annotations are accessed by later grammar phases, and can be manipulated
as necessary to resolve ambiguity or to merge consecutive annotations. The temporary
annotations can either be removed later, or left and simply ignored.
Generally, annotations about which we are more certain are created earlier on. Annotations
which are more dubious may be created temporarily, and then manipulated by later phases
as more information becomes available.
An annotation generated in one phase can be referred to in a later phase, in exactly the same
way as any other kind of annotation (by specifying the name of the annotation within curly
braces). The features and values can be referred to or omitted, as with all other annotations.
Make sure that if the Input speciﬁcation is used in the grammar, that the annotation to be
referred to is included in the list.

8.6

Using Java Code on the RHS

The RHS of a JAPE rule can consist of any Java code. This is useful for removing temporary
annotations and for percolating and manipulating features from previous annotations. In
the example below

JAPE: Regular Expressions over Annotations

209

The ﬁrst rule below shows a rule which matches a ﬁrst person name, e.g. ‘Fred’, and adds
a gender feature depending on the value of the minorType from the gazetteer list in which
the name was found. We ﬁrst get the bindings associated with the person label (i.e. the
Lookup annotation). We then create a new annotation called ‘personAnn’ which contains
this annotation, and create a new FeatureMap to enable us to add features. Then we get the
minorType features (and its value) from the personAnn annotation (in this case, the feature
will be ‘gender’ and the value will be ‘male’), and add this value to a new feature called
‘gender’. We create another feature ‘rule’ with value ‘FirstName’. Finally, we add all the
features to a new annotation ‘FirstPerson’ which attaches to the same nodes as the original
‘person’ binding.
Note that inputAS and outputAS represent the input and output annotation set. Normally,
these would be the same (by default when using ANNIE, these will be the ‘Default’ annotation set). Since the user is at liberty to change the input and output annotation sets in the
parameters of the JAPE transducer at runtime, it cannot be guaranteed that the input and
output annotation sets will be the same, and therefore we must specify the annotation set
we are referring to.
Rule: FirstName
(
{Lookup.majorType == person_first}
):person
-->
{
AnnotationSet person = bindings.get("person");
Annotation personAnn = person.iterator().next();
FeatureMap features = Factory.newFeatureMap();
features.put("gender", personAnn.getFeatures().get("minorType"));
features.put("rule", "FirstName");
outputAS.add(person.firstNode(), person.lastNode(), "FirstPerson",
features);
}

The second rule (contained in a subsequent grammar phase) makes use of annotations produced by the ﬁrst rule described above. Instead of percolating the minorType from the
annotation produced by the gazetteer lookup, this time it percolates the feature from the
annotation produced by the previous grammar rule. So here it gets the ‘gender’ feature value
from the ‘FirstPerson’ annotation, and adds it to a new feature (again called ‘gender’ for
convenience), which is added to the new annotation (in outputAS) ‘TempPerson’. At the
end of this rule, the existing input annotations (from inputAS) are removed because they are
no longer needed. Note that in the previous rule, the existing annotations were not removed,
because it is possible they might be needed later on in another grammar phase.
Rule: GazPersonFirst

210

JAPE: Regular Expressions over Annotations

(
{FirstPerson}
)
:person
-->
{
AnnotationSet person = bindings.get("person");
Annotation personAnn = person.iterator().next();
FeatureMap features = Factory.newFeatureMap();
features.put("gender", personAnn.getFeatures().get("gender"));
features.put("rule", "GazPersonFirst");
outputAS.add(person.firstNode(), person.lastNode(), "TempPerson",
features);
inputAS.removeAll(person);
}

You can combine Java blocks and normal assignments (separating each block or assignment
from the next with a comma), so the above RHS could be more simply expressed as
-->
:person.TempPerson = { gender = :person.FirstPerson.gender,
rule = "GazPersonFirst" },
{
inputAS.removeAll(bindings.get("person"));
}

8.6.1

A More Complex Example

The example below is more complicated, because both the title and the ﬁrst name (if present)
may have a gender feature. There is a possibility of conﬂict since some ﬁrst names are
ambiguous, or women are given male names (e.g. Charlie). Some titles are also ambiguous,
such as ‘Dr’, in which case they are not marked with a gender feature. We therefore take the
gender of the title in preference to the gender of the ﬁrst name, if it is present. So, on the
RHS, we ﬁrst look for the gender of the title by getting all Title annotations which have a
gender feature attached. If a gender feature is present, we add the value of this feature to a
new gender feature on the Person annotation we are going to create. If no gender feature is
present, we look for the gender of the ﬁrst name by getting all ﬁrstPerson annotations which
have a gender feature attached, and adding the value of this feature to a new gender feature
on the Person annotation we are going to create. If there is no ﬁrstPerson annotation and
the title has no gender information, then we simply create the Person annotation with no
gender feature.
Rule: PersonTitle

JAPE: Regular Expressions over Annotations

Priority: 35
/* allows Mr. Jones, Mr Fred Jones etc. */
(
(TITLE)
(FIRSTNAME | FIRSTNAMEAMBIG | INITIALS2)*
(PREFIX)?
{Upper}
({Upper})?
(PERSONENDING)?
)
:person -->
{
FeatureMap features = Factory.newFeatureMap();
AnnotationSet personSet = bindings.get("person");
// get all Title annotations that have a gender feature
HashSet fNames = new HashSet();
fNames.add("gender");
AnnotationSet personTitle = personSet.get("Title", fNames);
// if the gender feature exists
if (personTitle != null && personTitle.size()>0)
{
Annotation personAnn = personTitle.iterator().next();
features.put("gender", personAnn.getFeatures().get("gender"));
}
else
{
// get all firstPerson annotations that have a gender feature
AnnotationSet firstPerson = personSet.get("FirstPerson", fNames);
if (firstPerson != null && firstPerson.size()>0)
// create a new gender feature and add the value from firstPerson
{
Annotation personAnn = firstPerson.iterator().next();
features.put("gender", personAnn.getFeatures().get("gender"));
}
}
// create some other features
features.put("kind", "personName");
features.put("rule", "PersonTitle");
// create a Person annotation and add the features we’ve created
outputAS.add(personSet.firstNode(), personSet.lastNode(), "TempPerson",
features);
}

211

212

8.6.2

JAPE: Regular Expressions over Annotations

Adding a Feature to the Document

This is useful when using conditional controllers, where we only want to ﬁre a particular
resource under certain conditions. We ﬁrst test the document to see whether it fulﬁls these
conditions or not, and attach a feature to the document accordingly.
In the example below, we test whether the document contains an annotation of type ‘message’. In emails, there is often an annotation of this type (produced by the document format
analysis when the document is loaded in GATE). Note that annotations produced by document format analysis are placed automatically in the ‘Original markups’ annotation set,
so we must ensure that when running the processing resource containing this grammar that
we specify the Original markups set as the input annotation set. It does not matter what
we specify as the output annotation set, because the annotation we produce is going to
be attached to the document and not to an output annotation set. In the example, if an
annotation of type ‘message’ is found, we add the feature ‘genre’ with value ‘email’ to the
document.

Rule: Email
Priority: 150
(
{message}
)
-->
{
doc.getFeatures().put("genre", "email");
}

8.6.3

Finding the Tokens of a Matched Annotation

In this section we will demonstrate how by using Java on the right-hand side one can ﬁnd all
Token annotations that are covered by a matched annotation, e.g., a Person or an Organization. This is useful if one wants to transfer some information from the matched annotations
to the tokens. For example, to add to the Tokens a feature indicating whether or not they
are covered by a named entity annotation deduced by the rule-based system. This feature
can then be given as a feature to a learning PR, e.g. the HMM. Similarly, one can add a
feature to all tokens saying which rule in the rule based system did the match, the idea being
that some rules might be more reliable than others. Finally, yet another useful feature might
be the length of the coreference chain in which the matched entity is involved, if such exists.
The example below is one of the pre-processing JAPE grammars used by the HMM application. To inspect all JAPE grammars, see the muse/applications/hmm directory in the
distribution.

JAPE: Regular Expressions over Annotations

Phase:

NEInfo

Input: Token Organization Location Person
Options: control = appelt
Rule:

NEInfo

Priority:100
({Organization} | {Person} | {Location}):entity
-->
{
//get the annotation set
AnnotationSet annSet = bindings.get("entity");
//get the only annotation from the set
Annotation entityAnn = annSet.iterator().next();
AnnotationSet tokenAS = inputAS.get("Token",
entityAnn.getStartNode().getOffset(),
entityAnn.getEndNode().getOffset());
List<Annotation> tokens = new ArrayList<Annotation>(tokenAS);
//if no tokens to match, do nothing
if (tokens.isEmpty())
return;
Collections.sort(tokens, new gate.util.OffsetComparator());
Annotation curToken=null;
for (int i=0; i < tokens.size(); i++) {
curToken = tokens.get(i);
String ruleInfo = (String) entityAnn.getFeatures().get("rule1");
String NMRuleInfo = (String) entityAnn.getFeatures().get("NMRule");
if ( ruleInfo != null) {
curToken.getFeatures().put("rule_NE_kind", entityAnn.getType());
curToken.getFeatures().put("NE_rule_id", ruleInfo);
}
else if (NMRuleInfo != null) {
curToken.getFeatures().put("rule_NE_kind", entityAnn.getType());
curToken.getFeatures().put("NE_rule_id", "orthomatcher");
}
else {
curToken.getFeatures().put("rule_NE_kind", "None");
curToken.getFeatures().put("NE_rule_id", "None");
}
List matchesList = (List) entityAnn.getFeatures().get("matches");
if (matchesList != null) {

213

214

JAPE: Regular Expressions over Annotations

if (matchesList.size() == 2)
curToken.getFeatures().put("coref_chain_length", "2");
else if (matchesList.size() > 2 && matchesList.size() < 5)
curToken.getFeatures().put("coref_chain_length", "3-4");
else
curToken.getFeatures().put("coref_chain_length", "5-more");
}
else
curToken.getFeatures().put("coref_chain_length", "0");
}//for
}
Rule:
TokenNEInfo
Priority:10
({Token}):entity
-->
{
//get the annotation set
AnnotationSet annSet = bindings.get("entity");
//get the only annotation from the set
Annotation entityAnn = annSet.iterator().next();
entityAnn.getFeatures().put("rule_NE_kind", "None");
entityAnn.getFeatures().put("NE_rule_id", "None");
entityAnn.getFeatures().put("coref_chain_length", "0");
}

8.6.4

Using Named Blocks

For the common case where a Java block refers just to the annotations from a single lefthand-side binding, JAPE provides a shorthand notation:
Rule: RemoveDoneFlag
(
{Instance.flag == "done"}
):inst
-->
:inst{
Annotation theInstance = instAnnots.iterator().next();
theInstance.getFeatures().remove("flag");
}

JAPE: Regular Expressions over Annotations

215

This rule is equivalent to the following:
Rule: RemoveDoneFlag
(
{Instance.flag == "done"}
):inst
-->
{
AnnotationSet instAnnots = bindings.get("inst");
if(instAnnots != null && instAnnots.size() != 0) {
Annotation theInstance = instAnnots.iterator().next();
theInstance.getFeatures().remove("flag");
}
}

A label :<label> on a Java block creates a local variable <label>Annots within the Java
block which is the AnnotationSet bound to the <label> label. Also, the Java code in the
block is only executed if there is at least one annotation bound to the label, so you do not
need to check this condition in your own code. Of course, if you need more ﬂexibility, e.g.
to perform some action in the case where the label is not bound, you will need to use an
unlabelled block and perform the bindings.get() yourself.

8.6.5

Java RHS Overview

When a JAPE grammar is parsed, a Jape parser creates action classes for all Java RHSs in
the grammar. (one action class per RHS) RHS Java code will be embedded as a body of the
method doit and will work in context of this method. When a particular rule is ﬁred, the
method doit will be executed.
Method doit is speciﬁed by the interface gate.jape.RhsAction. Each action class implements this interface and is generated with the following template:
1
2
3
4
5
6
7
8
9
10
11
12
13

import java . io .*;
import java . util .*;
import gate .*;
import gate . jape .*;
import gate . creole . ontology .*;
import gate . annotation .*;
import gate . util .*;
// Import: block code will be embedded here
class < AutogeneratedActionClassName >
implements java . io . Serializable , gate . jape . RhsAction {
private ActionContext ctx ;
public void setActionContext () { ... }
public ActionContext getActionContext () { ... }

216

JAPE: Regular Expressions over Annotations

public void doit (
gate . Document doc ,
java . util . Map < java . lang . String , gate . AnnotationSet > bindings ,
gate . AnnotationSet annotations ,
gate . AnnotationSet inputAS ,
gate . AnnotationSet outputAS ,
gate . creole . ontology . Ontology ontology ) throws JapeException {
// your RHS Java code will be embedded here ...
}

14
15
16
17
18
19
20
21
22
23

}

Method doit has the following parameters that can be used in RHS Java code:
gate.Document doc - a document that is currently processed
java.util.Map<String, AnnotationSet> bindings - a map of binding variables
where a key is a (String) name of binding variable and value is (AnnotationSet) set of
annotations corresponding to this binding variable5
gate.AnnotationSet annotations - Do not use this (it’s a synonym for outputAS
that is still used in some grammars but is now deprecated).
gate.AnnotationSet inputAS - input annotations
gate.AnnotationSet outputAS - output annotations
gate.creole.ontology.Ontology ontology - a GATE’s transducer ontology
In addition, the ﬁeld ctx provides the ActionContext object to the RHS code (see the
ActionContext JavaDoc for more).
In your Java RHS you can use short names for all Java classes that are imported by the
action class (plus Java classes from the packages that are imported by default according to
JVM speciﬁcation: java.lang.*, java.math.*). But you need to use fully qualiﬁed Java class
names for all other classes. For example:
1
2
3
4
5
6
7
8

-->
{
// VALID line examples
AnnotationSet as = ...
InputStream is = ...
java . util . logging . Logger myLogger =
java . util . logging . Logger . getLogger ( " JAPELogger " );
java . sql . Statement stmt = ...

9

// INVALID line examples
Logger myLogger = Logger . getLogger ( " JapePhaseLogger " );
Statement stmt = ...

10
11
12
13

}

5
Prior to GATE 5.2 this parameter was a plain Map without type parameters, which is why you will see
a lot of now-unnecessary casts in existing JAPE grammars such as those in ANNIE.

JAPE: Regular Expressions over Annotations

217

In order to add additional Java import or import static statements to all Java RHS’ of the
rules in a JAPE grammar ﬁle, you can use the following code at the beginning of the JAPE
ﬁle:
1
2
3
4

Imports : {
import java . util . logging . Logger ;
import java . sql .*;
}

These import statements will be added to the default import statements for each action
class generated for a RHS and the corresponding classes can be used in the RHS Java
code without the need to use fully qualiﬁed names. A useful class to know about is
gate.Utils (see the javadoc documentation for details), which provides static utility methods to simplify some common tasks that are frequently used in RHS Java code. Adding an
import static gate.Utils.*; to the Imports block allows you to use these methods without
any preﬁx, for example:
1

{
AnnotationSet lookups = bindings . get ( " lookup " );
outputAS . add ( start ( lookups ) , end ( lookups ) , " Person " ,
featureMap ( " text " , stringFor ( doc , lookups )));

2
3
4
5

}

You can do the same with your own utility classes — JAPE rules can import any class
available to GATE, including classes deﬁned in a plugin.
A JAPE ﬁle can optionally also contain Java code blocks for handling the events of when
the controller (pipeline) running the JAPE processing resource starts processing, ﬁnishes
processing, or processing is aborted (see the JavaDoc for ControllerAwarePR for more information and warnings about using this feature). These code blocks have to be deﬁned after
any Import: block but before the ﬁrst phase in the ﬁle using the ControllerStarted:,
ControllerFinished: and ControllerAborted: keywords:
1
2
3
4
5
6
7
8
9
10

ControllerStarted : {
// code to run when the controller starts / before any transducing is done
}
Contro ll er Fi ni sh ed : {
// code to run right before the controller ﬁnishes / after all transducing
}
ControllerAborted : {
// code to run when processing is aborted by an exception or by a manual
// interruption
}

The Java code in each of these blocks can access the following predeﬁned ﬁelds:
controller: the Controller object running this JAPE transducer
corpus: the Corpus object on which this JAPE transducer is run, if it is run by a
CorpusController, null otherwise.

218

JAPE: Regular Expressions over Annotations

ontology: the Ontology object if an Ontology LR has been speciﬁed as a runtimeparameter for this JAPE transducer, null otherwise
ctx: the ActionContext object
verb!throwable!: inside the ControllerAborted block, the Throwable which signalled
the aborting exception

8.7

Optimising for Speed

The way in which grammars are designed can have a huge impact on the processing speed.
Some simple tricks to keep the processing as fast as possible are:
avoid the use of the * and + operators. Replace them with range queries where possible.
For example, instead of
({Token})*

use
({Token})[0,3]

if you can predict that you won’t need to recognise a string of Tokens longer than 3.
avoid specifying unnecessary elements such as SpaceTokens where you can. To do this,
use the Input speciﬁcation at the beginning of the grammar to stipulate the annotations
that need to be considered. If no Input speciﬁcation is used, all annotations will be
considered (so, for example, you cannot match two tokens separated by a space unless
you specify the SpaceToken in the pattern). If, however, you specify Tokens but not
SpaceTokens in the Input, SpaceTokens do not have to be mentioned in the pattern
to be recognised. If, for example, there is only one rule in a phase that requires
SpaceTokens to be speciﬁed, it may be judicious to move that rule to a separate phase
where the SpaceToken can be speciﬁed as Input.
avoid the shorthand syntax for copying feature values (newFeat = :bind.Type.oldFeat),
particularly if you need to copy multiple features from the left to the right hand side
of your rule.

8.8

Ontology Aware Grammar Transduction

GATE supports two diﬀerent methods for ontology aware grammar transduction. Firstly it
is possible to use the ontology feature both in grammars and annotations, while using the

JAPE: Regular Expressions over Annotations

219

default transducer. Secondly it is possible to use an ontology aware transducer by passing
an ontology language resource to one of the subsumes methods in SimpleFeatureMapImpl.
This second strategy does not check for ontology features, which will make the writing
of grammars easier, as there is no need to specify ontology when writing them. More
information about the ontology-aware transducer can be found in Section 14.10.

8.9

Serializing JAPE Transducer

JAPE grammars are written as ﬁles with the extension ‘.jape’, which are parsed and compiled at run-time to execute them over the GATE document(s). Serialization of the JAPE
Transducer adds the capability to serialize such grammar ﬁles and use them later to bootstrap new JAPE transducers, where they do not need the original JAPE grammar ﬁle. This
allows people to distribute the serialized version of their grammars without disclosing the
actual contents of their jape ﬁles. This is implemented as part of the JAPE Transducer PR.
The following sections describe how to serialize and deserialize them.

8.9.1

How to Serialize?

Once an instance of a JAPE transducer is created, the option to serialize it appears in the
context menu of that instance. The context menu can be activated by right clicking on the
respective PR. Having done so, it asks for the ﬁle name where the serialized version of the
respective JAPE grammar is stored.

8.9.2

How to Use the Serialized Grammar File?

The JAPE Transducer now also has an init-time parameter binaryGrammarURL, which
appears as an optional parameter to the grammarURL. The User can use this parameter
(i.e. binaryGrammarURL) to specify the serialized grammar ﬁle.

8.10

Notes for Montreal Transducer Users

In June 2008, the standard JAPE transducer implementation gained a number of features
inspired by Luc Plamondon’s ‘Montreal Transducer’, which was available as a GATE plugin
for several years, and was made obsolete in Version 5.1. If you have existing Montreal Transducer grammars and want to update them to work with the standard JAPE implementation
you should be aware of the following diﬀerences in behaviour:

220

JAPE: Regular Expressions over Annotations

Quantiﬁers (*, + and ?) in the Montreal transducer are always greedy, but this is not
necessarily the case in standard JAPE.
The Montreal Transducer deﬁnes {Type.feature != value} to be the same as
{!Type.feature == value} (and likewise the !~ operator in terms of =~). In standard JAPE these constructs have diﬀerent semantics. {Type.feature != value}
will only match if there is a Type annotation whose feature feature does not
have the given value, and if it matches it will bind the single Type annotation.
{!Type.feature == value} will match if there is no Type annotation at a given place
with this feature (including when there is no Type annotation at all), and if it matches
it will bind every other annotation that starts at that location. If you have used !=
in your Montreal grammars and want them to continue to behave the same way you
must change them to use the preﬁx-! form instead (see Section 8.1.10).
The =~ operator in standard JAPE looks for regular expression matches anywhere
within a feature value, whereas in the Montreal transducer it requires the whole string
to match. To obtain the whole-string matching behaviour in standard JAPE, use the
==~ operator instead (see Section 8.2.2).

Chapter 9
ANNIC: ANNotations-In-Context
ANNIC (ANNotations-In-Context) is a full-featured annotation indexing and retrieval system. It is provided as part of an extension of the Serial Data-stores, called Searchable Serial
Data-store (SSD).
ANNIC can index documents in any format supported by the GATE system (i.e., XML,
HTML, RTF, e-mail, text, etc). Compared with other such query systems, it has additional
features addressing issues such as extensive indexing of linguistic information associated with
document content, independent of document format. It also allows indexing and extraction of
information from overlapping annotations and features. Its advanced graphical user interface
provides a graphical view of annotation markups over the text, along with an ability to build
new queries interactively. In addition, ANNIC can be used as a ﬁrst step in rule development
for NLP systems as it enables the discovery and testing of patterns in corpora.
ANNIC is built on top of the Apache Lucene1 – a high performance full-featured search engine
implemented in Java, which supports indexing and search of large document collections. Our
choice of IR engine is due to the customisability of Lucene. For more details on how Lucene
was modiﬁed to meet the requirements of indexing and querying annotations, please refer to
[Aswani et al. 05].
As explained earlier, SSD is an extension of the serial data-store. In addition to the persist
location, SSD asks user to provide some more information (explained later) that it uses to
index the documents. Once the SSD has been initiated, user can add/remove documents/corpora to the SSD in a similar way it is done with other data-stores. When documents are
added to the SSD, it automatically tries to index them. It updates the index whenever there
is a change in any of the documents stored in the SSD and removes the document from the
index if it is deleted from the SSD. Be warned that only the annotation sets, types and features initially provided during the SSD creation time, will be updated when adding/removing
documents to the datastore.
1

http://lucene.apache.org

221

222

ANNIC: ANNotations-In-Context

SSD has an advanced graphical interface that allows users to issue queries over the SSD.
Below we explain the parameters required by SSD and how to instantiate it, how to use its
graphical interface and how to use SSD programmatically.

9.1

Instantiating SSD

Steps:
1. In GATE Developer, right click on ‘Datastores’ and select ‘Create Datastore’.
2. From a drop-down list select ‘Lucene Based Searchable DataStore’.
3. Here, you will see a ﬁle dialog. Please select an empty folder for your datastore. This
is similar to the procedure of creating a serial datastore.
4. After this, you will see an input window. Please provide these parameters:
(a) DataStore URL: This is the URL of the datastore folder selected in the previous
step.
(b) Index Location: By default, the location of index is calculated from the datastore
location. It is done by appending ‘-index’ to the datastore location. If user wants
to change this location, it is possible to do so by clicking on the folder icon and
selecting another empty folder. If the selected folder exists already, the system
will check if it is an empty folder. If the selected folder does not exist, the system
tries to create it.
(c) Annotation Sets: Here, you can provide one or more annotation sets that you
wish to index or exclude from being indexed. By default, the default annotation
set and the ‘Key’ annotation set are included. User can change this selection by
clicking on the edit list icon and removing or adding appropriate annotation set
names. In order to be able to readd the default annotation set, you must click on
the edit list icon and add an empty ﬁeld to the list. If there are no annotation
sets provided, all the annotation sets in all documents are indexed.
(d) Base-Token Type: (e.g. Token or Key.Token) These are the basic tokens of any
document. Your documents must have the annotations of Base-Token-Type in
order to get indexed. These basic tokens are used for displaying contextual information while searching patterns in the corpus. In case of indexing more than
one annotation set, user can specify the annotation set from which the tokens
should be taken (e.g. Key.Token- annotations of type Token from the annotation
set called Key). In case user does not provide any annotation set name (e.g.
Token), the system searches in all the annotation sets to be indexed and the basetokens from the ﬁrst annotation set with the base token annotations are taken.
Please note that the documents with no base-tokens are not indexed. However, if

ANNIC: ANNotations-In-Context

223

the ‘create tokens automatically’ option is selected, the SSD creates base-tokens
automatically. Here, each string delimited with white space is considered as a
token.
(e) Index Unit Type: (e.g. Sentence, Key.Sentence) This speciﬁes the unit of Index.
In other words, annotations lying within the boundaries of these annotations are
indexed (e.g. in the case of ‘Sentences’, no annotations that are spanned across
the boundaries of two sentences are considered for indexing). User can specify
from which annotation set the index unit annotations should be considered. If
user does not provide any annotation set, the SSD searches among all annotation
sets for index units. If this ﬁeld is left empty or SSD fails to locate index units,
the entire document is considered as a single unit.
(f) Features: Finally, users can specify the annotation types and features that should
be indexed or excluded from being indexed. (e.g. SpaceToken and Split). If user
wants to exclude only a speciﬁc feature of a speciﬁc annotation type, he/she can
specify it using a ’.’ separator between the annotation type and its feature (e.g.
Person.matches).
5. Click OK. If all parameters are OK, a new empty DS will be created.
6. Create an empty corpus and save it to the SSD.
7. Populate it with some documents. Each document added to the corpus and eventually
to the SSD is indexed automatically. If the document does not have the required
annotations, that document is skipped and not indexed.
SSDs are portable and can be moved across diﬀerent systems. However, the relative positions
of both the datastore folder and the respective index folder must be maintained. If it is not
possible to maintain the relative positions, the new location of the index must be speciﬁed
inside the ‘ GATE SerialDataStore ’ ﬁle inside the datastore folder.

9.2
9.2.1

Search GUI
Overview

Figure 9.1 shows the search GUI for a datastore. The top section contains a text area to
write a query, lists to select the corpus and annotation set to search in, sliders to set the size
of the results and context and icons to execute and clear the query.
The central section shows a graphical visualisation of stacked annotations and feature values
for the result row selected in the bottom results table. You can also see the stack view
conﬁguration window where you deﬁne which annotation type and feature to display in the
central section.

224

ANNIC: ANNotations-In-Context

Figure 9.1: Searchable Serial Datastore Viewer.

The bottom section contains the results table of the query, i.e. the text that matches the
query with their left and right contexts. The bottom section contains also a tabbed pane of
statistics.

9.2.2

Syntax of Queries

SSD enables you to formulate versatile queries using a subset of JAPE patterns. Below, we
give the JAPE pattern clauses which can be used as SSD queries. Queries can also be a
combination of one or more of the following pattern clauses.

1. String
2. {AnnotationType}
3. {AnnotationType == String}
4. {AnnotationType.feature == feature value}
5. {AnnotationType1, AnnotationType2.feature == featureValue}
6. {AnnotationType1.feature == featureValue, AnnotationType2.feature == featureValue}

ANNIC: ANNotations-In-Context

225

Figure 9.2: Searchable Serial Datastore Viewer - Auto-completion.

JAPE patterns also support the | (OR) operator. For instance, {A} ({B} | {C}) is a pattern
of two annotations where the ﬁrst is an annotation of type A followed by the annotation of
type either B or C.
ANNIC supports two operators, + and *, to specify the number of times a particular annotation or a sub pattern should appear in the main query pattern. Here, ({A})+n means one
and up to n occurrences of annotation {A} and ({A})*n means zero or up to n occurrences
of annotation {A}.
Below we explain the steps to search in SSD.
1. Double click on SSD. You will see an extra tab “Lucene DataStore Searcher”. Click
on it to activate the searcher GUI.
2. Here you can specify a query to search in your SSD. The query here is a L.H.S. part
of the JAPE grammar. Here are some examples:
(a) {Person} – This will return annotations of type Person from the SSD

(b) {Token.string == “Microsoft”} – This will return all occurrences of “Microsoft”
from the SSD.
(c) {Person}({Token})*2{Organization} – Person followed by zero or up to two tokens followed by Organization.
(d) {Token.orth==“upperInitial”, Organization} – Token with feature orth with
value set to “upperInitial” and which is also annotated as Organization.

9.2.3

Top Section

A text-area located in the top left part of the GUI is used to input a query. You can
copy/cut/paste with Control+C/X/V, undo/redo your changes with Control+Z/Y as usual.
To add a new line, use Control+Enter key combination.

226

ANNIC: ANNotations-In-Context

Auto-completion as shown in ﬁgure 9.2 for annotation type is triggered when typing ’{’ or ’,’
and for feature when typing ’.’ after a valid annotation type. It shows only the annotation
types and features related to the selected corpus and annotation set.
If you right-click on an expression it will automatically select the shortest valid enclosing
brace and if you click on a selection it will propose you to add quantiﬁers for allowing the
expression to appear zero, one or more times.
To execute the query, click on the magnifying glass icon, use Enter key or Alt+Enter key
combination. To clear the query, click on the red X icon or use Alt+Backspace key combination.
It is possible to have more than one corpus, each containing a diﬀerent set of documents,
stored in a single data-store. ANNIC, by providing a drop down box with a list of stored
corpora, also allows searching within a speciﬁc corpus. Similarly a document can have more
than one annotation set indexed and therefore ANNIC also provides a drop down box with
a list of indexed annotation sets for the selected corpus.
A large corpus can have many hits for a given query. This may take a long time to refresh
the GUI and may create inconvenience while browsing through results. Therefore you can
specify the number of results to retrieve. Use the Next Page of Results button to iterate
through results. Due to technical complexities, it is not possible to visit a previous page. To
retrieve all the results at the same time, push the results slider to the right end.

9.2.4

Central Section

Annotation types and features to show can be conﬁgured from the stack view conﬁguration
window by clicking on the Conﬁgure button in the bottom left part of the central section.
You can also change the feature value to be displayed by double clicking on the annotation
type name.
The central section shows coloured rectangles exactly below the spans of text where these
annotations occur. If only an annotation type is displayed, the rectangle remains empty.
When you hover the mouse over the rectangle, it shows all their features and values in a
popup window. If an annotation type and a feature are displayed, the value of that feature
is shown in the rectangle.
Shortcuts are expression that stand for an ”AnnotationType.Feature” expression. For example, on the ﬁgure 9.1, the shortcut ”POS” stands for the expression ”Token.category”.
When you double click on an annotation rectangle, the respective query expression is placed
at the caret position in the query text area. If you have selected anything in the query text
area, it gets replaced. You can also double click on a word on the ﬁrst line to add it to the
query.

ANNIC: ANNotations-In-Context

9.2.5

227

Bottom Section

The table of results contains the text matched by the query, the contexts, the features
displayed in the central view but only for the matching part, the eﬀective query, the document
and annotation set names. You can sort a table column by clicking on its header.
You can remove a result from the results table or open the document containing it by rightclicking on a result in the results table.
ANNIC provides an Export button to export results into an HTML ﬁle. You can also select
then copy/paste the table in your word processor or spreadsheet.
A statistics tabbed pane is displayed on the bottom-right. There is always a global statistics
pane that list the count of the occurrences of all annotation types for the selected corpus
and annotation set.
Statistics can be obtained for matched spans of the query in the results, with or without
contexts, just by annotation type, an annotation type + feature or an annotation type +
feature + value. A second pane contains the one item statistics that you can add by rightclicking on a non empty annotation rectangle or on the header of a row in the central section.
You can sort a table column by clicking on its header.

9.3

Using SSD from GATE Embedded

9.3.1

How to instantiate a searchabledatastore

1
2
3
4
5

// create an instance of datastore
Lucene Da t a St o r eI m p l ds = ( L u ce n e Da t a St o r eI m p l )
Factory . createDataStore ( ‘ ‘ gate . persist . L uc e n e Da t a St o r eI m p l ’ ’ ,
dsLocation );

6
7
8

// we need to set Indexer
Indexer indexer = new LuceneIndexer ( new URL ( indexLocation ));

9
10
11

// set the parameters
Map parameters = new HashMap ();

12
13
14

// specify the index url
parameters . put ( Constants . INDEX_LOCATION_URL , new URL ( indexLocation ));

15
16
17
18
19
20
21

// specify the base token type
// and specify that the tokens should be created automatically
// if not found in the document
parameters . put ( Constants . BASE_TOKEN_ANNOTATION_TYPE , ‘‘ Token ’ ’ );
parameters . put ( Constants . CREATE_TOKENS_AUTOMATICALLY ,
new Boolean ( true ));

228

ANNIC: ANNotations-In-Context

22
23
24

// specify the index unit type
parameters . put ( Constants . INDEX_UNIT_ANNOTATION_TYPE , ‘‘ Sentence ’ ’ );

25
26
27
28
29
30
31
32
33
34

// specifying the annotation sets ”Key” and ”Default Annotation Set”
// to be indexed
List < String > setsToInclude = new ArrayList < String >();
setsToInclude . add ( " Key " );
setsToInclude . add ( " < null > " );
parameters . put ( Constants . ANNOTATION_SETS_NAMES_TO_INCLUDE ,
setsToInclude );
parameters . put ( Constants . ANNOTATION_SETS_NAMES_TO_EXCLUDE ,
new ArrayList < String >());

35
36
37
38

// all features should be indexed
parameters . put ( Constants . FEATURES_TO_INCLUDE , new ArrayList < String >());
parameters . put ( Constants . FEATURES_TO_EXCLUDE , new ArrayList < String >());

39
40
41

// set the indexer
ds . setIndexer ( indexer , parameters );

42
43
44

// set the searcher
ds . setSearcher ( new LuceneSearcher ());

9.3.2

How to search in this datastore

1
2
3
4

// obtain the searcher instance
Searcher searcher = ds . getSearcher ();
Map parameters = new HashMap ();

5
6
7
8
9
10
11

// obtain the url of index
String indexLocation =
new File ((( URL ) ds . getIndexer (). getParameters ()
. get ( Constants . IN DE X_ LO CA TI ON _U RL )). getFile ()). getAbsolutePath ();
ArrayList indexLocations = new ArrayList ();
indexLocations . add ( indexLocation );

12
13
14

// corpus2SearchIn = mention corpus name that was indexed here.

15
16
17

// the annotation set to search in
String a n n o t a t i o n S e t 2 S e a r c h I n = " Key " ;

18
19
20
21
22
23
24

// set the parameter
parameters . put ( Constants . INDEX_LOCATIONS , indexLocations );
parameters . put ( Constants . CORPUS_ID , corpus2SearchIn );
parameters . put ( Constants . ANNOTATION_SET_ID , annotationSet );
parameters . put ( Constants . CONTEXT_WINDOW , contextWindow );
parameters . put ( Constants . NO_OF_PATTERNS , noOfPatterns );

25
26
27

// search
String query = " { Person } " ;

ANNIC: ANNotations-In-Context

28

Hit [] hits = searcher . search ( query , parameters );

229

230

ANNIC: ANNotations-In-Context

Chapter 10
Performance Evaluation of Language
Analysers
When you can measure what you are speaking about, and express it in numbers,
you know something about it; but when you cannot measure it, when you cannot
express it in numbers, your knowledge is of a meager and unsatisfactory kind:
it may be the beginning of knowledge, but you have scarcely in your thoughts
advanced to the stage of science. (Kelvin)
Not everything that counts can be counted, and not everything that can be
counted counts. (Einstein)

GATE provides a variety of tools for automatic evaluation. The Annotation Diﬀ tool compares two annotation sets within a document. Corpus QA extends Annotation Diﬀ to an
entire corpus. The Corpus Benchmark tool also provides functionality for comparing annotation sets over an entire corpus. Additionally, two plugins cover similar functionality; one
implements inter-annotator agreement, and the other, the balanced distance metric.
These tools are particularly useful not just as a ﬁnal measure of performance, but as a tool to
aid system development by tracking progress and evaluating the impact of changes as they
are made. Applications include evaluating the success of a machine learning or language
engineering application by comparing its results to a gold standard and also comparing
annotations prepared by two human annotators to each other to ensure that the annotations
are reliable.
This chapter begins by introducing the concepts and metrics relevant, before describing each
of the tools in turn.
231

232

10.1

Performance Evaluation of Language Analysers

Metrics for Evaluation in Information Extraction

When we evaluate the performance of a processing resource such as tokeniser, POS tagger,
or a whole application, we usually have a human-authored ‘gold standard’ against which to
compare our software. However, it is not always easy or obvious what this gold standard
should be, as diﬀerent people may have diﬀerent opinions about what is correct. Typically,
we solve this problem by using more than one human annotator, and comparing their annotations. We do this by calculating inter-annotator agreement (IAA), also known as inter-rater
reliability.
IAA can be used to assess how diﬃcult a task is. This is based on the argument that if
two humans cannot come to agreement on some annotation, it is unlikely that a computer
could ever do the same annotation ‘correctly’. Thus, IAA can be used to ﬁnd the ceiling for
computer performance.
There are many possible metrics for reporting IAA, such as Cohen’s Kappa, prevalence,
and bias [Eugenio & Glass 04]. Kappa is the best metric for IAA when all the annotators
have identical exhaustive sets of questions on which they might agree or disagree. In other
words, it is a classiﬁcation task. This could be a task like ‘are these names male or female
names’. However, sometimes there is disagreement about the set of questions, e.g. when the
annotators themselves determine which text spans they ought to annotate, such as in named
entity extraction. That could be a task like ‘read over this text and mark up all references
to politics’. When annotators determine their own sets of questions, it is appropriate to
use precision, recall, and F-measure to report IAA. Precision, recall and F-measure are
also appropriate choices when assessing performance of an automated application against a
trusted gold standard.
In this section, we will ﬁrst introduce some relevant terms, before outlining Cohen’s Kappa
and similar measures, in Section 10.1.2. We will then introduce precision, recall and Fmeasure in Section 10.1.3.

10.1.1

Annotation Relations

Before introducing the metrics we will use in this chapter, we will ﬁrst outline the ways in
which annotations can relate to each other. These ways of comparing annotations to each
other are used to determine the counts that then go into calculating the metrics of interest.
Consider a document with two annotation sets upon it. These annotation sets might for
example be prepared by two human annotators, or alternatively, one set might be produced
by an automated system and the other might be a trusted gold standard. We wish to assess
the extent to which they agree. We begin by counting incidences of the following relations:
Coextensive Two annotations are coextensive if they hit the same span of text in a document. Basically, both their start and end oﬀsets are equal.

Performance Evaluation of Language Analysers

233

Overlaps Two annotations overlap if they share a common span of text.
Compatible Two annotations are compatible if they are coextensive and if the features of
one (usually the ones from the key) are included in the features of the other (usually
the response).
Partially Compatible Two annotations are partially compatible if they overlap and if the
features of one (usually the ones from the key) are included in the features of the other
(response).
Missing This applies only to the key annotations. A key annotation is missing if either
it is not coextensive or overlapping, orif one or more features are not included in the
response annotation.
Spurious This applies only to the response annotations. A response annotation is spurious
if either it is not coextensive or overlapping, or if one or more features from the key
are not included in the response annotation.

10.1.2

Cohen’s Kappa

The three commonly used IAA measures are observed agreement, speciﬁc agreement, and
Kappa (κ) [Hripcsak & Heitjan 02]. Those measures can be calculated from a contingency
table, which lists the numbers of instances of agreement and disagreement between two
annotators on each category. To explain the IAA measures, a general contingency table for
two categories cat1 and cat2 is shown in Table 10.1.
Annotator-2
Annotator-1
cat1
cat1
a
cat2
c
marginal sum
a+c

cat2
b
d
b+d

marginal sum
a+b
c+d
a+b+c+d

Table 10.1: Contingency table for two-category problem
Observed agreement is the portion of the instances on which the annotators agree. For
the two annotators and two categories as shown in Table 10.1, it is deﬁned as
Ao =

a+d
a+b+c+d

(10.1)

The extension of the above formula to more than two categories is straightforward. The
extension to more than two annotators is usually taken as the mean of the pair-wise agreements [Fleiss 75], which is the average agreement across all possible pairs of annotators. An
alternative compares each annotator with the majority opinion of the others [Fleiss 75].

234

Performance Evaluation of Language Analysers

However, the observed agreement has two shortcomings. One is that a certain amount of
agreement is expected by chance. The Kappa measure is a chance-corrected agreement.
Another is that it sums up the agreement on all the categories, but the agreements on each
category may diﬀer. Hence the category speciﬁc agreement is needed.
Speciﬁc agreement quantiﬁes the degree of agreement for each of the categories separately.
For example, the speciﬁc agreement for the two categories list in Table 10.1 is the following,
respectively,
Acat1 =

2a
;
2a + b + c

Acat2 =

2d
b + c + 2d

(10.2)

Kappa is deﬁned as the observed agreements Ao minus the agreement expected by chance
Ae and is normalized as a number between -1 and 1.
κ=

Ao − Ae
1 − Ae

(10.3)

κ = 1 means perfect agreements, κ = 0 means the agreement is equal to chance, κ = −1
means ‘perfect’ disagreement.
There are two diﬀerent ways of computing the chance agreement Ae (for a detailed explanations about it see [Eugenio & Glass 04]; however, a quick outline will be given below). The
Cohen’s Kappa is based on the individual distribution of each annotator, while the Siegel
& Castellan’s Kappa is based on the assumption that all the annotators have the same
distribution. The former is more informative than the latter and has been used widely.
Let us consider an example:
Annotator-2
Annotator-1
cat1
cat1
1
cat2
3
marginal sum
4

cat2
2
4
6

marginal sum
3
7
10

Table 10.2: Example contingency table for two-category problem
Cohen’s Kappa requires that the expected agreement be calculated as follows. Divide
marginal sums by the total to get the portion of the instances that each annotator allocates
to each category. Multiply annotator’s proportions together to get the likelihood of chance
agreement, then total these ﬁgures. Table 10.3 gives a worked example.
The formula can easily be extended to more than two categories.
Siegel & Castellan’s Kappa is applicable for any number of annotators. Siegel & Castellan’s Kappa for two annotators is also known as Scott’s Pi (see [Lombard et al. 02]). It

Performance Evaluation of Language Analysers

cat1
cat2
Total

Annotator-1
3 / 10 = 0.3
7 / 10 = 0.7

Annotator 2
4 / 10 = 0.4
6 / 10 = 0.6

235
Multiplied
0.12
0.42
0.54

Table 10.3: Calculating Expected Agreement for Cohen’s Kappa

diﬀers from Cohen’s Kappa only in how the expected agreement is calculated. Table 10.4
shows a worked example. Annotator totals are added together and divided by the number
of decisions to form joint proportions. These are then squared and totalled.
cat1
cat2
Total

Ann-1
3
7

Ann-2
4
6

Sum
7
13

Joint Prop
7/20
13/20

JP-Squared
49/400=0.1225
169/400=0.4225
218/400 = 0.545

Table 10.4: Calculating Expected Agreement for Siegel & Castellan’s Kappa (Scott’s Pi)
The Kappa suﬀers from the prevalence problem which arises because imbalanced distribution of categories in the data increases Ae . The prevalence problem can be alleviated by
reporting the positive and negative speciﬁed agreement on each category besides the Kappa
[Hripcsak & Heitjan 02, Eugenio & Glass 04]. In addition, the so-called bias problem aﬀects
the Cohen’s Kappa, but not S&C’s. The bias problem arises as one annotator prefers one
particular category more than another annotator. [Eugenio & Glass 04] advised to compute
the S&C’s Kappa and the speciﬁc agreements along with the Cohen’s Kappa in order to
handle these problems.
Despite the problem mentioned above, the Cohen’s Kappa remains a popular IAA measure.
Kappa can be used for more than two annotators based on pair-wise ﬁgures, e.g. the mean
of all the pair-wise Kappa as an overall Kappa measure. The Cohen’s Kappa can also be
extended to the case of more than two annotators by using the following single formula
[Davies & Fleiss 82]
￿ ￿

2
IJ 2 − i c Yic
κ=1−
￿
￿ ￿
I(J(J − 1) c (pc (1 − pc )) + c j (pcj − pc )2 )

(10.4)

Where I and J are the number of instances and annotators, respectively; Yic is the number
of annotators who assigns the category c to the instance I; pcj is the probability of the
annotator j assigning category c; pc is the probability of assigning category by all annotators
(i.e. averaging pcj over all annotators).
The Krippendorﬀ’s alpha, another variant of Kappa, diﬀers only slightly from the S&C’s
Kappa on nominal category problem (see [Carletta 96, Eugenio & Glass 04]).
However, note that the Kappa (and the observed agreement) is not applicable to some
tasks. Named entity annotation is one such task [Hripcsak & Rothschild 05]. In the named

236

Performance Evaluation of Language Analysers

entity annotation task, annotators are given some text and are asked to annotate some
named entities (and possibly their categories) in the text. Diﬀerent annotators may annotate
diﬀerent instances of the named entity. So, if one annotator annotates one named entity in
the text but another annotator does not annotate it, then that named entity is a non-entity
for the latter. However, generally the non-entity in the text is not a well-deﬁned term, e.g.
we don’t know how many words should be contained in the non-entity. On the other hand,
if we want to compute Kappa for named entity annotation, we need the non-entities. This
is why people don’t compute Kappa for the named entity task.

10.1.3

Precision, Recall, F-Measure

Much of the research in IE in the last decade has been connected with the MUC competitions, and so it is unsurprising that the MUC evaluation metrics of precision, recall
and F-measure [Chinchor 92] also tend to be used, along with slight variations. These
metrics have a very long-standing tradition in the ﬁeld of IR [van Rijsbergen 79] (see also
[Manning & Sch¨tze 99, Frakes & Baeza-Yates 92]).
u
Precision measures the number of correctly identiﬁed items as a percentage of the number
of items identiﬁed. In other words, it measures how many of the items that the system
identiﬁed were actually correct, regardless of whether it also failed to retrieve correct items.
The higher the precision, the better the system is at ensuring that what is identiﬁed is
correct.
Error rate is the inverse of precision, and measures the number of incorrectly identiﬁed
items as a percentage of the items identiﬁed. It is sometimes used as an alternative to
precision.
Recall measures the number of correctly identiﬁed items as a percentage of the total number
of correct items. In other words, it measures how many of the items that should have been
identiﬁed actually were identiﬁed, regardless of how many spurious identiﬁcations were made.
The higher the recall rate, the better the system is at not missing correct items.
Clearly, there must be a tradeoﬀ between precision and recall, for a system can easily be
made to achieve 100% precision by identifying nothing (and so making no mistakes in what
it identiﬁes), or 100% recall by identifying everything (and so not missing anything). The
F-measure [van Rijsbergen 79] is often used in conjunction with Precision and Recall, as a
weighted average of the two. False positives are a useful metric when dealing with a wide
variety of text types, because it is not dependent on relative document richness in the same
way that precision is. By this we mean the relative number of entities of each type to be
found in a set of documents.
When comparing diﬀerent systems on the same document set, relative document richness
is unimportant, because it is equal for all systems. When comparing a single system’s
performance on diﬀerent documents, however, it is much more crucial, because if a particular

Performance Evaluation of Language Analysers

237

document type has a signiﬁcantly diﬀerent number of any type of entity, the results for that
entity type can become skewed. Compare the impact on precision of one error where the
total number of correct entities = 1, and one error where the total = 100. Assuming the
document length is the same, then the false positive score for each text, on the other hand,
should be identical.
Common metrics for evaluation of IE systems are deﬁned as follows:

P recision =

Recall =

Correct + 1/2P artial
Correct + Spurious + P artial

Correct + 1/2P artial
Correct + M issing + P artial

F − measure =

(β 2 + 1)P ∗ R
(β 2 P ) + R

(10.5)

(10.6)

(10.7)

where β reﬂects the weighting of P vs. R. If β is set to 1, the two are weighted equally. With
β set to 0.5, precision weights twice as much as recall. And with β set to 2, recall weights
twice as much as precision.

F alseP ositive =

Spurious
c

(10.8)

where c is some constant independent from document richness, e.g. the number of tokens or
sentences in the document.
Note that we consider annotations to be partially correct if the entity type is correct and the
spans are overlapping but not identical. Partially correct responses are normally allocated a
half weight.

10.1.4

Macro and Micro Averaging

Where precision, recall and f-measure are calculated over a corpus, there are options in terms
of how document statistics are combined.
Micro averaging essentially treats the corpus as one large document. Correct, spurious
and missing counts span the entire corpus, and precision, recall and f-measure are
calculated accordingly.

238

Performance Evaluation of Language Analysers

Macro averaging calculates precision, recall and f-measure on a per document basis,
and then averages the results.
The method of choice depends on the priorities of the case in question. Macro averaging
tends to increase the importance of shorter documents.
It is also possible to calculate a macro average across annotation types; that is to say,
precision, recall and f-measure are calculated separately for each annotation type and the
results then averaged.

10.2

The Annotation Diﬀ Tool

The Annotation Diﬀ tool enables two sets of annotations in one or two documents to be compared, in order either to compare a system-annotated text with a reference (hand-annotated)
text, or to compare the output of two diﬀerent versions of the system (or two diﬀerent systems). For each annotation type, ﬁgures are generated for precision, recall, F-measure. Each
of these can be calculated according to 3 diﬀerent criteria - strict, lenient and average. The
reason for this is to deal with partially correct responses in diﬀerent ways.
The Strict measure considers all partially correct responses as incorrect (spurious).
The Lenient measure considers all partially correct responses as correct.
The Average measure allocates a half weight to partially correct responses (i.e. it takes
the average of strict and lenient).
It can be accessed both from GATE Developer and from GATE Embedded. Annotation Diﬀ
compares sets of annotations with the same type. When performing the comparison, the
annotation oﬀsets and their features will be taken into consideration. and after that, the
comparison process is triggered.
All annotations from the key set are compared with the ones from the response set, and those
found to have the same start and end oﬀsets are displayed on the same line in the table.
Then, the Annotation Diﬀ evaluates if the features of each annotation from the response set
subsume those features from the key set, as speciﬁed by the features names you provide.
To use the annotation diﬀ tool, see Section 10.2.1. To create a gold standard, see section
10.2.2. To compare more than two annotation sets, see Section 3.4.3.

10.2.1

Performing Evaluation with the Annotation Diﬀ Tool

The Annotation Diﬀ tool is activated by selecting it from the Tools menu at the top of the
GATE Developer window. It will appear in a new window. Select the key and response

Performance Evaluation of Language Analysers

239

Figure 10.1: Annotation diﬀ window with the parameters at the top, the comparison table
in the center and the statistics panel at the bottom.

documents to be used (note that both must have been previously loaded into the system),
the annotation sets to be used for each, and the annotation type to be compared.
Note that the tool automatically intersects all the annotation types from the selected key
annotation set with all types from the response set.
On a separate note, you can perform a diﬀ on the same document, between two diﬀerent
annotation sets. One annotation set could contain the key type and another could contain
the response one.
After the type has been selected, the user is required to decide how the features will be
compared. It is important to know that the tool compares them by analysing if features
from the key set are contained in the response set. It checks for both the feature name and
feature value to be the same.
There are three basic options to select:
To take ‘all’ the features from the key set into consideration
To take only ‘some’ user selected features
To take ‘none’ of the features from the key set.

240

Performance Evaluation of Language Analysers

The weight for the F-Measure can also be changed - by default it is set to 1.0 (i.e. to
give precision and recall equal weight). Finally, click on ‘Compare’ to display the results.
Note that the window may need to be resized manually, by dragging the window edges as
appropriate).
In the main window, the key and response annotations will be displayed. They can be sorted
by any category by clicking on the central column header: ‘=?’. The key and response
annotations will be aligned if their indices are identical, and are color coded according to
the legend displayed at the bottom.
Precision, recall, F-measure are also displayed below the annotation tables, each according
to 3 criteria - strict, lenient and average. See Sections 10.2 and 10.1 for more details about
the evaluation metrics.
The results can be saves to an HTML ﬁle by using the ‘Export to HTML’ button. This
creates an HTML snapshot of what the Annotation Diﬀ table shows at that moment. The
columns and rows in the table will be shown in the same order, and the hidden columns will
not appear in the HTML ﬁle. The colours will also be the same.
If you need more details or context you can use the button ‘Show document’ to display the
document and the annotations selected in the annotation diﬀ drop down lists and table.

10.2.2

Creating a Gold Standard with the Annotation Diﬀ Tool

In order to create a gold standard set from two sets you need to show the ‘Adjudication’
panel at the bottom. It will insert two checkboxes columns in the central table. Tick boxes
in the columns ‘K(ey)’ and ‘R(esponse)’ then input a Target set in the text ﬁeld and use the
‘Copy selection to target’ button to copy all annotations selected to the target annotation
set.
There is a context menu for the checkboxes to tick them quickly.
Each time you will copy the selection to the target set to create the gold standard set, the
rows will be hidden in further comparisons. In this way, you will see only the annotations
that haven’t been processed. At the end of the gold standard creation you should have an
empty table.
To see again the copied rows, select the ‘Statistics’ tab at the bottom and use the button
‘Compare’.

Performance Evaluation of Language Analysers

241

Figure 10.2: Annotation diﬀ window with the parameters at the top, the comparison table
in the center and the adjudication panel at the bottom.

Figure 10.3: Corpus Quality Assurance showing the document statistics table

242

Performance Evaluation of Language Analysers

10.3

Corpus Quality Assurance

10.3.1

Description of the interface

A bottom tab in each corpus view is entitled ‘Corpus Quality Assurance’. This tab will
allow you to calculate precision, recall and F-score between two annotation sets in a corpus
without the need to load a plugin. It extends the Annotation Diﬀ functionality to the entire
corpus in a convenient interface.
The main part of the view consists of two tabs each containing a table. One tab is entitled
‘Corpus statistics’ and the other is entitled ‘Document statistics’.
To the right of the tabbed area is a conﬁguration pane in which you can select the annotation
sets you wish to compare, the annotation types you are interested in and the annotation
features you wish to specify for use in the calculation if any.
You can also choose whether to calculate agreement on a strict or lenient basis or take the
average of the two. (Recall that strict matching requires two annotations to have an identical
span if they are to be considered a match, where lenient matching accepts a partial match;
annotations are overlapping but not identical in span.)
At the top, several icons are for opening a document (double-clicking on a row is also working)
or Annotation Diﬀ only when a row in the document statistics table is selected, exporting the
tables to an HTML ﬁle, reloading the list of sets, types and features when some documents
have been modiﬁed in the corpus and getting this help page.
Corpus Quality Assurance works also with a corpus inside a datastore. Using a datastore is
useful to minimise memory consumption when you have a big corpus.
See the section 10.1 for more details about the evaluation metrics.

10.3.2

Step by step usage

Begin by selecting the annotation sets you wish to compare in the top list in the conﬁguration
pane. Clicking on an annotation set labels it annotation set A for the Key (an ‘(A)’ will
appear beside it to indicate that this is your selection for annotation set A). Now click on
another annotation set. This will be labelled annotation set B for the response.
To change your selection, deselect an annotation set by clicking on it a second time. You can
now choose another annotation set. Note that you do not need to hold the control key down
to select the second annotation set. This list is conﬁgured to accept two (and no more than
two) selections. If you wish, you may check the box ‘present in every document’ to reduce
the annotation sets list to only those sets present in every document.

Performance Evaluation of Language Analysers

243

You may now choose the annotation types you are interested in. If you don’t choose any
then all will be used. If you wish, you may check the box ‘present in every selected set’ to
reduce the annotation types list to only those present in every selected annotation set.
You can choose the annotation features you wish to include in the calculation. If you choose
features, then for an annotation to be considered a match to another, their feature values
must also match. If you select the box ‘present in every selected type’ the features list will
be reduced to only those present in every type you selected.
For the classiﬁcation measures you must select only one type and one feature.
The ‘Measures’ list allows you to choose whether to calculate strict or lenient ﬁgures or
average the two. You may choose as many as you wish, and they will be included as columns
in the table to the left. The BDM measures allow to accept a match when the two concept
are close enough in an ontology even if their name are diﬀerent. See section 10.6.
An ‘Options’ button above the ‘Measures’ list gives let you set some settings like the beta
for the Fscore or the BDM ﬁle.
Finally, click on the ‘Compare’ button to recalculate the tables. The ﬁgures that appear in
the several tables (one per tab) are described below.

10.3.3

Details of the Corpus statistics table

In this table you will see that one row appears for every annotation type you chose. Columns
give total counts for matching annotations (‘Match’ equivalent to TREC Correct), annotations only present in annotation set A/Key (‘Only A’ equivalent to TREC Missing), annotations only present in annotation set B/Response (‘Only B’ equivalent to TREC Spurious)
and annotations that overlapped (‘Overlap’ equivalent to TREC Partial).
Depending on whether one of your annotation sets is considered a gold standard, you might
prefer to think of ‘Only A’ as missing and ‘Only B’ as spurious, or vice versa, but the Corpus
Quality Assurance tool makes no assumptions about which if any annotation set is the gold
standard. Where it is being used to calculate Inter Annotator Agreement there is no concept
of a ‘correct’ set. However, in ‘MUC’ terms, ‘Match’ would be correct and ‘Overlap’ would
be partial.
After these columns, three columns appear for every measure you chose to calculate. If you
chose to calculate a strict F1, a recall, precision and F1 column will appear for the strict
counts. If you chose to calculate a lenient F1, precision, recall and F1 columns will also
appear for lenient counts.
In the corpus statistics table, calculations are done on a per type basis and include all
documents in the calculation. Final rows in the table provide summaries; total counts are
given along with a micro and a macro average.

244

Performance Evaluation of Language Analysers

Micro averaging treats the entire corpus as one big document where macro averaging, on
this table, is the arithmetic mean of the per-type ﬁgures. See Section 10.1.4 for more detail
on the distinction between a micro and a macro average.

10.3.4

Details of the Document statistics table

In this table you will see that one row appears for every document in the corpus. Columns
give counts as in the corpus statistics table, but this time on a per-document basis.
As before, for every measure you choose to calculate, precision, recall and F1 columns will
appear in the table.
Summary rows, again, give a macro average (arithmetic mean of the per-document measures)
and micro average (identical to the ﬁgure in the corpus statistics table).

10.3.5

GATE Embedded API for the measures

You can get the same results as the Corpus Quality Assurance tool from your program by
using the classes that compute the results.
They are three for the moment: AnnotationDiﬀer, ClassiﬁcationMeasures and OntologyMeasures. All in gate.util package.
To compute the measures respect the order below.
Constructors and methods to initialise the measure objects:
AnnotationDiffer differ = new AnnotationDiffer();
differ.setSignificantFeaturesSet(Set<String> features);
ClassificationMeasures classificationMeasures = new ClassificationMeasures();
OntologyMeasures ontologyMeasures = new OntologyMeasures();
ontologyMeasures.setBdmFile(URL bdmFileUrl);

With bdmFileUrl an URL to a ﬁle of the format described at section 10.6.
Methods for computing the measures:
differ.calculateDiff(Collection key, Collection response)
classificationMeasures.calculateConfusionMatrix(AnnotationSet key,
AnnotationSet response, String type, String feature, boolean verbose)
ontologyMeasures.calculateBdm(Collection<AnnotationDiffer> differs)

With verbose to be set to true if you want to get printed the annotations ignored on the
”standard” output stream.

Performance Evaluation of Language Analysers

245

Constructors, useful for micro average, no need to use calculateX methods as they must have
been already called:
AnnotationDiffer(Collection<AnnotationDiffer> differs)
ClassificationMeasures(Collection<ClassificationMeasures> tables)
OntologyMeasures(Collection<OntologyMeasures> measures)

Method for getting results for all 3 classes:
List<String> getMeasuresRow(Object[] measures, String title)

With measures an array of String with values to choose from:
F1.0-score strict
F1.0-score lenient
F1.0-score average
F1.0-score strict BDM
F1.0-score lenient BDM
F1.0-score average BDM
Observed agreement
Cohen’s Kappa
Pi’s Kappa
Note that the numeric value ‘1.0’ represents the beta coeﬃcient in the Fscore. See section 10.1
for more information on these measures.
Method only for ClassiﬁcationMeasures:
List<List<String>> getConfusionMatrix(String title)

The following example is taken from gate.gui.CorpusQualityAssurance#compareAnnotation
but hasn’t been ran so there could be some corrections to make.
1
2
3
4
5
6

final int FSCORE_MEASURES = 0;
final int C L A S S I F I C A T I O N _ M E A S U R E S = 1;
ArrayList < String > documentNames = new ArrayList < String >();
TreeSet < String > types = new TreeSet < String >();
Set < String > features = new HashSet < String >();

246

7
8
9
10
11
12
13
14
15
16
17
18
19

Performance Evaluation of Language Analysers

int measuresType = FSCORE_MEASURES ;
Object [] measures = new Object []
{ " F1 .0 - score strict " , " F0 .5 - score lenient BDM " };
String keySetName = " Key " ;
String responseSetName = " Response " ;
types . add ( " Person " );
features . add ( " gender " );
URL bdmFileUrl = null ;
try {
bdmFileUrl = new URL ( " file :/// tmp / bdm . txt " );
} catch ( M a l f o r m e d U R L E x c e p t i o n e ) {
e . printStackTrace ();
}

20
21
22
23
24

boolean useBdm = false ;
for ( Object measure : measures ) {
if ((( String ) measure ). contains ( " BDM " )) { useBdm = true ; break ; }
}

25
26
27
28
29
30
31
32
33
34
35
36
37
38
39

// for each document
for ( int row = 0; row < corpus . size (); row ++) {
boolean documentWasLoaded = corpus . isDocumentLoaded ( row );
Document document = ( Document ) corpus . get ( row );
documentNames . add ( document . getName ());
Set < Annotation > keys = new HashSet < Annotation >();
Set < Annotation > responses = new HashSet < Annotation >();
// get annotations from selected annotation sets
keys = document . getAnnotations ( keySetName );
responses = document . getAnnotations ( responseSetName );
if (! doc umentW asLoa ded ) { // in case of datastore
corpus . unloadDocument ( document );
Factory . deleteResource ( document );
}

40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59

// fscore document table
if ( measuresType == FSCORE_MEASURES ) {
HashMap < String , AnnotationDiffer > differsByType =
new HashMap < String , AnnotationDiffer >();
AnnotationDiffer differ ;
Set < Annotation > keysIter = new HashSet < Annotation >();
Set < Annotation > responsesIter = new HashSet < Annotation >();
for ( String type : types ) {
if (! keys . isEmpty () && ! types . isEmpty ()) {
keysIter = (( AnnotationSet ) keys ). get ( type );
}
if (! responses . isEmpty () && ! types . isEmpty ()) {
responsesIter = (( AnnotationSet ) responses ). get ( type );
}
differ = new AnnotationDiffer ();
differ . s e t S i g n i f i c a n t F e a t u r e s S e t ( features );
differ . calculateDiff ( keysIter , responsesIter ); // compare
differsByType . put ( type , differ );
}

Performance Evaluation of Language Analysers

247

d i f f e r s B y D o c T h e n T y p e . add ( differsByType );
differ = new AnnotationDiffer ( differsByType . values ());
List < String > measuresRow ;
if ( useBdm ) {
OntologyMeasures ontologyMeasures = new OntologyMeasures ();
ontologyMeasures . setBdmFile ( bdmFileUrl );
ontologyMeasures . calculateBdm ( differsByType . values ());
measuresRow = ontologyMeasures . getMeasuresRow (
measures , documentNames . get ( documentNames . size () -1));
} else {
measuresRow = differ . getMeasuresRow ( measures ,
documentNames . get ( documentNames . size () -1));
}
System . out . println ( Arrays . deepToString ( measuresRow . toArray ()));

60
61
62
63
64
65
66
67
68
69
70
71
72
73
74

// classiﬁcation document table
} else if ( measuresType == C L A S S I F I C A T I O N _ M E A S U R E S
&& ! keys . isEmpty () && ! responses . isEmpty ()) {
ClassificationMeasures classificationMeasures =
new C l a s s i f i c a t i o n M e a s u r e s ();
classificationMeasures . calculateConfusionMatrix (
( AnnotationSet ) keys , ( AnnotationSet ) responses ,
types . first () , features . iterator (). next () , false );
List < String > measuresRow = c l a s s i f i c a t i o n M e a s u r e s . getMeasuresRow (
measures , documentNames . get ( documentNames . size () -1));
System . out . println ( Arrays . deepToString ( measuresRow . toArray ()));
List < List < String > > matrix = c l a s s i f i c a t i o n M e a s u r e s
. ge tC on fu si on Ma tr ix ( documentNames . get ( documentNames . size () -1));
for ( List < String > matrixRow : matrix ) {
System . out . println ( Arrays . deepToString ( matrixRow . toArray ()));
}
}

75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92

}

See method gate.gui.CorpusQualityAssurance#printSummary for micro and macro average
like in the Corpus Quality Assurance.

10.3.6

Quality Assurance PR

We have also implemented a processing resource called Quality Assurance PR that wraps the
functionality of the QA Tool. At the time of writing this documentation, the only diﬀerence
the QA PR has in terms of functionality is that the PR only accepts one measure at a time.
The Quality Assuarance PR is included in the Tools plugin. The PR can be added to any
existing corpus pipeline. Since the QA tool works on the entire corpus, the PR has to be
executed after all the documents in the corpus have been processed. In order to achieve this,
we have designed the PR in such a way that it only gets executed when the pipeline reaches
to the last document in the corpus. There are no init-time parameters but users are required
to provide values for the following run-time parameters.

248

Performance Evaluation of Language Analysers

annotationTypes - annotation types to compare.
featuresNames - features of the annotation types (speciﬁed above) to compare.
keyASName - the annotation set that acts as a gold standard set and contains annotations of the types speciﬁed above in the ﬁrst parameter.
responseASName - the annotation set that acts as a test set and contains annotations
of the types speciﬁed above in the ﬁrst parameter.
measure - one of the six pre-deﬁned measures: F1 STRICT, F1 AVERAGE,
F1 LENIENT, F05 STRICT, F05 AVERAGE and F05 LENIENT.
outputFolderUrl - the PR produces two html ﬁles in the folder mentioned in this
parameter. The ﬁles are document-stats.html and the corpus-stats.html. The former
lists statistics for each document and the latter lists statistics for each annotation type
in the corpus. In case of the document-stats.html, each document is linked with an
html ﬁle that contains the output of the annotation diﬀ utility in GATE.

10.4

Corpus Benchmark Tool

Like the Corpus Quality Assurance functionality, the corpus benchmark tool enables evaluation to be carried out over a whole corpus rather than a single document. Unlike Corpus
QA, it uses matched corpora to achieve this, rather than comparing annotation sets within
a corpus. It enables tracking of the system’s performance over time. It provides more detailed information regarding the annotations that diﬀer between versions of the corpus (e.g.
annotations created by diﬀerent versions of an application) than the Corpus QA tool does.
The basic idea with the tool is to evaluate an application with respect to a ‘gold standard’.
You have a ‘marked’ corpus containing the gold standard reference annotations; you have
a ‘clean’ copy of the corpus that does not contain the annotations in question, and you
have an application that creates the annotations in question. Now you can see how you are
getting on, by comparing the result of running your application on ‘clean’ to the ‘marked’
annotations.

10.4.1

Preparing the Corpora for Use

You will need to prepare the following directory structure:
main directory (can have any name)
|
|__"clean" (directory containing unannotated documents in XML form)
|

Performance Evaluation of Language Analysers

249

|__"marked" (directory containing annotated documents in XML form)
|
|__"processed" (directory containing the datastore which is generated
when you ‘store corpus for future evaluation’)

main: you should have a main directory containing subdirectories for your matched
corpora. It does not matter what this directory is called. This is the directory you
will select when the program prompts, ‘Please select a directory which contains the
documents to be evaluated’.
clean: Make a directory called ‘clean’ (case-sensitive), and in it, make a copy of your
corpus that does not contain the annotations that your application creates (though it
may contain other annotations). The corpus benchmark tool will apply your application to this corpus, so it is important that the annotations it creates are not already
present in the corpus. You can create this corpus by copying your ‘marked’ corpus and
deleting the annotations in question from it.
marked: you should have a ‘gold standard’ copy of your corpus in a directory called
‘marked’ (case-sensitive), containing the annotations to which the program will compare those produced by your application. The idea of the corpus benchmark tool is to
tell you how good your application performance is relative to this annotation set. The
‘marked’ corpus should contain exactly the same documents as the ‘clean’ set.
processed: this directory contains a third version of the corpus. This directory will
be created by the tool itself, when you run ‘store corpus for future evaluation’. We
will explain how to do this in Section 10.4.3

10.4.2

Deﬁning Properties

The properties of the corpus benchmark tool are deﬁned in the ﬁle ‘corpus tool.properties’,
which should be located in the GATE home directory. GATE will tell you where it’s looking
for the properties ﬁle in the ‘message’ panel when you run the Corpus Benchmark Tool. It
is important to prepare this ﬁle before attempting to run the tool because there is no ﬁle
present by default, so unless you prepare this ﬁle, the corpus benchmark tool will not work!
The following properties should be set:
the precision/recall performance threshold for verbose mode, below which the annotation will be displayed in the results ﬁle. This enables problem annotations to be easily
identiﬁed. By default this is set to 0.5;
the name of the annotation set containing the human-marked annotations (annotSetName);

250

Performance Evaluation of Language Analysers

the name of the annotation set containing the system-generated annotations (outputSetName);
the annotation types to be considered (annotTypes);
the feature values to be considered, if any (annotFeatures).
The default annotation set has to be represented by an empty string. The outputSetName
and annotSetName must be diﬀerent, and cannot both be the default annotation set. (If
they are the same, then use the Annotation Set Transfer PR to change one of them.) If you
omit any line (or just leave the value blank), that property reverts to default. For example,
‘annotSetName=’ is the same as leaving that line out.
An example ﬁle is shown below:
threshold=0.7
annotSetName=Key
outputSetName=ANNIE
annotTypes=Person;Organization;Location;Date;Address;Money
annotFeatures=type;gender

Here is another example:
threshold=0.6
annotSetName=Filtered
outputSetName=
annotTypes=Mention
annotFeatures=class

10.4.3

Running the Tool

To use the tool, ﬁrst make sure the properties of the tool have been set correctly (see Section
10.4.2 for how to do this) and that the corpora and directory structure have been prepared
as outlined in Section 10.4.1. Also, make sure that your application is saved to ﬁle (see
Section 3.8.3). Then, from the ‘Tools’ menu, select ‘Corpus Benchmark’. You have four
options:
1. Default Mode
2. Store Corpus for Future Evaluation
3. Human Marked Against Stored Processing Results
4. Human Marked Against Current Processing Results

Performance Evaluation of Language Analysers

251

We will describe these options in a diﬀerent order to that in which they appear on the menu,
to facilitate explanation.
Store Corpus for Future Evaluation populates the ‘processed’ directory with a datastore
containing the result of running your application on the ‘clean’ corpus. If a ‘processed’
directory exists, the results will be placed there; if not, one will be created. This creates
a record of the current application performance. You can rerun this operation any time to
update the stored set.
Human Marked Against Stored Processing Results compares the stored ‘processed’
set with the ‘marked’ set. This mode assumes you have already run ‘Store corpus for future
evaluation’. It performs a diﬀ between the ‘marked’ directory and the ‘processed’ directory
and prints out the metrics.
Human Marked Against Current Processing Results compares the ‘marked’ set with
the result of running the application on the ‘clean’ corpus. It runs your application on the
documents in the ‘clean’ directory creating a temporary annotated corpus and performs a
diﬀ with the documents in the ‘marked’ directory. After the metrics (recall, precision, etc.)
are calculated and printed out, it deletes the temporary corpus.
Default Mode runs ‘Human Marked Against Current Processing Results’ and ‘Human
Marked Against Stored Processing Results’ and compares the results of the two, showing
you where things have changed between versions. This is one of the main purposes of the
benchmark tool; to show the diﬀerence in performance between diﬀerent versions of your
application.
Once the mode has been selected, the program prompts, ‘Please select a directory which
contains the documents to be evaluated’. Choose the main directory containing your corpus
directories. (Do not select ‘clean’, ‘marked’, or ‘processed’.) Then (except in ‘Human marked
against stored processing results’ mode) you will be prompted to select the ﬁle containing
your application (e.g. an .xgapp ﬁle).
The tool can be used either in verbose or non-verbose mode, by selecting or unselecting
the verbose option from the menu. In verbose mode, for any precision/recall ﬁgure below
the user’s pre-deﬁned threshold (stored in corpus tool.properties ﬁle) the tool will show the
the non-coextensive annotations (and their corresponding text) for that entity type, thereby
enabling the user to see where problems are occurring.

10.4.4

The Results

Running the tool (either in ‘Human marked against stored processing results’, ‘Human
marked against current processing results’ or ‘Default’ mode) produces an HTML ﬁle, in
tabular form, which is output in the main GATE Developer messages window. This can
then be pasted into a text editor and viewed in a web browser for easier viewing. See ﬁgure
10.4 for an example.

252

Performance Evaluation of Language Analysers

In each mode, the following statistics will be output:
1. Per-document ﬁgures, itemised by type: precision and recall, as well as detailed information about the diﬀering annotations;
2. Summary by type (‘Statistics’): correct, partially correct, missing and spurious totals,
as well as whole corpus (micro-average) precision, recall and f-measure (F1), itemised
by type;
3. Overall average ﬁgures: precision, recall and F1 calculated as a macro-average (arithmetic average) of the individual document precisions and recalls.
In ‘Default’ mode, information is also provided about whether the ﬁgures have increased or
decreased in comparison with the ‘Marked’ corpus.

Figure 10.4: Fragment of results from corpus benchmark tool

10.5

A Plugin Computing Inter-Annotator Agreement
(IAA)

The interannotator agreement plugin, ‘Inter Annotator Agreement’, computes the Fmeasures, namely precision, recall and F1, suitable for named entity annotations (see Sec-

Performance Evaluation of Language Analysers

253

tion 10.1.3), and agreement, Cohen’s kappa and Scott’s pi, suitable for text classiﬁcation
tasks (see Section 10.1.2). In the latter case, a confusion matrix is also provided. In this
section we describe those measures and the output results from the plugin. But ﬁrst we
explain how to load the plugin, and the input to and the parameters of the plugin.
First you need to load the plugin named ‘Inter Annotator Agreement’ into GATE Developer
using the tool Manage CREOLE Plugins, if it is not already loaded. Then you can create a
PR for the plugin from the ‘IAA Computation’ in the existing PR list. After that you can
put the PR into a Corpus Pipeline to use it.
The IAA Computation PR diﬀers from the Corpus Benchmark Tool in the data preparation
required. As in the Corpus Benchmark Tool, the idea is to compare annotation sets, for
example, prepared by diﬀerent annotators, but in the IAA Computation PR, these annotation sets should be on the same set of documents. Thus, one corpus is loaded into GATE
on which the PR is run. Diﬀerent annotation sets contain the annotations which will be
compared. These should (obviously) have diﬀerent names.
It falls to the user to decide whether to use annotation type or an annotation feature as
class; are two annotations considered to be in agreement because they have the same type
and the same span? Or do you want to mark up your data with an annotation type such
as ‘Mention’, thus deﬁning the relevant annotations, then give it a ‘class’ feature, the value
of which should be matched in order that they are considered to agree? This is a matter of
convenience. For example, data from the Batch Learning PR (see Section 17.2) uses a single
annotation type and a class feature. In other contexts, using annotation type might feel
more natural; the annotation sets should agree about what is a ‘Person’, what is a ‘Date’
etc. It is also possible to mix the two, as you will see below.
The IAA plugin has two runtime parameters annSetsForIaa and annTypesAndFeats for
specifying the annotation sets and the annotation types and features, respectively. Values
should be separated by semicolons. For example, to specify annotation sets ‘Ann1’, ‘Ann2’
and ‘Ann3’ you should set the value of annSetsForIaa to ‘Ann1;Ann2;Ann3’. Note that more
than two annotation sets are possible. Specify the value of annTypesAndFeats as ‘Per’ to
compute the IAA for the three annotation sets on the annotation type Per. You can also
specify more than one annotation type and separate them by ‘;’ too, and optionally specify
an annotation feature for a type by attaching a ‘->’ followed by feature name to the end of
the annotation name. For example, ‘Per->label;Org’ speciﬁes two annotation types Per and
Org and also a feature name label for the type Per. If you specify an annotation feature for
an annotation type, then two annotations of the same type will be regarded as being diﬀerent
if they have diﬀerent values of that feature, even if the two annotations occupy exactly the
same position in the document. On the other hand, if you do not specify any annotation
feature for an annotation type, then the two annotations of the type will be regarded as the
same if they occupy the same position in the document.
The parameter measureType speciﬁes the type of measure computed. There are two
measure types; the F-measure (i.e. Precision, Recall and F1), and the observed agreement
and Cohen’s Kappa. For classiﬁcation tasks such as document or sentence classiﬁcation, the

254

Performance Evaluation of Language Analysers

observed agreement and Cohen’s Kappa is often used, though the F-measure is applicable
too. In these tasks, the targets are already identiﬁed, and the task is merely to classify them
correctly. However, for the named entity recognition task, only the F-measure is applicable.
In such tasks, ﬁnding the ‘named entities’ (text to be annotated) is as much a part of the
task as correctly labelling it. Observed agreement and Cohen’s kappa are not suitable in this
case. See Section 10.1.2 for further discussion. The parameter has two values, FMEASURE
and AGREEMENTANDKAPPA. The default value of the parameter is FMEASURE.
Another parameter verbosity speciﬁes the verbosity level of the plugin’s output. Level 2
displays the most detailed output, including the IAA measures on each document and the
macro-averaged results over all documents. Level 1 only displays the IAA measures averaged
over all documents. Level 0 does not have any output. The default value of the parameter
is 1. In the following we will explain the outputs in detail.
Yet another runtime parameter bdmScoreFile speciﬁes the URL for a ﬁle containing the
BDM scores used for the BDM based IAA computation. The BDM score ﬁle should be
produced by the BDM computation plugin, which is described in Section 10.6. The BDMbased IAA computation will be explained below. If the parameter is not assigned any value,
or is assigned a ﬁle which is not a BDM score ﬁle, the PR will not compute the BDM based
IAA.

10.5.1

IAA for Classiﬁcation

IAA has been used mainly in classiﬁcation tasks, where two or more annotators are given
a set of instances and are asked to classify those instances into some pre-deﬁned categories.
IAA measures the agreements among the annotators on the class labels assigned to the instances by the annotators. Text classiﬁcation tasks include document classiﬁcation, sentence
classiﬁcation (e.g. opinionated sentence recognition), and token classiﬁcation (e.g. POS tagging). The important point to note is that the evaluation set and gold standard set have
exactly the same instances, but some instances in the two sets have diﬀerent class labels.
Identifying the instances is not part of the problem.
The three commonly used IAA measures are observed agreement, speciﬁc agreement, and
Kappa (κ) [Hripcsak & Heitjan 02]. See Section 10.1.2 for the detailed explanations of those
measures. If you select the value of the runtime parameter measureType as AGREEMENTANDKAPPA, the IAA plugin will compute and display those IAA measures for your classiﬁcation task. Below, we will explain the output of the PR for the agreement and Kappa
measures.
At the verbosity level 2, the output of the plugin is the most detailed. It ﬁrst prints out a list
of the names of the annotation sets used for IAA computation. In the rest of the results, the
ﬁrst annotation set is denoted as annotator 0, and the second annotation set is denoted as
annotator 1, etc. Then the plugin outputs the IAA results for each document in the corpus.

Performance Evaluation of Language Analysers

255

For each document, it displays one annotation type and optionally an annotation feature if
speciﬁed, and then the results for that type and that feature. Note that the IAA computations are based on the pairwise comparison of annotators. In other words, we compute
the IAA for each pair of annotators. The ﬁrst results for one document and one annotation
type are the macro-averaged ones over all pairs of annotators, which have three numbers
for the three types of IAA measures, namely Observed agreement, Cohen’s kappa and Scott’s
pi. Then for each pair of annotators, it outputs the three types of measures, a confusion
matrix (or contingency table), and the speciﬁc agreements for each label. The labels are
obtained from the annotations of that particular type. For each annotation type, if a feature
is speciﬁed, then the labels are the values of that feature. Please note that two terms may
be added to the label list: one is the empty one obtained from those annotations which
have the annotation feature but do not have a value for the feature; the other is ‘Non-cat’,
corresponding to those annotations not having the feature at all. If no feature is speciﬁed,
then two labels are used: ‘Anns’ corresponding to the annotations of that type, and ‘Noncat’ corresponding to those annotations which are annotated by one annotator but are not
annotated by another annotator.
After displaying the results for each document, the plugin prints out the macro-averaged
results over all documents. First, for each annotation type, it prints out the results for each
pair of annotators, and the macro-averaged results over all pairs of annotators. Finally it
prints out the macro-averaged results over all pairs of annotators, all types and all documents.
Please note that the classiﬁcation problem can be evaluated using the F-measure too. If you
want to evaluate a classiﬁcation problem using the F-measure, you just need to set the run
time parameter measureType to FMEASURE.

10.5.2

IAA For Named Entity Annotation

The commonly used IAA measures, such as kappa, have not been used in text mark-up
tasks such as named entity recognition and information extraction, for reasons explained
in Section 10.1.2 (also see [Hripcsak & Rothschild 05]). Instead, the F-measures, such as
Precision, Recall, and F1, have been widely used in information extraction evaluations such
as MUC, ACE and TERN for measuring IAA. This is because the computation of the Fmeasures does not need to know the number of non-entity examples. Another reason is
that F-measures are commonly used for evaluating information extraction systems. Hence
IAA F-measures can be directly compared with results from other systems published in the
literature.
For computing F-measure between two annotation sets, one can use one annotation set as
gold standard and another set as system’s output and compute the F-measures such as
Precision, Recall and F1. One can switch the roles of the two annotation sets. The Precision
and Recall in the former case become Recall and Precision in the latter, respectively. But
the F1 remains the same in both cases. For more than two annotators, we ﬁrst compute
F-measures between any two annotators and use the mean of the pair-wise F-measures as

256

Performance Evaluation of Language Analysers

an overall measure.
The computation of the F-measures (e.g. Precision, Recall and F1) are shown in Section
10.1. As noted in [Hripcsak & Rothschild 05], the F1 computed for two annotators for one
speciﬁc category is equivalent to the positive speciﬁc agreement of the category.
The outputs of the IAA plugins for named entity annotation are similar to those for classiﬁcation. But the outputs are the F-measures, such as Precision, Recall and F1, instead
of the agreements and Kappas. It ﬁrst prints out the results for each document. For one
document, it prints out the results for each annotation type, macro-averaged over all pairs of
annotators, then the results for each pair of annotators. In the last part, the micro-averaged
results over all documents are displayed. Note that the results are reported in both the strict
measure and the lenient measure, as deﬁned in Section 10.2.
Please note that, for computing the F-measures for the named entity annotations, the IAA
plugin carries out the same computation as the Corpus Benchmark tool. The IAA plugin is
simpler than the Corpus benchmark tool in the sense that the former needs only one set of
documents with two or more annotation sets, whereas the latter needs three sets of the same
documents, one without any annotation, another with one annotation set, and the third one
with another annotation set. Additionally, the IAA plugin can deal with more than two
annotation sets but the Corpus benchmark tool can only deal with two annotation sets.

10.5.3

The BDM-Based IAA Scores

For a named entity recognition system, if the named entity’s class labels are the names of
concepts in some ontology (e.g. in the ontology-based information extraction), the system
can be evaluated using the IAA measures based on the BDM scores. The BDM measures
the closeness of two concepts in an ontology. If an entity is identiﬁed but is assigned a label
which is close to but not the same as the true label, the system should obtain some credit
for it, which the BDM-based metric can do. In contrast, the conventional named entity
recognition measure does not take into account the closeness of two labels and does not give
any credit to one identiﬁed entity with a wrong label, regardless of how close the assigned
label is to the true label. For more explanation about BDM see Section 10.6.
In order to compute the BDM-based IAA, one has to assign the plugin’s runtime parameter
bdmScoreFile to the URL of a ﬁle containing the BDM scores. The ﬁle should be obtained
by using the BDM computation plugin, which is described in Section 10.6. Currently the
BDM-based IAA is only used for computing the F-measures for e.g. the entity recognition
problem. Please note that the F-measures can also be used for evaluation of classiﬁcation
problem. The BDM is not used for computing other measures such as the observed agreement
and Kappa, though it is possible to implement it. Therefore currently one has to select
FMEASURE for the run time parameter measureType in order to use the BDM based IAA
computation.

Performance Evaluation of Language Analysers

10.6

257

A Plugin Computing the BDM Scores for an Ontology

The BDM (balanced distance metric) measures the closeness of two concepts in an ontology
or taxonomy [Maynard 05, Maynard et al. 06]. It is a real number between 0 and 1. The
closer the two concepts are in an ontology, the greater their BDM score is. For detailed
explanation about the BDM, see the papers [Maynard 05, Maynard et al. 06]. The BDM can
be seen as an improved version of the learning accuracy [Cimiano et al. 03]. It is dependent
on the length of the shortest path connecting the two concepts and also the deepness of the
two concepts in ontology. It is also normalised with the size of ontology and also takes into
account the concept density of the area containing the two involved concepts.
The BDM has been used to evaluate the ontology based information extraction (qOBIE)
system [Maynard et al. 06]. The OBIE identiﬁes the instances for the concepts of an ontology. It’s possible that an OBIE system identiﬁes an instance successfully but does not
assign it the correct concept. Instead it assigns the instance a concept being close to the
correct one. For example, the entity ‘London’ is an instance of the concept Capital, and
an OBIE system assigns it the concept City which is close to the concept Capital in some
ontology. In that case the OBIE should obtain some credit according to the closeness of
the two concepts. That is where the BDM can be used. The BDM has also been used to
evaluate the hierarchical classiﬁcation system [Li et al. 07b]. It can also be used for ontology
learning and alignment.
The BDM computation plugin computes BDM score for each pair of concepts in an ontology.
It has two run time parameters:
ontologyURL – its value should be the URL of the ontology that one wants to
compute the BDM scores for.
outputBDMFile – its value is the URL of a ﬁle which will store the BDM scores
computed.
The plugin has the name Ontology BDM Computation and the corresponding processing
resource’s name is BDM Computation PR. The PR can be put into a Pipeline. If it is put
into a Corpus Pipeline, the corpus used should contain at least one document.
The BDM computation used the formula given in [Maynard et al. 06]. The resulting ﬁle
speciﬁed by the runtime parameter outputBDMFile contains the BDM scores. It is a text
ﬁle. The ﬁrst line of the ﬁle gives some meta information such as the name of ontology used
for BDM computation. From the second line of the ﬁle, each line corresponds to one pair of
concepts. One line is like
key=Service, response=Object, bdm=0.6617647, msca=Object, cp=1, dpk=1, dpr=0,
n0=2.0, n1=2.0, n2=2.8333333, bran=1.9565217

258

Performance Evaluation of Language Analysers

It ﬁrst shows the names of the two concepts (one as key and another as response, and the
BDM score, and then other parameters’ values used for the computation. Note that, since
the BDM is symmetric for the two concepts, the resulting ﬁle contains only one line for each
pair. So if you want to look for the BDM score for one pair of concepts, you can choose one
as key and another as response. If you cannot ﬁnd the line for the pair, you have to change
the order of two concepts and retrieve the ﬁle again.

10.7

Quality Assurance Summariser for Teamware

When documents are annotated using Teamware, anonymous annotation sets are created
for the annotating annotators. This makes it impossible to run Quality Assurance on such
documents as annotation sets with same names in diﬀerent documents may refer to the
annoations created by diﬀerent annotators. This is specially the case when a requirement is
to compute Inter Annotator Agreement (IAA). The QA Summariser for Teamware PR
generates a summary of agreements among annotators. It does this by pairing individual
annotators involved in the annotation task. It also compares annotations of each individual
annotator with those available in the consensus annotation set in the respective documents.
The PR is available from the Teamware Tools plugin. It internally uses the QualityAssurancePR to calculate agreement statistics. User has to provide the following run-time
parameters:
annotationTypes Annotation types for which the IAA has to be computed.
featureNames Features of annotations that should be used in IAA computations.
If no value is provided, only annotation boundaries for same annotation types are
compared.
measure one of the six pre-deﬁned measures: F1 STRICT, F1 AVERAGE,
F1 LENIENT, F05 STRICT, F05 AVERAGE and F05 LENIENT.
outputFolderUrl The PR produces a summary in this folder. More information on
the generated ﬁle is provided below.
The PR generates an index.html ﬁle in the output folder. This html ﬁle contains a table that
summarises the agreement statistics. Both the ﬁrst row and the ﬁrst column contain names
of annotators who were involved in the annotation task. For each pair of annotators who
did the annotations together on atleast one document, both the micro and macro averages
are produced.
Last two columns in each row give average macro and micro agreements of the respective
annotator with all the other annotators he or she did annotations together.

Performance Evaluation of Language Analysers

259

These ﬁgures are color coded. The color green is used for a cell background to indicate
full agreement (i.e. 1.0). The background color becomes lighter as the agreement reduces
towards 0.5. At 0.5 agreement, the background color of a cell is fully white. From 0.5
downwards, the color red is used and as the agreement reduces further, the color becomes
darker with dark red at 0.0 agreement. Use of such a color coding makes it easy for user to
get an idea of how annotators are performing and locate speciﬁc pairs of annotations who
need more training or may be someone who deserves a pat on his/her back.
For each pair of annotators, the summary table provides a link (with caption document) to
another html document that summarises annotations of the two respective annotators on
per document basis. The details include number of annotations they agreed and disagreed
and the scores for recall, precision and f-measure. Each document name in this summary
is linked with another html document with indepth comparison of annotations. User can
actually see the annotations on which the annotators had agreed and disagreed.

260

Performance Evaluation of Language Analysers

Chapter 11
Proﬁling Processing Resources
11.1

Overview

This is a reporting tool for GATE processing resources. It reports the total time taken by
processing resources and the time taken for each document to be processed by an application
of type corpus pipeline.
GATE use log4j, a logging system, to write proﬁling informations in a ﬁle. The GATE proﬁling reporting tool uses the ﬁle generated by log4j and produces a report on the processing
resources. It proﬁles JAPE grammars at the rule level, enabling the user precisely identify
the performance bottlenecks. It also produces a report on the time taken to process each
document to ﬁnd problematic documents.
This initial code for the reporting tool was written by Intelius employees Andrew Borthwick
and Chirag Viradiya and generously released under the LGPL licence to be part of GATE.

Figure 11.1: Example of HTML proﬁling report for ANNIE
261

262

Proﬁling Processing Resources

11.1.1

Features

Ability to generate the following two reports
– Report on processing resources. For each level of processing: application, processing resource (PR) and grammar rule, subtotalled at each level.
– Report on documents processed. For some or all PR, sorted in decreasing processing time.
Report on processing resources speciﬁc features
– Sort order by time or by execution.
– Show or hide processing elements which took 0 milliseconds.
– Generate HTML report with a collapsible tree.
Report on documents processed speciﬁc features
– Limit the number of document to show from the most time consuming.
– Filter the PR to display statistics for.
Features common to both reports
– Generate report as indented text or in HTML format.
– Generate a report only on the log entries from the last logical run of GATE.
– All processing times are reported in milliseconds and in terms of percentage
(rounded to nearest 0.1%) of total time.
– Command line interface and API.
– Detect if the benchmark.txt ﬁle is modiﬁed while generating the report.

11.1.2

Limitations

Be aware that the proﬁling doesn’t support non corpus pipeline as application type. There
is indeed no interest in proﬁling a non corpus pipeline that works on one or no document at
all. To get meaningful results you should run your corpus pipeline on at least 10 documents.

11.2

Graphical User Interface

The activation of the proﬁling and the creation of proﬁling reports are accessible from the
‘Tools’ menu in GATE with the submenu ‘Proﬁling Reports’.

Proﬁling Processing Resources

263

You can ‘Start Proﬁling Applications’ and ‘Stop Proﬁling Applications’ at any time. The
logging is cumulative so if you want to get a new report you must use the ‘Clear Proﬁling
History’ menu item when the proﬁling is stopped.
Be very careful that you must start the proﬁling before you load your application or you will
need to reload every Processing Resource that uses a Transducer. Otherwise you will get an
Exception similar to:
java.lang.IndexOutOfBoundsException: Index: 2, Size: 0
at java.util.ArrayList.RangeCheck(ArrayList.java:547)
at java.util.ArrayList.get(ArrayList.java:322)
at gate.jape.SinglePhaseTransducer.updateRuleTime(SinglePhaseTransducer.java:678)

Two types of reports are available: ‘Report on Processing Resources’ and ‘Report on Documents Processed’. See the previous section for more information.

11.3

Command Line Interface

Report on processing resources Usage: java gate.util.reporting.PRTimeReporter [Options]
Options:
-i input ﬁle path (default: benchmark.txt in the user’s .gate directory1 )
-m print media - html/text (default: html)
-z suppressZeroTimeEntries - true/false (default: true)
-s sorting order - exec order/time taken (default: exec order)
-o output ﬁle path (default: report.html/txt in the system temporary directory)
-l logical start (not set by default)
-h show help
Note that suppressZeroTimeEntries will be ignored if the sorting order is ‘time taken’
Report on documents processed Usage: java gate.util.reporting.DocTimeReporter
[Options]
1

GATE versions up to 5.2 placed benchmark.txt in the execution directory.

264

Proﬁling Processing Resources

Options:
-i input ﬁle path (default: benchmark.txt in the user’s .gate directory2 )
-m print media - html/text (default: html)
-d number of docs, use -1 for all docs (default: 10 docs)
-p processing resource name to be matched (default: all prs)
-o output ﬁle path (default: report.html/txt in the system temporary directory)
-l logical start (not set by default)
-h show help

Examples
Run report 1: Report on Total time taken by each processing element across corpus
– java -cp ”gate/bin:gate/lib/GnuGetOpt.jar” gate.util.reporting.PRTimeReporter
-i benchmark.txt -o report.txt -m text
Run report 2: Report on Time taken by document within given corpus.
– java -cp ”gate/bin:gate/lib/GnuGetOpt.jar” gate.util.reporting.DocTimeReporter
-i benchmark.txt -o report.html -m html

11.4

Application Programming Interface

11.4.1

Log4j.properties

This is required to direct the proﬁling information to the benchmark.txt ﬁle. The benchmark.txt generated by GATE will be used as input for GATE proﬁling report tool as input.
# File appender that outputs only benchmark messages
log4j.appender.benchmarklog=org.apache.log4j.RollingFileAppender
log4j.appender.benchmarklog.Threshold=DEBUG
log4j.appender.benchmarklog.File= user.home/.gate/benchmark.txt
2

GATE versions up to 5.2 placed benchmark.txt in the execution directory.

Proﬁling Processing Resources

265

log4j.appender.benchmarklog.MaxFileSize=5MB
log4j.appender.benchmarklog.MaxBackupIndex=1
log4j.appender.benchmarklog.layout=org.apache.log4j.PatternLayout
log4j.appender.benchmarklog.layout.ConversionPattern=%m%n
# Conﬁgure the Benchmark logger so that it only goes to the benchmark log ﬁle
log4j.logger.gate.util.Benchmark=DEBUG, benchmarklog
log4j.additivity.gate.util.Benchmark=false

11.4.2

Benchmark log format

The format of the benchmark ﬁle that logs the times is as follow:
timestamp START PR_name
timestamp duration benchmarkID class features
timestamp duration benchmarkID class features
...

with the timestamp being the diﬀerence, measured in milliseconds, between the current time
and midnight, January 1, 1970 UTC.
Example:
1257269774770 START Sections_splitter
1257269774773 0 Sections_splitter.doc_EP-1026523-A1_xml_00008.documentLoaded
gate.creole.SerialAnalyserController
{corpusName=Corpus for EP-1026523-A1.xml_00008,
documentName=EP-1026523-A1.xml_00008}
...

11.4.3

Enabling proﬁling

There are two ways to enable proﬁling of the processing resources:
1. In gate/build.properties, add the line: run.gate.enable.benchmark=true
2. In your Java code, use the method: Benchmark.setBenchmarkingEnabled(true)

266

Proﬁling Processing Resources

11.4.4

Reporting tool

Report on processing resources
1. Instantiate the Class PRTimeReporter
(a) PRTimeReporter report = new PRTimeReporter();
2. Set the input benchmark ﬁle
(a) File benchmarkFile = new File(”benchmark.txt”);
(b) report.setBenchmarkFile(benchmarkFile);
3. Set the output report ﬁle
(a) File reportFile = new File(”report.txt”); or
(b) File reportFile = new File(”report.html”);
(c) report.setReportFile(reportFile);
4. Set the output format: in html or text format (default: MEDIA HTML)
(a) report.setPrintMedia(PRTimeReporter.MEDIA TEXT); or
(b) report.setPrintMedia(PRTimeReporter.MEDIA HTML);
5. Set the sorting order: Sort in order of execution or descending order of time taken
(default: EXEC ORDER)
(a) report.setSortOrder(PRTimeReporter.SORT TIME TAKEN); or
(b) report.setSortOrder(PRTimeReporter.SORT EXEC ORDER);
6. Set if suppress zero time entries: True/False (default: True). Parameter ignored if
SortOrder speciﬁed is ‘SORT TIME TAKEN’
(a) report.setSuppressZeroTimeEntries(true);
7. Set the logical start: A string indicating the logical start to be operated upon for
generating reports
(a) report.setLogicalStart(”InteliusPipelineStart”);
8. Generate the text/html report
(a) report.executeReport();

Proﬁling Processing Resources

267

Report on documents processed
1. Instantiate the Class DocTimeReporter
(a) DocTimeReporter report = new DocTimeReporter();
2. Set the input benchmark ﬁle
(a) File benchmarkFile = new File(”benchmark.txt”);
(b) report.setBenchmarkFile(benchmarkFile);
3. Set the output report ﬁle
(a) File reportFile = new File(”report.txt”); or
(b) File reportFile = new File(”report.html”);
(c) report.setReportFile(reportFile);
4. Set the output format: Generate report in html or text format (default: MEDIA HTML)
(a) report.setPrintMedia(DocTimeReporter.MEDIA TEXT); or
(b) report.setPrintMedia(DocTimeReporter.MEDIA HTML);
5. Set the maximum number of documents: Maximum number of documents to be displayed in the report (default: 10 docs)
(a) report.setNoOfDocs(2); // 2 docs or
(b) report.setNoOfDocs(DocTimeReporter.ALL DOCS); // All documents
6. Set the PR matching regular expression: A PR name or a regular expression to ﬁlter
the results (default: MATCH ALL PR REGEX).
(a) report.setSearchString(”HTML”); // match ALL PRS having HTML as substring
7. Set the logical start: A string indicating the logical start to be operated upon for
generating reports
(a) report.setLogicalStart(”InteliusPipelineStart”);
8. Generate the text/html report
(a) report.executeReport();

268

Proﬁling Processing Resources

Chapter 12
Developing GATE
This chapter describes ways of getting involved in and contributing to the GATE project.
Sections 12.1 and 12.2 are good places to start. Sections 12.3 and 12.4 describe protocol and
provide information for committers; we cover creating new plugins and updating this user
guide. See Section 12.2 for information on becoming a committer.

12.1

Reporting Bugs and Requesting Features

The GATE bug tracker can be found on SourceForge, here. When reporting bugs, please give
as much detail as possible. Include the GATE version number and build number, the platform
on which you observed the bug, and the version of Java you were using (1.5.0 15, 1.6.0 03,
etc.). Include steps to reproduce the problem, and a full stack trace of any exceptions,
including ‘Caused by . . . ’. You may wish to ﬁrst check whether the bug is already ﬁxed in
the latest nightly build. You may also request new features.

12.2

Contributing Patches

Patches may be submitted on SourceForge. The best format for patches is an SVN diﬀ
against the latest subversion. The diﬀ can be saved as a ﬁle and attached; it should not
be pasted into the bug report. Note that we generally do not accept patches against earlier
versions of GATE. Also, GATE is intended to be compatible with Java 6, so if you regularly
develop using a later version of Java it is very important to compile and test your patches
on Java 6. Patches that use features from a later version of Java and do not compile and
run on Java 6 will not be accepted.
If you intend to submit larger changes, you might prefer to become a committer! We welcome
input to the development process of GATE. The code is hosted on SourceForge, providing
269

270

Developing GATE

anonymous Subversion access (see Section 2.2.3). We’re happy to give committer privileges
to anyone with a track record of contributing good code to the project. We also make the
current version available nightly on the ftp site.

12.3

Creating New Plugins

GATE provides a ﬂexible structure where new resources can be plugged in very easily. There
are three types of resources: Language Resource (LR), Processing Resource (PR) and Visual
Resource (VR). In the following subsections we describe the necessary steps to write new
PRs and VRs, and to add plugins to the nightly build. The guide on writing new LRs will
be available soon.

12.3.1

Where to Keep Plugins in the GATE Hierarchy

Each new resource added as a plugin should contain its own subfolder under the %GATEHOME%/plugins folder with an associated creole.xml ﬁle. A plugin can have one or more
resources declared in its creole.xml ﬁle and/or using source-level annotations as described in
section 4.7.

12.3.2

What to Call your Plugin

The plugins are many and the list is constantly expanding. The naming convention aims to
impose order and group plugins in a readable manner. When naming new plugins, please
adhere to the following guidelines:

Words comprising plugin names should be capitalized and separated by underscores
Like So. This means that they will format nicely in GATE Developer. For example,
‘Inter Annotator Agreement’.
Plugin names should begin with the word that best describes their function. Practically, this means that words are often reversed from the usual order, for example, the
Chemistry Tagger plugin should be called ‘Tagger Chemistry’. This means that for
example parsers will group together alphabetically and thus will be easy to ﬁnd when
someone is looking for parsers. Before naming your plugin, look at the existing plugins
and see where it might group well.

Developing GATE

12.3.3

271

Writing a New PR

Class Deﬁnition
Below we show a template class deﬁnition, which can be used in order to write a new
Processing Resource.
1
2

package example ;

3
4
5
6

import gate .*;
import gate . creole .*;
import gate . creole . metadata .*;

7
8
9
10
11
12
13
14
15

/
P r o c e s s i n g R e s o u r c e . The @ C r e o l e R e s o u r c e a n n o t a t i o n marks t h i s
c l a s s a s a GATE R e s o u r c e , and g i v e s t h e i n f o r m a t i o n GATE n e e d s
to configure the resource appropriately .
/
@CreoleResource ( name = " Example PR " ,
comment = " An example processing resource " )
public class NewPlugin extends A b s t r a c t L a n g u a g e A n a l y s e r {

16
17

/
t h i s method g e t s c a l l e d w h e n e v e r an o b j e c t o f t h i s
c l a s s i s c r e a t e d e i t h e r from GATE D e v e l o p e r GUI o r i f
i n i t i a t e d u s i n g F a c t o r y . c r e a t e R e s o u r c e ( ) method .

18
19
20
21
22
23
24
25

/
public Resource init () throws R e s o u r c e I n s t a n t i a t i o n E x c e p t i o n {
// here initialize all required variables, and may
// be throw an exception if the value for any of the
// mandatory parameters is not provided

26

if ( this . rulesURL == null )
throw new R e s o u r c e I n s t a n t i a t i o n E x c e p t i o n ( " rules URL null " );

27
28
29

return this ;

30
31

}

32
33
34
35
36
37
38
39
40
41
42

/
t h i s method s h o u l d p r o v i d e t h e a c t u a l f u n c t i o n a l i t y o f t h e PR
( from w h e r e t h e main e x e c u t i o n b e g i n s ) . T h i s method
g e t s c a l l e d when u s e r c l i c k on t h e ”RUN” b u t t o n i n t h e
GATE D e v e l o p e r GUI ’ s a p p l i c a t i o n window .
/
public void execute () throws Ex ecu ti on Ex ce pt io n {
// write code here
}

43
44
45

/ t h i s method i s c a l l e d t o r e i n i t i a l i z e t h e r e s o u r c e /
public void reInit () throws R e s o u r c e I n s t a n t i a t i o n E x c e p t i o n {

272

Developing GATE

// reinitialization code

46
47

}

48
49

/
There a r e two t y p e s o f p a r a m e t e r s
1 . I n i t time parameters − v a l u e s f o r t h e s e parameters need t o be
p r o v i d e d a t t h e t i m e o f i n i t i a l i z i n g a new r e s o u r c e and t h e s e
v a l u e s are not supposed to be changed .
2 . Runtime p a r a m e t e r s − v a l u e s f o r t h e s e p a r a m e t e r s a r e p r o v i d e d
a t t h e t i m e o f e x e c u t i n g t h e PR . These a r e r u n t i m e p a r a m e t e r s and
can b e c h a n g e d b e f o r e s t a r t i n g t h e e x e c u t i o n
( i . e . b e f o r e you c l i c k on t h e ”RUN” b u t t o n i n GATE D e v e l o p e r )
A p a r a m e t e r myParam i s s p e c i f i e d b y a p a i r o f m e t h o d s getMyParam
and setMyParam ( w i t h t h e f i r s t l e t t e r o f t h e p a r a m e t e r name
c a p i t a l i z e d i n t h e n o r m a l J a v a Beans s t y l e ) , w i t h t h e s e t t e r
annotated with a @CreoleParameter a n n o t a t i o n .

50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65

f o r example to s e t a v a l u e f o r outputAnnotationSetName
/
String o u t p u t A n n o t a t i o n S e t N a m e ;

66
67

//getter and setter methods

68
69
70
71
72

/ g e t <p a r a m e t e r name w i t h f i r s t l e t t e r C a p i t a l >
public String g e t O u t p u t A n n o t a t i o n S e t N a m e () {
return o u t p u t A n n o t a t i o n S e t N a m e ;
}

/

73
74

/

75
76
77
78
79
80
81
82
83

The s e t t e r method i s a n n o t a t e d t o
o p t i o n a l runtime parameter .

t e l l GATE t h a t

it

d e f i n e s an

/
@Optional
@RunTime
@CreoleParameter (
comment = " name of the annotationSet used for output " )
public void s e t O u p u t A n n o t a t i o n S e t N a m e ( String setName ) {
this . o u t p u t A n n o t a t i o n S e t N a m e = setName ;
}

84
85
86

/
I n i t −t i m e p a r a m e t e r
URL rulesURL ;

/

87
88
89
90
91

// getter and setter methods
public URL getRulesURL () {
return rulesFile ;
}

92
93
94
95
96
97
98

/

T h i s p a r a m e t e r i s n o t a n n o t a t e d @RunTime o r @ O p t i o n a l , s o i t
r e q u i r e d i n i t −t i m e p a r a m e t e r .

/
@CreoleParameter (
comment = " example of an inittime parameter " ,
defaultValue = " resources / morph / default . rul " )

is a

Developing GATE

public void setRulesURL ( URL rulesURL ) {
this . rulesURL = rulesURL ;
}

99
100
101
102

273

}

PR Creole Entry
The creole.xml ﬁle simply needs to tell GATE which JAR ﬁle to look in to ﬁnd the PR.

<?xml version="1.0"?>
<CREOLE-DIRECTORY>
<JAR SCAN="true">newplugin.jar</JAR>
</CREOLE-DIRECTORY>

Alternatively the conﬁguration can be given in the XML ﬁle directly instead of using source
annotations. Section 4.7 gives the full details.

Context Menu
Each resource (LR,PR) has some predeﬁned actions associated with it. These actions appear
in a context menu that appears in GATE Developer when the user right clicks on any of
the resources. For example if the selected resource is a Processing Resource, there will
be at least four actions available in its context menu: 1. Close 2. Hide 3. Rename and 4.
Reinitialize. New actions in addition to the predeﬁned actions can be added by implementing
the gate.gui.ActionsPublisher interface in either the LR/PR itself or in any associated VR.
Then the user has to implement the following method.

public List getActions() {
return actions;
}

Here the variable actions should contain a list of instances of type javax.swing.AbstractAction.
A string passed in the constructor of an AbstractAction object appears in the context menu.
Adding a null element adds a separator in the menu.

274

Developing GATE

Listeners
There are at least four important listeners which should be implemented in order to listen
to the various relevant events happening in the background. These include:
CreoleListener
Creole-register keeps information about instances of various resources and refreshes
itself on new additions and deletions. In order to listen to these events, a class should
implement the gate.event.CreoleListener. Implementing CreoleListener requires users
to implement the following methods:
– public void resourceLoaded(CreoleEvent creoleEvent);
– public void resourceUnloaded(CreoleEvent creoleEvent);
– public void resourceRenamed(Resource resource, String oldName, String newName);
– public void datastoreOpened(CreoleEvent creoleEvent);
– public void datastoreCreated(CreoleEvent creoleEvent);
– public void datastoreClosed(CreoleEvent creoleEvent);
DocumentListener
A traditional GATE document contains text and a set of annotationSets. To get
notiﬁed about changes in any of these resources, a class should implement the
gate.event.DocumentListener. This requires users to implement the following methods:
– public void contentEdited(DocumentEvent event);
– public void annotationSetAdded(DocumentEvent event);
– public void annotationSetRemoved(DocumentEvent event);
AnnotationSetListener
As the name suggests, AnnotationSet is a set of annotations.
To listen
to the addition and deletion of annotations, a class should implement the
gate.event.AnnotationSetListener and therefore the following methods:
– public void annotationAdded(AnnotationSetEvent event);
– public void annotationRemoved(AnnotationSetEvent event);
AnnotationListener
Each annotation has a featureMap associated with it, which contains a set of feature
names and their respective values. To listen to the changes in annotation, one needs
to implement the gate.event.AnnotationListener and implement the following method:
– public void annotationUpdated(AnnotationEvent event);

Developing GATE

275

Figure 12.1: GATE GUI

12.3.4

Writing a New VR

Each resource (PR and LR) can have its own associated visual resource. When double
clicked, the resource’s respective visual resource appears in GATE Developer. The GATE
Developer GUI is divided into three visible parts (See Figure 12.1). One of them contains
a tree that shows the loaded instances of resources. The one below this is used for various
purposes - such as to display document features and that the execution is in progress. This
part of the GUI is referred to as ‘small’. The third and the largest part of the GUI is referred
to as ‘large’. One can specify which one of these two should be used for displaying a new
visual resource in the creole.xml.

Class Deﬁnition
Below we show a template class deﬁnition, which can be used in order to write a new Visual
Resource.
1

package example . gui ;

2
3
4
5

import gate .*;
import gate . creole .*;
import gate . creole . metadata .*;

6
7
8
9
10
11
12
13

/
An e x a m p l e V i s u a l R e s o u r c e f o r t h e New P l u g i n
Note t h a t h e r e we e x t e n d s t h e A b s t r a c t V i s u a l R e s o u r c e c l a s s .
The @ C r e o l e R e s o u r c e a n n o t a t i o n a s s o c i a t e s t h i s VR w i t h t h e
u n d e r l y i n g PR t y p e i t d i s p l a y s .
/
@CreoleResource ( name = " Visual resource for new plugin " ,

276

14
15
16
17

Developing GATE

guiType = GuiType . LARGE ,
resourceDisplaye d = " example . NewPlugin " ,
mainViewer = true )
public class NewPluginVR extends A b s t r a c t V i s u a l R e s o u r c e {

18

/

19

An I n i t method c a l l e d when t h e GUI i s
the f i r s t time

20
21

initialized

for

/
public Resource init () {
// initialize GUI Components
return this ;
}

22
23
24
25
26
27

/

28

Here t a r g e t i s t h e PR c l a s s t o w h i c h t h i s V i s u a l R e s o u r c e
b e l o n g s . T h i s method i s c a l l e d a f t e r t h e i n i t ( ) method .

29
30

/
public void setTarget ( Object target ) {
// check if the target is an instance of what you expected
// and initialize local data structures if required
}

31
32
33
34
35
36

}

Every document has its own document viewer associated with it. It comes with a single
component that shows the text of the original document. GATE provides a way to attach new
GUI plugins to the document viewer. For example AnnotationSet viewer, AnnotationList
viewer and Co-Reference editor. These are the examples of DocumentViewer plugins shipped
as part of the core GATE build. These plugins can be displayed either on the right or on
top of the document viewer. They can also replace the text viewer in the center (See ﬁgure
12.1). A separate button is added at the top of the document viewer which can be pressed
to display the GUI plugin.
Below we show a template class deﬁnition, which can be used to develop a new DocumentViewer plugin.
1
2

/

3

Note t h a t t h e c l a s s n e e d s t o e x t e n d s t h e A b s t r a c t D o c u m e n t V i e w c l a s s
/
@CreoleResource
public class D o c u m e n t V i e w e r P l u g i n extends A b s t r a c t D o c u m e n t V i e w {

4
5
6
7
8

/

I m p l e m e n t e r s s h o u l d o v e r r i d e t h i s method and u s e i t
p o p u l a t i n g t h e GUI .

9
10
11
12
13

/
public void initGUI () {
// write code to initialize GUI
}

14
15
16

/ Returns the type of t h i s view
public int getType () {

/

for

Developing GATE

277

// it can be any of the following constants
// from the gate.gui.docview.DocumentView
// CENTRAL, VERTICAL, HORIZONTAL

17
18
19

}

20
21

/ R e t u r n s t h e a c t u a l UI component t h i s v i e w r e p r e s e n t s .
public Component getGUI () {
// return the top level GUI component
}

22
23
24
25

/

26

/ T h i s method c a l l e d w h e n e v e r v i e w b e c o m e s a c t i v e . /
public void registerHooks () {
// register listeners
}

27
28
29
30
31

/ T h i s method c a l l e d w h e n e v e r v i e w b e c o m e s i n a c t i v e .
public void unregisterHooks () {
// do nothing
}

32
33
34
35
36

/

}

12.3.5

Adding Plugins to the Nightly Build

If you add a new plugin and want it to be part of the build process, you should create a
build.xml ﬁle with targets ‘build’, ‘test’, ‘distro.prepare’, ‘javadoc’ and ‘clean’. The build
target should build the JAR ﬁle, test should run any unit tests, distro.prepare should clean
up any intermediate ﬁles (e.g. the classes/ directory) and leave just what’s in Subversion,
plus the compiled JAR ﬁle and javadocs. The clean target should clean up everything,
including the compiled JAR and any generated sources, etc. You should also add your
plugin to ‘plugins.to.build’ in the top-level build.xml to include it in the build. This is by
design - not all the plugins have build ﬁles, and of the ones that do, not all are suitable for
inclusion in the nightly build (viz. SUPPLE, Section 16.3).
Note that if you are currently building gate by doing ‘ant jar’, be aware that this does not
build the plugins. Running just ‘ant’ or ‘ant all’ will do so.

12.4

Updating this User Guide

The GATE User Guide is maintained in the GATE subversion repository at SourceForge. If
you are a developer at Sheﬃeld you do not need to check out the userguide explicitly, as it
will appear under the tao directory when you check out sale. For others, you can check it
out as follows:
svn checkout https://svn.sourceforge.net/svnroot/gate/userguide/trunk userguide

278

Developing GATE

A
The user guide is written in LTEX and translated to PDF using pdflatex and to HTML
using tex4ht. The main ﬁle that ties it all together is tao_main.tex, which deﬁnes the
various macros used in the rest of the guide and \inputs the other .tex ﬁles, one per
chapter.

12.4.1

Building the User Guide

You will need:
A standard POSIX shell environment including GNU Make. On Windows this generally means Cygwin, on Mac OS X the XCode developer tools and on Unix the relevant
packages from your distribution.
A copy of the userguide sources (see above).
A
A L TEX installation, including pdﬂatex if you want to build the PDF version, and
tex4ht if you want to build the HTML. MiKTeX should work for Windows, texlive
(available in MacPorts) for Mac OS X, or your choice of package for Unix.

The BibTeX database big.bib.
It must be located in the directory above
where you have checked out the userguide, i.e. if the guide sources are in
/home/bob/svn/userguide then big.bib needs to go in /home/bib/svn. Sheﬃeld
developers will ﬁnd that it is already in the right place, under sale, others will need
to download it from http://gate.ac.uk/sale/big.bib.
The ﬁle http://gate.ac.uk/sale/utils.tex.
A bit of luck.
Once these are all assembled it should be a case of running make to perform the actual build.
To build the PDF do make tao.pdf, for the one page HTML do make index.html and for
the several pages HTML do make split.html.
The PDF build generally works without problems, but the HTML build is known to hang
on some machines for no apparent reason. If this happens to you try again on a diﬀerent
machine.

12.4.2

Making Changes to the User Guide

To make changes to the guide simply edit the relevant .tex ﬁles, make sure the guide still
builds (at least the PDF version), and check in your changes to the source ﬁles only.
Please do not check in your own built copy of the guide, the oﬃcial user guide builds are
produced by a Hudson continuous integration server in Sheﬃeld.

Developing GATE

279

If you add a section or subsection you should use the \sect or \subsect commands rather
than the normal LaTeX \section or \subsection. These shorthand commands take an
optional ﬁrst parameter, which is the label to use for the section and should follow the
pattern of existing labels. The label is also set as an anchor in the HTML version of the
guide. For example a new section for the ‘Fish’ plugin would go in misc-creole.tex with
a heading of:
\sect[sec:misc-creole:fish]{The Fish Plugin}

and would have the persistent URL http://gate.ac.uk/userguide/sec:misc-creole:fish.
If your changes are to document a bug ﬁx or a new (or removed) feature then you should
also add an entry to the change log in recent-changes.tex. You should include a reference
to the full documentation for your change, in the same way as the existing changelog entries
do. You should ﬁnd yourself adding to the changelog every time except where you are just
tidying up or rewording existing documentation. Unlike in the other source ﬁles, if you add
a section or subsection you should use the \rcSect or \rcSubsect. Recent changes appear
both in the introduction and the appendix, so these commands enable nesting to be done
appropriately.
Section/subsection labels should comprise ‘sec’ followed by the chapter label and a descriptive
section identiﬁer, each colon-separated. New chapter labels should begin ‘chap:’.
Try to avoid changing chapter/section/subsection labels where possible, as this may break
links to the section. If you need to change a label, add it in the ﬁle ‘sections.map’. Entries
in this ﬁle are formatted one per line, with the old section label followed by a tab followed
by the new section label.
The quote marks used should be ‘ and ’.
Titles should be in title case (capitalise the ﬁrst word, nouns, pronouns, verbs, adverbs and
adjectives but not articles, conjunctions or prepositions). When referring to a numbered
chapter, section, subsection, ﬁgure or table, capitalise it, e.g. ‘Section 3.1’. When merely
using the words chapter, section, subsection, ﬁgure or table, e.g. ‘the next chapter’, do not
capitalise them. Proper nouns should be capitalised (‘Java’, ‘Groovy’), as should strings
where the capitalisation is signiﬁcant, but not terms like ‘annotation set’ or ‘document’.
The user guide is rebuilt automatically whenever changes are checked in, so your change
should appear in the online version of the guide within 20 or 30 minutes.

280

Developing GATE

Part III
CREOLE Plugins

281

Chapter 13
Gazetteers
...neurobiologists still go on openly studying reﬂexes and looking under the hood,
not huddling passively in the trenches. Many of them still keep wondering: how
does the inner life arise? Ever puzzled, they oscillate between two major ﬁctions:
(1) The brain can be understood; (2) We will never come close. Meanwhile they
keep pursuing brain mechanisms, partly from habit, partly out of faith. Their
premise: The brain is the organ of the mind. Clearly, this three-pound lump of
tissue is the source of our ‘insight information’ about our very being. Somewhere
in it there might be a few hidden guidelines for better ways to lead our lives.
Zen and the Brain, James H. Austin, 1998 (p. 6).

13.1

Introduction to Gazetteers

A gazetteer consists of a set of lists containing names of entities such as cities, organisations,
days of the week, etc. These lists are used to ﬁnd occurrences of these names in text, e.g.
for the task of named entity recognition. The word ‘gazetteer’ is often used interchangeably
for both the set of entity lists and for the processing resource that makes use of those lists
to ﬁnd occurrences of the names in text.
When a gazetteer processing resource is run on a document, annotations of type Lookup are
created for each matching string in the text. Gazetteers usually do not depend on Tokens
or on any other annotation and instead ﬁnd matches based on the textual content of the
document. (the Flexible Gazetteer, described in section 13.7, being the exception to
the rule). This means that an entry may span more than one word and may start or end
within a word. If a gazetteer that directly works on text does respect word boundaries, the
way how word boundaries are found might diﬀer from the way the GATE tokeniser ﬁnds
word boundaries. A Lookup annotation will only be created if the entire gazetteer entry is
matched in the text. The details of how gazetteer entries match text depend on the gazetteer
283

284

Gazetteers

processing resource and its parameters. In this chapter, we will cover several gazetteers.

13.2

ANNIE Gazetteer

The rest of this introductory section describes the ANNIE Gazetteer which is part of ANNIE
and also described in section 6.3. The ANNIE gazetteer is part of and proved by the ANNIE
plugin.
Each individual gazetteer list is a plain text ﬁle, with one entry per line.
Below is a section of the list for units of currency:
Ecu
European Currency Units
FFr
Fr
German mark
German marks
New Taiwan dollar
New Taiwan dollars
NT dollar
NT dollars

An index ﬁle (usually called lists.def) is used to describe all such gazetteer list ﬁles that
belong together. Each gazetteer list should reside in the same directory as the index ﬁle.
The gazetteer index ﬁles describes for each list the major type and optionally, a minor type
and a language, separated by colons. In the example below, the ﬁrst column refers to the
list name, the second column to the major type, and the third to the minor type. These lists
are compiled into ﬁnite state machines. Any text strings matched by these machines will be
annotated with features specifying the major and minor types.
currency_prefix.lst:currency_unit:pre_amount
currency_unit.lst:currency_unit:post_amount
date.lst:date:specific_date
day.lst:date:day
monthen.lst:date:month:en
monthde.lst:date:month:de
season.lst:date:season

The major and minor type as well as the language will be added as features to only Lookup
annotation generated from a matching entry from the respective list. For example, if an entry
from the currency unit.lst gazetteer list matches some text in a document, the gazetteer

Gazetteers

285

processing resource will generate a Lookup annotation spanning the matching text and assign
the features major="currency unit" and minor="post amount" to that annotation.
Grammar rules (JAPE rules) can specify the types to be identiﬁed in particular circumstances. The major and minor types enable this identiﬁcation to take place, by giving access
to items stored in particular lists or combinations of lists.
For example, if a day needs to be identiﬁed, the minor type ‘day’ would be speciﬁed in the
grammar, in order to match only information about speciﬁc days. If any kind of date needs
to be identiﬁed, the major type ‘date’ would be speciﬁed. This might include weeks, months,
years etc. as well as days of the week, and would give access to all the items stored in day.lst,
month.lst, season.lst, and date.lst in the example shown.

13.2.1

Creating and Modifying Gazetteer Lists

Gazetteer lists can be modiﬁed using any text editor or an editor inside GATE when you
double-click on the gazetteer in the resources tree. Use of an editor that can edit Unicode
UTF-8 ﬁles (e.g. the GATE Unicode editor) is advised, however, in order to ensure that the
lists are stored as UTF-8, which will minimise any language encoding problems, particularly
if e.g. accents, umlauts or characters from non-Latin scripts are present.
To create a new list, simply add an entry for that list to the deﬁnitions ﬁle and add the new
list in the same directory as the existing lists.
After any modiﬁcations have been made in an external editor, ensure that you reinitialise
the gazetteer PR in GATE, if one is already loaded, before rerunning your application.

13.2.2

ANNIE Gazetteer Editor

This editor is similar to GAZE editor but aims at being simpler by restricting itself to ANNIE
Gazetteers. To open it, double-click on the gazetteer in the resources tree.
It is composed of two tables:
a left table with 4 columns (List name, Major, Minor, Language) for the index, usually
a .def ﬁle
a right table with 1+2*n columns (Value, Feature 1, Value 1...Feature n, Value n) for
the lists, usually .lst ﬁles
When selecting a list in the left table you get its content displayed in the right table.

286

Gazetteers

Figure 13.1: ANNIE Gazetteer Editor

You can sort both tables by clicking on their column headers. A text ﬁeld ‘Filter’ at the
bottom of the right table allows to display only the rows that contain the expression you
typed.
To edit a value in a table, double click on a cell or press F2 then press Enter when ﬁnished
editing the cell. To add a new row in both tables use the text ﬁeld at the top and press
Enter or use the ‘New’ button next to it. When adding a new list you can select from the
list of existing gazetteer lists in the current directory or type a new ﬁle name. To delete a
row, press Shift+Delete or use the context menu. To delete more than one row select them
before.
You can reload a modiﬁed list by selecting it and right-clicking for the context menu item
‘Reload List’ or by pressing Control+R. When a list is modiﬁed its name in the left table is
coloured in red.
If you have set ‘gazetteerFeatureSeparator’ parameter then the right table will show a ‘Feature’ and ‘Value’ columns for each feature. To add a new couple of columns use the button
‘Add Cols’.
Note that in the left table, you can only select one row at a time.
The gazetteer like other language resource has a context menu in the resources tree to
‘Reinitialise’, ‘Save’ or ‘Save as...’ the resource.
The right table has a context menu for the current selection to help you creating new
gazetteer. It is similar with the actions found in a spreadsheet application like ‘Fill Down
Selection’, ‘Clear Selection’, ‘Copy Selection’, ‘Paste Selection’, etc.

Gazetteers

13.3

287

Gazetteer Visual Resource - GAZE

Gaze is a tool for editing the gazetteer lists , deﬁnitions and mapping to ontology. It
is suitable for use both for Plain/Linear Gazetteers (Default and Hash Gazetteers) and
Ontology-enabled Gazetteers (OntoGazetteer). The Gazetteer PR associated with the viewer
is reinitialised every time a save operation is performed. Note that GAZE does not scale up
to very large lists (we suggest not using it to view over 40,000 entries and not to copy inside
more than 10, 000 entries).
Gaze is part of and provided by the ANNIE plugin. To make it possible to visualize gazetteers
with the Gaze visualizer, the ANNIE plugin must be loaded ﬁrst. Double clicking on a
gazetteer PR that uses a gazetteer deﬁnition (index) ﬁle will display the contents of the
gazetteer in the main window. The ﬁrst pane will display the deﬁnition ﬁle, while the right
pane will display whichever gazetteer list has been selected from it.
A gazetteer list can be modiﬁed simply by typing in it. it can be saved by clicking the Save
button. When a list is saved, the whole gazetteer is automatically reinitialised (and will be
ready for use in GATE immediately).
To edit the deﬁnition ﬁle, right click inside the pane and choose from the options (Inset, Edit,
Remove). A pop-up menu will appear to guide you through the remaining process. Save the
deﬁnition ﬁle by selecting Save. Again, the gazetteer will be reinitialised automatically.

13.3.1

Display Modes

The display mode depends on the type of gazetteer loaded in the VR. The mode in which
Linear/Plain Gazetteers are loaded is called Linear/Plain Mode. In this mode, the Linear
Deﬁnition is displayed in the left pane, and the Gazetteer List is displayed in the right pane.
The Ontology/Extended mode is on when the displayed gazetteer is ontology-aware, which
means that there exists a mapping between classes in the ontology and lists of phrases. Two
more panes are displayed when in this mode. On the top in the left-most pane there is a tree
view of the ontology hierarchy, and at the bottom the mapping deﬁnition is displayed. This
section describes the Linear/Plain display mode, the Ontology/Extended mode is described
in section 13.5.
Whenever a gazetteer PR that uses a gazetteer deﬁnition (index) ﬁle is loaded, the Gaze
gazetteer visualisation will appear on double-click over the gazetteer in the Processing Resources branch of the Resources Tree.

288

13.3.2

Gazetteers

Linear Deﬁnition Pane

This pane displays the nodes of the linear deﬁnition, and allows manipulation of the whole
deﬁnition as a ﬁle, as well as the single nodes. Whenever a gazetteer list is modiﬁed, its
node in the linear deﬁnition is coloured in red.

13.3.3

Linear Deﬁnition Toolbar

All the functionality explained in this section (New, Load, Save, Save As) is accessible also
via File — Linear Deﬁnition in the menu bar of Gaze.
New – Pressing New invokes a ﬁle dialog where the location of the new deﬁnition is speciﬁed.
Load – Pressing Load invokes a ﬁle dialog, and after locating the new deﬁnition it is loaded
by pressing Open.
Save – Pressing Save saves the deﬁnition to the location from which it has been read.
Save As – Pressing Save As allows another location to be chosen, and the deﬁnition saved
there.

13.3.4

Operations on Linear Deﬁnition Nodes

Double-click node – Double-clicking on a deﬁnition node forces the displaying of the
gazetteer list of the node in the right-most pane of the viewer.
Insert – On right-click over a node and choosing Insert, a dialog is displayed, requesting
List, Major Type, Minor Type and Languages. The mandatory ﬁelds are List and Major
Type. After pressing OK, a new linear node is added to the deﬁnition.
Remove – On right-click over a node and choosing Remove, the selected linear node is
removed from the deﬁnition.
Edit – On right-click over a node and choosing Edit a dialog is displayed allowing changes
of the ﬁelds List, Major Type, Minor Type and Languages.

13.3.5

Gazetteer List Pane

The gazetteer list pane has a toolbar with similar to the linear deﬁnition’s buttons (New,
Load, Save, Save As). They work as predicted by their names and as explained in the Linear
Deﬁnition Pane section, and are also accessible from File / Gazetteer List in the menu bar
of Gaze. The only addition is Save All which saves all modiﬁed gazetteer lists. The editing

Gazetteers

289

of the gazetteer list is as simple as editing a text ﬁle. One could use Ctrl+A to select the
whole list, Ctrl+C to copy the selected, Ctrl+V to paste it, Del to delete the selected text
or a single character, etc.

13.3.6

Mapping Deﬁnition Pane

The mapping deﬁnition is displayed one mapping node per row. It consists of a gazetteer
list, ontology URL, and class id. The content of the gazetteer list in the node is accessible
through double-clicking. It is displayed in the Gazetteer List Pane. The toolbar allows the
creation of a new deﬁnition (New), the loading of an existing one (Load), saving to the same
or new location (Save/Save As). The functionality of the toolbar buttons is also available
via File.

13.4

OntoGazetteer

The Ontogazetteer, or Hierarchical Gazetteer, is a processing resource which can associate the
entities from a speciﬁc gazetteer list with a class in a GATE ontology language resource. The
OntoGazetteer assigns classes rather than major or minor types, and is aware of mappings
between lists and class IDs. The Gaze visual resource can display the lists, ontology mappings
and the class hierarchy of the ontology for a OntoGazetteer processing resource and provides
ways of editing these components.

13.5

Gaze Ontology Gazetteer Editor

This section describes the Gaze gazetteer editor when it displays an OntoGazetteer processing
resource. The editor consists of two parts: one for the editing of the lists and the mapping
of lists and one for editing the ontology. These two parts are described in the following
subsections.

13.5.1

The Gaze Gazetteer List and Mapping Editor

This is a VR for editing the gazetteer lists, and mapping them to classes in an ontology. It
provides load/store/edit for the lists, load/store/edit for the mapping information, loading
of ontologies, load/store/edit for the linear deﬁnition ﬁle, and mapping of the lists ﬁle to the
major type, minor type and language.
Left pane: A single ontology is visualized in the left pane of the VR. The mapping between
a list and a class is displayed by showing the list as a subclass with a diﬀerent icon. The

290

Gazetteers

mapping is speciﬁed by drag and drop from the linear deﬁnition pane (in the middle) and/or
by right click menu.
Middle pane: The middle pane displays the nodes/lines in the linear deﬁnition ﬁle. By
double clicking on a node the corresponding list is opened. Editing of the line/node is done
by right clicking and choosing edit: a dialogue appears (lower part of the scheme) allowing
the modiﬁcation of the members of the node.
Right pane: In the right pane a single gazetteer list is displayed. It can be edited and parts
of it can be cut/copied/pasted.

13.5.2

The Gaze Ontology Editor

Note: to edit ontologies within gate, the more recent ontology viewer editor provided by the
Ontology Tools which provides many more features can be used, see section 14.5.
This is a VR for editing the class hierarchy of an ontology. it provides storing to and loading
from RDF/RDFS, and provides load/edit/store of the class hierarchy of an ontology.
Left pane: The various ontologies loaded are listed here. On double click or right click and
edit from the menu the ontology is visualized in the Right pane.
Right pane: Besides the visualization of the class hierarchy of the ontology the following
operations are allowed:
expanding/collapsing parts of the ontology
adding a class in the hierarchy: by right clicking on the intended parent of the new
class and choosing add sub class.
removing a class: via right clicking on the class and choosing remove.
As a result of this VR, the ontology deﬁnition ﬁle is aﬀected/altered.

13.6

Hash Gazetteer

The Hash Gazetteer is a gazetteer implemented by the OntoText Lab (http://www.ontotext.com/).
Its implementation is based on simple lookup in several java.util.HashMap objects, and is
inspired by the strange idea of Atanas Kiryakov, that searching in HashMaps will be faster
than a search in a Finite State Machine (FSM). The Hash Gazetteer processing resource is
part of the ANNIE plugin.

Gazetteers

291

This gazetteer processing resource is implemented in the following way: Every phrase i.e.
every list entry is separated into several parts. The parts are determined by the whitespaces
lying among them. e.g. the phrase : ”form is emptiness” has three parts : “form”, “is”, and
“emptiness”. There is also a list of HashMaps: mapsList which has as many elements as
the longest (in terms of ‘count of parts’) phrase in the lists. So the ﬁrst part of a phrase is
placed in the ﬁrst map. The ﬁrst part + space + second part is placed in the second map,
etc. The full phrase is placed in the appropriate map, and a reference to a Lookup object is
attached to it.
On ﬁrst sight it seems that this algorithm is certainly much more memory-consuming than a
ﬁnite state machine (FSM) with the parts of the phrases as transitions, but this is actually not
so important since the average length of the phrases (in parts) in the lists is 1.1. On the other
hand, one advantage of the algorithm is that, although unconventional, on average it takes
four times less memory and works three times faster than an optimized FSM implementation.

13.6.1

Prerequisites

The phrases to be recognised should be listed in a set of ﬁles, one for each type of occurrence
(as for the standard gazetteer).
The gazetteer is built with the information from a ﬁle that contains the set of lists (which
are ﬁles as well) and the associated type for each list. The ﬁle deﬁning the set of lists should
have the following syntax: each list deﬁnition should be written on its own line and should
contain:
the ﬁle name (required)
the major type (required)
the minor type (optional)
the language(s) (optional)
The elements of each deﬁnition are separated by ‘:’. The following is an example of a valid
deﬁnition:
personmale.lst:person:male:english

Each ﬁle named in the lists deﬁnition ﬁle is just a list containing one entry per line.
When this gazetteer is run over some input text (a GATE document) it will generate annotations of type Lookup having the attributes speciﬁed in the deﬁnition ﬁle.

292

Gazetteers

13.6.2

Parameters

The Hash Gazetteer processing resource allows the speciﬁcation of the following parameters
when it is created:
caseSensitive: this can be switched between true and false to indicate if matches should
be done in a case-sensitive way.
encoding: the encoding of the gazetteer lists
listsURL: the URL of the list deﬁnitions (index) ﬁle, i.e. the ﬁle that contains the ﬁlenames,
major types and optionally minor types and languages of all the list ﬁles.
There is one run-time parameter, annotationSetName that allows the speciﬁcation of the
annotation set in which the Lookup annotations will be created. If nothing is speciﬁed the
default annotation set will be used.

13.7

Flexible Gazetteer

The Flexible Gazetteer provides users with the ﬂexibility to choose their own customized
input and an external Gazetteer. For example, the user might want to replace words in the
text with their base forms (which is an output of the Morphological Analyser) or to segment
a Chinese text (using the Chinese Tokeniser) before running the Gazetteer on the Chinese
text.
The Flexible Gazetteer performs lookup over a document based on the values of an arbitrary
feature of an arbitrary annotation type, by using an externally provided gazetteer. It is
important to use an external gazetteer as this allows the use of any type of gazetteer (e.g.
an Ontological gazetteer).
Input to the Flexible Gazetteer:
Runtime parameters:
Document – the document to be processed
inputAnnotationSetName The annotationSet where the Flexible Gazetteer should
search for the AnnotationType.feature speciﬁed in the inputFeatureNames.
outputAnnotationSetName The AnnotationSet where Lookup annotations should
be placed.
Creation time parameters:

Gazetteers

293

inputFeatureNames – when selected, these feature values are used to replace the
corresponding original text. A temporary document is created from the values of the
speciﬁed features on the speciﬁed annotation types. For example: for Token.string
the temporary document will have the same content as the original one but all the
SpaceToken annotations will have been replaced by single spaces.
gazetteerInst – the actual gazetteer instance, which should run over a temporary
document. This generates the Lookup annotations with features. This must be an
instance of gate.creole.gazetteer.Gazetteer which has already been created. All
such instances will be shown in the dropdown menu for this parameter in GATE
Developer.
Once the external gazetteer has annotated text with Lookup annotations, Lookup annotations on the temporary document are converted to Lookup annotations on the original
document. Finally the temporary document is deleted.

13.8

Gazetteer List Collector

The gazetteer list collector collects occurrences of entities directly from a set of annotated
training texts, and populates gazetteer lists with the entities. The entity types and structure
of the gazetteer lists are deﬁned as necessary by the user. Once the lists have been collected,
a semantic grammar can be used to ﬁnd the same entities in new texts.
An empty list must be created ﬁrst for each annotation type, if no list exists already. The
set of lists must be loaded into GATE before the PR can be run. If a list already exists, the
list will simply be augmented with any new entries. The list collector will only collect one
occurrence of each entry: it ﬁrst checks that the entry is not present already before adding
a new one.
There are 4 runtime parameters:
annotationTypes: a list of the annotation types that should be collected
gazetteer: the gazetteer where the results will be stored (this must be already loaded
in GATE)
markupASname: the annotation set from which the annotation types should be collected
theLanguage: sets the language feature of the gazetteer lists to be created to the
appropriate language (in the case where lists are collected for diﬀerent languages)
Figure 13.2 shows a screenshot of a set of lists collected automatically for the Hindi language.
It contains 4 lists: Person, Organisation, Location and a list of stopwords. Each list has a

294

Gazetteers

majorType whose value is the type of list, a minorType ‘inferred’ (since the lists have been
inferred from the text), and the language ‘Hindi’.

Figure 13.2: Lists collected automatically for Hindi
The list collector also has a facility to split the Person names that it collects into their
individual tokens, so that it adds both the entire name to the list, and adds each of the
tokens to the list (i.e. each of the ﬁrst names, and the surname) as a separate entry. When
the grammar annotates Persons, it can require them to be at least 2 tokens or 2 consecutive Person Lookups. In this way, new Person names can be recognised by combining
a known ﬁrst name with a known surname, even if they were not in the training corpus.
Where only a single token is found that matches, an Unknown entity is generated, which
can later be matched with an existing longer name via the orthomatcher component which
performs orthographic coreference between named entities. This same procedure can also
be used for other entity types. For example, parts of Organisation names can be combined
together in diﬀerent ways. The facility for splitting Person names is hardcoded in the ﬁle
gate/src/gate/creole/GazetteerListsCollector.java and is commented.

13.9

OntoRoot Gazetteer

OntoRoot Gazetteer is a type of a dynamically created gazetteer that is, in combination
with few other generic GATE resources, capable of producing ontology-based annotations
over the given content with regards to the given ontology. This gazetteer is a part of
‘Gazetteer Ontology Based’ plugin that has been developed as a part of the TAO project.

Gazetteers

13.9.1

295

How Does it Work?

To produce ontology-based annotations i.e. annotations that link to the speciﬁc concepts
or relations from the ontology, it is essential to pre-process the Ontology Resources (e.g.,
Classes, Instances, Properties) and extract their human-understandable lexicalisations.
As a precondition for extracting human-understandable content from the ontology, ﬁrst a
list of the following is being created:
names of all ontology resources i.e. fragment identiﬁers

1

and

assigned property values for all ontology resources (e.g., label and datatype property
values)
Each item from the list is further processed so that:
any name containing dash ("-") or underline ("_") character(s) is processed so that
each of these characters is replaced by a blank space. For example, Project_Name or
Project-Name would become a Project Name.
any name that is written in camelCase style is actually split into its constituent words,
so that ProjectName becomes a Project Name (optional).
any name that is a compound name such as ‘POS Tagger for Spanish’ is split so that
both ‘POS Tagger’ and ‘Tagger’ are added to the list for processing. In this example,
‘for’ is a stop word, and any words after it are ignored (optional).
Each item from this list is analysed separately by the Onto Root Application (ORA) on
execution (see ﬁgure 13.3). The Onto Root Application ﬁrst tokenises each linguistic term,
then assigns part-of-speech and lemma information to each token.
As a result of that pre-processing, each token in the terms will have additional feature named
‘root’, which contains the lemma as created by the morphological analyser. It is this lemma
or a set of lemmas which are then added to the dynamic gazetteer list, created from the
ontology.
For instance, if there is a resource with a short name (i.e., fragment identiﬁer) ProjectName,
without any assigned properties the created list before executing the OntoRoot gazetteer
collection will contain the following strings:
‘ProjectName’,
1
An ontology resource is usually identiﬁed by an URI concatenated with a set of characters starting with
‘#’. This set of characters is called fragment identiﬁer. For example, if the URI of a class representing
GATE POS Tagger is: ’http://gate.ac.uk/ns/gate-ontology#POSTagger’, the fragment identiﬁer will be
’POSTagger’.

296

Gazetteers

Figure 13.3: Building Ontology Resource Root (OntoRoot) Gazetteer from the Ontology

‘Project Name’ after separating camelCased word and
‘Name’ after applying heuristic rules.
Each of the item from the list is then analysed separately and the results would be the same
as the input strings, as all of entries are nouns given in singular form.

13.9.2

Initialisation of OntoRoot Gazetteer

To initialise the gazetteer there are few mandatory parameters:
Ontology to be processed;
Tokeniser, POS Tagger and GATE Morphological Analyser to be used during processing.
and few optional ones:
useResourceUri, default is set to true - should this gazetteer analyse resource URIs or
not;
considerProperties, default is set to true - should this gazetteer consider properties or
not;

Gazetteers

297

propertiesToInclude - checked only if considerProperties is set to true - this parameter
contains the list of property names (URIs) to be included, comma separated;
propertiesToExclude - checked only if considerProperties is set to true - this parameter
contains the list of property names to be excluded, comma separated;
caseSensitive, default set to be false -should this gazetteer diﬀerentiate on case;
separateCamelCasedWords, default set to true - should this gazetteer separate emphcamelCased words, e.g. ‘ProjectName’ into ‘Project Name’;
considerHeuristicRules, default set to false - should this gazetteer consider several
heuristic rules or not. Rules include splitting the words containing spaces, and using
prepositions as stop words; for example, if ’pos tagger for Spanish’ would be analysed,
‘for’ would be considered as a stop word; heuristically derived would be ‘pos tagger’
and this would be further used to add ‘pos tagger’ to the gazetteer list, with a feature
emphheuristical level set to be 0, and ‘tagger’ with emphheuristical level 1; at runtime
lower heuristical level should be preferred. NOTE: setting considerHeuristicRules to
true can cause a lot of noise for some ontologies and is likely to require implementing
an additional ﬁltering resource that will prefer the annotations with the lower heuristic
level;

13.9.3

Simple steps to run OntoRoot Gazetteer

OntoRoot Gazetteer is a part of the Gazetteer Ontology Based plugin.

Easy way
For a quick start with the OntoRoot Gazetteer, consider running it from the GATE Developer
(GATE GUI):
Start GATE
Load a sample application from resources folder (exampleApp.xgapp). This will load
CAT App application.
Run CAT App application and open query-doc to see a set of Lookup annotations
generated as a result (see Figure 13.4).

298

Gazetteers

Figure 13.4: Sample ontology-based annotation as a result of running OntoRoot Gazetteer.
Feature URI refers to the URI of the ontology resource, while type identiﬁes the type of the
resource such as class, instance, property, or datatypePropertyValue

Hard way
OntoRoot Gazetteer can easily be set up to be used with any ontology. To generate a GATE
application which demonstrates the use of the OntoRoot Gazetteer, follow these steps:
1. Start GATE
2. Load necessary plugins: Click on Manage CREOLE plugins and check the following:
Tools
Ontology
Ontology Based Gazetteer
Ontology Tools (optional); this parameter is required in order to view ontology
using the GATE Ontology Editor.
ANNIE.
Make sure that these plugins are loaded from GATE/plugins/[plugin name] folder.
3. Load an ontology. Right click on Language Resource, and select the last option
to create an OWLIM Ontology LR. Specify the format of the ontology, for example rdfXmlURL, and give the correct path to the ontology: either the absolute path on your local machine such as c:/myOntology.owl or the URL such as
http://gate.ac.uk/ns/gate-ontology. Specify the name such as myOntology (this
is optional).
4. Create Processing Resources: Right click on the Processing Resource and create the
following PRs (with default parameters):

Gazetteers

299

Document Reset PR
ANNIE English Tokeniser
ANNIE POS Tagger
GATE Morphological Analyser
RegEx Sentence Splitter (or ANNIE Sentence Splitter)
5. Create an Onto Root Gazetteer and set the init parameters. Mandatory ones are:
Ontology: select previously created myOntology;
Tokeniser : select previously created Tokeniser;
POS Tagger : select previously created POS Tagger;
Morpher : select previously created Morpher.
OntoRoot gazetteer is quite ﬂexible in that it can be conﬁgured using the optional
parameters. List of all parameters is detailed in Section 13.9.2.
When all parameters are set click OK. It can take some time to initialise OntoRoot Gazetteer. For example, loading GATE knowledge base from
http://gate.ac.uk/ns/gate-kb takes around 6-15 seconds. Larger ontologies can
take much longer.
6. Create another PR which is a Flexible Gazetteer. As init parameters it is mandatory to
select previously created OntoRoot Gazetteer for gazetteerInst. For another parameter,
inputFeatureNames, click on the button on the right and when prompt with a window,
add ’Token.root’ in the provided textbox, then click Add button. Click OK, give name
to the new PR (optional) and then click OK.
7. Create an application. Right click on Application, then New Pipeline (or Corpus
Pipeline). Add the following PRs to the application in this particular order:
Document Reset PR
RegEx Sentence Splitter (or ANNIE Sentence Splitter)
ANNIE English Tokeniser
ANNIE POS Tagger
GATE Morphological Analyser
Flexible Gazetteer
8. Create a document to process with the new application; for example, if the ontology
was http://gate.ac.uk/ns/gate-kb, then the document could be the GATE home
page: http://gate.ac.uk. Run application and then investigate the results further.
All annotations are of type Lookup, with additional features that give details about
the resources they are referring to in the given ontology.

300

Gazetteers

13.10

Large KB Gazetteer

The large KB gazetteer provides support for ontology-aware NLP. You can load any ontology
from RDF and then use the gazetteer to obtain lookup annotations that have both instance
and class URI.
The large KB gazetteer is available as the plugin Gazetteer LKB.
The current version of the large KB gazetteer does not use GATE ontology language resources. Instead, it uses its own mechanism to load and process ontologies. The current
version is likely to change signiﬁcantly in the near future.
The Large KB gazetteer grew from a component in the semantic search platform Ontotext KIM. The gazetteer is developed by people from the KIM team (see http://nmwiki.
ontotext.com/lkb_gazetteer/team-list.html). You may ﬁnd the name kim left in several places in the source code, documentation or source ﬁles.

13.10.1

Quick usage overview

To use the Large KB gazetteer, set up your dictionary ﬁrst.
The dictionary is a folder with some conﬁguration ﬁles.
Use the samples at
GATE HOME/plugins/Gazetteer LKB/samples as a guide or download a prebuilt dictionary from http://ontotext.com/kim/lkb gazetteer/dictionaries.
Load GATE HOME/plugins/Gazetteer LKB as a CREOLE plugin. See Section 3.5 for
details.
Create a new ‘Large KB Gazetteer’ processing resource (PR). Put the folder of the
dictionary you created in the ‘dictionaryPath’ parameter. You can leave the rest of
the parameters as defaults.
Add the PR to your GATE application. The gazetteer doesn’t require a tokenizer or
the output of any other processing resources.
The gazetteer will create annotations with type ‘Lookup’ and two features; ‘inst’, which
contains the URI of the ontology instance, and ‘class’ which contains the URI of the
ontology class that instance belongs to.

13.10.2

Dictionary setup

The dictionary is a folder with some conﬁguration ﬁles.
GATE HOME/plugins/Gazetteer LKB/samples.

You can ﬁnd samples at

Gazetteers

301

Setting up your own dictionary is easy. You need to deﬁne your RDF ontology and then
specify a SPARQL or SERQL query that will retrieve a subset of that ontology as a dictionary.
conﬁg.ttl is a Turtle RDF ﬁle which conﬁgures a local RDF ontology or connection to a
remote Sesame RDF database.
If you want to see examples of how to use local RDF ﬁles, please check samples/dictionary from local ontology/conﬁg.ttl. The Sesame repository conﬁguration section conﬁgures a local Ontotext SwiftOWLIM database that loads a list of RDF ﬁles. Simply create a list of your RDF ﬁles and reuse the rest of the conﬁguration. The sample conﬁguration support datasets with 10,000,000 triples with acceptable performance. For
working with larger datasets, advanced users can substitute SwiftOWLIM with another
Sesame RDF engine. In that case, make sure you add the necessary JARs to the list
in GATE HOME/plugins/Gazetteer LKB/creole.xml. For example, Ontotext BigOWL is
a Sesame RDF engine that can load billions of triples on desktop hardware.
Since any Sesame repository can be conﬁgured in conﬁg.ttl, the Large KB Gazetteer can extract dictionaries from all signiﬁcant RDF databases. See the page on database compatibility
for more information.
query.txt contains a SPARQL query. You can write any query you like, as long as its
projection contains at least two columns in the following order: label and instance. As an
option, you can also add a third column for the ontology class of the RDF entity. Below you
can see a sample query, which creates a dictionary from the names and the unique identiﬁers
of 10,000 entertainers in DbPedia.
PREFIX opencyc: <http://sw.opencyc.org/2008/06/10/concept/en/>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
SELECT ?Name ?Person WHERE {
?Person a opencyc:Entertainer ; rdfs:label ?Name .
FILTER (lang(?Name) = "en")
} LIMIT 10000

Try this query at the Linked Data Semantic Repository.
When you load the dictionary conﬁguration in GATE for the ﬁrst time, it creates a binary
snapshot of the dictionary. Thereafter it will load only this binary snapshot. If the dictionary conﬁguration is changed, the snapshot will be reinitialized automatically. For more
information, please see the dictionary lifecycle speciﬁcation.

302

Gazetteers

13.10.3

Additional dictionary conﬁguration

The conﬁg.ttl may contain additional dictionary conﬁguration. Such conﬁguration concerns
only the initial loading of the dictionary from the RDF database. The options are still
being determined and more will appear in future versions. They must be placed below the
repository conﬁguration section as attributes of a dictionary conﬁguration. Here is a sample
conﬁg.ttl ﬁle with additional conﬁguration.
# Sesame configuration template for a (proxy for a) remote repository
#
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>.
@prefix rep: <http://www.openrdf.org/config/repository#>.
@prefix hr: <http://www.openrdf.org/config/repository/http#>.
@prefix lkbg: <http://www.ontotext.com/lkb_gazetteer#>.
[] a rep:Repository ;
rep:repositoryImpl [
rep:repositoryType "openrdf:HTTPRepository" ;
hr:repositoryURL <http://ldsr.ontotext.com/openrdf-sesame/repositories/owlim>
];
rep:repositoryID "owlim" ;
rdfs:label "LDSR" .
[] a lkbg:DictionaryConfiguration ;
lkbg:caseSensitivity "CASE_INSENSITIVE" .

13.10.4

Processing Resource Conﬁguration

The following options can be set when the gazetteer PR is initialized:
dictionaryPath; the dictionary folder described above.
forceCaseSensitive; whether the gazetteer should return case-sensitive matches regardless of the loaded dictionary.

13.10.5

Runtime conﬁguration

annotationSetName - The annotation set, which will receive the generated lookup
annotations.
annotationLimit - The maximum number of the generated annotations. NULL or 0
for no limit. Setting limit of the number of the created annotations will reduce the
memory consumption of GATE on large documents. Note that GATE documents

Gazetteers

303

consume gigabytes of memory if there are tens of thousands of annotations in the
document. All PRs that create large number of annotations like the gazetteers and
tokenizers may cause an Out Of Memory error on large texts. Setting that options
limits the amount of memory that the gazetteer will use.

13.10.6

Semantic Enrichment PR

The Semantic Enrichment PR allows adding new data to semantic annotations by querying
external RDF (Linked Data) repositories. It is a companion to the large KB gazetteer that
showcases the usefulness of using Linked Data URI as identiﬁers.
Here a semantic annotation is an annotation that is linked to an RDF entity by having the
URI of the entity in the ‘inst’ feature of the annotation. For all such annotation of a given
type, this PR runs a SPARQL query against the deﬁned repository and puts a commaseparated list of the values mentioned in the query output in the ‘connections’ feature of the
same annotation.
There is a sample pipeline that features the Semantic Enrichment PR.
Parameters
inputASName; the annotation set, which annotation will be processed.
server; the URL of the Sesame 2 HTTP repository. Support for generic SPARQL
endpoints can be implemented if required.
repositoryId; the ID of the Sesame repository.
annotationTypes; a list of types of annotation that will be processed.
query; a SPARQL query pattern.
The query will be processed like this String.format(query, uriFromAnnotation), so you can use parameters like %s or %1 s.
deleteOnNoRelations; whether we want to delete the annotation that weren’t enriched.
Helps to clean up the input annotations.

13.11

The Shared Gazetteer for multithreaded processing

The DefaultGazetteer (and its subclasses such as the OntoRootGazetteer) compiles its
gazetteer data into a ﬁnite state matcher at initialization time. For large gazetteers this
FSM requires a considerable amount of memory. However, once the FSM has been built

304

Gazetteers

then (as long as you do not modify it dynamically using Gaze) it is accessed in a readonly manner at runtime. For a multi-threaded application that requires several identical
copies of its processing resources (see section 7.13), GATE provides a mechanism whereby a
single compiled FSM can be shared between several gazetteer PRs that can then be executed
concurrently in diﬀerent threads, saving the memory that would otherwise be required to
load the lists several times.
This feature is not available in the GATE Developer GUI, as it is only intended for
use in embedded code. To make use of it, ﬁrst create a single instance of the regular
DefaultGazetteer or OntoRootGazetteer:
FeatureMap params = Factory.newFeatureMap();
params.put("listsUrl", listsDefLocation);
LanguageAnalyser mainGazetteer = (LanguageAnalyser)Factory.createResource(
"gate.creole.gazetteer.DefaultGazetteer", params);

Then create any number of SharedDefaultGazetteer instances, passing this regular
gazetteer as a parameter:
FeatureMap params = Factory.newFeatureMap();
params.put("bootstrapGazetteer", mainGazetteer);
LanguageAnalyser sharedGazetteer = (LanguageAnalyser)Factory.createResource(
"gate.creole.gazetteer.SharedDefaultGazetteer", params);

The SharedDefaultGazetteer instance will re-use the FSM that was built by the
mainGazetteer instead of loading its own.

Chapter 14
Working with Ontologies
GATE provides an API for modeling and manipulating ontologies and comes with two plugins
that provide implementations for the API and several tools for editing ontologies and using
ontologies for document annotation.
Ontologies in GATE are classiﬁed as language resources. In order to create an ontology
language resource, the user must ﬁrst load one of the two plugins containing an ontology
implementation.
The following implementations and ontology related tools are provided as plugins:
Plugin Ontology OWLIM2 provides an implementation that is fully backwardscompatible with the implementation that was part of GATE prior to version 5.1 (see
Section 14.4).
Plugin Ontology provides a modiﬁed and current implementation (see Section 14.3).
Unless noted otherwise, all information in this chapter applies to this implementation.
Plugin Ontology Tools provides a simple graphical ontology editor (see Section 14.5)
and OCAT, a tool for interactive ontology based document annotation (see Section 14.6). It also provides a gazetteer processing resource, OntoGaz, that allows
the mapping of linear gazetteers to classes in an ontology (see Section 13.4).
Plugin Gazetteer Ontology Based provides the ‘Onto Root Gazetteer’ for the automatic creating of a gazetteer from an ontology (see Section 13.9)
Plugin Ontology BDM Computation can be used to compute BDM scores (see Section 10.6).
Plugin Gazetteer LKB provides a processing resource for creating annotations based
on the contents of a large ontology.
305

306

Working with Ontologies

GATE ontology support aims to simplify the use of ontologies both within the set of GATE
tools and for programmers using the GATE ontology API. The GATE ontology API hides
the details of the actual backend implementation and allows a simpliﬁed manipulation of
ontologies by modeling ontology resources as easy-to-use Java objects. Ontologies can be
loaded from and saved to various serialization formats.
The GATE ontology support roughly conforms to the representation, manipulation and
inference that conforms to what is supported in OWL-Lite (see http://www.w3.org/TR/
owl-features/). This means that a user can represent information in an ontology that
conforms to OWL-Lite and that the GATE ontology model will provide inferred information
equivalent to what an OWL-Lite reasoner would provide. The GATE ontology model makes
an attempt to also to some extend provide useful information for ontologies that do not
conform to OWL-Lite: RDFS, OWL-DL, OWL-Full or OWL2 ontologies can be loaded but
GATE might ignore part of all contents of those ontologies, or might only provide part of,
or incorrect inferred facts for such ontologies.
The GATE API tries to prevent clients from modifying an ontology that conforms to OWLLite to become OWL-DL or OWL-Full and also tries to prevent or warn about some of
the most common errors that would make the ontology inconsistent. However, the current
implementation is not able to prevent all such errors and has no way of ﬁnding out if an
ontology conforms to OWL-Lite or is inconsistent.

14.1

Data Model for Ontologies

14.1.1

Hierarchies of Classes and Restrictions

Class hierarchy (or taxonomy) plays the central role in the ontology data model. This
consists of a set of ontology classes (represented by OClass objects in the ontology API)
linked by subClassOf, superClassOf and equivalentClassAs relations. Each ontology
class is identiﬁed by an URI (unless it is a restriction or an anonymous class, see below).
The URI of each ontology resource must be unique.
Each class can have a set of superclasses and a set of subclasses; these are used to build the
class hierarchy. The subClassOf and superClassOf relations are transitive and methods
are provided by the API for calculating the transitive closure for each of these relations given
a class. The transitive closure for the set of superclasses for a given class is a set containing
all the superclasses of that class, as well as all the superclasses of its direct superclasses, and
so on until no more are found. This calculation is ﬁnite, the upper bound being the set of all
the classes in the ontology. A class that has no superclasses is called a top class. An ontology
can have several top classes. Although the GATE ontology API can deal with cycles in the
hierarchy graph, these can cause problems for processes using the API and probably indicate
an error in the deﬁnition of the ontology. Also other components of GATE, like the ontology
editor cannot deal with cyclic class structures and will terminate with an error. Care should

Working with Ontologies

307

be taken to avoid such situations.
A pair of ontology classes can also have an equivalentClassAs relation, which indicates
that the two classes are virtually the same and all their properties and instances should be
shared.
A restriction (represented by Restriction objects in the GATE ontology API) is an anonymous class (i.e., the class is not identiﬁed by an URI/IRI) and is set on an object or a
datatype property to restrict some instances of the speciﬁed domain of the property to have
only certain values (also known as value constraint) or certain number of values (also known
as cardinality restriction) for the property. Thus for each restriction there exists at least
three triples in the repository. One that deﬁnes resource as a restriction, another one that
indicates on which property the restriction is speciﬁed, and ﬁnally the third one that indicates what is the constraint set on the cardinality or value on the property. There are six
types of restrictions:
1. Cardinality Restriction (owl:cardinalityRestriction): the only valid values for this
restriction in OWL-Lite are 0 and 1. A cardinality restriction set to either 0 or 1 implies
both a MinCardinality Restriction and a MaxCardinality Restriction set to the same
value.
2. MinCardinality Restriction (owl:minCardinalityRestriction)
3. MaxCardinality Restriction (owl:maxCardinalityRestriction)
4. HasValue Restriction (owl:hasValueRestriction)
5. AllValuesFrom Restriction (owl:allValuesFromRestriction)
6. SomeValuesFrom Restriction (owl:someValuesFromRestriction)
Please visit the OWL Reference for more detailed information on restrictions.

14.1.2

Instances

Instances, also often called individuals are objects that belong to classes. Like named classes,
each instance is identiﬁed by an URI. Each instance can belong to one or more classes and
can have properties with values. Two instances can have the sameInstanceAs relation,
which indicates that the property values assigned to both instances should be shared and
that all the properties applicable to one instance are also valid for the other. In addition,
there is a differentInstanceAs relation, which declares the instances as disjoint.
Instances are represented by OInstance objects in the API. API methods are provided for
getting all the instances in an ontology, all the ones that belong to a given class, and all the
property values for a given instance. There is also a method to retrieve a list of classes that
the instance belongs to, using either transitive or direct closure.

308

Working with Ontologies

14.1.3

Hierarchies of Properties

The last part of the data model is made up of hierarchies of properties that can be associated
with objects in the ontology. The speciﬁcation of the type of objects that properties apply to
is done through the means of domains. Similarly, the types of values that a property can take
are restricted through the deﬁnition of a range. A property with a domain that is an empty
set can apply to instances of any type (i.e. there are no restrictions given). Like classes,
properties can also have superPropertyOf, subPropertyOf and equivalentPropertyAs
relations among them.
GATE supports the following property types:
1. Annotation Property:
An annotation property is associated with an ontology resource (i.e. a class,
property or instance) and can have a Literal as value.
A Literal is a
Java object that can refer to the URI of any ontology resource or a string
(http://www.w3.org/2001/XMLSchema#string) with the speciﬁed language or a data
type (discussed below) with a compatible value. Two annotation properties can not
be declared as equivalent. It is also not possible to specify a domain or range for an
annotation property or a super or subproperty relation between two annotation properties. Five annotation properties, predeﬁned by OWL, are made available to the user
whenever a new ontology instance is created:
owl:versionInfo,
rdfs:label,
rdfs:comment,
rdfs:seeAlso, and
rdfs:isDeﬁnedBy.
In other words, even when the user creates an empty ontology, these annotation properties are created automatically and available to users.
2. Datatype Property:
A datatype property is associated with an ontology instance and can have a Literal
value that is compatible with its data type . A data type can be one of the pre-deﬁned
data types in the GATE ontology API:
http://www.w3.org/2001/XMLSchema#boolean
http://www.w3.org/2001/XMLSchema#byte
http://www.w3.org/2001/XMLSchema#date
http://www.w3.org/2001/XMLSchema#decimal
http://www.w3.org/2001/XMLSchema#double
http://www.w3.org/2001/XMLSchema#duration

Working with Ontologies

309

http://www.w3.org/2001/XMLSchema#float
http://www.w3.org/2001/XMLSchema#int
http://www.w3.org/2001/XMLSchema#integer
http://www.w3.org/2001/XMLSchema#long
http://www.w3.org/2001/XMLSchema#negativeInteger
http://www.w3.org/2001/XMLSchema#nonNegativeInteger
http://www.w3.org/2001/XMLSchema#nonPositiveInteger
http://www.w3.org/2001/XMLSchema#positiveInteger
http://www.w3.org/2001/XMLSchema#short
http://www.w3.org/2001/XMLSchema#string
http://www.w3.org/2001/XMLSchema#time
http://www.w3.org/2001/XMLSchema#unsignedByte
http://www.w3.org/2001/XMLSchema#unsignedInt
http://www.w3.org/2001/XMLSchema#unsignedLong
http://www.w3.org/2001/XMLSchema#unsignedShort

A set of ontology classes can be speciﬁed as a property’s domain; in that case the
property can be associated with the instance belonging to all of the classes speciﬁed in
that domain only (the intersection of the set of domain classes).
Datatype properties can have other datatype properties as subproperties.
3. Object Property:
An object property is associated with an ontology instance and has an instance as
value. A set of ontology classes can be speciﬁed as property’s domain and range. Then
the property can only be associated with the instances belonging to all of the classes
speciﬁed as the domain. Similarly, only the instances that belong to all the classes
speciﬁed in the range can be set as values.
Object properties can have other object properties as subproperties.
4. RDF Property:
RDF properties are more general than datatype or object properties. The GATE
ontology API uses RDFProperty objects to hold datatype properties, object properties,
annotation properties or actual RDF properties (rdf:Property).
Note: The use of RDFProperty objects for creating, or manipulating RDF properties
is carried over from previous implementations for compatibility reasons but should be
avoided.
All properties (except the annotation properties) can be marked as functional properties,
which means that for a given instance in their domain, they can only take at most one value,
i.e. they deﬁne a function in the algebraic sense. Properties inverse to functional properties
are marked as inverse functional. If one likes ontology properties with algebraic relations,
the semantics of these become apparent.

310

Working with Ontologies

14.1.4

URIs

URIs are used to identify resources (instances, classes, properties) in an ontology. All URIs
that identify classes, instances, or properties in an ontology must consist of two parts:
a name part: this is the part after the last slash (#) or the ﬁrst hash (#) in the URI.
This part of the URI is often used as a shorthand name for the entity (e.g. in the
ontology editor) and is often called a fragment identiﬁer
a namespace part: the part that precedes the name, including the trailing slash or hash
character.
URIs uniquely identify resources: each resource can have at most one URI and each URI
can be associated with at most one resource.
URIs are represented by OURI objects in the API. The Ontology object provides factory
methods to create OURIs from a complete URI string or by appending a name to the default
namespace of the ontology. However it is the responsibility of the caller to ensure that any
strings that are passed to these factory methods do in fact represent valid URIs. GATE
provides some helper methods in the OUtils class to help with encoding and decoding URI
strings.

14.2

Ontology Event Model

An Ontology Event Model (OEM) is implemented and incorporated into the new GATE
ontology API. Under the new OEM, events are ﬁred when a resource is added, modiﬁed or
deleted from the ontology.
An interface called OntologyModificationListener is created with ﬁve methods (see below) that need to be implemented by the listeners of ontology events.
public void resourcesRemoved(Ontology ontology, String[] resources);

This method is invoked whenever an ontology resource (a class, property or instance) is
removed from the ontology. Deleting one resource can also result into the deletion of the
other dependent resources. For example, deleting a class should also delete all its instances
(more details on how deletion works are explained later). The second parameter, an array
of strings, provides a list of URIs of resources deleted from the ontology.
public void resourceAdded(Ontology ontology, OResource resource);

This method is invoked whenever a new resource is added to the ontology. The parameters
provide references to the ontology and the resource being added to it.

Working with Ontologies

311

public void ontologyRelationChanged(Ontology ontology, OResource resource1,
OResource resource2, int eventType);

This method is invoked whenever a relation between two resources (e.g. OClass and OClass,
RDFPRoeprty, RDFProeprty, etc) is changed. Example events are addition or removal of
a subclass or a subproperty, two classes or properties being set as equivalent or diﬀerent
and two instances being set as same or diﬀerent. The ﬁrst parameter is the reference to the
ontology, the next two parameters are the resources being aﬀected and the ﬁnal parameters
is the event type. Please refer to the list of events speciﬁed below for diﬀerent types of
events.
public void resourcePropertyValueChanged(Ontology ontology,
OResource resource, RDFProperty
property, Object value, int eventType)

This method is invoked whenever any property value is added or removed to a resource.
The ﬁrst parameter provides a reference to the ontology in which the event took place. The
second provides a reference to the resource aﬀected, the third parameter provides a reference
to the property for which the value is added or removed, the fourth parameter is the actual
value being set on the resource and the ﬁfth parameter identiﬁes the type of event.
public void ontologyReset(Ontology ontology)

This method is called whenever ontology is reset. In other words when all resources of the
ontology are deleted using the ontology.cleanup method.
The OConstants class deﬁnes the static constants, listed below, for various event types.
public
public
public
public
public
public
public
public
public
public
public
public
public
public
public
public
public

static
static
static
static
static
static
static
static
static
static
static
static
static
static
static
static
static

final
final
final
final
final
final
final
final
final
final
final
final
final
final
final
final
final

int
int
int
int
int
int
int
int
int
int
int
int
int
int
int
int
int

OCLASS_ADDED_EVENT;
ANONYMOUS_CLASS_ADDED_EVENT;
CARDINALITY_RESTRICTION_ADDED_EVENT;
MIN_CARDINALITY_RESTRICTION_ADDED_EVENT;
MAX_CARDINALITY_RESTRICTION_ADDED_EVENT;
HAS_VALUE_RESTRICTION_ADDED_EVENT;
SOME_VALUES_FROM_RESTRICTION_ADDED_EVENT;
ALL_VALUES_FROM_RESTRICTION_ADDED_EVENT;
SUB_CLASS_ADDED_EVENT;
SUB_CLASS_REMOVED_EVENT;
EQUIVALENT_CLASS_EVENT;
ANNOTATION_PROPERTY_ADDED_EVENT;
DATATYPE_PROPERTY_ADDED_EVENT;
OBJECT_PROPERTY_ADDED_EVENT;
TRANSTIVE_PROPERTY_ADDED_EVENT;
SYMMETRIC_PROPERTY_ADDED_EVENT;
ANNOTATION_PROPERTY_VALUE_ADDED_EVENT;

312

Working with Ontologies

public
public
public
public
public
public
public
public
public
public
public
public
public
public
public

static
static
static
static
static
static
static
static
static
static
static
static
static
static
static

final
final
final
final
final
final
final
final
final
final
final
final
final
final
final

int
int
int
int
int
int
int
int
int
int
int
int
int
int
int

DATATYPE_PROPERTY_VALUE_ADDED_EVENT;
OBJECT_PROPERTY_VALUE_ADDED_EVENT;
RDF_PROPERTY_VALUE_ADDED_EVENT;
ANNOTATION_PROPERTY_VALUE_REMOVED_EVENT;
DATATYPE_PROPERTY_VALUE_REMOVED_EVENT;
OBJECT_PROPERTY_VALUE_REMOVED_EVENT;
RDF_PROPERTY_VALUE_REMOVED_EVENT;
EQUIVALENT_PROPERTY_EVENT;
OINSTANCE_ADDED_EVENT;
DIFFERENT_INSTANCE_EVENT;
SAME_INSTANCE_EVENT;
RESOURCE_REMOVED_EVENT;
RESTRICTION_ON_PROPERTY_VALUE_CHANGED;
SUB_PROPERTY_ADDED_EVENT;
SUB_PROPERTY_REMOVED_EVENT;

An ontology is responsible for ﬁring various ontology events. Object wishing to listen to
the ontology events must implement the methods above and must be registered with the
ontology using the following method.
addOntologyModificationListener(OntologyModificationListener oml);

The following method cancels the registration.
removeOntologyModificationListener(OntologyModificationListener oml);

14.2.1

What Happens when a Resource is Deleted?

Resources in an ontology are connected with each other. For example, one class can be a
sub or superclass of another classes. A resource can have multiple properties attached to it.
Taking these various relations into account, change in one resource can aﬀect other resources
in the ontology. Below we describe what happens (in terms of what does the GATE ontology
API do) when a resource is deleted.
When a class is deleted
– A list of all its super classes is obtained. For each class in this list, a list of its
subclasses is obtained and the deleted class is removed from it.
– All subclasses of the deleted class are removed from the ontology. A list of all its
equivalent classes is obtained. For each class in this list, a list of its equivalent
classes is obtained and the deleted class is removed from it.
– All instances of the deleted class are removed from the ontology.

Working with Ontologies

313

– All properties are checked to see if they contain the deleted class as a member
of their domain or range. If so, the respective property is also deleted from the
ontology.
When an instance is deleted
– A list of all its same instances is obtained. For each instance in this list, a list of
its same instances is obtained and the deleted instance is removed.
– A list of all instances set as diﬀerent from the deleted instance is obtained. For
each instance in this list, a list of instances set as diﬀerent from it is obtained and
the deleted instance is removed.
– All the instances of ontology are checked to see if any of their set properties have
the deleted instance as value. If so, the respective set property is altered to remove
the deleted instance.
When a property is deleted
– A list of all its super properties is obtained. For each property in this list, a list
of its sub properties is obtained and the deleted property is removed.
– All sub properties of the deleted property are removed from the ontology.
– A list of all its equivalent properties is obtained. For each property in this list, a
list of its equivalent properties is obtained and the deleted property is removed.
– All instances and resources of the ontology are checked to see if they have the
deleted property set on them. If so the respective property is deleted.

14.3

The Ontology Plugin: Current Implementation

The plugin Ontology contains the current ontology API implementation. This implementation provides the additions and enhancements introduced into the GATE ontology API as
of release 5.1. It is based on a backend that uses Sesame version 2 and OWLIM version 3.
Before any ontology-based functionality can be used, the plugin must be loaded into GATE.
To do this in the GATE Developer GUI, select the ‘Manage CREOLE Plugins’ option from
the ‘File’ menu and check the ‘Load now’ checkbox for the ‘Ontology’ plugin, then click
OK. After this, the context menu for Language Resources will include the following ontology
language resources:
OWLIMOntology: this is the standard language resource to use in most situations.
It allows the user to create a new ontology backed by ﬁles in a local directory and
optionally load ontology data into it.

314

Working with Ontologies

OWLIMOntology DEPRECATED: this language resource has the same functionality as
OWLIMOntology but uses the exactly same package and class name as the language
resource in the plugin Ontology_OWLIM2. This LR is provided to allow an easier
upgrade of existing pipelines to the new implementation but users should move the the
OWLIMOntology LR as soon as possible.
ConnectSesameOntology: This language resources allows the use of ontologies that are
already stored in a Sesame2 repository which is either stored in a directory or accessible
from a server. This is useful for quickly re-using a very large ontology that has been
previously created as a persistent OWLIMOntology language resource.
CreateSesameOntology: This language resource allows the user to create a new empty
ontology by specifying the repository conﬁguration for creating the sesame repository.
Note:This is for advanced uses only!
Each of these language resources is explained in more detail in the following sections.
To make the plugin available to your GATE Embedded application, load the plugin prior to
creating one of the ontology language resources using the following code:
1
2
3
4
5

// Find the directory for the Ontology plugin
File pluginHome =
new File ( new File ( Gate . getGateHome () , " plugins " ) , " Ontology " );
// Load the plugin from that directory
Gate . getCreoleRegister (). regi s te r D ir e c to r i es ( pluginHome . toURI (). toURL ());

14.3.1

The OWLIMOntology Language Resource

The OWLIMOntology language resource is the main ontology language resource provided by
the plugin and provides a similar functionality to the OWLIMOntologyLR language resource
provided by the pre-5.1 implementation and provided by the Ontology_OWLIM2 plugin from
version 5.1 on. This language resource creates an in-memory store backed by ﬁles in a
directory on the ﬁle system to hold the ontology data.
To create a new OWLIM Ontology resource, select ‘OWLIM Ontology’ from the right-click
‘New’ menu for language resources. A dialog as shown in Figure 14.1 appears with the
following parameters to ﬁll in or change:
Name (optional): if no name is given, a default name will be generated, if an ontology
is loaded from an URL, based on that URL, otherwise based on the language resource
name.
baseURI (optional): the URI to be used for resolving relative URI references in the
ontology during loading.

Working with Ontologies

315

dataDirectoryName (optional): the name of an existing directory on the ﬁle system
where the directory will be created that backs the ontology store. The name of the
directory that will be created within the data directory will be GATE_OWLIMOntology_
followed by a string representation of the system time. If this parameter is not speciﬁed,
the value for system property java.io.tmpdir is used, if this is not set either an error
is raised.
loadImports (optional): either true or false. If set to false all ontology import speciﬁcations found in the loaded ontology are ignored. This parameter is ignored if no
ontology is loaded when the language resource is created.
mappingsURL (optional): the URL of a text ﬁle containing import mappings speciﬁcations. See section 14.3.5 for a description of the mappings ﬁle. If no URL is speciﬁed,
the GATE will interpret each import URI found as an URL and try to import the data
from that URL. If the URI is not absolute it will get resolved against the base URI.
persistent (optional): true or false: if false, the directory created inside the data directory is removed when the language resource is closed, otherwise, that directory is kept.
The ConnectSesameOntology language resource can be used at a later time to connect
to such a directory and create an ontology language resource for it (see Section 14.3.2).
rdfXmlUrl (optional): an URL specifying the location of an ontology in RDF/XML serialization format (see http://www.w3.org/TR/rdf-syntax-grammar/) from which to
load initial ontology data from. The parameter name can be changed from rdfXmlUrl
to n3Url to indicate N3 serialization format (see http://www.w3.org/DesignIssues/
Notation3.html), to ntriplesUrl to indicate N-Triples format (see http://www.
w3.org/TR/2004/REC-rdf-testcases-20040210/#ntriples), and to turtleUrl to
indicate TURTLE serialization format (see http://www.w3.org/TeamSubmission/
turtle/). If this is left blank, no ontology is loaded and an empty ontology language
resource is created.

Figure 14.1: The New OWLIM Ontology Dialog
Note: you could create a language resource such as OWLIM Ontology from GATE Developer
successfully, but you will not be able to browse/edit the ontology unless you loaded Ontology
Tools plugin beforehand.

316

Working with Ontologies

Additional ontology data can be loaded into an existing ontology language resource by selecting the ‘Load’ option from the language resource’s context menu. This will show the
dialog shown in ﬁgure 14.2. The parameters in this dialog correspond to the parameters
in the dialog for creating a new ontology with the addition of one new parameter: ‘load as
import’. If this parameter is checked, the ontology data is loaded speciﬁcally as an ontology
import. Ontology imports can be excluded from what is saved at a later time.

Figure 14.2: The Load Ontology Dialog
Figure 14.3 shows the ontology save dialog that is shown when the option ‘Save as. . . ’ is
selected from the language resource’s context menu. The parameter ‘include imports’ allows
the user to specify if the data that has been loaded through imports should be included in
the saved data or not.

Figure 14.3: The Save Ontology Dialog

14.3.2

The ConnectSesameOntology Language Resource

This ontology language resource can be created from either a directory on the local ﬁle
system that holds an ontology backing store (as created in the ‘data directory’ for the
‘OWLIM Ontology’ language resource), or from a sesame repository on a server that holds
an OWLIM ontology store.
This is very useful when using very large ontologies with GATE. Loading a very large ontology from a serialized format takes a signiﬁcant amount of time because the ﬁle has to be
deserialized and all implied facts have to get generated. Once an ontology has been loaded
into a persisting OWLIMOntology language resource, the ConnectSesameOntology language
resource can be used with the directory created to re-connect to the already de-serialized
and inferred data much faster.

Working with Ontologies

317

Figure 14.4 shows the dialog for creating a ConnectSesameOntology language resource.
repositoryID: the name of the sesame repository holding the ontology store. For a
backing store created with the ‘OWLIM Ontology’ language resource, this is always
‘owlim3’.
repositoryLocation: the URL of the location where to ﬁnd the repository holding the
ontology store. The URL can either specify a local directory or an HTTP server.
For a backing store created with the ‘OWLIM Ontology’ language resource this is
the directory that was created inside the data directory (the name of the directory
starting with GATE_OWLIMOntology_). If the URL speciﬁes a HTTP server which
requires authentiﬁcation, the user-ID and password have to be included in the URL
(e.g. http://userid:passwd@localhost:8080/openrdf-sesame).
Note that this ontology language resource is only supported when connected with an
OWLIM3 repository with the owl-max ruleset and partialRDFS optimizations disabled!
Connecting to any other repository is experimental and for expert users only! Also note that
connecting to a repository that is already in use by GATE or any other application is not
supported and might result in unwanted or erroneous behavior!

Figure 14.4: The New ConnectSesameOntology Dialog

14.3.3

The CreateSesameOntology Language Resource

This ontology language resource can be directly created from a Sesame2 repository conﬁguration ﬁle. This is an experimental language resource intended for expert users only. This
can be used to create any kind of Sesame2 repository, but the only repository conﬁguration supported by GATE and the GATE ontology API is an OWLIM repository with the
owl-max ruleset and partialRDFS optimizations disabled. The dialog for creating this
language resource is shown in Figure 14.5.

14.3.4

The OWLIM2 Backwards-Compatible Language Resource

This language resource is shown as “OWLIM Ontology DEPRECATED” in the ‘New
Language Resource’ submenu from the ‘File’ menu. It provides the “OWLIM Ontology” language resource in a way that attempts maximum backwards-compatibility with

318

Working with Ontologies

Figure 14.5: The New CreateSesameOntology Dialog

the ontology language resource provided by prior versions or the Ontology OWLIM2 language resource. This means, the class name is identical to those language resources
gate.creole.ontology.owlim.OWLIMOntologyLR) and the parameters are made compatible. This means that the parameter defaultNameSpace is added as an alias for the parameter
baseURI (also the methods setPersistsLocation and getPersistLocation are available
for legacy Java code that expects them, but the persist location set that way is not actually
used).
In addition, this language resource will still automatically add the resource name of a resource
as the String value for the annotation property “label”.

14.3.5

Using Ontology Import Mappings

If an ontology is loaded that contains the URIs of imported ontologies using owl:imports,
the plugin will try to automatically resolve those URIs to URLs and load the ontology ﬁle
to be imported from the location corresponding to the URL. This is done transitively, i.e.
import speciﬁcations contained in freshly imported ontologies are resolved too.
In some cases one might want to suppress the import of certain ontologies or one might want
to load the data from a diﬀerent locatin, e.g. from a ﬁle on the local ﬁle system instead.
With the OWLIMOntology language resource this can be achieved by specifying an import
mappings ﬁle when creating the ontology.
An import mappings ﬁle (see ﬁgure 14.6 for an example) is a plain ﬁle that maps speciﬁc
import URIs to URLs or to nothing at all. Each line that is not empty or does not start
with a hash (#) indicating a comment line must contain a URI. If the URI is not followed
by anything, this URI will be ignored when processing imports. If the URI is followed by
something, this is interpreted as a URL that is used for resolving the import of the URI. Local
ﬁles can be speciﬁed as file: URLs or by just giving the absolute or relative pathname
of the ﬁle in Linux path notation (forward slashes as path separators). At the moment,
ﬁlenames with embedded whitespace are not supported. If a pathname is relative it will be
resolved relative to the directory which contains the mappings ﬁle.

Working with Ontologies

319

# map this import to another web url
http://proton.semanticweb.org/2005/04/protont http://mycompany.com/owl/protont.owl
# map this import to a file in the same directory as the mappings file
http://proton.semanticweb.org/2005/04/protons protons.owl
# ignore this import
http://somewhere.com/reallyhugeimport

Figure 14.6: An example import mappings ﬁle

14.3.6

Using BigOWLIM

The GATE ontology plugin is based on SwiftOWLIM for storing the ontology and managing
inference. SwiftOWLIM is an in-memory store and the maximum size of ontologies that can
be stored is limited by the available memory.
BigOWLIM (see http://www.ontotext.com/owlim/big/) can handle huge ontologies and
is not limited by available memory. BigOWLIM is a commercial product and needs to be separately obtained and installed for use with the GATE ontology plugin. See the BigOWLIM
installation guide on how to set up BigOWLIM on a Tomcat server and how to create
BigOWLIM on the server with the Sesame console program.
The ontology plugin can easily and without any additional installation be used with
BigOWLIM repositories by using the ConnectSesameOntology LR (see section 14.3.2) to
connect to a BigOWLIM repository on a remote Tomcat server.

14.4

The Ontology OWLIM2 plugin:
compatible implementation

backwards-

14.4.1

The OWLIMOntologyLR Language Resource

This implementation is identical to the implementation that was part of GATE core before
version 5.1. It is based on SwiftOWLIM version 2 and Sesame version 1.
In order to load an ontology in an OWLIM repository, the user has to provide certain
conﬁguration parameters. These include the name of the repository, the URL of the ontology,
the default name space, the format of the ontology (RDF/XML, N3, NTriples and Turtle),
the URLs or absolute locations of the other ontologies to be imported, their respective name
spaces and so on. Ontology ﬁles, based on their format, are parsed and persisted in the
NTriples format.

320

Working with Ontologies

In order to utilize the power of OWLIM and the simplicity of GATE ontology API, GATE
provides an implementation of the OWLIM Ontology. Its basic purpose is to hide all the
complexities of OWLIM and Sesame and provide an easy to use API and interface to create,
load, save and update ontologies. Based on certain parameters that the user provides when
instantiating the ontology, a conﬁguration ﬁle is dynamically generated to create a dummy
repository in memory (unless persistence is speciﬁed).
When creating a new ontology, one can use an existing ﬁle to pre-populate it with data.
If no such ﬁle is provided, an empty ontology is created. A detailed description for all the
parameters that are available for new ontologies follows:
1. defaultNameSpace is the base URI to be used for all new items that are only mentioned
using their local name. This can safely be left empty, in which case, while adding new
resources to the ontology, users are asked to provide name spaces for each new resource.
2. As indicated earlier, OWLIM supports four diﬀerent formats: RDF/XML, NTriples,
Turtle and N3. According to the format of the ontology ﬁle, user should select one of
the four URL options (rdfXmlURL, ntriplesURL, turtleURL and n3URL (not supported
yet)) and provide a URL pointing to the ontology data.
Once an ontology is created, additional data can be loaded that will be merged with the
existing information. This can be done by right-clicking on the ontology in the resources tree
in GATE Developer and selecting ‘Load ... data’ where ‘...’ is one of the supported formats.
Other options available are cleaning the ontology (deleting all the information from it) and
saving it to a ﬁle in one of the supported formats.
Ontology can be saved in diﬀerent formats (rdf/xml, ntriples, n3 and turtle) using the
options provided in the context menu that can be invoked by right clicking on the instance
of an ontology in GATE Developer. All the changes made to the ontology are logged and
stored as an ontology feature. Users can also export these changes to a ﬁle by selecting the
‘Save Ontology Event Log’ option from the context menu. Similarly, users can also load
the exported event log and apply the changes on a diﬀerent ontology by using the ‘Load
Ontology Event Log’ option. Any change made to the ontology can be described by a set
of triples either added or deleted from the repository. For example, in GATE Embedded,
addition of a new instance results into addition of two statements into the repository:

// Adding a new instance "Rec1" of type "Recognized"
// Here + indicates the addition
+ <http://proton.semanticweb.org/2005/04/protons#Rec1>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://proton.semanticweb.org/2005/04/protons#Recognized>

Working with Ontologies

321

// Adding a label (annotation property) to the instance with
// value "Rec Instance"
+ <http://proton.semanticweb.org/2005/04/protons#Rec1>
<http://www.w3.org/2000/01/rdf-schema#label>
<Rec Instance>
<http://www.w3.org/2001/XMLSchema#string>

The event log therefore contains a list of such triples, the latest change being at the bottom
of the change log. Each triple consists of a subject followed by a predicate followed by an
object. Below we give an illustration explaining the syntax used for recording the changes.

// Adding a new instance "Rec1" of type "Recognized"
// Here + indicates the addition
+ <http://proton.semanticweb.org/2005/04/protons#Rec1>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://proton.semanticweb.org/2005/04/protons#Recognized>
// Adding a label (annotation property) to the instance with
// value "Rec Instance"
+ <http://proton.semanticweb.org/2005/04/protons#Rec1>
<http://www.w3.org/2000/01/rdf-schema#label>
<Rec Instance>
<http://www.w3.org/2001/XMLSchema#string>
// Adding a new class called TrustSubClass
+ <http://proton.semanticweb.org/2005/04/protons#TrustSubClass>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
<http://www.w3.org/2002/07/owl#Class>
// TrustSubClass is a subClassOf the class Trusted
+ <http://proton.semanticweb.org/2005/04/protons#TrustSubClass>
<http://www.w3.org/2000/01/rdf-schema#subClassOf>
<http://proton.semanticweb.org/2005/04/protons#Trusted>
// Deleting a property called hasAlias and all relevant statements
// Here - indicates the deletion
// * indicates any value in place
- <http://proton.semanticweb.org/2005/04/protons#hasAlias> <*> <*>
- <*> <http://proton.semanticweb.org/2005/04/protons#hasAlias> <*>
- <*> <*> <http://proton.semanticweb.org/2005/04/protons#hasAlias>
// Deleting a label set on the instance Rec1
- <http://proton.semanticweb.org/2005/04/protons#Rec1>
<http://www.w3.org/2000/01/rdf-schema#label>

322

Working with Ontologies

<Rec Instance>
<http://www.w3.org/2001/XMLSchema#string>
// Reseting the entire ontology (Deleting all statements)
- <*> <*> <*>

14.5

GATE Ontology Editor

GATE’s ontology support also includes a viewer/editor that can be used within GATE
Developer to navigate an ontology and quickly inspect the information relating to any of the
objects deﬁned in it—classes and restrictions, instances and their properties. Also, resources
can be deleted and new resources can be added through the viewer.
Before the ontology editor can be used, one of the ontology implementation plugins must be
loaded. In addition the Ontology_Tools must be loaded.
Note: To make it possible to show a loaded ontology in the ontology editor, the
Ontology_Tools plugin must be loaded before the ontology language resource is created.

Figure 14.7: The GATE Ontology Viewer

Working with Ontologies

323

The viewer is divided into two areas. One on the left shows separate tabs for hierarchy of
classes and instances and for (as of Gate 4) hierarchy of properties. The view on right hand
side shows the details pertaining of the object currently selected in the other two.
First tab on the left view displays a tree which shows all the classes and restrictions deﬁned in
the ontology. The tree can have several root nodes—one for each top class in the ontology.
The same tree also shows each instances for each class. Note: Instances that belong to
several classes are shown as children of all the classes they belong to.
Second tab on the left view displays a tree of all the properties deﬁned in the ontology. This
tree can also have several root nodes—one for each top property in the ontology. Diﬀerent
types of properties are distinguished by using diﬀerent icons.
Whenever an item is selected in the tree view, the right-hand view is populated with the
details that are appropriate for the selected object. For an ontology class, the details include
the brief information about the resource such as the URI of the selected class, type of the
selected class etc., set of direct superclasses, the set of all superclasses using the transitive
closure, the set of direct subclasses, the set of all the subclasses, the set of equivalent classes,
the set of applicable property types, the set of property values set on the selected class, and
the set of instances that belong to the selected class. For a restriction, in addition to the
above information, it displays on which property the restriction is applicable to and what
type of the restriction that is.
For an instance, the details displayed include the brief information about the instance, set
of direct types (the list of classes this instance is known to belong to), the set of all types
this instance belongs to (through the transitive closure of the set of direct types), the set of
same instances, the set of diﬀerent instances and the values for all the properties that are
set.
When a property is selected, diﬀerent information is displayed in the right-hand view according to the property type. It includes the brief information about the property itself,
set of direct superproperties, the set of all superproperties (obtained through the transitive
closure), the set of direct subproperties, the set of all subproperties (obtained through the
transitive closure), the set of equivalent properties, and domain and range information.
As mentioned in the description of the data model, properties are not directly linked to the
classes, but rather deﬁne their domain of applicability through a set of domain restrictions.
This means that the list of properties should not really be listed as a detail for class objects
but only for instances. It is however quite useful to have an indication of the types of
properties that could apply to instances of a given class. Because of the semantics of property
domains, it is not possible to calculate precisely the list of applicable properties for a given
class, but only an estimate of it. If a property for instance requires its domain instances to
belong to two diﬀerent classes then it cannot be known with certitude whether it is applicable
to either of the two classes—it does not apply to all instances of any of those classes, but
only to those instances the two classes have in common. Because of this, such properties
will not be listed as applicable to any class.

324

Working with Ontologies

The information listed in the details pane is organised in sub-lists according to the type of
the items. Each sub-list can be collapsed or expanded by clicking on the little triangular
button next to the title. The ontology viewer is dynamic and will update the information
displayed whenever the underlying ontology is changed through the API.
When you double click on any resource in the details table, the respective resource is selected
in the class or in the property tree and the selected resource’s details are shown in the details
table. To change a property value, user can double click on a value of the property (second
column) and the relevant window is shown where user is asked to provide a new value. Along
with each property value, a button (with red X caption) is provided. If user wants to remove
a property value he or she can click on the button and the property value is deleted.
A new toolbar has been added at the top of the ontology viewer, which contains the following
buttons to add and delete ontology resources:
Add new top class (TC)
Add new subclass (SC)
Add new instance (I)
Add new restriction (R)
Add new Annotation property (A)
Add new Datatype property (D)
Add new Object property (O)
Add new Symmetric property (S)
Add new Transitive property (T)
Remove the selected resource(s) (X)
Search
Refresh ontology
The tree components allow the user to select more than one node, but the details table on
the right-hand side of the GATE Developer GUI only shows the details of the ﬁrst selected
node. The buttons in the toolbar are enabled and disabled based on users’ selection of nodes
in the tree.
1. Creating a new top class:
A window appears which asks the user to provide details for its namespace (default
name space if speciﬁed), and class name. If there is already a class with same name in
ontology, GATE Developer shows an appropriate message.

Working with Ontologies

325

2. Creating a new subclass:
A class can have multiple super classes. Therefore, selecting multiple classes in the
ontology tree and then clicking on the ‘SC’ button, automatically considers the selected
classes as the super classes. The user is then asked for details for its namespace and
class name.
3. Creating a new instance:
An instance can belong to more than one class. Therefore, selecting multiple classes
in the ontology tree and then clicking on the ‘I’ button, automatically considers the
selected classes as the type of new instance. The user is then prompted to provide
details such as namespace and instance name.
4. Creating a new restriction:
As described above, restriction is a type of an anonymous class and is speciﬁed on a
property with a constraint set on either the number of values it can take or the type
of value allowed for instances to have for that property. User can click on the blue ‘R’
square button which shows a window for creating a new restriction. User can select
a type of restriction, property and a value constraint for the same. Please note that
restrictions are considered as anonymous classes and therefore user does not have to
specify any URI for the same but restrictions are named automatically by the system.
5. Creating a new property:
Editor allows creating ﬁve diﬀerent types of properties:
Annotation property: Since an annotation property cannot have any domain or
range constraints, clicking on the new annotation property button brings up a
dialog that asks the user for information such as the namespace and the annotation
property name.
Datatype property: A datatype property can have one or more ontology classes
as its domain and one of the pre-deﬁned datatypes as its range. Selecting one or
more classes and clicking on the new Datatype property icon, brings up a window
where the selected classes in the tree are taken as the property’s domain. The
user is then asked to provide information such as the namespace and the property
name. A drop down box allows users to select one of the data types from the list.
Object, Symmetric and Transitive properties: These properties can have one or
more classes as their domain and range. For a symmetric property the domain and
range are the same. Clicking on any of these options brings up a window where
user is asked to provide information such as the namespace and the property
name. The user is also given two buttons to select one or more classes as values
for domain and range.
6. Removing the selected resources:
All the selected nodes are removed when user clicks on the ‘X’ button. Please note that
since ontology resources are related in various ways, deleting a resource can aﬀect other

326

Working with Ontologies

resources in the ontology; for example, deleting a resource can cause other resources
in the same ontology to be deleted too.
7. Searching in ontology:
The Search button allows users to search for resources in the ontology. A window pops
up with an input text ﬁeld that allows incremental searching. In other words, as user
types in name of the resource, the drop-down list refreshes itself to contain only the
resources that start with the typed string. Selecting one of the resources in this list
and pressing OK, selects the appropriate resource in the editor. The Search function
also allows selecting resources by the property values set on them.
8. Refresh Ontology
The refresh button reloads the ontology and updates the editor.
9. Setting properties on instances/classes:
Right-clicking on an instance brings up a menu that provides a list of properties that
are inherited and applicable to its classes. Selecting a speciﬁc property from the menu
allows the user to provide a value for that property. For example, if the property
is an Object property, a new window appears which allows the user to select one or
more instances which are compatible to the range of the selected property. The selected
instances are then set as property values. For classes, all the properties (e.g. annotation
and RDF properties) are listed on the menu.
10. Setting relations among resources:
Two or more classes, or two or more properties, can be set as equivalent; similarly
two or more instances can be marked as the same. Right-clicking on a resource brings
up a menu with an appropriate option (Equivalent Class for ontology classes, Same
As Instance for instances and Equivalent Property for properties) which when clicked
then brings up a window with a drop down box containing a list of resources that the
user can select to specify them as equivalent or the same.

14.6

Ontology Annotation Tool

The Ontology Annotation Tool (OAT) is a GATE plugin available from the Ontology Tools
plugin set, which enables a user to manually annotate a text with respect to one or more
ontologies. The required ontology must be selected from a pull-down list of available ontologies.
The OAT tool supports annotation with information about the ontology classes, instances
and properties.

Working with Ontologies

327

Figure 14.8: Viewing Ontology-Based Annotations

14.6.1

Viewing Annotated Text

Ontology-based annotations in the text can be viewed by selecting the desired classes or
instances in the ontology tree in GATE Developer (see Figure 14.8). By default, when a
class is selected, all of its sub-classes and instances are also automatically selected and their
mentions are highlighted in the text. There is an option to disable this default behaviour
(see Section 14.6.4).
Figure 14.8 shows the mentions of each class and instance in a diﬀerent colour. These colours
can be customised by the user by clicking on the class/instance names in the ontology tree.
It is also possible to expand and collapse branches of the ontology.

14.6.2

Editing Existing Annotations

In order to view the class/instance of a highlighted annotation in the text (e.g., United States
- see Figure 14.9), hover the mouse over it and an edit dialogue will appear. It shows the
current class or instance (Country in our example) and allows the user to delete it or change
it. To delete an existing annotation, press the Delete button.
A class or instance can be changed by starting to type the name of the new class in the

328

Working with Ontologies

Figure 14.9: Editing Existing Annotations

combo-box. Then it displays a list of available classes and instances, which start with the
typed string. For example, if we want to change the type from Country to Location, we can
type ‘Lo’ and all classes and instances which names start with Lo will be displayed. The
more characters are typed, the fewer matching classes remain in the list. As soon as one sees
the desired class in the list, it is chosen by clicking on it.
It is possible to apply the changes to all occurrences of the same string and the same previous
class/instance, not just to the current one. This is useful when annotating long texts. The
user needs to make sure that they still check the classes and instances of annotations further
down in the text, in case the same string has a diﬀerent meaning (e.g., bank as a building
vs. bank as a river bank).
The edit dialogue also allows correcting annotation oﬀset boundaries. In other words, user
can expand or shrink the annotation oﬀsets’ boundaries by clicking on the relevant arrow
buttons.
OAT also allows users to assign property values as annotation features to the existing class
and instance annotations. In the case of class annotation, all annotation properties from the
ontology are displayed in the table. In the case of instance annotations, all properties from
the ontology applicable to the selected instance are shown in the table. The table also shows
existing features of the selected annotation. User can then add, delete or edit any value(s)
of the selected feature. In the case of a property, user is allowed to provide an arbitrary

Working with Ontologies

329

Figure 14.10: Add New Annotation

number of values. User can, by clicking on the editList button, add, remove or edit any
value to the property. In case of object properties, users are only allowed to select values
from a pre-selected list of values (i.e. instances which satisfy the selected property’s range
constraints).

14.6.3

Adding New Annotations

New annotations can be added in two ways: using a dialogue (see Figure 14.10) or by
selecting the text and clicking on the desired class or instance in the ontology tree.
When adding a new annotation using the dialogue, select a text and after a very short while,
if the mouse is not moved, a dialogue will appear (see Figure 14.10). Start typing the name
of the desired class or instance, until you see it listed in the combo-box, then select it with
the mouse. This operation is the same, as in changing the class/instance of an existing
annotation. One has the option of applying this choice to the current selection only or to all
mentions of the selected string in the current document (Apply to All check box).
User can also create an instance from the selected text. If user checks the ‘create instance’
checkbox prior to selecting the class, the selected text is annotated with the selected class
and a new instance of the selected class (with the name equivalent to the selected text) is
created (provided there isn’t any existing instance available in the ontology with that name).

330

Working with Ontologies

Figure 14.11: Tool Options

14.6.4

Options

There are several options that control the OAT behaviour (see Figure 14.11):
Disable child feature: By default, when a class is selected, all of its sub-classes are
also automatically selected and their mentions are highlighted in the text. This option
disables that behaviour, so only mentions of the selected class are highlighted.
Delete conﬁrmation: By default, OAT deletes ontological information without asking for conﬁrmation, when the delete button is pressed. However, if this leads to too
many mistakes, it is possible to enable delete conﬁrmations from this option.
Disable Case-Sensitive Feature: When user decides to annotate all occurrences
of the selected text (‘apply to all’ option) in the document and if the ‘disable casesensitive feature’ is selected, the tool, when searching for the identical strings in the
document text, ignores the case-sensitivity.
Setting up a ﬁlter to disable resources from the OAT GUI: When user wants
to annotate the text of a document with certain classes/instances of the ontology, s/he
may disable the resources which s/he is not going to use. This option allows users to
select a ﬁle which contains class or instance names, one per line. These names are case
sensitive. After selecting a ﬁle, when user turns on the ‘ﬁlter’ check box, the resources

Working with Ontologies

331

speciﬁed in the ﬁlter ﬁle are disabled and removed from the annotation editor window.
User can also add new resources to this list or remove some or all from the list by right
clicking on the respective resource and by selecting the relevant option. Once modiﬁed,
the ‘save’ button allows users to export this list to a ﬁle.
Annotation Set: GATE stores information in annotation sets and OAT allows you
to select which set to use as input and output.
Annotation Type: By default, this is annotation of type Mention, but that can be
changed to any other name. This option is required because OAT uses Gate annotations
to store and read the ontological data. However, to do that, it needs a type (i.e. name)
so ontology-based annotations can be distinguished easily from other annotations (e.g.
tokens, gazetteer lookups).

14.7

Relation Annotation Tool

This tool is designed to annotate a document with ontology instances and to create relations
between annotations with ontology object properties. It is close and compatible with OAT
but focus on relations between annotations, see section 14.6 for OAT.
To use it you must load the Ontology Tools plugin, load a document and an ontology then
show the document and in the document editor click on the button named ‘RAT-C’ (Relation
Annotation Tool Class view) which will also display the ‘RAT-I’ view (Relation Annotation
Tool Instance view).

14.7.1

Description of the two views

The right vertical view shows the loaded ontologies as trees.
To show/hide the annotations in the document, use the class checkbox. The selection of a
class and the ticking of a checkbox are independent and work the same as in the annotation
sets view.
To change the annotation set used to load/save the annotations, use the drop down list at
the bottom of the vertical view.
To hide/show the classes in the tree in order to decrease the amount of elements displayed,
use the context menu on classes selection. The setting is saved in the user preferences.
The bottom horizontal view shows two tables: one for instances and one for properties. The
instances table shows the instances and their labels for the selected class in the ontology
trees and the properties table shows the properties values for the selected instance in the
instances table.

332

Working with Ontologies

Figure 14.12: Relation Annotation Tool vertical and horizontal document views

Two buttons allow to add a new instance from the text selection in the document or as a
new label for the selected instance.
To ﬁlter on instance labels, use the ﬁlter text ﬁeld. You can clear the ﬁeld with the X button
at the end of the ﬁeld.
You can use ‘Show In Ontology Editor’ on the context menu of an instance in the instance
table. Then in the ontology editor you can add class or object properties.

14.7.2

Create new annotation and instance from text selection

select a class in the ontology tree at the right
select some text in the document editor and hover the mouse over it
use the button ‘New Inst.’ in the view at the bottom
in the bottom left table you have your new instance
don’t forget to save your document AND the ontology before to quit

14.7.3

Create new annotation and add label to existing instance
from text selection

select a class in the ontology tree at the right
select some text in the document editor and hover the mouse on it

Working with Ontologies

333

if the instances table is empty then clear the ﬁlter text ﬁeld
select an existing instance in the instances table
use the button ‘Add to Selected Inst.’ in the view at the bottom
in the bottom left table you have your new label
don’t forget to save your document AND the ontology before to quit

14.7.4

Create and set properties for annotation relation

open an ontology with the ontology editor
if not existing add at least an object property for one class
set the domain and range accordingly to the type of annotation relation
add an instance or label as explained previously for the same class
in the bottom right table you have the properties for this instance
click in the ‘Value’ column cell to set the object property
if the list of choices is empty, add ﬁrst other instances
don’t forget to save your document AND the ontology before to quit

14.7.5

Delete instance, label or property

select one or more instances or properties in their respective table
right-click on the selection for the context menu and choose an item

14.7.6

Diﬀerences with OAT and Ontology Editor

This tool is very close to OAT but without the annotation editor popup and instead a bottom
tables view, with multiple ontologies support, with only instance annotation and no class
annotation.
To make OAT compatible with this tool you must use ‘Mention’ as annotation type, ‘class’
and ‘inst’ as feature names. They are the defaults in OAT. You must also select the same
annotation set in the drop down list at the bottom right corner.
You should enable the option ‘Selected Text As Property Value’ in the Options panel of
OAT. So it will add a label from the selected text for each instance.

334

Working with Ontologies

The ontology editor is useful to check that an instance is correctly added to the ontology
and to add new annotation relation as object property.

14.8

Using the ontology API

The following code demonstrates how to use the GATE API to create an instance of the
OWLIM Ontology language resource. This example shows how to use the current version of
the API and ontology implementation.
For an example of using the old API and the backwards compatibility plugin, see 14.9.
1
2

// step 1: initialize GATE
if (! Gate . isInitialized ()) { Gate . init (); }

3
4
5
6

// step 2: load the Ontology plugin that contains the implementation
File ontoHome = new File ( Gate . getPluginsHome () , " Ontology " );
Gate . getCreoleRegister (). addDirectory ( ontoHome . toURL ());

7
8
9
10
11
12
13

// step 3: set the parameters
FeatureMap fm = Factory . newFeatureMap ();
fm . put ( " rdfXmlURL " , urlOfTheOntology );
fm . put ( " baseURI " , theBaseURI );
fm . put ( " mappingsURL " , urlOfT h e M a p p i n g s F i l e );
// .. any other parameters

14
15
16
17
18

// step 4: ﬁnally create an instance of ontology
Ontology ontology = ( Ontology )
Factory . createResource ( " gate . creole . ontology . impl . sesame . OWLIMOntology " ,
fm );

19
20
21

// retrieving a list of top classes
Set < OClass > topClasses = ontology . getOClasses ( true );

22
23
24
25
26
27
28
29
30

// for all top classes, printing their direct sub classes and print
// their URI or blank node ID in turtle format.
for ( OClass c : topClasses ) {
Set < OClass > dcs = c . getSubClasses ( OConstants . DIRECT_CLOSURE );
for ( OClass sClass : dcs ) {
System . out . println ( sClass . getONodeID (). toTurtle ());
}
}

31
32
33
34

// creating a new class from a full URI
OURI aURI1 = ontology . createOURI ( " http :// sample . en / owlim # Organization " );
OClass or ganiza tionCl ass = ontology . addOClass ( aURI1 );

35
36
37
38
39

// create a new class from a name and the default name space set for
// the ontology
OURI aURI2 = ontology . createOU RIForN ame ( " someOtherName " );
OClass someOtherClass = ontology . addOClass ( aURI2 );

Working with Ontologies

335

40
41
42

// set the label for the class
someOtherClass . setLabel ( " some other name " , OConstants . ENGLISH );

43
44
45
46
47
48
49
50
51

// creating a new Datatype property called name
// with domain set to Organization
// with datatype set to string
URI dURI = new URI ( " http :// sample . en / owlim # Name " , false );
Set < OClass > domain = new HashSet < OClass >();
domain . add ( or ganiza tionCl ass );
DatatypeProperty dp =
ontology . addDatatypeProper t y ( dURI , domain , Datatype . getS tringD ataTyp e ());

52
53
54
55

// creating a new instance of class organization called IBM
OURI iURI = ontology . createOURI ( " http :// sample . en / owlim # IBM " );
OInstance ibm = Ontology . addOInstance ( iURI , organ izatio nClass );

56
57
58

// assigning a Datatype property, name to ibm
ibm . a d dD a t a ty p e P ro p e r ty V a l u e ( dp , new Literal ( " IBM Corporation " ,

59
60
61
62
63
64
65
66
67
68
69
70

// get all the set values of all Datatype properties on the instance ibm
Set < DatatypeProperty > dps = Ontology . g e t D a t a t y p e P r o p e r t i e s ();
for ( DatatypeProperty dp : dps ) {
List < Literal > values = ibm . g e t D a t a t y p e P r o p e r t y V a l u e s ( dp );
System . out . println ( " DP : " + dp . getOURI ());
for ( Literal l : values ) {
System . out . println ( " Value : " + l . getValue ());
System . out . println ( " Datatype : " + l . getDataType (). getXmlSchemaURI ());
}
}

71
72
73
74
75

// export data to a ﬁle in Turtle format
BufferedWriter writer = new BufferedWriter ( new FileWriter ( someFile ));
ontology . writeOntologyData ( writer , OConstants . OntologyFormat . TURTLE );
writer . close ();

14.9

Using the ontology API (old version)

The following code demonstrates how to use the GATE API to create an instance of the
OWLIM Ontology language resource. This example shows how to use the API with the
backwards-compatibility plugin Ontology OWLIM2
For how to use the API with the current implementation plugin, see 14.8.
1
2

// step 1: initialize GATE
Gate . init ();

3
4
5

// step 2: load the plugin
File ontoHome = new File ( Gate . getPluginsHome () , " Ontology_OWLIM2 " );

336

6

Working with Ontologies

Gate . getCreoleRegister (). addDirectory ( ontoHome . toURL ());

7
8
9
10

// step 3: set the parameters
FeatureMap fm = Factory . newFeatureMap ();
fm . put ( " rdfXmlURL " , url - of - the - ontology );

11
12
13
14

// step 4: ﬁnally create an instance of ontology
Ontology ontology = ( Ontology )
Factory . createResource ( " gate . creole . ontology . owlim . OWLIMOntologyLR " , fm );

15
16
17

// retrieving a list of top classes
Set < OClass > topClasses = ontology . getOClasses ( true );

18
19
20
21
22
23
24
25
26

// for all top classes, printing their direct sub classes
Iterator < OClass > iter = topClasses . iterator ();
while ( iter . hasNext ()) {
Set < OClass > dcs = iter . next (). getSubClasses ( OConstants . DIRECT_CLOSURE );
for ( OClass aClass : dcs ) {
System . out . println ( aClass . getURI (). toString ());
}
}

27
28
29
30
31

// creating a new class
// false indicates that it is not an anonymous URI
URI aURI = new URI ( " http :// sample . en / owlim # Organization " , false );
OClass or ganiza tionCl ass = ontology . addOClass ( aURI );

32
33
34
35
36
37
38
39
40

// creating a new Datatype property called name
// with domain set to Organization
// with datatype set to string
URI dURI = new URI ( " http :// sample . en / owlim # Name " , false );
Set < OClass > domain = new HashSet < OClass >();
domain . add ( or ganiza tionCl ass );
DatatypeProperty dp = ontology . a dd D a t at y p eP r o pe r t y ( dURI , domain ,
Datatype . getS tringD ataTyp e ());

41
42
43
44

// creating a new instance of class organization called IBM
URI iURI = new URI ( " http :// sample . en / owlim # IBM " , false );
OInstance ibm = Ontology . addOInstance ( iURI , organ izatio nClass );

45
46
47
48

// assigning a Datatype property, name to ibm
ibm . a d dD a t a ty p e P ro p e r ty V a l u e ( dp , new Literal ( " IBM Corporation " ,
dp . getDataType ());

49
50
51
52
53
54
55
56
57
58

// get all the set values of all Datatype properties on the instance ibm
Set < DatatypeProperty > dps = Ontology . g e t D a t a t y p e P r o p e r t i e s ();
for ( DatatypeProperty dp : dps ) {
List < Literal > values = ibm . g e t D a t a t y p e P r o p e r t y V a l u e s ( dp );
System . out . println ( " DP : " + dp . getURI (). toString ());
for ( Literal l : values ) {
System . out . println ( " Value : " + l . getValue ());
System . out . println ( " Datatype : "
+ l . getDataType (). getXmlSchemaURI (). toString ()); }

Working with Ontologies

59

337

}

60
61
62
63
64
65
66
67

// export data to a ﬁle in the ntriples format
BufferedWriter writer = new BufferedWriter ( new FileWriter ( someFile ));
String output = ontology . getOntologyData (
OConstants . O N T O LO G Y _ FO R M A T_ N T R IP L E S );
writer . write ( output );
writer . flush ();
writer . close ();

14.10

Ontology-Aware JAPE Transducer

One of the GATE components that makes use of the ontology support is the JAPE transducer (see Chapter 8). Combining the power of ontologies with JAPE’s pattern matching
mechanisms can ease the creation of applications.
In order to use ontologies with JAPE, one needs to load an ontology in GATE before loading
the JAPE transducer. Once the ontology is known to the system, it can be set as the value
for the optional ontology parameter for the JAPE grammar. Doing so alters slightly the
way the matching occurs when the grammar is executed. If a transducer is ontology-aware
(i.e. it has a value set for the ’ontology’ parameter) it will treat all occurrences of the feature
named class diﬀerently from the other features of annotations. The values for the feature
class on any type of annotation will be considered as referring to classes in the ontology as
follows:
if the class feature value is a valid URI (e.g. http://sample.en/owlim#Organization)
then it is treated as a reference to the class (if any) with that URI in the ontology.
otherwise, it is treated as a name in the ontology’s default namespace. The default
namespace is prepended to the value to give a URI and the feature is treated as referring
to the class with that URI.
For example, if the default namespace of the ontology is http://gate.ac.uk/example# then
a class feature with the value “Person” refers to the http://gate.ac.uk/example#Person
class in the ontology. If the ontology imports other ontologies then it may be useful to deﬁne
templates for the various namespace URIs to avoid excessive repetition. There is an example
of this for the PROTON ontology in section 8.1.5.
In ontology-aware mode the matching between two class values will not be based on simple equality but rather hierarchical compatibility. For example if the ontology contains a
class named ‘Politician’, which is a sub class of the class ‘Person’, then a pattern of
{Entity.class == ‘Person’} will successfully match an annotation of type Entity with
a feature class having the value ‘Politician’. If the JAPE transducer were not ontologyaware, such a test would fail.

338

Working with Ontologies

This behaviour allows a larger degree of generalisation when designing a set of rules. Rules
that apply several types of entities mentioned in the text can be written using the most
generic class they apply to and need not be repeated for each subtype of entity. One could
have rules applying to Locations without needing to know whether a particular location
happens to be a country or a city.
If a domain ontology is available at the time of building an application, using it in conjunction
with the JAPE transducers can signiﬁcantly simplify the set of grammars that need to be
written.
The ontology does not normally aﬀect actions on the right hand side of JAPE rules, but
when Java is used on the right hand side, then the ontology becomes accessible via a local
variable named ontology, which may be referenced from within the right-hand-side code.
In Java code, the class feature should be referenced using the static ﬁnal variable,
LOOKUP CLASS FEATURE NAME, that is deﬁned in gate.creole.ANNIEConstants.

14.11

Annotating Text with Ontological Information

The ontology-aware JAPE transducer enables the text to be linked to classes in an ontology
by means of annotations. Essentially this means that each annotation can have a class and
ontology feature. To add the relevant class feature to an annotation is very easy: simply
add a feature ‘class’ with the classname as its value. To add the relevant ontology, use
ontology.getURL().
Below is a sample rule which looks for a location annotation and identiﬁes it as a ‘Mention’
annotation with the class ‘Location’ and the ontology loaded with the ontology-aware JAPE
transducer (via the runtime parameter of the transducer).
Rule: Location
({Location}):mention
-->
:mention{
// create the ontology and class features
FeatureMap features = Factory.newFeatureMap();
features.put("ontology", ontology.getURL());
features.put("class", "Location");
// create the new annotation
try {
annotations.add(mentionAnnots.firstNode().getOffset(),
mentionAnnots.lastNode().getOffset(), "Mention", features);

Working with Ontologies

339

}
catch(InvalidOffsetException e) {
throw new JapeException(e);
}
}

14.12

Populating Ontologies

Another typical application that combines the use of ontologies with NLP techniques is
ﬁnding mentions of entities in text. The scenario is that one has an existing ontology and
wants to use Information Extraction to populate it with instances whenever entities belonging
to classes in the ontology are mentioned in the input texts.
Let us assume we have an ontology and an IE application that marks the input text with
annotations of type ‘Mention’ having a feature ‘class’ specifying the class of the entity
mentioned. The task we are seeking to solve is to add instances in the ontology for every
Mention annotation.
The example presented here is based on a JAPE rule that uses Java code on the action side
in order to access directly the GATE ontology API:
1
2
3
4
5
6
7
8

Rule : FindEntities
({ Mention }): mention
-->
: mention {
//ﬁnd the annotation matched by LHS
//we know the annotation set returned
//will always contain a single annotation
Annotation mentionAnn = mentionAnnots . iterator (). next ();

9
10
11
12
13
14
15
16
17
18

//ﬁnd the class of the mention
String className = ( String ) mentionAnn . getFeatures ().
get ( gate . creole . ANNIEConstants . L O O K U P _ C L A S S _ F E A T U R E _ N A M E );
// should normalize class name and avoid invalid class names here!
OClass aClass = ontology . getOClass ( ontology . cr eateOU RIForN ame ( className ));
if ( aClass == null ) {
System . err . println ( " Error class \" " + className + " \" does not exist ! " );
return ;
}

19
20
21

//ﬁnd the text covered by the annotation
String theMentionText = gate . Utils . stringFor ( doc , mentionAnn );

22
23
24
25
26
27
28

// when creating a URI from text that came from a document you must take care
// to ensure that the name does not contain any characters that are illegal
// in a URI. The following method does this nicely for English but you may
// want to do your own normalization instead if you have non-English text.
String mentionName = OUtils . toResourceName ( theMentionText );

340

Working with Ontologies

// get the property to store mention texts for mention instances
DatatypeProperty prop =
ontology . getDatatypeProper t y ( ontology . crea teOURI ForNam e ( " mentionText " ));

29
30
31
32

OURI mentionURI = ontology . cr eateOU RIForN ame ( mentionName );
// if that mention instance does not already exist, add it
if (! ontology . co ntains OInsta nce ( mentionURI )) {
OInstance inst = ontology . addOInstance ( mentionURI , aClass );
// add the actual mention text to the instance
try {
inst . a dd D a t at y p e Pr o p e rt y V a l u e ( prop ,
new Literal ( theMentionText , OConstants . ENGLISH ));
}
catch ( I n v a l i d V a l u e E x c e p t i o n e ) {
throw new JapeException ( e );
}
}

33
34
35
36
37
38
39
40
41
42
43
44
45
46

}

This will match each annotation of type Mention in the input and assign it to a label
‘mention’. That label is then used in the right hand side to ﬁnd the annotation that was
matched by the pattern (lines 5–10); the value for the class feature of the annotation is
used to identify the ontological class name (lines 12–14); and the annotation span is used to
extract the text covered in the document (lines 16–26). Once all these pieces of information
are available, the addition to the ontology can be done. First the right class in the ontology
is identiﬁed using the class name (lines 28–37) and then a new instance for that class is
created (lines 38–50).
Beside JAPE, another tool that could play a part in this application is the Ontological
Gazetteer, see Section 13.4, which can be useful in bootstrapping the IE application that
ﬁnds entity mentions.
The solution presented here is purely pedagogical as it does not address many issues that
would be encountered in a real life application solving the same problem. For instance,
it is na¨ to assume that the name for the entity would be exactly the text found in the
ıve
document. In many cases entities have several aliases – for example the same person name
can be written in a variety of forms depending on whether titles, ﬁrst names, or initials are
used. A process of name normalisation would probably need to be employed in order to
make sure that the same entity, regardless of the textual form it is mentioned in, will always
be linked to the same ontology instance.
For a detailed description of the GATE ontology API, please consult the JavaDoc documentation.

Working with Ontologies

14.13

341

Ontology API and Implementation Changes

This section describes the changes in the API and the implementation made in GATE Developer version 5.1. The most important change is that the implementation of the ontology API
has been removed from the GATE core and is now being made available as plugins. Currently
the plugin Ontology OWLIM2 provides the implementation that was present in the GATE core
previously and the plugin Ontology provides a new and upgraded implementation that also
implements some new features that were added to the API. The Ontology OWLIM2 plugin
is intended to provide maximum backwards compatibility but will not be developed further
and be phased out in the future, while the Ontology plugin provides the current actively
developed implementation.
Before any ontology-related function can be used in GATE, one of the ontology implementation plugins must be loaded.

14.13.1

Diﬀerences between the implementation plugins

The implementation provided in plugin Ontology OWLIM2 is based on Sesame version 1 and
OWLIM version 2, while the changed implementation provided in plugin Ontology is based
on Sesame version 2 and OWLIM version 3.
The plugin Ontology provides the ontology language resource OWLIM Ontology with new
and changed parameters. In addition, there are two language resources for advanced users,
Create Sesame Ontology and Connect Sesame Ontology. Finally the new implementation provides the language resource OWLIM Ontology DEPRECATED to make the move from
the old to the new implementation easier: this language resource has the same name, parameters and Java package as the language resource OWLIMOntologyLR in backwards-compatibility
plugin Ontology OWLIM2. This allows to test existing pipelines and applications with the
new implementation without the necessity to adapt the names of the language resource or
parameters.
The implementation in plugin Ontology makes various attempts to reduce the amount of
memory needed to load an ontology. This will allow to load signiﬁcantly larger ontologies
into GATE. This comes at the price of some methods needing more time than before, as the
implementation does not cache all ontology entities in GATE’s memory any more.
The new implementation does not provide access to any implementation detail anymore, the
method getSesameRepository will therefore throw an exception. The return type of this
method in the old implementation has been changed to Object to remove the dependency
on a Sesame class in the GATE API.

342

14.13.2

Working with Ontologies

Changes in the Ontology API

The class gate.creole.ontology.URI has been deprecated. Instead, the ontology API
client must use objects that implement the new OURI, OBnodeID or ONodeID interfaces.
An API client can only directly create OURI objects and must use the ontology factory
methods createOURI, createOURIFroName or generateOURI to create such objects.
Also, the intended way how ontologies are modeled has been changed: the API tries to
prevent a user from adding anything to an ontology that would make an ontology that
conforms to OWL-Lite go beyond that sublanguage (and e.g. become OWL-Full). However,
if an ontology already is not conforming to OWL-Lite, the API tries to make as much
information visible to the client as possible. That means for instance that RDF classes will
be included in the list of classes returned by method getOClasses, but there is no support
for adding RDF classes to an ontology. Similarly, all methods that already existed which
would allow to add entities to an ontology that do not conform to OWL-Lite have been
deprecated.
Most methods that use a constant from class OConstants which is deﬁned as a byte value
have been deprecated and replaced by methods that use enums that replace the byte constants instead. (however, the byte constants used for literal string languages are still used).
The API now supports the handling of ontology imports more ﬂexibly. Ontology imports
are internally kept in a named graph that is diﬀerent from the named graph data from
loaded ontologies is kept in. Imported ontology data is still visible to the ontology API but
can be ignored when storing (serializing) an ontology. The ontology API now also allows
to explicitly resolve ontology imports and it allows the speciﬁcation of mappings between
import URIs and URLs of either local ﬁles or substitute web URLs. The import map can
also specify patterns for substituting import URIs with replacement URLs (or ignoring them
altogether).
The default namespace URI is now set automatically from the ontology if possible and the
API allows getting and setting the ontology URI.
The ontology API now oﬀers methods for getting an iterator when accessing some ontology
resources, e.g. when getting all classes in the ontology. This helps to prevent the excessive
use of memory when retrieving a large number of such resources from a large ontology.
Ontology objects do not internally store copies of all ontology resources in hash maps
any more. This means that re-fetching ontology resources will be a slower operation and
old methods that rely on this mechanism are either deprecated (getOResourcesByName,
getOResourceByName) or do not work at all any more (getOResourceFromMap,
addOResourceToMap, removeOResourceFromMap).

Chapter 15
Non-English Language Support
There are plugins available for processing the following languages: French, German, Spanish,
Italian, Chinese, Arabic, Romanian, Hindi and Cebuano. Some of the applications are quite
basic and just contain some useful processing resources to get you started when developing
a full application. Others (Cebuano and Hindi) are more like toy systems built as part of an
exercise in language portability.
Note that if you wish to use individual language processing resources without loading the
whole application, you will need to load the relevant plugin for that language in most cases.
The plugins all follow the same kind of format. Load the plugin using the plugin manager
in GATE Developer, and the relevant resources will be available in the Processing Resources
set.
Some plugins just contain a list of resources which can be added ad hoc to other applications.
For example, the Italian plugin simply contains a lexicon which can be used to replace the
English lexicon in the default English POS tagger: this will provide a reasonable basic POS
tagger for Italian.
In most cases you will also ﬁnd a directory in the relevant plugin directory called data which
contains some sample texts (in some cases, these are annotated with NEs).
There are also a number of plugins, documented elsewhere in this manual that while they
default to processing English can be conﬁgured to support other languages. These include
the TaggerFramework (Section 20.3), the Numbers Tagger (Section 20.5.1), and the Snowball
based stemmer (Section 20.9). The LingPipe POS Tagger PR (Section 20.25.3) now includes
two models for Bulgarian.

343

344

15.1

Non-English Language Support

French Plugin

The French plugin contains two applications for NE recognition: one which includes the
TreeTagger for POS tagging in French (french+tagger.gapp) , and one which does not
(french.gapp). Simply load the application required from the plugins/Lang French directory.
You do not need to load the plugin itself from the GATE Developer’s Plugin Management
Console. Note that the TreeTagger must ﬁrst be installed and set up correctly (see Section
20.3 for details). Check that the runtime parameters are set correctly for your TreeTagger in your application. The applications both contain resources for tokenisation, sentence
splitting, gazetteer lookup, NE recognition (via JAPE grammars) and orthographic coreference. Note that they are not intended to produce high quality results, they are simply a
starting point for a developer working on French. Some sample texts are contained in the
plugins/Lang French/data directory.

15.2

German Plugin

The German plugin contains two applications for NE recognition: one which includes the
TreeTagger for POS tagging in German (german+tagger.gapp) , and one which does not (german.gapp). Simply load the application required from the plugins/Lang German/resources
directory. You do not need to load the plugin itself from the GATE Developer’s Plugin
Management Console. Note that the TreeTagger must ﬁrst be installed and set up correctly (see Section 20.3 for details). Check that the runtime parameters are set correctly
for your TreeTagger in your application. The applications both contain resources for tokenisation, sentence splitting, gazetteer lookup, compound analysis, NE recognition (via JAPE
grammars) and orthographic coreference. Some sample texts are contained in the plugins/Lang German/data directory. We are grateful to Fabio Ciravegna and the Dot.KOM
project for use of some of the components for the German plugin.

15.3

Romanian Plugin

The Romanian plugin contains an application for Romanian NE recognition (romanian.gapp). Simply load the application from the plugins/Lang Romanian/resources directory. You do not need to load the plugin itself from the GATE Developer’s Plugin
Management Console. The application contains resources for tokenisation, gazetteer lookup,
NE recognition (via JAPE grammars) and orthographic coreference. Some sample texts are
contained in the plugins/romanian/corpus directory.

Non-English Language Support

15.4

345

Arabic Plugin

The Arabic plugin contains a simple application for Arabic NE recognition (arabic.gapp).
Simply load the application from the plugins/Lang Arabic/resources directory. You do not
need to load the plugin itself from the GATE Developer’s Plugin Management Console. The
application contains resources for tokenisation, gazetteer lookup, NE recognition (via JAPE
grammars) and orthographic coreference. Note that there are two types of gazetteer used in
this application: one which was derived automatically from training data (Arabic inferred
gazetteer), and one which was created manually. Note that there are some other applications
included which perform quite speciﬁc tasks (but can generally be ignored). For example,
arabic-for-bbn.gapp and arabic-for-muse.gapp make use of a very speciﬁc set of training
data and convert the result to a special format. There is also an application to collect new
gazetteer lists from training data (arabic lists collector.gapp). For details of the gazetteer
list collector please see Section 13.8.

15.5

Chinese Plugin

The Chinese plugin contains two components: a simple application for Chinese NE recognition (chinese.gapp) and a component called “Chinese Segmenter”.
In order to use the former, simply load the application from the plugins/Lang Chinese/resources
directory. You do not need to load the plugin itself from the GATE Developer’s Plugin Management Console. The application contains resources for tokenisation, gazetteer lookup, NE
recognition (via JAPE grammars) and orthographic coreference. The application makes
use of some gazetteer lists (and a grammar to process them) derived automatically from
training data, as well as regular hand-crafted gazetteer lists. There are also applications
(listscollector.gapp, adj collector.gapp and nounperson collector.gapp) to create such lists,
and various other application to perform special tasks such as coreference evaluation (coreference eval.gapp) and converting the output to a diﬀerent format (ace-to-muse.gapp).

15.5.1

Chinese Word Segmentation

Unlike English, Chinese text does not have a symbol (or delimiter) such as blank space to
explicitly separate a word from the surrounding words. Therefore, for automatic Chinese
text processing, we may need a system to recognise the words in Chinese text, a problem
known as Chinese word segmentation. The plugin described in this section performs the
task of Chinese word segmentation. It is based on our work using the Perceptron learning
algorithm for the Chinese word segmentation task of the Sighan 20051 . [Li et al. 05c]. Our
Perceptron based system has achieved very good performance in the Sighan-05 task.
1

See http://www.sighan.org/bakeoﬀ2005/ for the Sighan-05 task

346

Non-English Language Support

The plugin is called Lang Chinese and is available in the GATE distribution. The corresponding processing resource’s name is Chinese Segmenter PR. Once you load the PR into
GATE, you may put it into a Pipeline application. Note that it does not process a corpus
of documents, but a directory of documents provided as a parameter (see description of
parameters below). The plugin can be used to learn a model from segmented Chinese text
as training data. It can also use the learned model to segment Chinese text. The plugin
can use diﬀerent learning algorithms to learn diﬀerent models. It can deal with diﬀerent
character encodings for Chinese text, such as UTF-8, GB2312 or BIG5. These options can
be selected by setting the run-time parameters of the plugin.
The plugin has ﬁve run-time parameters, which are described in the following.
learningAlg is a String variable, which speciﬁes the learning algorithm used for producing the model. Currently it has two values, PAUM and SVM, representing the two
popular learning algorithms Perceptron and SVM, respectively. The default value is
PAUM.
Generally speaking, SVM may perform better than Perceptron, in particular for small
training sets. On the other hand, Perceptron’s learning is much faster than SVM’s.
Hence, if you have a small training set, you may want to use SVM to obtain a better
model. However, if you have a big training set which is typical for the Chinese word
segmentation task, you may want to use Perceptron for learning, because the SVM’s
learning may take too long time. In addition, using a big training set, the performance
of the Perceptron model is quite similar to that of the SVM model. See [Li et al. 05c]
for the experimental comparison of SVM and Perceptron on Chinese word segmentation.
learningMode determines the two modes of using the plugin, either learning a model
from training data or applying a learned model to segment Chinese text. Accordingly it
has two values, SEGMENTING and LEARNING. The default value is SEGMENTING,
meaning segmenting the Chinese text.
Note that you ﬁrst need to learn a model and then you can use the learned model to
segment the text. Several models using the training data used in the Sighan-05 Bakeoﬀ
are available for this plugin, which you can use to segment your Chinese text. More
descriptions about the provided models will be given below.
modelURL speciﬁes an URL referring to a directory containing the model. If the
plugin is in the LEARNING runmode, the model learned will be put into the directory.
If it is in the SEGMENTING runmode, the plugin will use the model stored in the
directory to segment the text. The models learned from the Sighan-05 bakeoﬀ training
data will be discussed below.
textCode speciﬁes the encoding of the text used. For example it can be UTF-8, BIG5,
GB2312 or any other encoding for Chinese text. Note that, when you segment some
Chinese text using a learned model, the Chinese text should use the same encoding as
the one used by the training text for obtaining the model.

Non-English Language Support

347

textFilesURL speciﬁes an URL referring to a directory containing the Chinese documents. All the documents contained in this directory (but not those documents contained in its sub-directory if there is any) will be used as input data. In the LEARNING
runmode, those documents contain the segmented Chinese text as training data. In
the SEGMENTING runmode, the text in those documents will be segmented. The
segmented text will be stored in the corresponding documents in the sub-directory
called segmented.
The following PAUM models are distributed with plugins and are available as compressed zip
ﬁles under the plugins/Lang Chinese/resources/models directory. Please unzip them to use.
In detail, those models were learned using the PAUM learning algorithm from the corpora
provided by Sighan-05 bakeoﬀ task.
the PAUM model learned from PKU training data, using the PAUM learning algorithm
and the UTF-8 encoding, is available as model-paum-pku-utf8.zip.
the PAUM model learned from PKU training data, using the PAUM learning algorithm
and the GB2312 encoding, is available as model-paum-pku-gb.zip.
the PAUM model learned from AS training data, using the PAUM learning algorithm
and the UTF-8 encoding, is available as model-as-utf8.zip.
the PAUM model learned from AS training data, using the PAUM learning algorithm
and the BIG5 encoding, is available as model-as-big5.zip.
As you can see, those models were learned using diﬀerent training data and diﬀerent Chinese
text encodings of the same training data. The PKU training data are news articles published
in mainland China and use simpliﬁed Chinese, while the AS training data are news articles
published in Taiwan and use traditional Chinese. If your text are in simpliﬁed Chinese, you
can use the models trained by the PKU data. If your text are in traditional Chinese, you
need to use the models trained by the AS data. If your data are in GB2312 encoding or any
compatible encoding, you need use the model trained by the corpus in GB2312 encoding.
Note that the segmented Chinese text (either used as training data or produced by this
plugin) use the blank space to separate a word from its surrounding words. Hence, if your
data are in Unicode such as UTF-8, you can use the GATE Unicode Tokeniser to process the
segmented text to add the Token annotations into your text to represent the Chinese words.
Once you get the annotations for all the Chinese words, you can perform further processing
such as POS tagging and named entity recognition.

15.6

Hindi Plugin

The Hindi plugin (‘Lang Hindi’) contains a set of resources for basic Hindi NE recognition
which mirror the ANNIE resources but are customised to the Hindi language. You need to

348

Non-English Language Support

have the ANNIE plugin loaded ﬁrst in order to load any of these PRs. With the Hindi, you
can create an application similar to ANNIE but replacing the ANNIE PRs with the default
PRs from the plugin.

Chapter 16
Parsers
16.1

MiniPar Parser

MiniPar is a shallow parser. In its shipped version, it takes one sentence as an input and
determines the dependency relationships between the words of a sentence. It parses the
sentence and brings out the information such as:
the lemma of the word;
the part of speech of the word;
the head modiﬁed by this word;
name of the dependency relationship between this word and the head;
the lemma of the head.
In the version of MiniPar integrated in GATE (‘Parser Minipar’ plugin), it generates annotations of type ‘DepTreeNode’ and the annotations of type ‘[relation]’ that exists between the
head and the child node. The document is required to have annotations of type ‘Sentence’,
where each annotation consists of a string of the sentence.
Minipar takes one sentence at a time as an input and generates the tokens of type ‘DepTreeNode’. Later it assigns relation between these tokens. Each DepTreeNode consists of
feature called ‘word’: this is the actual text of the word.
For each and every annotation of type ‘[Rel]’, where ‘Rel’ is obj, pred etc. This is the name of
the dependency relationship between the child word and the head word (see Section 16.1.5).
Every ‘[Rel]’ annotation is assigned four features:
child word: this is the text of the child annotation;
349

350

Parsers

Figure 16.1: a MiniPar annotated document

child id: IDs of the annotations which modify the current word (if any).
head word: this is the text of the head annotation;
head id: ID of the annotation modiﬁed by the child word (if any);

Figure 16.1 shows a MiniPar annotated document in GATE Developer.

16.1.1

Platform Supported

MiniPar in GATE is supported for the Linux and Windows operating systems. Trying to
instantiate this PR on any other OS will generate the ResourceInstantiationException.

Parsers

16.1.2

351

Resources

MiniPar in GATE is shipped with four basic resources:
MiniparWrapper.jar: this is a JAVA Wrapper for MiniPar;
creole.XML: this deﬁnes the required parameters for MiniPar Wrapper;
minipar.linux: this is a modiﬁed version of pdemo.cpp.
minipar-windows.exe : this is a modiﬁed version of pdemo.cpp compiled to work on
windows.

16.1.3

Parameters

The MiniPar wrapper takes six parameters:
annotationTypeName: new annotations are created with this type, default is ”DepTreeNode”;
annotationInputSetName: annotations of Sentence type are provided as an input
to MiniPar and are taken from the given annotationSet;
annotationOutputSetName: All annotations created by Minipar Wrapper are
stored under the given annotationOutputSet;
document: the GATE document to process;
miniparBinary: location of the MiniPar Binary ﬁle (i.e. either minipar.linux or
minipar-windows.exe. These ﬁles are available under gate/plugins/minipar/ directory);
miniparDataDir: location of the ‘data’ directory under the installation directory of
MINIPAR. default is ”%MINIPAR HOME%/data”.

16.1.4

Prerequisites

The MiniPar wrapper requires the MiniPar library to be available on the underlying Linux/Windows machine. It can be downloaded from the MiniPar homepage.

352

16.1.5

Parsers

Grammatical Relationships

appo
"ACME president, --appo-> P.W. Buckman"
aux "should <-aux-- resign"
be "is <-be-- sleeping"
c
"that <-c-- John loves Mary"
comp1
first complement
det "the <-det ‘-- hat"
gen "Jane’s <-gen-- uncle"
i
the relationship between a C clause and its I clause
inv-aux
inverted auxiliary: "Will <-inv-aux-- you stop it?"
inv-be
inverted be: "Is <-inv-be-- she sleeping"
inv-have
inverted have: "Have <-inv-have-- you slept"
mod the relationship between a word and its adjunct modifier
pnmod
post nominal modifier
p-spec
specifier of prepositional phrases
pcomp-c
clausal complement of prepositions
pcomp-n
nominal complement of prepositions
post
post determiner
pre
pre determiner
pred
predicate of a clause
rel
relative clause
vrel
passive verb modifier of nouns
wha, whn, whp: wh-elements at C-spec positions
obj
object of verbs
obj2
second object of ditransitive verbs
subj
subject of verbs
s
surface subjec

16.2

RASP Parser

RASP (Robust Accurate Statistical Parsing) is a robust parsing system for English, developed by the Natural Language and Computational Linguistics group at the University of
Sussex.
This plugin, ‘Parser RASP’, developed by DigitalPebble, provides four wrapper PRs that
call the RASP modules as external programs, as well as a JAPE component that translates
the output of the ANNIE POS Tagger (Section 6.6).
RASP2 Tokenizer This PR requires Sentence annotations and creates Token annotations with a string feature. Note that sentence-splitting must be carried out before
tokenization; the the RegEx Sentence Splitter (see Section 6.5) is suitable for this.
(Alternatively, you can use the ANNIE Tokenizer (Section 6.2) and then the ANNIE

Parsers

353

Sentence Splitter (Section 6.4); their output is compatible with the other PRs in this
plugin).
RASP2 POS Tagger This requires Token annotations and creates WordForm annotations
with pos, probability, and string features.
RASP2 Morphological Analyser This requires WordForm annotations (from the POS
Tagger) and adds lemma and suffix features.
RASP2 Parser This requires the preceding annotation types and creates multiple
Dependency annotations to represent a parse of each sentence.
RASP POS Converter This PR requires Token annotations with a category feature as
produced by the ANNIE POS Tagger (see Section 6.6 and creates WordForm annotations
in the RASP Format. The ANNIE POS Tagger and this Converter can together be
used as a substitute for the RASP2 POS Tagger.
Here are some examples of corpus pipelines that can be correctly constructed with these
PRs.
1. RegEx Sentence Splitter
2. RASP2 Tokenizer
3. RASP2 POS Tagger
4. RASP2 Morphological Analyser
5. RASP2 Parser
1. RegEx Sentence Splitter
2. RASP2 Tokenizer
3. ANNIE POS Tagger
4. RASP POS Converter
5. RASP2 Morphological Analyser
6. RASP2 Parser
1. ANNIE Tokenizer
2. ANNIE Sentence Splitter
3. RASP2 POS Tagger

354

Parsers

4. RASP2 Morphological Analyser
5. RASP2 Parser
1. ANNIE Tokenizer
2. ANNIE Sentence Splitter
3. ANNIE POS Tagger
4. RASP POS Converter
5. RASP2 Morphological Analyser
6. RASP2 Parser
Further documentation is included in the directory gate/plugins/Parser\_RASP/doc/.
The RASP package, which provides the external programs, is available from the RASP web
page.
RASP is only supported for Linux operating systems. Trying to run it on any other operating
systems will generate an exception with the message: ‘The RASP cannot be run on any other
operating systems except Linux.’
It must be correctly installed on the same machine as GATE, and must be installed in a
directory whose path does not contain any spaces (this is a requirement of the RASP scripts
as well as the wrapper). Before trying to run scripts for the ﬁrst time, edit rasp.sh and
rasp_parse.sh to set the correct value for the shell variable RASP, which should be the ﬁle
system pathname where you have installed the RASP tools (for example, RASP=/opt/RASP or
RASP=/usr/local/RASP. You will need to enter the same path for the initialization parameter
raspHome for the POS Tagger, Morphological Analyser, and Parser PRs.
(On some systems the arch command used in the scripts is not available; a work-around is
to comment that line out and add arch=’ix86_linux’, for example.)
(The previous version of the RASP plugin can now be found in plugins/Obsolete/rasp.)

16.3

SUPPLE Parser

SUPPLE is a bottom-up parser that constructs syntax trees and logical forms for English
sentences. The parser is complete in the sense that every analysis licensed by the grammar
is produced. In the current version only the ‘best’ parse is selected at the end of the parsing
process. The English grammar is implemented as an attribute-value context free grammar
which consists of subgrammars for noun phrases (NP), verb phrases (VP), prepositional

Parsers

355

phrases (PP), relative phrases (R) and sentences (S). The semantics associated with each
grammar rule allow the parser to produce logical forms composed of unary predicates to
denote entities and events (e.g., chase(e1), run(e2)) and binary predicates for properties
(e.g. lsubj(e1,e2)). Constants (e.g., e1, e2) are used to represent entity and event identiﬁers.
The GATE SUPPLE Wrapper stores syntactic information produced by the parser in the
gate document in the form of parse annotations containing a bracketed representation of the
parse; and semantics annotations that contains the logical forms produced by the parser.
It also produces SyntaxTreeNode annotations that allow viewing of the parse tree for a
sentence (see Section 16.3.4).

16.3.1

Requirements

The SUPPLE parser is written in Prolog, so you will need a Prolog interpreter to run
the parser. A copy of PrologCafe (http://kaminari.scitec.kobe-u.ac.jp/PrologCafe/), a
pure Java Prolog implementation, is provided in the distribution. This should work on
any platform but it is not particularly fast. SUPPLE also supports the open-source
SWI Prolog (http://www.swi-prolog.org) and the commercially licenced SICStus prolog
(http://www.sics.se/sicstus, SUPPLE supports versions 3 and 4), which are available for
Windows, Mac OS X, Linux and other Unix variants. For anything more than the simplest
cases we recommend installing one of these instead of using PrologCafe.

16.3.2

Building SUPPLE

The SUPPLE plugin must be compiled before it can be used, so you will require a suitable
Java SDK (GATE itself requires only the JRE to run). To build SUPPLE, ﬁrst edit the ﬁle
build.xml in the Parser SUPPLE directory under plugins, and adjust the user-conﬁgurable
options at the top of the ﬁle to match your environment. In particular, if you are using SWI
or SICStus Prolog, you will need to change the swi.executable or sicstus.executable
property to the correct name for your system. Once this is done, you can build the plugin
by opening a command prompt or shell, going to the Parser SUPPLE directory and running:
../../bin/ant swi

(on Windows, use ..\..\bin\ant). For PrologCafe or SICStus, replace swi with plcafe or
sicstus as appropriate.

16.3.3

Running the Parser in GATE

In order to parse a document you will need to construct an application that has:

356

Parsers

tokeniser
splitter
POS-tagger
Morphology
SUPPLE Parser with parameters
mapping ﬁle (conﬁg/mapping.conﬁg)
feature table ﬁle (conﬁg/feature table.conﬁg)
parser ﬁle (supple.plcafe or supple.sicstus or supple.swi)
prolog implementation (shef.nlp.supple.prolog.PrologCafe,
shef.nlp.supple.prolog.SICStusProlog3, shef.nlp.supple.prolog.SICStusProlog4,
shef.nlp.supple.prolog.SWIProlog or shef.nlp.supple.prolog.SWIJavaProlog1 ).
You can take a look at build.xml to see examples of invocation for the diﬀerent implementations.
Note that prior to GATE 3.1, the parser ﬁle parameter was of type java.io.File. From
3.1 it is of type java.net.URL. If you have a saved application (.gapp ﬁle) from before
GATE 3.1 which includes SUPPLE it will need to be updated to work with the new version.
Instructions on how to do this can be found in the README ﬁle in the SUPPLE plugin
directory.

16.3.4

Viewing the Parse Tree

GATE Developer provides a syntax tree viewer in the Tools plugin which can display the
parse tree generated by SUPPLE for a sentence. To use the tree viewer, be sure that the
Tools plugin is loaded, then open a document in GATE Developer that has been processed
with SUPPLE and view its Sentence annotations. Right-click on the relevant Sentence
annotation in the annotations table and select ‘Edit with syntax tree viewer’. This viewer
can also be used with the constituency output of the Stanford Parser PR (Section 16.4).

16.3.5

System Properties

The SICStusProlog (3 and 4) and SWIProlog implementations work by calling the native
prolog executable, passing data back and forth in temporary ﬁles. The location of the prolog
executable is speciﬁed by a system property:
1
shef.nlp.supple.prolog.SICStusProlog exists for backwards compatibility and behaves the same as SICStusProlog3.

Parsers

357

for SICStus: supple.sicstus.executable - default is to look for sicstus.exe (Windows) or sicstus (other platforms) on the PATH.
for SWI: supple.swi.executable - default is to look for plcon.exe (Windows) or
swipl (other platforms) on the PATH.
If your prolog is installed under a diﬀerent name, you should specify the correct name in
the relevant system property. For example, when installed from the source distribution, the
Unix version of SWI prolog is typically installed as pl, most binary packages install it as
swipl, though some use the name swi-prolog. You can also use the properties to specify
the full path to prolog (e.g. /opt/swi-prolog/bin/pl) if it is not on your default PATH.
For details of how to pass system properties to GATE, see the end of Section 2.3.

16.3.6

Conﬁguration Files

Two ﬁles are used to pass information from GATE to the SUPPLE parser: the mapping ﬁle
and the feature table ﬁle.

Mapping File
The mapping ﬁle speciﬁes how annotations produced using GATE are to be passed to the
parser. The ﬁle is composed of a number of pairs of lines, the ﬁrst line in a pair speciﬁes
a GATE annotation we want to pass to the parser. It includes the AnnotationSet (or
default), the AnnotationType, and a number of features and values that depend on the
AnnotationType. The second line of the pair speciﬁes how to encode the GATE annotation
in a SUPPLE syntactic category, this line also includes a number of features and values. As
an example consider the mapping:
Gate;AnnotationType=Token;category=DT;string=&S
SUPPLE;category=dt;m_root=&S;s_form=&S

It speciﬁes how a determinant (’DT’) will be translated into a category ‘dt’ for the parser.
The construct ‘&S’ is used to represent a variable that will be instantiated to the appropriate
value during the mapping process. More speciﬁcally a token like ‘The’ recognised as a DT
by the POS-tagging will be mapped into the following category:
dt(s_form:’The’,m_root:’The’,m_affix:’_’,text:’_’).

As another example consider the mapping:

358

Parsers

Gate;AnnotationType=Lookup;majorType=person_first;minorType=female;string=&S
SUPPLE;category=list_np;s_form=&S;ne_tag=person;ne_type=person_first;gender=female

It speciﬁed that an annotation of type ‘Lookup’ in GATE is mapped into a category ‘list np’
with speciﬁc features and values. More speciﬁcally a token like ‘Mary’ identiﬁed in GATE
as a Lookup will be mapped into the following SUPPLE category:
list_np(s_form:’Mary’,m_root:’_’,m_affix:’_’,
text:’_’,ne_tag:’person’,ne_type:’person_first’,gender:’female’).

Feature Table
The feature table ﬁle speciﬁes SUPPLE ‘lexical’ categories and its features. As an example
an entry in this ﬁle is:
n;s_form;m_root;m_affix;text;person;number

which speciﬁes which features and in which order a noun category should be written. In this
case:
n(s_form:...,m_root:...,m_affix:...,text:...,person:...,number:....).

16.3.7

Parser and Grammar

The parser builds a semantic representation compositionally, and a ‘best parse’ algorithm
is applied to each ﬁnal chart, providing a partial parse if no complete sentence span can be
constructed. The parser uses a feature valued grammar. Each Category entry has the form:
Category(Feature1:Value1,...,FeatureN:ValueN)

where the number and type of features is dependent on the category type (see Section 5.1).
All categories will have the features s form (surface form) and m root (morphological root);
nominal and verbal categories will also have person and number features; verbal categories
will also have tense and vform features; and adjectival categories will have a degree feature.
The list np category has the same features as other nominal categories plus ne tag and
ne type.
Syntactic rules are speciﬁed in Prolog with the predicate rule(LHS, RHS) where LHS is a
syntactic category and RHS is a list of syntactic categories. A rule such as BN P HEAD ⇒
N (‘a basic noun phrase head is composed of a noun’) is written as follows:

Parsers

359

rule(bnp_head(sem:E^[[R,E],[number,E,N]],number:N),
[n(m_root:R,number:N)]).

where the feature ‘sem’ is used to construct the semantics while the parser processes input,
and E, R, and N are variables to be instantiated during parsing.
The full grammar of this distribution can be found in the prolog/grammar directory, the ﬁle
load.pl speciﬁes which grammars are used by the parser. The grammars are compiled when
the system is built and the compiled version is used for parsing.

16.3.8

Mapping Named Entities

SUPPLE has a prolog grammar which deals with named entities, the only information required is the Lookup annotations produced by Gate, which are speciﬁed in the mapping ﬁle.
However, you may want to pass named entities identiﬁed with your own Jape grammars in
GATE. This can be done using a special syntactic category provided with this distribution.
The category sem cat is used as a bridge between Gate named entities and the SUPPLE
grammar. An example of how to use it (provided in the mapping ﬁle) is:
Gate;AnnotationType=Date;string=&S
SUPPLE;category=sem_cat;type=Date;text=&S;kind=date;name=&S

which maps a named entity ‘Date’ into a syntactic category ’sem cat’. A grammar ﬁle
called semantic rules.pl is provided to map sem cat into the appropriate syntactic category
expected by the phrasal rules. The following rule for example:
rule(ne_np(s_form:F,sem:X^[[name,X,NAME],[KIND,X]]),[
sem_cat(s_form:F,text:TEXT,type:’Date’,kind:KIND,name:NAME)]).

is used to parse a ‘Date’ into a named entity in SUPPLE which in turn will be parsed into
a noun phrase.

16.3.9

Upgrading from BuChart to SUPPLE

In theory upgrading from BuChart to SUPPLE should be relatively straightforward. Basically any instance of BuChart needs to be replaced by SUPPLE. Speciﬁc changes which
must be made are:
The compiled parser ﬁles are now supple.swi, supple.sicstus, or supple.plcafe

360

Parsers

The GATE wrapper parameter buchartFile is now SUPPLEFile, and it is now of type
java.net.URL rather than java.io.File. Details of how to compensate for this in
existing saved applications are given in the SUPPLE README ﬁle.
The Prolog wrappers now start shef.nlp.supple.prolog instead of shef.nlp.buchart.prolog
The mapping.conf ﬁle now has lines starting SUPPLE; instead of Buchart;
Most importantly the main wrapper class is now called nlp.shef.supple.SUPPLE
Making these changes to existing code should be trivial and allow application to beneﬁt from
future improvements to SUPPLE.

16.4

Stanford Parser

The Stanford Parser is a probabilistic parsing system implemented in Java by Stanford
University’s Natural Language Processing Group. Data ﬁles are available from Stanford for
parsing Arabic, Chinese, English, and German.
This plugin, ‘Parser Stanford’, developed by the GATE team, provides a PR
(gate.stanford.Parser) that acts as a wrapper around the Stanford Parser (version 1.6.5)
and translates GATE annotations to and from the data structures of the parser itself. The
plugin is supplied with the unmodiﬁed jar ﬁle and one English data ﬁle obtained from
Stanford. Stanford’s software itself is subject to the full GPL.
The parser itself can be trained on other corpora and languages, as documented on the
website, but this plugin does not provide a means of doing so. Trained data ﬁles are not necessarily compatible between diﬀerent versions of the parser; in particular ﬁles from versions
before 1.6.1 are always incompatible with the software from version 1.6.1 onwards. (GATE
switched from 1.6 to 1.6.1 at build 3120 in January 2009, and to 1.6.5 in December 2010.)
Creating multiple instances of this PR in the same JVM—even with the same language
model ﬁle—does not work. The PRs can be instantiated, but runtime errors will almost
certainly occur. In particular, this PR is not safe for use in a multithreaded environment
(see Section 7.13).

16.4.1

Input Requirements

Documents to be processed by the Parser PR must already have Sentence and Token annotations, such as those produced by either ANNIE Sentence Splitter (Sections 6.4 and 6.5)
and the ANNIE English Tokeniser (Section 6.2).

Parsers

361

If the reusePosTags parameter is true, then the Token annotations must have category
features with compatible POS tags. The tags produced by the ANNIE POS Tagger are
compatible with Stanford’s parser data ﬁles for English (which also use the Penn treebank
tagset).

16.4.2

Initialization Parameters

parserFile the path to the trained data ﬁle; the default value points to the English data
ﬁle2 included with the GATE distribution. You can also use other ﬁles downloaded
from the Stanford Parser website or produced by training the parser.
mappingFile the optional path to a mapping ﬁle: a ﬂat, two-column ﬁle which the wrapper
can use to ‘translate’ tags. A sample ﬁle is included.3 By default this value is null
and mapping is ignored.
tlppClass an implementation of TreebankLangParserParams, used by the parser itself to
extract the dependency relations from the constituency structures. The default value
is compatible with the English data ﬁle supplied. Please refer to the Stanford NLP
Group’s documentation and the parser’s javadoc for a further explanation.

16.4.3

Runtime Parameters

annotationSetName the name of the annotationSet used for input (Token and Sentence
annotations) and output (SyntaxTreeNode and Dependency annotations, and
category and dependencies features added to Tokens).
debug a boolean value which controls the verbosity of the wrapper’s output.
reusePosTags if true, the wrapper will read category features (produced by an earlier
POS-tagging PR) from the Token annotations and force the parser to use them.
useMapping if this is true and a mapping ﬁle was loaded when the PR was initialized, the
POS and syntactic tags produced by the parser will be translated using that ﬁle. If no
mapping ﬁle was loaded, this parameter is ignored.
The following boolean parameters switch on and oﬀ the various types of output that the
parser can produce. Any or all of them can be true, but if all are false the PR will simply
print a warning to save time (instead of running the parser).
addPosTags if this is true, the wrapper will add category features to the Token annotations.
2
3

resources/englishPCFG.ser.gz
resources/english-tag-map.txt

362

Parsers

addConstituentAnnotations if true, the wrapper will mark the syntactic constituents
with SyntaxTreeNode annotations that are compatible with the Syntax Tree Viewer
(see Section 16.3.4).
addDependencyAnnotations if true, the wrapper will add Dependency annotations to
indicate the dependency relations in the sentence.
addDependencyFeatures if true, the wrapper will add dependencies features to the
Token annotations to indicate the dependency relations in the sentence.
The parser will derive the dependency structures only if either or both of the dependency
output options is enabled, so if you do not need the dependency analysis, you can disable
both of them and the PR will run faster.
Two sample GATE applications for English are included in the plugins/Parser Stanford
directory: sample_parser_en.gapp runs the Regex Sentence Splitter and ANNIE Tokenizer and then this PR to annotate constituency and dependency structures, whereas
sample_pos+parser_en.gapp also runs the ANNIE POS Tagger and makes the parser re-use
its POS tags.

Chapter 17
Machine Learning
This chapter presents machine learning PRs available in GATE. Currently, two PRs are
available:

The Batch Learning PR (in the Learning plugin) is GATE’s most comprehensive
and developed machine learning oﬀering. It is speciﬁcally targetted at NLP tasks
including text classiﬁcation, chunk learning (e.g. for named entity recognition) and
relation learning. It integrates LibSVM for improved speed, along with the PAUM
algorithm, oﬀering very competitive performance and speed. It also oﬀers a Weka
interface. It is documented in Section 17.2.
The Machine Learning PR (in the Machine Learning plugin) is GATE’s older
machine learning oﬀering. It oﬀers wrappers for Maxent, Weka and SVM Light. It is
documented in Section 17.3.

The rest of the chapter is organised as follows. Section 17.1 introduces machine learning in
general, focusing on the terminology used and the meaning of the terms within GATE. We
then move on to describe the two Machine Learning processing resources, beginning with the
Batch Learning PR in Section 17.2. Section 17.2.1 describes all the conﬁguration settings of
the Batch Learning PR one by one; i.e. all the elements in the conﬁguration ﬁle for setting
the Batch Learning PR (the learning algorithm to be used and the options for learning) and
deﬁning the NLP features for the problem. Section 17.2.2 presents three case studies with
example conﬁguration ﬁles for the three types of NLP learning problems. Section 17.2.3
lists the steps involved in using the Batch Learning PR. Finally, Section 17.2.4 explains the
outputs of the Batch Learning PR for the four usage modes; namely training, application,
evaluation and producing feature ﬁles only, and in particular, the format of the feature ﬁles
and label list ﬁle produced by the Batch Learning PR. Section 17.3 outlines the original
Machine Learning PR in GATE.
363

364

Machine Learning

17.1

ML Generalities

There are two main types of ML; supervised learning and unsupervised learning. Supervised
learning is more eﬀective and much more widely used in NLP. Classiﬁcation is a particular
example of supervised learning, in which the set of training examples is split into multiple
subsets (classes) and the algorithm attempts to distribute new examples into the existing
classes. This is the type of ML that is used in GATE, and all further references to ML
actually refer to classiﬁcation.
An ML algorithm ‘learns’ about a phenomenon by looking at a set of occurrences of that
phenomenon that are used as examples. Based on these, a model is built that can be used
to predict characteristics of future (unseen) examples of the phenomenon.
An ML implementation has two modes of functioning: training and application. The training
phase consists of building a model (e.g. a statistical model, a decision tree, a rule set, etc.)
from a dataset of already classiﬁed instances. During application, the model built during
training is used to classify new instances.
Machine Learning in NLP falls broadly into three categories of task type; text classiﬁcation,
chunk recognition, and relation extraction
Text classiﬁcation classiﬁes text into pre-deﬁned categories. The process can be
equally well applied at the document, sentence or token level. Typical examples of
text classiﬁcation might be document classiﬁcation, opinionated sentence recognition,
POS tagging of tokens and word sense disambiguation.
Chunk recognition often consists of two steps. First, it identiﬁes the chunks of
interest in the text. It then assigns a label or labels to these chunks. However some
problems comprise simply the ﬁrst step; identifying the relevant chunks. Examples of
chunk recognition include named entity recognition (and more generally, information
extraction), NP chunking and Chinese word segmentation.
Relation extraction determines whether or not a pair of terms in the text has some
type(s) of pre-deﬁned relations. Two examples are named entity relation extraction
and co-reference resolution.
Typically, the three types of NLP learning use diﬀerent linguistic features and feature representations. For example, it has been recognised that for text classiﬁcation the so-called
tf − idf representation of n-grams is very eﬀective (e.g. with SVM). For chunk recognition,
identifying the start token and the end token of the chunk by using the linguistic features
of the token itself and the surrounding tokens is eﬀective and eﬃcient. Relation extraction
beneﬁts from both the linguistic features from each of the two terms involved in the relation
and the features of the two terms combined.
The rest of this section explains some basic deﬁnitions in ML and their speciﬁcation in the
ML plugin.

Machine Learning

17.1.1

365

Some Deﬁnitions

instance: an example of the studied phenomenon. An ML algorithm learns a model
from a set of known instances, called a (training) dataset. It can then apply the learned
model to another (application) dataset.
attribute: a characteristic of the instances. Each instance is deﬁned by the values
of its attributes. The set of possible attributes is well deﬁned and is the same for
all instances in the training and application datasets. ‘Feature’ is also often used.
However, in this context, this can cause confusion with GATE annotation features.
class: an attribute for which the values are available in the training dataset for learning,
but which are not present in the application dataset. ML is used to ﬁnd the value of
this attribute in the application dataset.

17.1.2

GATE-Speciﬁc Interpretation of the Above Deﬁnitions

instance: an annotation. In order to use ML in GATE, users will need to choose the
type of annotations used as instances. Token annotations are a good candidate for
many NLP learning tasks such as information extraction and POS tagging, but any
type of annotation could be used (e.g. things that were found by a previously run
JAPE grammar, such as sentence annotations and document annotations for sentence
and document classiﬁcation respectively).
attribute: an attribute is the value of a named feature of a particular annotation
type, which can either (partially) cover the instance annotation considered or another
instance annotation which is related to the instance annotation considered. The value
of the attribute can refer to the current instance or to an instance either situated at
a speciﬁed location relative to the current instance or having special relation with the
current instance.
class: any attribute referring to the current instance can be marked as class attribute.

17.2

Batch Learning PR

This section describes the newest machine learning PR in GATE. The implementation focuses on the three main types of learning in NLP, namely chunk recognition (e.g. named
entity recognition), text classiﬁcation and relation extraction. The implementation for chunk
recognition is based on our work using support vector machines (SVM) for information extraction [Li et al. 05a]. The text classiﬁcation is based on our work on opinionated sentence
classiﬁcation and patent document classiﬁcation (see [Li et al. 07c] and [Li et al. 07d], respectively). The relation extraction is based on our work on named entity relation extraction
[Wang et al. 06].

366

Machine Learning

The Batch Learning PR, given a set of documents, can also produce feature ﬁles, containing
linguistic features and feature vectors, and labels if there are any in the documents. It can
also produce document-term matrices and n-gram based language models. Feature ﬁles are
in text format and can be used outside of GATE. Hence, users can use GATE-produced
feature ﬁles oﬀ-line, for their own purpose, e.g. evaluating new learning algorithms.
The PR also provides facilities for active learning, based on support vector machines (SVM),
mainly ranking the unlabelled documents according to the conﬁdence scores of the current
SVM models for those documents.
The primary learning algorithm implemented is SVM, which has achieved state of the art
performances for many NLP learning tasks. The training of SVM uses a Java version of
the SVM package LibSVM [CC001]. Application of SVM is implemented by ourselves. The
PAUM (Perceptron Algorithm with Uneven Margins) is also included [Li et al. 02], and
on our test datasets has consistently produced a performance to rival the SVM with much
reduced training times. Moreover, the ML implementation provides an interface to the opensource machine learning package Weka [Witten & Frank 99], and can use machine learning
algorithms implemented in Weka. Three widely-used learning algorithms are available in the
current implementation: Naive Bayes, KNN and the C4.5 decision tree algorithm.
Access to ML implementations is provided in GATE by the ‘Batch Learning PR’ (in the
‘learning’ plugin). The PR handles training and application of an ML model, evaluation
of learning on GATE documents, producing feature ﬁles and ranking documents for Active
Learning. It also makes it possible to view the primal forms of a linear SVM. This PR is a
Language Analyser so it can be used in all default types of GATE controllers.
In order to use the Batch Learning processing resource, the user has to do three things.
First, the user has to annotate some training documents with the labels that s/he wants
the learning system to annotate in new documents. Those label annotations should be
GATE annotations. Secondly, the user may need to pre-process the documents to obtain
linguistic features for the learning. Again, these features should be in the form of GATE
annotations. GATE’s plugin ANNIE might be helpful for producing the linguistic features.
Other resources such as the NP Chunker and parser may also be helpful. By providing the
machine learning algorithm with more and better information on which to base learning,
chances of a good result are increased, so this preprocessing stage is important. Finally the
user has to create a conﬁguration ﬁle for setting the ML PR, e.g. selecting the learning
algorithm and deﬁning the linguistic features used in learning. Three example conﬁguration
ﬁles are presented in this section; it might be helpful to take one of them as a starting point
and modify it.

17.2.1

Batch Learning PR Conﬁguration File Settings

In order to allow for more ﬂexibility, all conﬁguration parameters for the PR are set through
one external XML ﬁle, except for the learning mode, which is selected through normal PR

Machine Learning

367

parameterisation. The XML ﬁle contains both the conﬁguration parameters of the Batch
Learning PR itself and of the linguistic data (namely the deﬁnitions of the instance and
attributes) used by the Batch Learning PR. The XML ﬁle is speciﬁed when creating a new
Batch Learning PR.
The parent directory of the XML conﬁguration ﬁle becomes the working directory. A subdirectory in the working directory, named ‘savedFiles’, will be created (if it does not already
exist). All the ﬁles produced by the Batch Learning PR, including the NLP features ﬁles,
label list ﬁle, feature vector ﬁle and learned model ﬁle, will be stored in that subdirectory.
A log ﬁle recording the learning session is also created in this directory.
Below, we ﬁrst describe the parameters of the Batch Learning PR. Then we explain those
settings speciﬁed in the conﬁguration ﬁle.

PR Parameters: Settings not Speciﬁed in the Conﬁguration File
For the sake of convenience, a few settings are not speciﬁed in the conﬁguration ﬁle. Instead
the user should specify them as initialization or run-time parameters of the PR, as in other
PRs.

URL (or path and name) of the conﬁguration ﬁle. The user is required to give
the URL of the conﬁguration ﬁle when creating the PR. The conﬁguration ﬁle should
be in XML format with the extension name .xml. It contains most of learning settings
and will be explained in detail in the next subsection.
Corpus. This is a run-time parameter, meaning that the user should specify it after
creating the PR, and may change it between runs. The corpus contains the documents
that the PR will use as learning data (training or application). For application, the
documents should include all the annotations speciﬁed in the conﬁguration ﬁle, except
the class attribute. The annotations for class attribute should be available in the
documents used for training or evaluation.
inputASName is the annotation set containing the annotations for the linguistic
features to be used and the class labels.
outputASName is the annotation set in which the results of applying the models
will be put. Note that it should be set the same as the inputASName when doing the
evaluation (i.e. setting the learningMode as ‘EVALUATION’).
learningMode is a run-time parameter. It can be set as one of the following
values, ‘TRAINING’, ‘APPLICATION’, ‘EVALUATION’, ‘ProduceFeatureFilesOnly’,
‘MITRAINING’, ‘VIEWPRIMALFORMMODELS’ and ‘RankingDocsForAL’. The default learning mode is ‘TRAINING’.

368

Machine Learning

– In TRAINING mode, the PR learns from the data provided and saves the
models into a ﬁle called ‘learnedModels.save’ under the sub-directory ‘savedFiles’
of the working directory.
– If the user wants to apply the learned model to the data, s/he should select
APPLICATION mode. In application mode, the PR reads the learned model
from the ﬁle ‘learnedModels.save’ in the subdirectory ‘savedFiles’ and then applies
the model to the data.
– In EVALUATION mode, the PR will do k-fold or hold-out test set evaluation on
the corpus provided (the method of the evaluation is speciﬁed in the conﬁguration
ﬁle, see below), and output the evaluation results to the messages window of
GATE Developer, or standard out when using GATE Embedded, and into the
log ﬁle. When using evaluation mode, please make sure that the outputASName
is set to the same annotation set as the inputASName.
– If the user only wants to produce feature data and feature vectors but does not
want to train or apply a model, s/he may select the ProduceFeatureFilesOnly
mode. The feature ﬁles that the PR produces will be explained in detail in Section
17.2.4.
– In MITRAINING (mixed initiative training) mode, the training data are appended to the end of any existing feature ﬁle. In contrast, in training mode,
the training data created in the current session overwrite any existing feature ﬁle.
Consequently, mixed initiative training mode uses both the training data obtained
in this session and the data that existed in the feature ﬁle before starting the session. Hence, training mode is for batch learning, while mixed initiative training
mode can be used for on-line (or adaptive, or mixed-initiative) learning. There is
one parameter for mixed initiative training mode specifying the minimal number
of newly added documents before starting the learning procedure to update the
learned model. The parameter can be deﬁned in the conﬁguration ﬁle.
– VIEWPRIMALFORMMODELS mode is used for displaying the most salient
NLP features in the learned models. In the current implementation, the mode is
only valid with the linear SVM model, in which the most salient NLP features
correspond to the biggest (absolute values of) weights in the weight vector. In
the conﬁguration ﬁle one can specify two parameters to determine the number
of displayed NLP features for positive and negative weights. Note that if e.g.
the number for negative weight is set as 0, then no NLP feature is displayed for
negative weights.
– RankingDocsForAL applies the current learned SVM models (in the subdirectory ‘savedFiles’) to the feature vectors stored in the ﬁle ‘fvsDataSelecting.save’ in the sub-directory ‘savedFiles’ and ranks the documents according to
the margins of the examples in one document to the SVM models. The ranked
list of documents will be put into the ﬁle ‘ALRankedDocs.save’.
In most cases it is not safe to run more than one instance of the batch learning PR with the
same working directory at the same time, because the PR needs to update the model (in

Machine Learning

369

TRAINING, MITRAINING or EVALUATION mode) or other data ﬁles. It is safe to run
multiple instances at once provided they are all in APPLICATION mode1 .

Order of document processing In the usual case, in a GATE corpus pipeline application, documents are processed one at a time, and each PR is applied in turn to the document,
processing it fully, before moving on to the next document. The Batch Learning PR breaks
from this rule. ML training algorithms, including SVM, typically run as a batch process
over a training set, and require all the data to be fully prepared and passed to the algorithm
in one go. This means that in training (or evaluation) mode, the Batch Learning PR will
wait for all the documents to be processed and will then run as a single operation at the
end. Therefore, the Batch Learning PR needs to be positioned last in the pipeline. Postprocessing cannot be done within the pipeline after the Batch Learning PR. Where further
processing needs to be done, this should take the form of a separate application, and be
applied to the data afterwards.
There is an exception to the above, however. In application mode, the situation is slightly
diﬀerent, since the ML model has already been created, and the PR only applies it to the
data. This can be done on a document by document basis, in the manner of a normal PR.
However, although it can be done document by document, there may be advantages in terms
of eﬃciency to grouping documents into batches before applying the algorithm. A parameter
in the conﬁguration ﬁle, BATCH-APP-INTERVAL, described later, allows the user to specify
the size of such batches, and by default this is set to 1; in other words, by default, the Batch
Learning PR in application mode behaves like a normal PR and processes each document
separately. There may be substantial eﬃciency gains to be had through increasing this
parameter (although higher values require more memory consumption), but if the Batch
Learning PR is applied in application mode and the parameter BATCH-APP-INTERVAL
is set to 1, the PR can be treated like any other, and other PRs may be positioned after it
in a pipeline.

Settings in the Batch Learning PR XML Conﬁguration File
The root element of the XML conﬁguration ﬁle needs to be called ‘ML-CONFIG’, and it
must contain two basic elements; DATASET and ENGINE, and optionally other settings. In
the following, we ﬁrst describe the optional settings, then the ENGINE element, and ﬁnally
the DATASET element. In the next section, some examples of the XML conﬁguration ﬁle
are given for illustration. Please also refer to the conﬁguration ﬁles in the test directory (i.e.
plugs/learning/test/ under the main gate directory) for more examples.
1
This is only true for GATE 5.2 or later; in earlier versions all modes were unsafe for multiple instances
of the PR.

370

Machine Learning

Optional Settings in the Conﬁguration File The Batch Learning PR provides a variety of optional settings, which facilitate diﬀerent tasks. Every optional setting has a default
value; if an optional setting is not speciﬁed in the conﬁguration ﬁle, the Batch Learning PR
will adopt its default value. Each of the following optional settings can be set as an element
in the XML conﬁguration ﬁle.
SURROUND should be set to ‘true’ if the user wants the Batch Learning PR to learn
chunks by identifying the start token and the end token of the chunk. This approach to
chunk learning, for example, named entity recognition, where a span of several tokens
is to be identiﬁed, often produces better results than trying to learn every token in the
chunk. For classiﬁcation problems and relation extraction, set its value as ‘false’. This
element appears in the conﬁguration ﬁle as:
<SURROUND VALUE=’X’/>
where the variable X has two possible values: ‘true’ or ‘false’. The default value is
‘false’.
FILTERING relates to SVM training. Where the ratio of positive examples to negative examples is low, i.e. the instances belonging in the class are much outweighed
by instances outside of the class (e.g. ‘one against others’ is used, see multiClassiﬁcation2Binary below) SVMs can run into diﬃculties. The positive examples may
be swamped by outlying negative examples. The ML plugin provides functionality
developed through research (e.g. [Li & Bontcheva 08]) to assist in such cases. One example is the FILTERING parameter. The ﬁltering functionality performs initial SVM
training, then removes negative examples on the basis of their position relative to the
separator. It then retrains on the smaller dataset. Typically, negative instances close
to the boundary are removed. Note that this two-step process takes longer than simple
training. However, the second training step will be quicker than the ﬁrst, as it is performed on a somewhat reduced dataset. If the item dis is set as ‘near’, the PR selects
and removes those negative examples which are closest to the SVM hyper-plane. If it
is set as ‘far’, those negative examples that are furthest from the SVM hyper-plane are
removed. The value of the item ratio determines what proportion of negative examples
will be ﬁltered out. This element appears in the conﬁguration ﬁle as:
< FILTERING ratio=’X’ dis=’Y’/>
where X represents a number between 0 and 1 and Y can be set as ‘near’ or ‘far’. If the
ﬁltering element is not present in the conﬁguration ﬁle, or the value of ratio is set as
0.0, the PR does not perform ﬁltering. The default value of ratio is 0.0. The default
value of dis is ‘far’.
EVALUATION As outlined above, if the learning mode parameter learningMode is
set to ‘EVALUATION’, the PR will perform evaluation of the ML model; it will split
the documents in the corpus into two parts, the training dataset and the test dataset,
learn a model from the training dataset, apply the model to the testing dataset, and
ﬁnally compare the annotations assigned by the model on the test set with the true
annotations and output measures of success (e.g. F-measure). The evaluation element
speciﬁes the method of splitting the corpus. The item method determines which method

Machine Learning

371

to use for evaluation. Currently two commonly used methods are implemented, namely
k-fold cross-validation and hold-out test. In k-fold cross-validation the PR segments
the corpus into k partitions of equal size, and uses each of the partitions in turn as
a test set, with all the remaining documents as a training set. For hold-out test, the
system randomly selects some documents as testing data and uses all other documents
as training data. The value of the item runs speciﬁes the number ‘k’ for k-fold crossvalidation. The value of the item ratio speciﬁes the ratio of the data used for training
in the hold-out test method. The element in the conﬁguration ﬁle appears as so:
<EVALUATION method=”X” runs=”Y” ratio=”Z”/>
where the variable X has two possible values ‘kfold’ and ‘holdout’, Y is a positive integer,
and Z is a ﬂoat number between 0 and 1. The default value of method is ‘holdout’.
The default value of runs is ‘1’. The default value of ratio is ‘0.66’.
multiClassiﬁcation2Binary. Certain machine learning algorithms, including SVM,
are designed to operate on two class problems; they ﬁnd a separator between two groups
of instances. In order to use such algorithms to classify items into a larger number of
classes, the problem has to be converted into a series of ‘binary’ (two class) problems.
The ML plugin implements two common methods for converting a multi-class problem
into several binary problems, namely one against others and one against another. The
two methods may have slightly diﬀerent names in other publications, but the principle
is the same. Suppose we have a multi-class classiﬁcation problem with n classes. For
the one against others method, one binary classiﬁcation problem is derived for each of
the n classes. Examples belonging to the class in question are considered to be positive
examples and all other examples in the training set are negative examples. In contrast,
for the one against another method, one binary classiﬁcation problem is derived for
each pair (c1, c2) of the n classes. Training examples belonging to the class c1 are the
positive examples and those belonging to the other class, c2, are the negative examples.
The user can select one of the two methods by specifying the value of the item method
of the element. The element appears as so:
<multiClassiﬁcation2Binary method=”X” thread-pool-size=”N”/>
where the variable X has two values, ‘one-vs-others’ and ‘one-vs-another’. Note that
depending on the sample size, the two methods may diﬀer greatly in their speed of
execution. The default method is the one-vs-others method. If the conﬁguration ﬁle
does not have the element or the item method is missed, then the PR will use the onevs-others method. Since the derived binary classiﬁers are independent it is possible to
learn several of them in parallel. The ‘thread-pool-size’ attribute gives the number of
threads that will be used to learn and apply the binary classiﬁers. If omitted, a single
thread will be used to process all the classiﬁers in sequence.
thresholdProbabilityBoundary sets a conﬁdence threshold on start and end tokens
for chunk learning. It is used in post-processing the learning results. Only those
boundary tokens in which the conﬁdence level is above the threshold are selected as
candidates for the entities. The element in conﬁguration ﬁle appears as so:
<PARAMETER name=”thresholdProbabilityBoundary” value=”X”/>
The value X is between 0 and 1. The default value is 0.4.

372

Machine Learning

thresholdProbabilityEntity sets a conﬁdence threshold on chunks (which is the
multiplication of the probabilities of the start token and end token of the chunk) for
chunk learning. Only those entities in which the conﬁdence level is above the threshold
are selected as candidates of the entities. The element in conﬁguration ﬁle appears as
so:
<PARAMETER name=”thresholdProbabilityEntity” value=”X”/>
The value X is between 0 and 1. The default value is 0.2.
The threshold parameter thresholdProbabilityClassiﬁcation is the conﬁdence
threshold for classiﬁcation (e.g. text classiﬁcation and relation extraction tasks. In
contrast, the above two probabilities are for the chunking recognition task.) The corresponding element in conﬁguration ﬁle appears as so:
<PARAMETER name=”thresholdProbabilityClassiﬁcation” value=”X”/>
The value X is between 0 and 1. The default value is 0.5.
IS-LABEL-UPDATABLE is a Boolean parameter. If its value is set to ‘true’, the
label list is updated from the labels in the training data. Otherwise, a pre-deﬁned label
list will be used and cannot be updated from the training data. The conﬁguration
element appears as so:
<IS-LABEL-UPDATABLE value=”X”/>
The value X is ‘true’ or ‘false’. The default value is ‘true’.
IS-NLPFEATURELIST-UPDATABLE is a Boolean parameter. If its value is set
to ‘true’, the NLP feature list is updated from the features in the training or application
data. Otherwise, a pre-deﬁned NLP feature list will be used and cannot be updated.
The conﬁguration element appears as so:
<IS-NLPFEATURELIST-UPDATABLE value=”X”/>
The value X is ‘true’ or ‘false’. The default value is ‘true’.
The parameter VERBOSITY speciﬁes the verbosity level of the output of the system,
both to the message window of GATE Developer (or standard out when using GATE
Embedded) and into the log ﬁle. Currently there are three verbosity levels. Level 0
only allows the output of warning messages. Level 1 outputs some important setting
information and the results for evaluation mode. Level 2 is used for debugging purposes.
The element in the conﬁguration ﬁle appears as so:
<VERBOSITY level=”X”/>
The value X can be set as 0, 1 or 2. The default value is 1.
MI-TRAINING-INTERVAL speciﬁes the minimal number of newly added documents needed to trigger retraining the model. This parameter is used in MITRAINING.
The number is speciﬁed by the value of the feature ‘num’ as so:
<MI-TRAINING-INTERVAL num=”X”/>
The default value of X is 1.
BATCH-APP-INTERVAL is used in application mode, and speciﬁes the number
of documents to be collected and passed as a batch for classiﬁcation. Please refer to

Machine Learning

373

Section 17.2.1 for a detailed explanation of this option. The corresponding element in
the conﬁguration ﬁle is:
<BATCH-APP-INTERVAL num=”X”/>
The default value of X is 1.
DISPLAY-NLPFEATURES-LINEARSVM relates to ‘VIEWPRIMALFORMMODELS’ mode. In this mode, the most signiﬁcant features are displayed for each
class. For more information about this mode see Section 17.2.1. Two numbers are
speciﬁed; the number of positively weighted features to display and the number of
negatively weighted features to display. It has the following form in the conﬁguration
ﬁle;
<DISPLAY-NLPFEATURES-LINEARSVM numP=”X” numN=”Y”/>
where X and Y represent the numbers of positively and negatively weighted features to
display, respectively. The default values of X and Y are 10 and 0.
ACTIVELEARNING speciﬁes the settings for active learning. Active learning ranks
documents based on the average of a sample of ML annotation conﬁdence scores. A
larger sample gives a more accurate ranking but takes longer to calculate. The option
has the following form:
<ACTIVELEARNING numExamplesPerDoc=’X’/>
where X represents the number of examples per document used to obtain the conﬁdence
score with respect to the learned model. The default value of numExamplesPerDoc is
3.
The ENGINE Element The ENGINE element speciﬁes which ML algorithm will be
used, and also allows the options to be set for that algorithm.
For SVM learning, the user can choose one of two learning engines. We will discuss the two
SVM learning engines below. Note that only linear and polynomial kernels are supported.
This is despite the fact that the original SVM packages implemented other types of kernel.
Linear and polynomial kernels are popular in natural language learning, and other types of
kernel are rarely used. However, if you want to experiment with other types of kernel, you
can do so by ﬁrst running the Batch Learning PR in GATE to produce the training and
testing data, then using the data with the SVM implementation outside of GATE.
The conﬁguration ﬁles in the test directory (i.e. plugins/learning/test/ under the main gate
directory) contain examples for setting the learning engine.
The ENGINE element in the conﬁguration ﬁle is speciﬁed as follows:
<ENGINE nickname=’X’ implementationName=’Y’ options=’Z’/>
It has three items:
nickname can be the name of the learning algorithm or whatever the user wants it to

374

Machine Learning

be.
implementationName refers to the implementation of the particular learning algorithm that the user wants to use. Its value should be one of the following:
– SVMLibSvmJava, the binary classiﬁcation SVM algorithm implemented in the
Java version of the SVM package LibSVM.
– SVMExec, a binary SVM implementation of your choice, potentially in a language other than Java, run as a separate process outside of GATE. Currently
it can use the SV M light SVM package2 ; see the XML ﬁle in the GATE distribution (at gate/plugins/learning/test/chunklearning/engines-svm-svmlight.xml) for
an example of how to specify the learning engine to be used. The learning engines SVMExec and SVMLibSvmJava should produce the same results in theory
but may get slightly diﬀerent results in practice due to implementational diﬀerences. SVMLibSvmJava tends to be faster than SVMExec for smaller training
sets. There may be cases where it is an advantage to run SVM as a separate
process however, in which case, SVMExec would be preferable.
– PAUM, the Perceptron with uneven margins, a simple and fast classiﬁcation learning algorithm. (For details about the learning algorithm PAUM, see
[Li et al. 02]).
– PAUMExec, a binary PAUM implementation of your choice, potentially in a
language other than Java, run as a separate process outside of GATE. The relationship between the PAUM and PAUMExec is similar to that of SVMLibSvmJava and SVMExec. You may download and use an implementation in C from
http://www.dcs.shef.ac.uk/∼yaoyong/paum/paum-learning.zip. See the XML ﬁle
in the GATE distribution (at gate/plugins/learning/test/chunklearning/enginespaum-exec.xml) for an example of how to specify the learning engine to be used.
– NaiveBayesWeka, the Naive Bayes learning algorithm implemented in Weka.
– KNNWeka, the K nearest neighbour (KNN) algorithm implemented in Weka.
– C4.5Weka, the decision tree algorithm C4.5 implemented in Weka.
Options: the value of this item, which is dependent on the particular learning algorithm, will be passed verbatim to the ML engine used. Where an option is absent,
defaults for that engine will be used.
– The options for SVMLibSvmJava are similar to those for LibSVM but with the
exception that since SVMLibSvmJava implements the uneven margins SVM algorithms described in [Li & Shawe-Taylor 03], it takes the uneven margins parameter as an option. SVMLibSvmJava options are as follows:
-s svm type; whether the SVM should be binary or multiclass. Default
value is 0. Since only binary is supported, the option should be set to 0 or
excluded.
2

The SVM package SV M light can be downloaded from http://svmlight.joachims.org/.

Machine Learning

375

-t kernel type; 0 for a linear kernel or 1 for a polynomial kernel. Default
value is 0. Note that the current implementation does not support other
kernel types such as radial and sigmoid function.
-d degree; the degree in polynomial kernel, e.g. 2 for quadratic kernel.
Default value is 3.
-c cost; the cost parameter C in the SVM. Default value is 1. This parameter
determines the cost associated with allowing training errors (‘soft margins’).
Allowing some points to be misclassiﬁed by the SVM may produce a more
generalizable result.
-m cachesize; the cache memory size in MB (default 100).
-tau value; setting the value of uneven margins parameter of the SVM.
τ = 1 corresponds to the standard SVM. If the training data has just a small
number of positive examples and a large number of negative examples, setting
the parameter τ to a value less than 1 (e.g. τ = 0.4) often results in better
F-measure than the standard SVM (see [Li & Shawe-Taylor 03]).
– The options for SVMExec, using SV M light , are similar to those for using SV M light
directly for training. Options set the type of kernel, the parameters in the kernel function, the cost parameter, the memory used, etc. The parameter tau is
also included, to set the uneven margins parameter, as explained above. The
last two terms in the parameter options are the training data ﬁle and the model
ﬁle. An example of the options for SVMExec might be ‘-c 0.7 -t 0 -m 100 -v
0 -tau 0.6 /yaoyong/software/svm-light/data svm.dat /yaoyong/software/svmlight/model svm.dat’, meaning that the learner uses a linear kernel, the uneven margins parameter is set as 0.6, and two data ﬁles /yaoyong/software/svmlight/data svm.dat and /yaoyong/software/svm-light/model svm.dat for writing
and reading data. Note that both the data ﬁles speciﬁed here are temporary
ﬁles, which are used only by the svm-light training program, can be in anywhere
in your computer, and are independent of the data ﬁles produced by the GATE
learning plugin. SVMExec also takes a further argument, executableTraining,
which speciﬁes the SVM learning program svm learn.exe in the SV M light . For example, executableTraining=‘/yaoyong/software/svm-light/svm learn.exe’ speciﬁes one particular svm learn.exe obtained from the package SV M light .
– The PAUM engine has three options; ‘-p’ for the positive margin, ‘-n’ fo the
negative margin, and ‘-optB’ for the modiﬁcation of the bias term. For example,
options=‘-p 50 -n 5 -optB 0.3’ means τ+ = 50, τ− = 5 and b = b + 0.3 in the
PAUM algorithm.
– The KNN algorithm has one option; the number of neighbours used. It is set via
‘-k X’. The default value is 1.
– There are no options for Naive Bayes and C4.5 algorithms.
The DATASET Element The DATASET element deﬁnes the type of annotation to
be used as training instance and the set of attributes that characterise the instances. The

376

Machine Learning

INSTANCE-TYPE sub-element is used to select the annotation type to be used for instances.
There will be one training instance for every one of the instance annotations in the corpus.
For example, if INSTANCE-TYPE has ‘Token’ as its value, there will be one training instance
in the document per token. This also means that the positions (see below) are deﬁned
in relation to tokens. INSTANCE-TYPE can be seen as the basic unit to be taken into
account for machine learning. The attributes of the instance are deﬁned by a sequence of
ATTRIBUTE, ATTRIBUTE REL or ATTRIBUTELIST elements.
Diﬀerent NLP learning tasks may have diﬀerent instance types and use diﬀerent kinds of
attribute elements. Chunking recognition often uses the token as instance type and the linguistic features of ‘Token’ and other annotations as features. Text classiﬁcation’s instance
type is the text unit for classiﬁcation, e.g. the whole document, or sentence, or token. If
classifying for example a sentence, n-grams (see below) are often a good feature representation for many statistical learning algorithms. For relation extraction, the instance type
is a pair of terms that may be related, and the features come from not only the linguistic
features of each of the two terms but also those related to both terms taken together.
The DATASET element should deﬁne an INSTANCE-TYPE sub-element, it should deﬁne
an ATTRIBUTE sub-element or an ATTRIBUTE REL sub-element as class, and it should
deﬁne some linguistic feature related sub-elements (‘linguistic feature’ or ‘NLP feature’ is
used here to distinguish features or attributes used for machine learning from features in the
sense of a feature of a GATE annotation). All the annotation types involved in the dataset
deﬁnition should be in the same annotation set. Each of the sub-elements deﬁning the
linguistic features (attributes) should contain an element deﬁning the annotation TYPE to
be used and an element deﬁning the FEATURE of the annotation type to use. For instance,
TYPE might be ‘Person’ and FEATURE might be ‘gender’. For an ATTRIBUTE subelement, if you do not specify FEATURE, the entire sub-element will be ignored. Therefore,
if an annotation type you want to use does not have any annotation features, you should add
an annotation feature to it and assign the same value to the feature for all annotations of
that type. Note that if blank spaces are contained in the values of the annotation features,
they will be replaced by the character ‘ ’ in each occurrence. So it is advisable that the
values of the annotation features used, in particular for the class label, do not contain any
blank space.
Below, we explain all the sub-elements one by one. Please also refer to the example conﬁguration ﬁles presented in next section. Note that each sub-element should have a unique
name, if it requires a name, unless we explicitly state otherwise.
The INSTANCE-TYPE sub-element is deﬁned as
<INSTANCE-TYPE>X</INSTANCE-TYPE> where X is the annotation type used
as instance unit for learning, for example ‘Token’. For relation extraction, the user
should also specify the two arguments of the relation, as so:
<INSTANCE-ARG1>A</INSTANCE-ARG1>
<INSTANCE-ARG2>B</INSTANCE-ARG2>
The values of A and B should be identiﬁers for the ﬁrst and second terms of the relation,

Machine Learning

377

respectively. These names will be used later in the conﬁguration ﬁle. An example can
be found at /gate/plugins/learning/test/relation-learning/engines-svm.xml.
An ATTRIBUTE element has the following sub-elements:
– NAME; the name of the attribute. Its value should not end with ‘gram’, since
this is reserved for n-gram features as mentioned below. This attribute name will
appear in output ﬁles, so it is useful to give a descriptive name.
– SEMTYPE; type of the attribute value. It can be ‘NOMINAL’ or ‘NUMERIC’.
Currently only nominal is supported.
– TYPE; the annotation type used to extract the attribute.
– FEATURE; the value of the attribute will be the value of the named feature on
the annotation of the speciﬁed type.
– POSITION; the position of the instance annotation to be used for extracting
the feature relative to the current instance annotation. 0 refers to the current
instance annotation, -1 refers to the preceding instance annotation, 1 refers to
the following one and so forth. Recall that we deﬁned INSTANCE-TYPE at the
start of the DATASET element. This type might for example be ‘Token’. In the
current ATTRIBUTE element we are deﬁning an annotation type to use to get
the feature from, separate and possibly diﬀerent from the INSTANCE-TYPE. For
example, we might be interested in the ‘majorType’ of a ‘Lookup’. By specifying
-1, we would be saying, move to the preceding ‘Token’ and then try to extract the
‘majorType’ of the ‘Lookup’ on that token. The default value of the parameter
is 0. Note that if our INSTANCE-TYPE were to be for example a named entity
annotation comprising multiple tokens, and we wanted to extract a feature on the
‘Token’ annotation, then all the tokens within it would be considered to be in the
zero position relative to the current instance annotation, and the current implementation would simply pick the ﬁrst. (Useful in this case might be the NGRAM
attribute type, described later, which can be used to extract features for each
member of a multi-token annotation.) In the current implementation, features
are weighted according to their distance from the current instance annotation. In
other words, features which are further removed from the current instance annotation are given reduced importance. The component value in the feature vector
for one attribute feature is 1 if the attribute’s position p is 0. Otherwise its value
is 1.0/|p|.
– <CLASS/>: an empty element used to mark the class attribute. There can
only be one attribute marked as class in a dataset deﬁnition. The attribute, as
described above, has speciﬁed TYPE and FEATURE; the features of the type
are the class labels. Since only one attribute can be marked as class, it may be
necessary to preprocess your data to put all class labels into a feature of one type
of annotation, e.g. you might create a ‘Mention’ annotation, with the feature
‘Class’, which is set to the class name.

378

Machine Learning

The ATTRIBUTELIST element is similar to ATTRIBUTE except that it has no POSITION sub-element but instead a RANGE element. This will be converted into several
attributes with position ranging from the value of ‘from’ to the value of ‘to’. It deﬁnes
a ‘context window’ containing several consecutive examples. The ATTRIBUTELIST
should be preferred when deﬁning a context window for features, because not only it
can avoid the duplication of ATTRIBUTE elements, but also because processing is
speeded up (see the discussion for the element WINDOWSIZE below).
The WINDOWSIZE element speciﬁes the size of the context window. This will
override the context window size deﬁned in every ATTRIBUTELIST. If the WINDOWSIZE element is not present in the conﬁguration ﬁle, the window size deﬁned in
each element ATTRIBUTELIST will be used; otherwise, the window size speciﬁed by
this element will be used for each ATTRIBUTELIST if it contains one ATTRIBUTE
at position 0 (otherwise the ATTRIBUTELIST will be ignored). This element can be
used for speeding up the process of extracting the feature vectors from the documents.
The element has two features specifying the length of left and right sides of context
window. It has the following form:
<WINDOWSIZE windowSizeLeft=”X” windowSizeRight=”Y”/>
where X and Y represent the the length of left and right sides of context window, respectively. For example, if X = 2 and Y = 1, then the context window will be from the
position -2 to 1 ( e.g. from the second token in the left through the current token to
the ﬁrst token in the right).
An NGRAM feature is used for characterising an instance annotation in terms of
constituent sequences of subsumed feature annotations. It is essentially a reversal of the
ATTRIBUTELIST principle; where ATTRIBUTELIST uses a sequence surrounding an
instance in order to classify the instance, NGRAM uses sequences within the instance as
features. It simply creates a series of attributes that constitute a sliding window across
the entire of the current instance annotation. For example, INSTANCE-TYPE might
be sentences, in sentence classiﬁcation, and the NGRAM attribute speciﬁcation could
be used for example to create a series of unigram features for the sentence, eﬀectively
a ‘bag of words’ representation. Conventionally, one would use the string of the token,
or perhaps its lemma, as the feature for the NGRAM; however, it is possible to specify
multiple features of choice, as shown below.
– NAME; name of the n-gram. Its value should end with ‘gram’.
– NUMBER; the ‘n’ of the n-gram, with value 1 for unigram, and 2 for bigram,
etc.
– CONSNUM; several features can be used to generate n-grams. For example,
n-grams of token strings could be used as well as n-grams of lemmas. Where
CONSNUM is ‘k’, the NGRAM element should have ‘k’ CONS-X sub-elements,
where X= 1, ..., k. Each CONS-X element has one TYPE sub-element and one
FEATURE sub-element, which deﬁne feature to be used for that term to create
n-grams.

Machine Learning

379

– The WEIGHT sub-element speciﬁes a weight for the n-gram feature. The ngram part of the feature vector for one instance is normalised, thus having a
default value of 1.0. If the user wants to adjust the contributions of the n-gram to
the whole feature vector, s/he can do so by setting the WEIGHT parameter. For
example, if the user is doing sentence classiﬁcation and s/he uses two features;
the unigram of tokens in a sentence and the length of the sentence, by default the
entire of the NGRAM attribute speciﬁcation is given only the same importance as
the sentence length feature. In order to experiment with increasing the importance
of the n-gram element, the user can set the weight sub-element of the n-gram
element with a number bigger than 1.0 (like 10.0). Then every component of the
n-gram part of the feature vector would be multiplied by the parameter.
The ValueTypeNgram element speciﬁes the type of value used in the n-gram. Currently it can take one of the three types; ‘binary, tf, and tf-idf, which are explained in
Section 17.2.4. The value is speciﬁed by the X in
<ValueTypeNgram>X</ValueTypeNgram>
X = 1 for binary, = 2 for tf, and = 3 for tf-idf. The default value is 3.
The FEATURES-ARG1 element deﬁnes the features related to the ﬁrst argument of
the relation for relation learning. It should include one ARG sub-element referring to
the GATE annotation of the argument (see below for a detailed explanation). It may include other sub-elements, such as ATTRIBUTE, ATTRIBUTELIST and/or NGRAM,
to deﬁne the linguistic features related to the argument. Features pertaining particularly to one or the other argument of a relation should be deﬁned in FEATURES-ARG1
or FEATURES-ARG2 as appropriate. Features relating to both arguments should be
deﬁned using an ATTRIBUTE REL.
The FEATURES-ARG2 element deﬁnes the features related to the second argument of relation. Like the element FEATURES-ARG1, it should include one ARG
sub-element. It may also include other sub-elements. The ARG sub-element in the
FEATURES-ARG2 should have a unique name which is diﬀerent from the name for
the ARG sub-element in the FEATURES-ARG1. However, other sub-elements may
have the same name as corresponding ones in the FEATURES-ARG1, if they refer to
the same annotation type and feature in the text.
The ARG element is used in both FEATURES-ARG1 and FEATURES-ARG2. It
speciﬁes the annotation corresponding to one argument of a relation. It has four subelements, as follows;
– NAME; a unique name for the argument (e.g. ‘ARG1’).
– SEMTYPE; the type of the arg value. This can be ‘NOMINAL’ or ‘NUMERIC’.
Currently only nominal is implemented.
– TYPE; the annotation type for the argument.
– FEATURE; the value of the named feature on the annotation of speciﬁed type is
the identiﬁer of the argument. Only if the value of the feature is same as the value

380

Machine Learning

of the feature speciﬁed in the sub-element <INSTANCE-ARG1>A</INSTANCEARG1> (or <INSTANCE-ARG2>B</INSTANCE-ARG2>), the argument is regarded as one argument of the relation instance considered.
ATTRIBUTE REL element is similar to the ATTRIBUTE element. However, it
does not have the POSITION sub-element, and it has two other sub-elements, ARG1
and ARG2, relating to the two argument features of the (relation) instance type. In
other words, if and only if the value X in the sub-element <ARG1>X</ARG1> is same
as the value A in the ﬁrst argument instance <INSTANCE-ARG1>A</INSTANCEARG1> and the value Y in the sub-element <ARG2>Y</ARG2> is same as the value
B in the second argument instance <INSTANCE-ARG2>B</INSTANCE-ARG2> is
the feature deﬁned in this ATTRIBUTE REL sub-element assigned to the instance
considered. For relation learning, an ATTRIBUTE REL is denoted as the class attribute by including <CLASS/>.

17.2.2

Case Studies for the Three Learning Types

The following are three illustrated examples of conﬁguration ﬁles for information extraction,
sentence classiﬁcation and relation extraction. Note that the conﬁguration ﬁle is in the XML
format, and should be stored in a ﬁle with the ‘.xml’ extension.

Information Extraction
The ﬁrst example is for information extraction. The corpus is prepared with annotations
providing class information as well as the features to be used. Class information is provided in
the form of a single annotation type, ‘Mention’, which contains a feature ‘class’. Within the
class feature is the name of the class of the textual chunk. Other annotations in the dataset
include ‘Token’ and ‘Lookup’ annotations as provided by ANNIE. All of these annotations
are in the same annotation set, the name of which will be passed as a runtime parameter.
The conﬁguration ﬁle is given below. The optional settings are in the ﬁrst part. It ﬁrst
speciﬁes surround mode as ‘true’; we will ﬁnd the chunks that correspond to our entities
by using machine learning to locate the start and end of the chunks. Then it speciﬁes the
ﬁltering settings. Since we are going to use SVM in this problem, we can ﬁlter our data to
remove some of the negative instances that can cause problems if they are too dominant.
The ratio’s value is ‘0.1’ and the dis’s value is ‘near’, meaning that an initial SVM learning
step will be executed and the 10% of negative examples which are closest to the learned
SVM hyper-plane will be removed in the ﬁltering stage, before the ﬁnal learning is executed.
The threshold probabilities for the boundary tokens and information entity are set as ‘0.4’
and ‘0.2’, respectively; boundary tokens found with a lower conﬁdence than the threshold
will be rejected. The threshold probability for classiﬁcation is also set as ‘0.5’; this, however,
will not be used in this case since we are doing chunk learning with surround mode set as

Machine Learning

381

‘true’. The parameter will be ignored. multiClassiﬁcation2Binary is set as ‘one-vs-others’,
meaning that the ML API will convert the multi-class classiﬁcation problem into a series of
binary classiﬁcation problems using the one against others approach. In evaluation mode,
‘2-fold’ cross-validation will be used, dividing the corpus into two equal parts and running
two training/test cycles with each part as the training data.
The second part is the sub-element ENGINE, specifying the learning algorithm. The PR
will use the LibSVM SVM implementation. The options determine that it will use the linear
kernel with the cost C as 0.7 and the cache memory as 100M. Additionally it will use uneven
margins, with τ as 0.4.
The last part is the DATASET sub-element, deﬁning the linguistic features used. It ﬁrst
speciﬁes the ‘Token’ annotation as instance type. The ﬁrst ATTRIBUTELIST allows the
token’s string as a feature of an instance. The range from ‘-5’ to ‘5’ means that the strings
of the current token instance as well as its ﬁve preceding tokens and its ﬁve ensuing tokens
will be used as features for the current token instance. The next two attribute lists deﬁne
features based on the tokens’ capitalisation information and types. The ATTRIBUTELIST
named ‘Gaz’ uses as attributes the values of the feature ‘majorType’ of the annotation type
‘Lookup’. The ﬁnal ATTRIBUTE feature deﬁnes the class attribute; it has the sub-element
<CLASS/>. The values of the feature ‘class’ of the annotation type ‘Mention’ are the class
labels.
<?xml version="1.0"?>
<ML-CONFIG>
<SURROUND value="true"/>
<FILTERING ratio="0.1" dis="near"/>
<PARAMETER name="thresholdProbabilityEntity" value="0.2"/>
<PARAMETER name="thresholdProbabilityBoundary" value="0.4"/>
<PARAMETER name="thresholdProbabilityClassification" value="0.5"/>
<multiClassification2Binary method="one-vs-others"/>
<EVALUATION method="kfold" runs="2"/>
<ENGINE nickname="SVM" implementationName="SVMLibSvmJava"
options=" -c 0.7 -t 0 -m 100 -tau 0.4 "/>
<DATASET>
<INSTANCE-TYPE>Token</INSTANCE-TYPE>
<ATTRIBUTELIST>
<NAME>Form</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Token</TYPE>
<FEATURE>string</FEATURE>
<RANGE from="-5" to="5"/>
</ATTRIBUTELIST>
<ATTRIBUTELIST>
<NAME>Orthography</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Token</TYPE>

382

Machine Learning

<FEATURE>orth</FEATURE>
<RANGE from="-5" to="5"/>
</ATTRIBUTELIST>
<ATTRIBUTELIST>
<NAME>Tokenkind</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Token</TYPE>
<FEATURE>kind</FEATURE>
<RANGE from="-5" to="5"/>
</ATTRIBUTELIST>
<ATTRIBUTELIST>
<NAME>Gaz</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Lookup</TYPE>
<FEATURE>majorType</FEATURE>
<RANGE from="-5" to="5"/>
</ATTRIBUTELIST>
<ATTRIBUTE>
<NAME>Class</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Mention</TYPE>
<FEATURE>class</FEATURE>
<POSITION>0</POSITION>
<CLASS/>
</ATTRIBUTE>
</DATASET>
</ML-CONFIG>

Sentence Classiﬁcation
We will now consider the case of sentence classiﬁcation. The corpus in this example is annotated with ‘Sentence’ annotations, which contain the feature ‘sent size’, as well as the class
of the sentence. Furthermore, ‘Token’ annotations are applied, having features ‘category’
and ‘root’. As before, all annotations are in the same set, and the annotation set name will
be passed to the PR at run time.
Below is an example conﬁguration ﬁle. It ﬁrst speciﬁes surround mode as ‘false’, because it
is a text classiﬁcation problem; we are interested in classifying single instances rather than
chunks of instances. Our targets of interest, sentences, have already been found (unlike in
the information extraction example, where identifying the limits of the entity was part of the
problem). The next two options allow the label list and the NLP feature list to be updated
from the training data when retraining. It also speciﬁes probability thresholds for entity and
entity boundary. Note that these two speciﬁcations will not be used in this case. However,

Machine Learning

383

their presence is not problematic; they will simply be ignored. The probability threshold for
classiﬁcation is set as ‘0.5’. This will be used to decide which classiﬁcations to accept and
which to reject as being too unlikely. (Altering this parameter can trade oﬀ precision against
recall and vice versa.) The evaluation will use the hold-out test method. It will randomly
select 66% of the documents from the corpus for training, and the other 34% documents
will be used for testing. It will run the evaluation twice, and average the results over the
two runs. Note that it does not specify the method of converting a multi-class classiﬁcation
problem into several binary class problem, meaning that it will adopt the default (namely
one against all others).
The conﬁguration ﬁle speciﬁes KNN (K-Nearest Neighbour) as the learning algorithm. It
also speciﬁes the number of neighbours used as 5. Of course other learning algorithms can
be used as well. For example, the ENGINE element in the previous example, which speciﬁes
SVM as learning algorithm, can be put into this conﬁguration ﬁle to replace the current one.
In the DATASET element, the annotation ‘Sentence’ is used as instance type. Two kinds of
linguistic features are deﬁned; one is NGRAM and the other is ATTRIBUTE. The n-gram
is based on the annotation ‘Token’. It is a unigram, as its NUMBER element has the value
1. This means that a ‘bag of words’ feature will be formed from the tokens comprising the
sentence. It is based on the two features, ‘root’ and ‘category’, of the annotation ‘Token’.
This introduces a new aspect to the n-gram. The n-gram feature comprises counts of the
unigrams appearing in the sentence. For example, if the sentence were ‘the man walked
the dog”, the unigram feature would contain the information that ‘the’ appeared twice,
and ‘man’, ‘walked’ and ‘dog’ appeared once. However, since our n-gram has two features,
‘root’ and ‘category’, two tokens will be considered the same term if and only if they have
the same ‘root’ feature and the same ‘category’ feature. The weight of the ngram is set
as 10.0, meaning its contribution is ten times that of the contribution of the other feature,
the sentence length. The feature ‘sent size’ of the annotation ‘Sentence’ is given as an
ATTRIBUTE feature. Finally the values of the feature ‘class’ of the annotation ‘Sentence’
are nominated as the class labels.

<?xml version="1.0"?>
<ML-CONFIG>
<SURROUND value="false"/>
<IS-LABEL-UPDATABLE value="true"/>
<IS-NLPFEATURELIST-UPDATABLE value="true"/>
<PARAMETER name="thresholdProbabilityEntity" value="0.2"/>
<PARAMETER name="thresholdProbabilityBoundary" value="0.42"/>
<PARAMETER name="thresholdProbabilityClassification" value="0.5"/>
<EVALUATION method="holdout" runs="2" ratio="0.66"/>
<ENGINE nickname="KNN" implementationName="KNNWeka" options = " -k 5 "/>
<DATASET>
<INSTANCE-TYPE>Sentence</INSTANCE-TYPE>
<NGRAM>
<NAME>Sent1gram</NAME>

384

Machine Learning

<NUMBER>1</NUMBER>
<CONSNUM>2</CONSNUM>
<CONS-1>
<TYPE>Token</TYPE>
<FEATURE>root</FEATURE>
</CONS-1>
<CONS-2>
<TYPE>Token</TYPE>
<FEATURE>category</FEATURE>
</CONS-2>
<WEIGHT>10.0</WEIGHT>
</NGRAM>
<ATTRIBUTE>
<NAME>Class</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Sentence</TYPE>
<FEATURE>sent_size</FEATURE>
<POSITION>0</POSITION>
</ATTRIBUTE>
<ATTRIBUTE>
<NAME>Class</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Sentence</TYPE>
<FEATURE>class</FEATURE>
<POSITION>0</POSITION>
<CLASS/>
</ATTRIBUTE>
</DATASET>
</ML-CONFIG>

Relation Extraction
The last example is for relation extraction. The relation extraction support in the PR is
based on the work described in [Wang et al. 06].
Two concepts are key in a relation extraction corpus. Entities are the things that may be
related, and relations describe the relationship between the entities if any. In our example,
entities are pre-identiﬁed, and the task is to identify the relationships between them. The
corpus for this example is annotated with the following:
‘ACEEntity’ annotations indicate the entities of interest in the corpus.
‘RE INS’ annotations form the instances, and there is an instance for every pair of
‘ACEEntities’ within a sentence. ‘RE INS’ annotations span the entire of the text

Machine Learning

385

between and including their ‘ACEEntity’ annotations. For example, ‘the commander
of Israeli troops’ might be a potential relationship between a person, ‘the commander’, and an entity, ‘Israeli troops’. Its ‘RE INS’ annotation covers the entire of this
text. It contains ‘arg1’ and ‘arg2’ features containing the numerical identiﬁers of the
two ‘ACEEntities’ to which it pertains. These numerical identiﬁers match the ‘MENTION ID’ feature of the ‘ACEEntity’ annotation.
‘ACERelation’ annotations indicate the relations we wish to learn, and also span
the entire of the text involved in the relationship. They include the features ‘MENTION ARG1’ and ‘MENTION ARG2’, which, again, contain the numerical identiﬁer
found in the ‘MENTION ID’ feature of the ‘ACEEntity’ annotations, as well as ‘Relation type’, indicating the type of the relation.
Various ANNIE-style annotations are also included.
Our task is to select the ‘RE INS’ instances that match the ‘ACERelations’. You will see
that throughout the conﬁguration ﬁle, annotation types are speciﬁed in conjunction with
argument identiﬁers. This is because we need to ensure that the annotation in question
pertains to the right entities. Therefore, argument identiﬁers are used to constrain the
match.
The conﬁguration ﬁle does not specify any optional settings, meaning that it uses all the
default values for those settings (see Section 17.2.1 for the default values of all possible
settings).
it sets the surround mode as ‘false’;
both the label list and NLP feature list are updatable;
the probability threshold for classiﬁcation is set as 0.5;
it uses ‘one against others’ for converting multi-class problem into binary class problems
for SVM learning;
for evaluation it uses hold-out testing with a ratio of 0.66 and only one run.
The conﬁguration ﬁle speciﬁes the learning algorithm as the Naive Bayes method implemented in Weka. However, other learning algorithms could equally well be used.
We begin by deﬁning ‘RE INS’ as the instance type. Next, we provide the numeric identiﬁers of each argument of the relationship by specifying elements INSTANCE-ARG1 and
INSTANCE-ARG2 as the feature names ‘arg1’ and ‘arg2’ respectively. This indicates that
the argument identiﬁers of the instances can be found in the ‘arg1’ and ‘arg2’ features of the
‘RE INS’ annotations.

386

Machine Learning

Attributes might pertain to the entire relation or they might pertain to one or other argument
within the relation. We are going to begin by deﬁning the features speciﬁc to each argument
of the relation. Recall that our ‘RE INS’ annotations have as arguments two ‘ACEEntity’ annotations, and that these are identiﬁed by their ‘MENTION ID’ being the same as the ‘arg1’
or ‘arg2’ features of the ‘RE INS’. It is from these ‘ACEEntity’ annotations that we wish
to obtain argument-speciﬁc features. FEATURES-ARG1 and FEATURES-ARG1 elements
begin by specifying which annotation we are referring to. We use the ARG element to explain this. We are interested in annotations of type ‘ACEEntity’, and their ‘MENTION ID’
must match ‘arg1’ or ‘arg2’ of ‘RE INS’ as appropriate. Having identiﬁed precisely which
‘ACEEntity’ we are interested in we can go on to give argument-speciﬁc features; in this
case, unigrams of the ‘Token’ feature ‘string’.
We now wish to deﬁne features pertaining to the entire relation. We indicate that the ‘t12’
feature of ‘RE INS’ annotations is to be used (this feature contains type information derived
from ‘ACEEntity’). Again, rather than just specifying the ‘RE INS’ annotation, we also
indicate that the ‘arg1’ and ‘arg2’ feature values must match the argument identiﬁers of
the instance, as deﬁned in the INSTANCE-ARG1 and INSTANCE-ARG2 elements at the
beginning. This ensures that we are taking our features from the correct annotation.
Finally, we deﬁne the class attribute. We indicate that the class attribute is contained in
the ‘Relation type’ feature of the ‘ACERelation’ annotation. The ‘ACERelation’ annotation
type has features ‘MENTION ARG1’ and ‘MENTION ARG1’, indicating its arguments.
Again, we use the elements ARG1 and ARG2 to indicate that it is these features that must
be matched to the arguments of the instance if that instance is to be considered a positive
example of the class.

<?xml version="1.0"?>
<ML-CONFIG>
<ENGINE nickname="NB" implementationName="NaiveBayesWeka"/>
<DATASET>
<INSTANCE-TYPE>RE_INS</INSTANCE-TYPE>
<INSTANCE-ARG1>arg1</INSTANCE-ARG1>
<INSTANCE-ARG2>arg2</INSTANCE-ARG2>
<FEATURES-ARG1>
<ARG>
<NAME>ARG1</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>ACEEntity</TYPE>
<FEATURE>MENTION_ID</FEATURE>
</ARG>
<ATTRIBUTE>
<NAME>Form</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Token</TYPE>
<FEATURE>string</FEATURE>

Machine Learning

387

<POSITION>0</POSITION>
</ATTRIBUTE>
</FEATURES-ARG1>
<FEATURES-ARG2>
<ARG>
<NAME>ARG2</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>ACEEntity</TYPE>
<FEATURE>MENTION_ID</FEATURE>
</ARG>
<ATTRIBUTE>
<NAME>Form</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>Token</TYPE>
<FEATURE>string</FEATURE>
<POSITION>0</POSITION>
</ATTRIBUTE>
</FEATURES-ARG2>
<ATTRIBUTE_REL>
<NAME>EntityCom1</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>RE_INS</TYPE>
<ARG1>arg1</ARG1>
<ARG2>arg2</ARG2>
<FEATURE>t12</FEATURE>
</ATTRIBUTE_REL>
<ATTRIBUTE_REL>
<NAME>Class</NAME>
<SEMTYPE>NOMINAL</SEMTYPE>
<TYPE>ACERelation</TYPE>
<ARG1>MENTION_ARG1</ARG1>
<ARG2>MENTION_ARG2</ARG2>
<FEATURE>Relation_type</FEATURE>
<CLASS/>
</ATTRIBUTE_REL>
</DATASET>
</ML-CONFIG>

17.2.3

How to Use the Batch Learning PR in GATE Developer

The Batch Learning PR implements the procedure of using supervised machine learning
for NLP, which generally has two steps; training and application. The training step learns
models from labelled data. The application step applies the learned models to the unlabelled
data in order to add labels. Therefore, in order to use supervised ML for NLP, one should
have some labelled data, which can be obtained either by manually annotating documents

388

Machine Learning

or from other resources. One also needs to determine which linguistic features are to be
used in training. (The same features should be used in the application as well.) In this
implementation, all machine learning attributes are GATE annotation features. Finally, one
should determine which learning algorithm will be used.
Based on the general procedure outlined above, we explain how to use the Batch Learning
PR step by step below:
1. Annotate some documents with labels that you want to learn. The labels should be
represented by the values of a feature of a GATE annotation type (not the annotation
type itself).
2. Determine the linguistic features that you want the PR to use for learning.
3. Annotate the documents (training and application) with the desired features. ANNIE
can be useful in this regard. Other PRs such as GATE morphological analyser and
the parsers may produce useful features as well. You may need to write some JAPE
scripts to produce the features you want.
4. Create an XML conﬁguration ﬁle for your learning problem. The ﬁle should contain one
DATASET element specifying the NLP features used, one ENGINE element specifying
the learning algorithm, and some optional settings as necessary. (Tip: it may be easier
to copy one of the conﬁguration ﬁles presented above and modify it for your problem
than to write a conﬁguration ﬁle from scratch.)
5. Load the training documents containing the required annotations representing the
linguistic features and the class label, and put them into a corpus. All linguistic
features and the class feature should be in the same annotation set. (The Annotation
Set Transfer PR in the ‘Tools’ plugin can be useful here.)
6. Load the Batch Learning PR into GATE Developer. First you need load the plugin
named ‘learning’ using the tool Manage CREOLE Plugins. Then you can create a new
‘Batch Learning PR’. You will need to provide the conﬁguration ﬁle as an initialization
parameter. After that you can put the PR into a Corpus Pipeline application to use
it. Add the corpus containing the training documents to the application too. Set the
inputASName to the annotation set containing the annotations for linguistic features
and class labels.
7. Set the run-time parameter learningMode to ‘TRAINING’ to learn a model from the
training data, or set learningMode to ‘EVALUATION’ to do evaluation on the training
data and get ﬁgures indicating the success of the learning. When using evaluation
mode, make sure that the outputASName is the same as the inputASName. (Tip: it
may save time if you ﬁrst try evaluation mode on a small number of documents to
make sure that the ML PR works well on your problem and outputs reasonable results
before training on the large data.)

Machine Learning

389

8. If you want to apply the learned model to new documents, load those new documents
into GATE and pre-process them in the same way as the training documents, to ensure
that the same features are present. (Class labels need not be present, of course.) Then
set learningMode to ‘APPLICATION’ and run the PR on this corpus. The application
results, namely the new annotations containing the class labels, will be added into the
annotation set speciﬁed by the outputASName.
9. If you just want the feature ﬁles produced by the system and do not want to do any
learning or application, select the learning mode ‘ProduceFeatureFilesOnly’.

17.2.4

Output of the Batch Learning PR

The Batch Learning PR outputs several diﬀerent kinds of information. Firstly, it outputs
information about the learning settings. This information will be printed in the Messages
Window of the GATE Developer (or standard out if using GATE Embedded) and also into
the log ﬁle ‘logFileForNLPLearning.save’. The amount of information displayed can be determined via the VERBOSITY parameter in the conﬁguration ﬁle. The main output of the
learning system is diﬀerent for diﬀerent usage modes. In training mode the system produces the learned models. In application mode it annotates the documents using the learned
models. In evaluation mode it displays the evaluation results. Finally, in ‘ProduceFeatureFilesOnly’ mode, it produces feature ﬁles for the current corpus. Below, we explain the
outputs for diﬀerent learning modes.
Note that all the ﬁles produced by the Batch Learning PR, including the log ﬁle, are placed
in the sub-directory ‘savedFiles’ of the ML working directory. The ML working directory is
the directory containing the conﬁguration ﬁle.

Training results
When the Batch Learning PR is used in training mode, its main output is the learned model,
stored in a ﬁle named ‘learnedModels.save’. For the SVM algorithm, the learned model ﬁle
is a text ﬁle. For the learning algorithms implemented in Weka, the model ﬁle is a binary
ﬁle. The output also includes the feature ﬁles described in Section 17.2.4.

Application Results
The main application result is the annotations added to the documents. Those annotations
are the results of applying the ML model to the documents. In the conﬁguration ﬁle, the
annotation type and feature of the class labels are speciﬁed; class labels must be the value of
a feature of an annotation type. In application mode, those annotation types are created in
the new documents, and the feature speciﬁed will hold the class label. An additional feature

390

Machine Learning

will also be included on the speciﬁed annotation type; ‘prob’ will hold the conﬁdence level
for the annotation.
Evaluation Results
The Batch Learning PR outputs the evaluation results for each run and also the averaged
results over all runs. For each run, it ﬁrst prints a message about the names of the documents
in training and testing corpora respectively. Then it displays the evaluation results of this
run; ﬁrst the results for each class label and then the micro-averaged results over all labels.
For each label, it presents the name of the label, the number of instances belonging to the
label in the training data and results on the test data; the numbers of correct, partially
correct, spurious and missing instances in the testing data, and the precision, recall and
F1, calculated using correct only (strict) and correct plus partial (lenient). The F-measure
results are obtained using the AnnotationDiﬀ Tool which is described in Chapter 10. Finally,
the system presents the means of the results of all runs for each label and the micro-averaged
results.
Feature Files
The Batch Learning PR is able to produce several feature ﬁles. These feature ﬁles could
be used for evaluating learning algorithms not implemented in this plugin. We describe the
formats of those feature ﬁles below. Note that all the data ﬁles described below can be
obtained by setting the run time parameter learningMode to ‘ProduceFeatureFilesOnly’, but
some may be produced as part of other learning modes.
The NLP feature ﬁle, named NLPFeatureData.save, contains the NLP features of the
instances deﬁned in the conﬁguration ﬁle. Below is an example of the ﬁrst few lines of an
NLP feature ﬁle for information extraction:
Class(es) Form(-1) Form(0) Form(1) Ortho(-1) Ortho(0) Ortho(1)
0 ft-airlines-27-jul-2001.xml 512
1 Number_BB _NA[-1] _Form_Seven _Form_UK[1] _NA[-1] _Ortho_upperInitial
_Ortho_allCaps[1]
1 Country_BB _Form_Seven[-1] _Form_UK _Form_airlines[1] _Ortho_upperInitial[-1]
_Ortho_allCaps _Ortho_lowercase[1]
0 _Form_UK[-1] _Form_airlines _Form_including[1] _Ortho_allCaps[-1]
_Ortho_lowercase _Ortho_lowercase[1]
0 _Form_airlines[-1] _Form_including _Form_British[1] _Ortho_lowercase[-1]
_Ortho_lowercase _Ortho_upperInitial[1]
1 Airline_BB _Form_including[-1] _Form_British _Form_Airways[1]
_Ortho_lowercase[-1] _Ortho_upperInitial _Ortho_upperInitial[1]
1 Airline _Form_British[-1] _Form_Airways _Form_[1], _Ortho_upperInitial[-1]
_Ortho_upperInitial _NA[1]

Machine Learning

391

0 _Form_Airways[-1] _Form_, _Form_Virgin[1] _Ortho_upperInitial[-1] _NA
_Ortho_upperInitial[1]

The ﬁrst line of the NLP feature ﬁle lists the names of all features used. These names are the
names the user gave to their features in the conﬁguration ﬁle. The number in the parenthesis
following a feature name indicates the position of the feature. For example, ‘Form(-1)’ means
the Form feature of the token which is immediately before the current token, and ‘Form(0)’
means the Form feature of the current token. The NLP features for all instances are listed
for one document before moving on to the next. For each document, the ﬁrst line shows the
index of the document, the document’s name and the number of instances in the document,
as shown in the second line above. After that, each line corresponds to an instance in the
document, in their order of appearance. The ﬁrst item on the line is a number n, representing
the number of class labels of the instance. Then, the following n items are the labels. If the
current instance is the ﬁrst instance of an entity, its corresponding label has a suﬃx ‘ BB’.
The other items following the label item(s) are the NLP features of the instance, in the order
listed in the ﬁrst line of the ﬁle. Each NLP feature contains the feature’s name and value,
separated by ‘ ’. At the end of one NLP feature, there may be an integer in square brackets,
which represents the position of the feature relative to the current instance. If there is no
square-bracketed integer at the end of one NLP feature, then the feature is at the position
0.
The Feature vector ﬁle has the ﬁle name ‘featureVectorsData.save’, and stores the feature
vector in sparse format for each instance. The ﬁrst few lines of the feature vector ﬁle
corresponding to the NLP feature ﬁle shown above are as follows:
0
1
2
3
4
5
6
7

512 ft-airlines-27-jul-2001.xml
2 1 2 439:1.0 761:1.0 100300:1.0 100763:1.0
2 3 4 300:1.0 763:1.0 50439:1.0 50761:1.0 100440:1.0 100762:1.0
0 440:1.0 762:1.0 50300:1.0 50763:1.0 100441:1.0 100762:1.0
0 441:1.0 762:1.0 50440:1.0 50762:1.0 100020:1.0 100761:1.0
1 5 20:1.0 761:1.0 50441:1.0 50762:1.0 100442:1.0 100761:1.0
1 6 442:1.0 761:1.0 50020:1.0 50761:1.0 100066:1.0
0 66:1.0 50442:1.0 50761:1.0 100443:1.0 100761:1.0

The feature vectors are also listed for each document in sequence. For each document, the
ﬁrst line shows the index of the document, the number of instances in the document and the
document’s name. Each of the following lines is for each of the instances in the document.
The ﬁrst item in the line is the index of the instance in the document. The second item is
a number n, representing the number of labels the instance has. The following n items are
indices representing the class labels.
For text classiﬁcation and relation learning, the label’s index comes directly from the label
list ﬁle, described below. For chunk learning, the label’s index presented in the feature vector
ﬁle is a bit more complicated. If an instance (e.g. token) is the ﬁrst one of a chunk with

392

Machine Learning

label k, then the instance has as the label’s index 2 ∗ k − 1, as shown in the ﬁfth instance.
If it is the last instance of the chunk, it has the label’s index as 2 ∗ k, as shown in the sixth
instance. If the instance is both the ﬁrst one and the last one of the chunk (namely the
chunk consists of one instance), it has two label indices, 2 ∗ k − 1 and 2 ∗ k, as shown in the
ﬁrst and second instances.
The items following the label(s) are the non-zero components of the feature vector. Each
component is represented by two numbers separated by ‘:’. The ﬁrst number is the dimension
(position) of the component in the feature vector, and the second one is the value of the
component.
The Label list ﬁle has the name ‘LabelsList.save’, and stores a list of labels and their
indices. The following is a part of a label list. Each line shows one label name and its index
in the label list.
Airline 3
Bank 13
CalendarMonth 11
CalendarYear 10
Company 6
Continent 8
Country 2
CountryCapital 15
Date 21
DayOfWeek 4

The NLP feature list has the name ‘NLPFeaturesList.save’, and contains a list of NLP
features and their indices in the list. The following are the ﬁrst few lines of an NLP feature
list ﬁle.
totalNumDocs=14915
_EntityType_Date 13 1731
_EntityType_Location 170 1081
_EntityType_Money 523 3774
_EntityType_Organization 12 2387
_EntityType_Person 191 421
_EntityType_Unknown 76 218
_Form_’ 112 775
_Form_\$ 527 74
_Form_’ 508 37
_Form_’s 63 731
_Form_( 526 111

The ﬁrst line of the ﬁle shows the number of instances from which the NLP features were
collected. The number of instances will be used for computating of the idf (inverse document

Machine Learning

393

frequency) in document or sentence classiﬁcation. The following lines are for the NLP
features. Each line is for one unique feature. The ﬁrst item in the line represents the NLP
feature, which is a combination of the feature’s name deﬁned in the conﬁguration ﬁle and
the value of the feature. The second item is a positive integer representing the index of the
feature in the list. The last item is the number of times that the feature occurs, which is
needed for computing the idf.
The N-grams (or language model) ﬁle has the name ‘NgramList.save’, and can only be
produced by setting the learning mode to ‘ProduceFeatureFilesOnly’. In order to produce
n-gram data, the user may use a very simple conﬁguration ﬁle, i.e. it need only contain the
DATASET element, and the data element need contain only an NGRAM element to specify
the type of n-gram and the INSTANCE-TYPE element to deﬁne the annotation type from
which the n-gram data are created (e.g. sentence). The NGRAM element in conﬁguration
ﬁle speciﬁes what type of n-grams the PR produces (see Section 17.2.1 for the explanation
of the n-gram deﬁnition). For example, if you specify a bigram based on the string form of
‘Token’, you will obtain a list of bigrams from the corpus you used. The following are the
ﬁrst lines of a bigram list based on the token annotation’s ‘string’ feature, and was calculated
over 3 documents.
## The following 2-gram were obtained from 3 documents or examples
Aug<>, 3
Female<>; 3
Human<>; 3
2004<>Aug 3
;<>Female 3
.<>The 3
of<>a 3
)<>: 3
,<>and 3
to<>be 3
;<>Human 3

The two terms of the bigram are separated by ‘<>’. The number following one n-gram is the
number of occurrences of that n-gram in the corpus. The n-gram list is ordered according
to the number of occurrences of the n-gram terms. The most frequent terms in the corpus
are therefore at the start of the list.
The n-gram data produced can be based on any features of annotations available in the
documents. Hence it can not only produce the conventional n-gram data based on the
token’s form or lemma, but also n-grams based on e.g. the token’s POS, or a combination
of the token’s POS and form, or any feature of the ‘sentence’ annotation (see Section 17.2.1
for how to deﬁne diﬀerent types of n-gram).
The Document-term matrix ﬁle has the name ‘documentByTermMatrix.save’, and can
only be produced by setting the learning mode to ‘ProduceFeatureFilesOnly’. The documentterm matrix presents the weights of terms appearing in each document (see Section 20.14 for

394

Machine Learning

more explanation). Currently three types of weight are implemented; binary, term frequency
(tf) and tf-idf. The binary weight is simply 1 if the term appears in document and 0 if it does
not. tf (term frequency) refers to the number of occurrences of one term in a document. tf-idf
is popular in information retrieval and text mining. It is a multiplication of term frequency
and inverse document frequency. Inverse document frequency is calculated as follows:
idfi = log

|D|
|{dj : ti ∈ dj }|

where |D| is the total number of documents in the corpus, and |{dj : ti ∈ dj }| is the number
of documents in which the term ti appears. The type of weight is speciﬁed by the sub-element
ValueTypeNgram in the DATASET element in conﬁguration ﬁle (see Section 17.2.1).
Like the n-gram data, in order to produce the document-term matrix, the user may use
a very simple conﬁguration ﬁle, i.e. it need only contain the DATASET element, and the
data element need only contain two elements; the INSTANCE-TYPE element, to deﬁne
the annotation type from which the terms are counted, and an NGRAM element to specify
the type of n-gram. As mentioned previously, the element ValueTypeNgram speciﬁes the
type of value used in the matrix. If it is not present, the default type tf-idf will be used.
The conventional document-term matrix can be produced using a unigram based on the
token’s form or lemma and the instance type covering the whole document. In other words,
INSTANCE-TYPE is set to an annotation type such as for example ‘body’, which covers the
entire document, and the n-gram deﬁnition then speciﬁes the ‘string’ feature of the ‘Token’
annotation type.
The following was extracted from the beginning of a document-term matrix ﬁle, produced
using unigrams of the token’s form. It presents a part of the matrix of terms and their term
frequency values in the document named ‘27.xml’. Each term and its term frequency are
separated by ‘:’. The terms are in alphabetic order.
0 Documentname="27.xml", has 1 parts: ":2 (:6 ):6 ,:14 -:1 .:16 /:1
124:1 2004:1 22:1 29:1 330:1 54:1 8:2 ::5 ;:11 Abstract:1 Adaptation:1
Adult:1 Atopic:2 Attachment:3 Aug:1 Bindungssicherheit:1 Cross-:1
Dermatitis:2 English:1 F-SOZU:1 Female:1 Human:1 In:1 Index:1
Insecure:1 Interpersonal:1 Irrespective:1 It:1 K-:1 Lebensqualitat:1
Life:1 Male:1 NSI:2 Neurodermitis:2 OT:1 Original:1 Patients:1
Psychological:1 Psychologie:1 Psychosomatik:1 Psychotherapie:1
Quality:1 Questionnaire:1 RSQ:1 Relations:1 Relationship:1 SCORAD:1
Scales:1 Sectional:1 Securely:1 Severity:2 Skindex-:1 Social:1
Studies:1 Suffering:1 Support:1 The:1 Title:1 We:3 [:1 ]:1 a:4
absence:1 affection:1 along:2 amount:1 an:1 and:9 as:1 assessed:1
association:2 atopic:5 attached:7

A list of names of documents processed can also be obtained. The ﬁle has the name
‘docsName.save’, and only can be produced by setting the learning mode to ‘ProduceFea-

Machine Learning

395

tureFilesOnly’. It contains the names of all the documents processed. The ﬁrst line shows
the number of documents in the list. Then, each line lists one document’s name. The ﬁrst
lines of an example ﬁle are shown below:
##totalDocs=3
ft-bank-of-england-02-aug-2001.xml
ft-airtours-08-aug-2001.xml
ft-airlines-27-jul-2001.xml

A list of names of the selected documents for active learning purposes can also
be produced. The ﬁle has the name ‘ALSelectedDocs.save’. It is a text ﬁle. It is produced
in ‘ProduceFeatureFilesOnly’ mode. The ﬁle contains the names of documents which have
been selected for annotating and training in the active learning process. It is used by the
‘RankingDocsForAL’ learning mode to exclude those selected documents from the ranked
documents for active learning purposes. When one or more documents are selected for
annotating and training, their names should be put into this ﬁle, one line per document.
A list of names of ranked documents for active learning purposes; the ﬁle has
the name ‘ALRankedDocs.save’, and is produced in ‘RankingDocsForAL’ mode. The ﬁle
contains the list of names of the documents ranked for active learning, according to their
usefulness for learning. Those in the front of the list are the most useful documents for
learning. The ﬁrst line in the ﬁle shows the total number of documents in the list. Each of
other lines in the ﬁle lists one document and the averaged conﬁdence score for classifying
the document. An example of the ﬁle is shown below:
##numDocsRanked=3
ft-airlines-27-jul-2001.xml_000201 8.61744
ft-bank-of-england-02-aug-2001.xml_000221 8.672693
ft-airtours-08-aug-2001.xml_000211 9.82562

17.2.5

Using the Batch Learning PR from the API

Using the Batch Learning PR from the API is a simple matter if you have some familiarity
with GATE Embedded. Chapter 7 provides a more comprehensive introduction to programming with GATE Embedded, and should be consulted for any general points. There is also
a complete example program on the code examples page.
The following snippet shows creating a pipeline application, with a corpus, then creating a
batch learning PR and adding it to the application. The location of the conﬁguration ﬁle
and the mode in which the PR is to be run are added to the PR. The application is then run.
‘corpus’ is a GATE corpus that you have previously set up. (To learn more about creating
a corpus from GATE Embedded, see chapter 7 or the example at the code examples page.)

396

1
2

Machine Learning

File configFile = new File ( " / home / you / ml_config . xml " ); //Wherever it is
RunMode mode = RunMode . EVALUATION ; //or TRAINING, or APPLICATION ..

3
4
5
6
7
8
9
10

//Make a pipeline and add the corpus
FeatureMap pfm = Factory . newFeatureMap ();
pfm . put ( " corpus " , corpus );
gate . creole . S er i a l An a l y se r C o n t r o l l e r pipeline =
( gate . creole . S e r i a l A n a ly s e r Co n t r ol l e r )
gate . Factory . createResource (
" gate . creole . S e r i a l A n a l y s e r C o n t r o l l e r " , pfm );

11
12
13
14
15
16
17
18
19
20
21

//Set up the PR and add it to the pipeline.
//As with using the PR from GATE Developer, it needs a conﬁg ﬁle
//and a mode.
FeatureMap fm = Factory . newFeatureMap ();
fm . put ( " configFileURL " , configFile . toURI (). toURL ());
fm . put ( " learningMode " , mode );
gate . learning . LearningAPIMain learner =
( gate . learning . LearningAPIMain )
gate . Factory . createResource ( " gate . learning . LearningAPIMain " , fm );
pipeline . add ( learner );

22
23
24

//Run it!
pipeline . execute ();

Having run the PR in EVALUATION mode, you can access the results programmatically:
1
2
3
4
5
6
7
8

Eva luat io n B a s e d O n D o c s ev = learner . getEvaluation ();
System . out . println (
ev . m a c r o M e a s u r e s O f R e s u l t s . precision + " ," +
ev . m a c r o M e a s u r e s O f R e s u l t s . recall + " ," +
ev . m a c r o M e a s u r e s O f R e s u l t s . f1 + " ," +
ev . m a c r o M e a s u r e s O f R e s u l t s . precisionLenient + " ," +
ev . m a c r o M e a s u r e s O f R e s u l t s . recallLenient + " ," +
ev . m a c r o M e a s u r e s O f R e s u l t s . f1Lenient + " \ n " );

17.3

Machine Learning PR

The ‘Machine Learning PR’ is GATE’s earlier machine learning PR. It handles both the
training and application of ML model on GATE documents. This PR is a Language Analyser so it can be used in all default types of GATE controllers. It can be found in the
‘Machine Learning’ plugin.
In order to allow for more ﬂexibility, all the conﬁguration parameters for the Machine Learning PR are set through an external XML ﬁle and not through the normal PR parameterisation. The root element of the ﬁle needs to be called ‘ML-CONFIG’ and it contains two
elements: ‘DATASET’ and ‘ENGINE’. An example XML conﬁguration ﬁle is given in Section
17.3.6.

Machine Learning

17.3.1

397

The DATASET Element

The DATASET element deﬁnes the type of annotation to be used as instance and the set of
attributes that characterise all the instances.
An ‘INSTANCE-TYPE’ element is used to select the annotation type to be used for instances,
and the attributes are deﬁned by a sequence of ‘ATTRIBUTE’ elements.
For example, if an ‘INSTANCE-TYPE’ has a ‘Token’ for value, there will one instance in
the dataset per ‘Token’. This also means that the positions (see below) are deﬁned in
relation to Tokens. The ‘INSTANCE-TYPE’ can be seen as the smallest unit to be taken
into account for the Machine Learning.
An ATTRIBUTE element has the following sub-elements:
NAME: the name of the attribute
TYPE: the annotation type used to extract the attribute.
FEATURE (optional): if present, the value of the attribute will be the value of the
named feature on the annotation of speciﬁed type.
POSITION: the position of the annotation used to extract the feature relative to the
current instance annotation.
VALUES(optional): includes a list of VALUE elements.
<CLASS/>: an empty element used to mark the class attribute. There can only be
one attribute marked as class in a dataset deﬁnition.
The VALUES being deﬁned as XML entities, the characters <, > and & must be replaced
by &lt;, &rt; and &amp;. It is recommended to write the XML conﬁguration ﬁle in UTF-8
in order that uncommon characters are correctly parsed.
Semantically, there are three types of attributes:
nominal attributes: both type and features are deﬁned and a list of allowed values
is provided;
numeric: both type and features are deﬁned but no list of allowed values is provided;
it is assumed that the feature can be converted to a number (a double value).
boolean: no feature or list of values is provided; the attribute will take one of the
‘true’ or ‘false’ values based on the presence (or absence) of the speciﬁed annotation
type at the required position.

398

Machine Learning

Figure 17.1: Sample attributes and their values

Figure 17.1 gives some examples of what the values of speciﬁed attributes would be in a
situation when ‘Token’ annotations are used as instances.
An ATTRIBUTELIST element is similar to ATTRIBUTE except that it has no POSITION
sub-element but a RANGE element. This will be converted into several ATTRIBUTELIST
with position ranging from the value of the attribute ‘from’ to the value of the attribute ‘to’.
This can be used in order to avoid the duplication of ATTRIBUTE elements.

17.3.2

The ENGINE Element

The ENGINE element deﬁnes which particular ML implementation will be used, and allows
the setting of options for that particular implementation.
The ENGINE element has three sub-elements:
WRAPPER: deﬁnes the class name for the ML implementation (or implementation
wrapper). The speciﬁed class needs to extend gate.creole.ml.MLEngine.
BATCH-MODE-CLASSIFICATION: this element is optional. If present (as
an empty element <BATCH-MODE-CLASSIFICATION />), the training instances will be

Machine Learning

399

passed to the engine in a single batch. If absent, the instances are passed to the engine
one at a time. Not every engine supports this option, but for those that do, it can
greatly improve performance.
OPTIONS: the contents of the OPTIONS element will be passed verbatim to the ML
engine used.

17.3.3

The WEKA Wrapper

The PR provides a wrapper for the WEKA ML Library (http://www.cs.waikato.ac.nz/ml/weka/)
in the form of the gate.creole.ml.weka.Wrapper class.

Options for the WEKA Wrapper
The WEKA wrapper accepts the following options:
CLASSIFIER: the class name for the classiﬁer to be used.
CLASSIFIER-OPTIONS: the options string as required for the classiﬁer.
CONFIDENCE-THRESHOLD: a double value. If the classiﬁer can provide a
probability distribution rather than a simple classiﬁcation then all possible classiﬁcations that have a probability value larger or equal to the conﬁdence threshold will be
considered.
DATASET-FILE: location of the weka arﬀ ﬁle. This item is not mandatory, it is
possible to specify the ﬁle using the saving option on the GUI.

Training an ML Model with the WEKA Wrapper
The Machine Learning PR has a Boolean runtime parameter named ”training”. When the
value of this parameter is set to true, the PR will collect a dataset of instances from the
documents on which it is run. If the classiﬁer used is an updatable classiﬁer then the ML
model will be built while collecting the dataset. If the selected classiﬁer is not updatable,
then the model will be built the ﬁrst time a classiﬁcation is attempted.
Training a model consists of designing a deﬁnition ﬁle for the ML PR, and creating an
application containing a Machine Learning PR. When the application is run over a corpus,
the dataset (and the model if possible) is built.

400

Machine Learning

Applying a Learnt Model
Using the same PR, set the ‘training’ parameter to false and run your application.
Depending on the type of the attribute that is marked as class, diﬀerent actions will be
performed when a classiﬁcation occurs:
if the attribute is boolean, a new annotation of the speciﬁed type will be created with
no features;
if the attribute is nominal or numeric, a new annotation of the speciﬁed type will be
created with the feature named in the attribute deﬁnition having the value predicted
by the classiﬁer.
Once a model is learnt, it can be saved and reloaded at a later time. The WEKA wrapper
also provides an operation for saving only the dataset in the ARFF format, which can be
used for experiments in the WEKA interface. This could be useful for determining the best
algorithm to be used and the optimal options for the selected algorithm.

17.3.4

The MAXENT Wrapper

GATE also provides a wrapper for the Open NLP MAXENT library
(http://maxent.sourceforge.net/about.html). The MAXENT library provides an implementation of the maximum entropy learning algorithm, and can be accessed using the
gate.creole.ml.maxent.MaxentWrapper class.
The MAXENT library requires all attributes except for the class attribute to be boolean, and
that the class attribute be boolean or nominal. (It should be noted that, within maximum
entropy terminology, the class attribute is called the ‘outcome’.) Because the MAXENT
library does not provide a speciﬁc format for data sets, there is no facility to save or load
data sets separately from the model, but if there should be a need to do this, the WEKA
wrapper can be used to collect the data.
Training a MAXENT model follows the same general procedure as for WEKA models, but
the following diﬀerence should be noted. MAXENT models are not updateable, so the model
will always be created and trained the ﬁrst time a classiﬁcation is attempted. The training
of the model might take a considerable amount of time, depending on the amount of training
data and the parameters of the model.
Options for the MAXENT Wrapper
CUT-OFF: MAXENT features will only be included in the model if they occur at
least this many times. (The default value of this parameter is zero.)

Machine Learning

401

ITERATIONS: The number of times the training procedure should iterate when
ﬁnding the model’s parameters (default is 10). In general no more than about 100
iterations should be needed to train a model, and it is recommended that less are used
during development to allow for shorter training times.
CONFIDENCE-THRESHOLD: Same as for the WEKA wrapper (see above).
However, if this parameter is not set, or is set to zero, the model will not use a
conﬁdence threshold, but will simply return the most likely classiﬁcation.
SMOOTHING: Use smoothing when training the model. Smoothing can improve the
accuracy of the learned models, but it will result in longer training times, and training
will use more memory. The size of the learned models will also be larger. Generally
smoothing will only improve performance for those models trained from small data
sets with a few outcomes. With larger data sets with lots of outcomes, it may make
performance worse.
SMOOTHING-OBSERVATION: When using smoothing, this will specify the
number of times that trainer will imagine that it has seen features which it did not see
(default value is 0.1).
VERBOSE: If selected, this will cause the classiﬁer to output more details of its
operation during execution.

17.3.5

The SVM Light Wrapper

The PR provides a wrapper for the SVM Light ML system (http://svmlight.joachims.org).
SVM Light is a support vector machine implementation, written in C, which is provided as
a set of command line programs. The wrapper takes care of the mundane work of converting
the data structures between GATE and SVM Light formats, and calls the command line
programs in the right sequence, passing the data back and forth in temporary ﬁles. The
<WRAPPER> value for this engine is gate.creole.ml.svmlight.SVMLightWrapper.
The SVM Light binaries themselves are not distributed with GATE – you should download
the version for your platform from http://svmlight.joachims.org and place svm learn and
svm classify on your path.
Classifying documents using the SVMLightWrapper is a two phase procedure. In its ﬁrst
phase, SVMWrapper collects data from the pre-annotated documents and builds the SVM
model using the collected data to classify the unseen documents in its second phase. Below
we describe brieﬂy an example of classifying the start time of the seminar in a corpus of
email announcing seminars and provide more details later in the section.
Figure 17.2 explains step by step the process of collecting training data for the SVM classiﬁer.
GATE documents, which are pre-annotated with the annotations of type Class and feature
type=’stime’, are used as the training data. In order to build the SVM model, we require start

402

Machine Learning

and end annotations for each stime annotation. We use a pre-processor JAPE transduction
script to mark the sTimeStart and sTimeEnd annotations on stime annotations. Following
this step, the Machine Learning PR (SVMLightWrapper) with training mode set to true
collects the training data from all training documents. A GATE corpus pipeline, given a
set of documents and PRs to execute on them, executes all PRs one by one, only on one
document at a time. Unless provided in a separate pipeline, it makes it impossible to send
all training data (i.e. collected from all documents) altogether to the SVMWrapper using
the same pipeline to build the SVM model. This results in the model not being built at the
time of collecting training data. The state of the SVMWrapper can be saved to an external
ﬁle once the training data is collected.

Figure 17.2: Flow diagram explaining the SVM training data collection
Before classifying any unseen document, SVM requires the SVM model to be available. In
the absence of an up-to-date SVM model, SVMWrapper builds a new one using a command
line SVM learn utility and the training data collected from the training corpus. In other
words, the ﬁrst SVM model is built when a user tries to classify the ﬁrst document. At this
point the user has an option to save the model somewhere. This is to enable reloading of
the model prior to classifying other documents and to avoid rebuilding of the SVM model
everytime the user classiﬁes a new set of documents. Once the model becomes available,
SVMWrapper classiﬁes the unseen documents which creates new sTimeStart and sTimeEnd
annotations over the text. Finally, a post-processor JAPE transduction script is used to
combine them into the sTime annotation. Figure 17.3 explains this process.
The wrapper allows support vector machines to be created which do either boolean classiﬁcation or regression (estimation of numeric parameters), and so the class attribute can be
boolean or numeric. Additionally, when learning a classiﬁer, SVM Light supports transduction, whereby additional examples can be presented during training which do not have the
value of the class attribute marked. Presenting such examples can, in some circumstances,
greatly improve the performance of the classiﬁer. To make use of this, the class attribute
can be a three value nominal, in which case the ﬁrst value speciﬁed for that nominal in the
conﬁguration ﬁle will be interpreted as true, the second as false and the third as unknown.
Transduction will be used with any instances for which this attribute is set to the unknown
value. It is also possible to use a two value nominal as the class attribute, in which case it
will simply be interpreted as true or false.
The other attributes can be boolean, numeric or nominal, or any combination of these. If
an attribute is nominal, each value of that attribute maps to a separate SVM Light feature.
Each of these SVM Light features will be given the value 1 when the nominal attribute has
the corresponding value, and will be omitted otherwise. If the value of the nominal is not
speciﬁed in the conﬁguration ﬁle or there is no value for an instance, then no feature will be

Machine Learning

403

Figure 17.3: Flow diagram explaining document classifying process

added.
An extension to the basic functionality of SVM Light is that each attribute can receive a
weighting. These weighting can be speciﬁed in the conﬁguration ﬁle by adding <WEIGHTING>
tags to the parts of the XML ﬁle specifying each attribute. The weighting for the attribute
must be speciﬁed as a numeric value, and be placed between an opening <WEIGHTING> tag
and a closing </WEIGHTING> one. Giving an attribute a greater weighting, will cause it to
play a greater role in learning the model and classifying data. This is achieved by multiplying
the value of the attribute by the weighting before creating the training or test data that is
passed to SVM Light. Any attribute left without an explicitly speciﬁed weighting is given a
default weighting of one. Support for these weightings is contained in the Machine Learning
PR itself, and so is available to other wrappers, though at time of writing only the SVM
Light wrapper makes use of weightings.
As with the MAXENT wrapper, SVM Light models are not updateable, so the model
will be trained at the ﬁrst classiﬁcation attempt. The SVM Light wrapper supports
<BATCH-MODE-CLASSIFICATION />, which should be used unless you have a very good reason
not to.
The SVM Light wrapper allows both data sets and models to be loaded and saved to ﬁles
in the same formats as those used by SVM Light when it is run from the command line.
When a model is saved, a ﬁle will be created which contains information about the state
of the SVM Light Wrapper, and which is needed to restore it when the model is loaded
again. This ﬁle does not, however, contain any information about the SVM Light model
itself. If an SVM Light model exists at the time of saving, and that model is up to date with
respect to the current state of the training data, then it will be saved as a separate ﬁle, with

404

Machine Learning

the same name as the ﬁle containing information about the state of the wrapper, but with
.NativePart appended to the ﬁlename. These ﬁles are in the standard SVM Light model
format, and can be used with SVM Light when it is run from the command line. When a
model is reloaded by GATE, both of these ﬁles must be available, and in the same directory,
otherwise an error will result. However, if an up to date trained model does not exist at
the time the model is saved, then only one ﬁle will be created upon saving, and only that
ﬁle is required when the model is reloaded. So long as at least one training instance exists,
it is possible to bring the model up to date at any point simply by classifying one or more
instances (i.e. running the model with the training parameter set to false).

Options for the SVM Light Engine
Only one <OPTIONS> subelement is currently supported:
<CLASSIFIER-OPTIONS> a string of options to be passed to svm learn on the command
line. The only diﬀerence is that the user should not specify whether regression or
classiﬁcation is to be used, as the wrapper will detect this automatically, based on the
type of the class attribute, and set the option accordingly.

17.3.6

Example Conﬁguration File

<?xml version="1.0" encoding="UTF-8"?>
<ML-CONFIG>
<DATASET>
<!-- The type of annotation used as instance -->
<INSTANCE-TYPE>Token</INSTANCE-TYPE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>Lookup(0)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Lookup</TYPE>
<!-- The position relative to the instance annotation -->
<POSITION>0</POSITION>
</ATTRIBUTE>

<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>Lookup_MT(-1)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Lookup</TYPE>
<!-- Optional: the feature name for the feature used to extract values
for the attribute -->
<FEATURE>majorType</FEATURE>
<!-- The position relative to the instance annotation -->
<POSITION>-1</POSITION>
<!-- The list of permitted values.
if present, marks a nominal attribute;
if absent, the attribute is numeric (double)
-->
<VALUES>
<!-- One permitted value -->

Machine Learning

<VALUE>address</VALUE>
<VALUE>cdg</VALUE>
<VALUE>country_adj</VALUE>
<VALUE>currency_unit</VALUE>
<VALUE>date</VALUE>
<VALUE>date_key</VALUE>
<VALUE>date_unit</VALUE>
<VALUE>facility</VALUE>
<VALUE>facility_key</VALUE>
<VALUE>facility_key_ext</VALUE>
<VALUE>govern_key</VALUE>
<VALUE>greeting</VALUE>
<VALUE>ident_key</VALUE>
<VALUE>jobtitle</VALUE>
<VALUE>loc_general_key</VALUE>
<VALUE>loc_key</VALUE>
<VALUE>location</VALUE>
<VALUE>number</VALUE>
<VALUE>org_base</VALUE>
<VALUE>org_ending</VALUE>
<VALUE>org_key</VALUE>
<VALUE>org_pre</VALUE>
<VALUE>organization</VALUE>
<VALUE>organization_noun</VALUE>
<VALUE>percent</VALUE>
<VALUE>person_ending</VALUE>
<VALUE>person_first</VALUE>
<VALUE>person_full</VALUE>
<VALUE>phone_prefix</VALUE>
<VALUE>sport</VALUE>
<VALUE>spur</VALUE>
<VALUE>spur_ident</VALUE>
<VALUE>stop</VALUE>
<VALUE>surname</VALUE>
<VALUE>time</VALUE>
<VALUE>time_modifier</VALUE>
<VALUE>time_unit</VALUE>
<VALUE>title</VALUE>
<VALUE>year</VALUE>
</VALUES>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>Lookup_MT(0)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Lookup</TYPE>
<!-- Optional: the feature name for the feature used to extract values
for the attribute -->
<FEATURE>majorType</FEATURE>
<!-- The position relative to the instance annotation -->
<POSITION>0</POSITION>
<!-- The list of permitted values.
if present, marks a nominal attribute;
if absent, the attribute is numeric (double)
-->
<VALUES>
<!-- One permitted value -->
<VALUE>address</VALUE>
<VALUE>cdg</VALUE>
<VALUE>country_adj</VALUE>
<VALUE>currency_unit</VALUE>
<VALUE>date</VALUE>
<VALUE>date_key</VALUE>
<VALUE>date_unit</VALUE>

405

406

Machine Learning

<VALUE>facility</VALUE>
<VALUE>facility_key</VALUE>
<VALUE>facility_key_ext</VALUE>
<VALUE>govern_key</VALUE>
<VALUE>greeting</VALUE>
<VALUE>ident_key</VALUE>
<VALUE>jobtitle</VALUE>
<VALUE>loc_general_key</VALUE>
<VALUE>loc_key</VALUE>
<VALUE>location</VALUE>
<VALUE>number</VALUE>
<VALUE>org_base</VALUE>
<VALUE>org_ending</VALUE>
<VALUE>org_key</VALUE>
<VALUE>org_pre</VALUE>
<VALUE>organization</VALUE>
<VALUE>organization_noun</VALUE>
<VALUE>percent</VALUE>
<VALUE>person_ending</VALUE>
<VALUE>person_first</VALUE>
<VALUE>person_full</VALUE>
<VALUE>phone_prefix</VALUE>
<VALUE>sport</VALUE>
<VALUE>spur</VALUE>
<VALUE>spur_ident</VALUE>
<VALUE>stop</VALUE>
<VALUE>surname</VALUE>
<VALUE>time</VALUE>
<VALUE>time_modifier</VALUE>
<VALUE>time_unit</VALUE>
<VALUE>title</VALUE>
<VALUE>year</VALUE>
</VALUES>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>Lookup_MT(1)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Lookup</TYPE>
<!-- Optional: the feature name for the feature used to extract values
for the attribute -->
<FEATURE>majorType</FEATURE>
<!-- The position relative to the instance annotation -->
<POSITION>1</POSITION>
<!-- The list of permitted values.
if present, marks a nominal attribute;
if absent, the attribute is numeric (double)
<VALUES>
<!-- One permitted value -->
<VALUE>address</VALUE>
<VALUE>cdg</VALUE>
<VALUE>country_adj</VALUE>
<VALUE>currency_unit</VALUE>
<VALUE>date</VALUE>
<VALUE>date_key</VALUE>
<VALUE>date_unit</VALUE>
<VALUE>facility</VALUE>
<VALUE>facility_key</VALUE>
<VALUE>facility_key_ext</VALUE>
<VALUE>govern_key</VALUE>
<VALUE>greeting</VALUE>
<VALUE>ident_key</VALUE>

-->

Machine Learning

407

<VALUE>jobtitle</VALUE>
<VALUE>loc_general_key</VALUE>
<VALUE>loc_key</VALUE>
<VALUE>location</VALUE>
<VALUE>number</VALUE>
<VALUE>org_base</VALUE>
<VALUE>org_ending</VALUE>
<VALUE>org_key</VALUE>
<VALUE>org_pre</VALUE>
<VALUE>organization</VALUE>
<VALUE>organization_noun</VALUE>
<VALUE>percent</VALUE>
<VALUE>person_ending</VALUE>
<VALUE>person_first</VALUE>
<VALUE>person_full</VALUE>
<VALUE>phone_prefix</VALUE>
<VALUE>sport</VALUE>
<VALUE>spur</VALUE>
<VALUE>spur_ident</VALUE>
<VALUE>stop</VALUE>
<VALUE>surname</VALUE>
<VALUE>time</VALUE>
<VALUE>time_modifier</VALUE>
<VALUE>time_unit</VALUE>
<VALUE>title</VALUE>
<VALUE>year</VALUE>
</VALUES>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>POS_category(-1)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Token</TYPE>
<!-- Optional: the feature name for the feature used to extract values
for the attribute -->
<FEATURE>category</FEATURE>
<!-- The position relative to the instance annotation -->
<POSITION>-1</POSITION>
<!-- The list of permitted values.
if present, marks a nominal attribute;
if absent, the attribute is numeric (double)
<VALUES>
<!-- One permitted value -->
<VALUE>NN</VALUE>
<VALUE>NNP</VALUE>
<VALUE>NNPS</VALUE>
<VALUE>NNS</VALUE>
<VALUE>NP</VALUE>
<VALUE>NPS</VALUE>
<VALUE>JJ</VALUE>
<VALUE>JJR</VALUE>
<VALUE>JJS</VALUE>
<VALUE>JJSS</VALUE>
<VALUE>RB</VALUE>
<VALUE>RBR</VALUE>
<VALUE>RBS</VALUE>
<VALUE>VB</VALUE>
<VALUE>VBD</VALUE>
<VALUE>VBG</VALUE>
<VALUE>VBN</VALUE>
<VALUE>VBP</VALUE>
<VALUE>VBZ</VALUE>

-->

408

Machine Learning

<VALUE>FW</VALUE>
<VALUE>CD</VALUE>
<VALUE>CC</VALUE>
<VALUE>DT</VALUE>
<VALUE>EX</VALUE>
<VALUE>IN</VALUE>
<VALUE>LS</VALUE>
<VALUE>MD</VALUE>
<VALUE>PDT</VALUE>
<VALUE>POS</VALUE>
<VALUE>PP</VALUE>
<VALUE>PRP</VALUE>
<VALUE>PRP$</VALUE>
<VALUE>PRPR$</VALUE>
<VALUE>RP</VALUE>
<VALUE>TO</VALUE>
<VALUE>UH</VALUE>
<VALUE>WDT</VALUE>
<VALUE>WP</VALUE>
<VALUE>WP$</VALUE>
<VALUE>WRB</VALUE>
<VALUE>SYM</VALUE>
<VALUE>\"</VALUE>
<VALUE>#</VALUE>
<VALUE>$</VALUE>
<VALUE>’</VALUE>
<VALUE>(</VALUE>
<VALUE>)</VALUE>
<VALUE>,</VALUE>
<VALUE>--</VALUE>
<VALUE>-LRB-</VALUE>
<VALUE>.</VALUE>
<VALUE>’’</VALUE>
<VALUE>:</VALUE>
<VALUE>::</VALUE>
<VALUE>‘</VALUE>
</VALUES>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>POS_category(0)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Token</TYPE>
<!-- Optional: the feature name for the feature used to extract values
for the attribute -->
<FEATURE>category</FEATURE>
<!-- The position relative to the instance annotation -->
<POSITION>0</POSITION>
<!-- The list of permitted values.
if present, marks a nominal attribute;
if absent, the attribute is numeric (double)
<VALUES>
<!-- One permitted value -->
<VALUE>NN</VALUE>
<VALUE>NNP</VALUE>
<VALUE>NNPS</VALUE>
<VALUE>NNS</VALUE>
<VALUE>NP</VALUE>
<VALUE>NPS</VALUE>
<VALUE>JJ</VALUE>
<VALUE>JJR</VALUE>
<VALUE>JJS</VALUE>

-->

Machine Learning

409

<VALUE>JJSS</VALUE>
<VALUE>RB</VALUE>
<VALUE>RBR</VALUE>
<VALUE>RBS</VALUE>
<VALUE>VB</VALUE>
<VALUE>VBD</VALUE>
<VALUE>VBG</VALUE>
<VALUE>VBN</VALUE>
<VALUE>VBP</VALUE>
<VALUE>VBZ</VALUE>
<VALUE>FW</VALUE>
<VALUE>CD</VALUE>
<VALUE>CC</VALUE>
<VALUE>DT</VALUE>
<VALUE>EX</VALUE>
<VALUE>IN</VALUE>
<VALUE>LS</VALUE>
<VALUE>MD</VALUE>
<VALUE>PDT</VALUE>
<VALUE>POS</VALUE>
<VALUE>PP</VALUE>
<VALUE>PRP</VALUE>
<VALUE>PRP$</VALUE>
<VALUE>PRPR$</VALUE>
<VALUE>RP</VALUE>
<VALUE>TO</VALUE>
<VALUE>UH</VALUE>
<VALUE>WDT</VALUE>
<VALUE>WP</VALUE>
<VALUE>WP$</VALUE>
<VALUE>WRB</VALUE>
<VALUE>SYM</VALUE>
<VALUE>\"</VALUE>
<VALUE>#</VALUE>
<VALUE>$</VALUE>
<VALUE>’</VALUE>
<VALUE>(</VALUE>
<VALUE>)</VALUE>
<VALUE>,</VALUE>
<VALUE>--</VALUE>
<VALUE>-LRB-</VALUE>
<VALUE>.</VALUE>
<VALUE>’’</VALUE>
<VALUE>:</VALUE>
<VALUE>::</VALUE>
<VALUE>‘</VALUE>
</VALUES>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>POS_category(1)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Token</TYPE>
<!-- Optional: the feature name for the feature used to extract values
for the attribute -->
<FEATURE>category</FEATURE>
<!-- The position relative to the instance annotation -->
<POSITION>1</POSITION>
<!-- The list of permitted values.
if present, marks a nominal attribute;
if absent, the attribute is numeric (double)
<VALUES>

-->

410

<!-- One permitted value -->
<VALUE>NN</VALUE>
<VALUE>NNP</VALUE>
<VALUE>NNPS</VALUE>
<VALUE>NNS</VALUE>
<VALUE>NP</VALUE>
<VALUE>NPS</VALUE>
<VALUE>JJ</VALUE>
<VALUE>JJR</VALUE>
<VALUE>JJS</VALUE>
<VALUE>JJSS</VALUE>
<VALUE>RB</VALUE>
<VALUE>RBR</VALUE>
<VALUE>RBS</VALUE>
<VALUE>VB</VALUE>
<VALUE>VBD</VALUE>
<VALUE>VBG</VALUE>
<VALUE>VBN</VALUE>
<VALUE>VBP</VALUE>
<VALUE>VBZ</VALUE>
<VALUE>FW</VALUE>
<VALUE>CD</VALUE>
<VALUE>CC</VALUE>
<VALUE>DT</VALUE>
<VALUE>EX</VALUE>
<VALUE>IN</VALUE>
<VALUE>LS</VALUE>
<VALUE>MD</VALUE>
<VALUE>PDT</VALUE>
<VALUE>POS</VALUE>
<VALUE>PP</VALUE>
<VALUE>PRP</VALUE>
<VALUE>PRP$</VALUE>
<VALUE>PRPR$</VALUE>
<VALUE>RP</VALUE>
<VALUE>TO</VALUE>
<VALUE>UH</VALUE>
<VALUE>WDT</VALUE>
<VALUE>WP</VALUE>
<VALUE>WP$</VALUE>
<VALUE>WRB</VALUE>
<VALUE>SYM</VALUE>
<VALUE>\"</VALUE>
<VALUE>#</VALUE>
<VALUE>$</VALUE>
<VALUE>’</VALUE>
<VALUE>(</VALUE>
<VALUE>)</VALUE>
<VALUE>,</VALUE>
<VALUE>--</VALUE>
<VALUE>-LRB-</VALUE>
<VALUE>.</VALUE>
<VALUE>’’</VALUE>
<VALUE>:</VALUE>
<VALUE>::</VALUE>
<VALUE>‘</VALUE>
</VALUES>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>
<ATTRIBUTE>
<!-- The name given to the attribute -->
<NAME>Entity(0)</NAME>
<!-- The type of annotation used as attribute -->
<TYPE>Entity</TYPE>
<!-- The position relative to the instance annotation -->

Machine Learning

Machine Learning

<POSITION>0</POSITION>
<CLASS/>
<!-- Optional: if present marks the attribute used as CLASS
Only one attribute can be marked as class -->
</ATTRIBUTE>

</DATASET>
<ENGINE>
<WRAPPER>gate.creole.ml.weka.Wrapper</WRAPPER>
<OPTIONS>
<CLASSIFIER OPTIONS="-S -C 0.25 -B -M 2">weka.classifiers.trees.J48</CLASSIFIER>
<CONFIDENCE-THRESHOLD>0.85</CONFIDENCE-THRESHOLD>
</OPTIONS>
</ENGINE>
</ML-CONFIG>

411

412

Machine Learning

Chapter 18
Tools for Alignment Tasks
18.1

Introduction

This chapter introduces a new plugin called ‘Alignment’ that comprises of tools to perform
text alignment at various level (e.g word, phrase, sentence etc). It allows users to integrate
other tools that can be useful for speeding up the alignment process.
Text alignment can be achieved at a document, section, paragraph, sentence and a word level.
Given two parallel corpora, where the ﬁrst corpus contains documents in a source language
and the other in a target language, the ﬁrst task is to ﬁnd out the parallel documents and
align them at the document level. For these tasks one would need to refer to more than one
document at the same time. Hence, a need arises for Processing Resources (PRs) which can
accept more than one document as parameters. For example given two documents, a source
and a target, a Sentence Alignment PR would need to refer to both of them to identify
which sentence of the source document aligns with which sentence of the target document.
However, the problem occurs when such a PR is part of a corpus pipeline. In a corpus
pipeline, only one document from the selected corpus at a time is set on the member PRs.
Once the PRs have completed their execution, the next document in the corpus is taken and
set on the member PRs. Thus it is not possible to use a corpus pipeline and at the same
time supply for than one document to the underlying PRs.

18.2

The Tools

We have introduced a few new resources in GATE that allows processing parallel data.
These include resources such as CompoundDocument, CompositeDocument, and a new
AlignmentEditor to name a few. Below we describe these components. Please note that
all these resources are distributed as part of the ‘Alignment’ plugin and therefore the users
should load the plugin ﬁrst in order to use these resources.
413

414

Tools for Alignment Tasks

18.2.1

Compound Document

A new Language Resource (LR), called CompoundDocument, is introduced which is a collection of documents and allow various documents to be grouped together under a single
document. The CompoundDocument allows adding more documents to it and removing
them if required. It implements the gate.Document interface allowing users to carry out
all operations that can be done on a normal gate document. For example, if a PR such as
Sentence Aligner needs access to two documents (e.g. source and target documents), these
documents can be grouped under a single compound document and supplied to the Sentence
Alignment PR.
To instantiate CompoundDocument user needs to provide the following parameters.
encoding - encoding of the member documents. All document members must have the
same encoding (e.g. Unicode, UTF-8, UTF-16).
collectRepositioningInfo - this parameter indicates whether the underlying documents
should collect the repositioning information in case the contents of these documents
change.
preserveOriginalContent - if the original content of the underlying documents should
be preserved.
documentIDs - users need to provide a unique ID for each document member. These
ids are used to locate the appropriate documents.
sourceUrl - given a URL of one of the member documents, the instance of CompoundDocument searches for other members in the same folder based on the ids provided
in the documentIDs parameter. Following document name conventions are followed to
search other member documents:
– FileName.id.extension (ﬁlename followed by id followed the extension and all of
these separated by a ‘.’ (dot)).
– For example if user provides three document IDs (e.g. ‘en’, ‘hi’ and ‘gu’) and
selects a ﬁle with name ‘File.en.xml’, the CompoundDocument will search for
rest of the documents (i.e. ‘File.hi.xml’ and ‘File.gu.xml’). The ﬁle name (i.e.
‘File’) and the extension (i.e. ‘xml’) remain common for all three members of the
compound document.
Figure 18.1 shows a snapshot for instantiating a compound document from GATE Developer.
Compound document provides various methods that help in accessing their individual members.
public Document getDocument(String docid);

Tools for Alignment Tasks

415

Figure 18.1: Compound Document

The following method returns a map of documents where the key is a document ID and the
value is its respective document.

public Map getDocuments();

Please note that only one member document in a compound document can have focus set
on it. Then all the standard document methods of gate.Document interface apply to the
document with focus set on it. For example, if there are two documents, ‘hi’ and ‘en’, and
the focus is set on the document ‘hi’ then the getAnnotations() method will return a default
annotation set of the ‘hi’ document. One can use the following method to switch the focus
of a compound document to a diﬀerent document:

public void setCurrentDocument(String documentID);
public Document getCurrentDocument();

As explained above, new documents can be added to or removed from the compound document using the following method:

public void addDocument(String documentID, Document document);
public void removeDocument(String documentID);

The following code snippet demonstrates how to create a new compound document using
GATE Embedded:

416

Tools for Alignment Tasks

1
2
3

// step 1: initialize GATE
Gate . init ();

4
5
6
7

// step 2: load the Alignment plugin
File alignmentHome = new File ( Gate . getPluginsHome () , " Alignment " );
Gate . getCreoleRegister (). addDirectory ( alignmentHome . toURL ());

8
9
10

// step 3: set the parameters
FeatureMap fm = Factory . newFeatureMap ();

11
12
13
14
15
16
17
18

// for example you want to create a compound document for
// File.id1.xml and File.id2.xml
List docIDs = new ArrayList ();
docIDs . add ( " id1 " );
docIDs . add ( " id2 " );
fm . put ( " documentIDs " , docIDs );
fm . put ( " sourceUrl " , new URL ( " file :/// url / to / File . id1 . xml " ));

19
20
21
22

// step 4: ﬁnally create an instance of compound document
Document aDocument = ( gate . compound . CompoundDocument )
Factory . createResource ( " gate . compound . impl . C o m p o u n d D o c u m e n t I m p l " , fm );

18.2.2

CompoundDocumentFromXml

As described later in the chapter, the entire compound document can be saved in a single
xml ﬁle. In order to load such a compound document from the saved xml ﬁle, we provide
a language resource called CompoundDocumentFromXml. This is same as the Compound
Document. The only diﬀerence is in the parameters needed to instantiate this resource. This
LR requires only one parameter called compoundDocumentUrl. The parameter is the url to
the xml ﬁle.

18.2.3

Compound Document Editor

The compound document editor is a visual resource (VR) associated with the compound
document. The VR contains several tabs - each representing a diﬀerent member of the
compound document. All standard functionalities such as GATE document editor, with all
its add-on plugins such as AnnotationSetView, AnnotationsList, coreference editor etc., are
available to be used with each individual member.
Figure 18.2 shows a compound document editor with English and Hindi documents as members of the compound document.
As shown in the ﬁgure 18.2, there are several buttons at the top of the editor that provide different functionalities. For instance, the Add button, allows adding a new member document

Tools for Alignment Tasks

417

Figure 18.2: Compound Document Editor

to the compound document. The Remove button removes the current visible member from
the document. The buttons Save and Save As XML allow saving the documents individually
and in a single xml document respectively. The Switch button allows changing focus of the
compound document from one member to the other (this functionality is explained later).
Finally, the Alignment Editor allows one to start the alignment editor to align text.

18.2.4

Composite Document

The composite document allows users to merge the texts of member documents and keep the
merged text linked with their respective member documents. In other words, if users make
any change to the composite document (e.g. add new annotations or remove any existing
annotations), the relevant eﬀect is made to their respective documents.
A PR called CombineMembersPR allows creation of a new composite document. It asks for

418

Tools for Alignment Tasks

a class name that implements the CombiningMethod interface. The CombiningMethod tells
the CombineMembersPR how to combine texts and create a new composite document.
For example, a default implementation of the CombiningMethod, called DefaultCombiningMethod, takes the following parameters and puts the text of the compound document’s
members into a new composite document.
unitAnnotationType=Sentence
inputASName=Key
copyUnderlyingAnnotations=true;

The ﬁrst parameter tells the combining method that it is the ‘Sentence’ annotation type
whose text needs to be merged and it should be taken from the ‘Key’ annotation set (second
parameter) and ﬁnally all the underlying annotations of every Sentence annotation must be
copied in the composite document.
If there are two members of a compound document (e.g. ‘hi’ and ‘en’), given the above
parameters, the combining method ﬁnds out all the annotations of type Sentence from each
document and sorts them in ascending order, and one annotation from each document is
put one after another in a composite document. This operation continues until all the
annotations have been traversed.
Document en
Sen1
Sen2
Sen3

Document hi
Shi1
Shi2
Shi3

Document Composite
Sen1
Shi1
Sen2
Shi2
Sen3
Shi3

The composite document also maintains a mapping of text oﬀsets such that if someone adds
a new annotation to or removes any annotation from the composite document, they are
added to or removed from their respective documents. Finally the newly created composite
document becomes a member of the same compound document.

18.2.5

DeleteMembersPR

This PR allows deletion of a speciﬁc member of the compound document. It takes a parameter called ‘documentID’ and deletes a document with this name.

Tools for Alignment Tasks

18.2.6

419

SwitchMembersPR

As described above, only one member of the compound document can have focus set on it.
PRs trying to use the getDocument() method get a pointer to the compound document;
however all the other methods of the compound document give access to the information
of the document member with the focus set on it. So if user wants to process a particular
member of the compound document with some PRs, s/he should use the SwitchMembersPR
that takes one parameter called documentID and sets focus to the document with that
speciﬁc id.

18.2.7

Saving as XML

Calling the toXml() method on a compound document returns the XML representation of
the member which has focus. However, GATE Developer provides an option to save all
member documents in diﬀerent ﬁles. This option appears in the options menu when the
user right-clicks on the compound document. The user is asked to provide a name for the
directory in which all the members of the compound document will be saved in separate
ﬁles.
It is also possible to save all members of the compound document in a single XML ﬁle. The
option, ‘Save in a single XML Document’, also appears in the options menu. After saving it
in a single XML document, the user can use the option ‘Compound Document from XML’
to load the document back into GATE Developer.

18.2.8

Alignment Editor

Inspired by various tools, we have implemented a new version of alignment editor that is
comprised of several new features. We preserve standard ways of aligning text but at the
same time provide advanced features that can be used for facilitating incremental learning.
The alignment editor can be used for performing alignment at any annotation level. When
performing alignment at word or sentence level, the texts being aligned need to be preprocessed in order to identify tokens and sentences boundaries.
Information about the alignments carried over the text of a compound document is stored
as a document feature in the compound document itself. Since the document features are
stored in a map, every object stored as a document feature needs to have a unique name that
identiﬁes that feature. There is no limit on how many features one can store provided they
all have diﬀerent names. This allows storing alignment information, carried out at diﬀerent
levels, in separate alignment instances. For example, if a user is carrying out alignment
at a word level, he/she can store it in an alignment object with a name word-alignment.
Similarly, sentence alignment information can be stored with a name sentence-alignment.
If multiple users are annotating the same document, alignments produced by diﬀerent users

420

Tools for Alignment Tasks

can be stored with diﬀerent names (e.g. word-alignment-user1, word-alignment-user2
etc.). Alignment objects can be used for:
aligning and unaligning two annotations;
checking if the two annotations are aligned with each other;
obtaining all the aligned annotations in a document;
obtaining all the annotations that are aligned to a particular annotation.
Given a compound document containing a source and a target document, the alignment
editor starts in the alignment viewer mode. In this mode the texts of the two documents are
shown side-by-side in parallel windows. The purpose of the alignment viewer is to highlight
the annotations that are already aligned. The ﬁgure 18.3 shows the alignment viewer. In
this case the selected documents are English and Hindi, titled as en and hi respectively.

Figure 18.3: Alignment Viewer
To see alignments, user needs to select the alignment object that he/she wants to see alignments from. Along with this, user also needs to select annotation sets - one for the source

Tools for Alignment Tasks

421

document and one for the target document. Given these parameters, the alignment viewer
highlights the annotations that belong to the selected annotation sets and have been aligned
in the selected alignment object. When the mouse is placed on one of the aligned annotations, the selected annotation and the annotations that are aligned to the selected annotation
are highlighted in red. In this case (see ﬁgure 18.3) the word go is aligned with the words
chalate hein.
Before the alignment process can be started, the tool needs to know few parameters about
the alignment task.
Unit Of Alignment: this is the annotation type that users want to perform alignment
at. Data Source: generally, if performing a word alignment task, people consider a pair
of aligned sentences one at a time and align words within sentences. If the sentences are
annotated, for example as Sentence, the Sentence annotation type is called Parent of
Unit of Alignment. The Data Source contains information about the aligned parents
of unit of alignment. In this case, it would refer to the alignment object that contains
alignment information about the annotations of type Sentence. The editor iterates through
the aligned sentences and forms pairs of parent of unit of alignments to be shown to the
user one by one. If user does not provide any data source, a single pair is formed containing
entire documents. Alignment Feature Name: this is the name given to the alignment
object where the information about new alignments is stored.
The purpose of the alignment viewer is to highlight the annotations that are already aligned.
The editor comes with three diﬀerent views for performing alignment which the user can
select at the time of creating a new alignment task: the Links view (see 18.4 - suitable for
character, word and phrase level alignments), the Parallel view (see 18.5 - suitable for
annotations which have longer texts, e.g. sentences, paragraphs, sections) and the Matrix
view (see 18.6) - suitable for character, word and phrase level alignment.
Let us assume that the user wants to align words in sentences using the Links view. The
ﬁrst thing he needs to do is to create a new Alignment task. This can be achieved by
clicking on the File menu and selecting the New Task option. User is asked to provide
certain parameters as discussed above. The editor also allows to store task conﬁgurations in
an xml ﬁle which can be at later stage reloaded in the alignment editor. Also, if there are
more than one task created, the editor allows users to switch between them.
To align one or more words in the source language with one or more words in the target
language, the user needs to select individual words by clicking on them individually. Clicking
on words highlights them with an identical colour. Right clicking on any of the selected words
brings up a menu with the two default options: Reset Selection and Align. Diﬀerent
colours are used for highlighting diﬀerent pairs of alignments. This helps distinguishing one
set of aligned words from other sets of aligned pairs. Also a link between the aligned words
in the two texts is drawn to show the alignment. To unalign, user needs to right click on
the aligned words and click on the Remove Alignment option. Only the word on which user
right-clicks is taken out of the alignment and rest of the words in the pair remain unaﬀected.
We use the term Orphaned Annotation to refer to the annotation which does not have any

422

Tools for Alignment Tasks

alignment in the target document. If after removing an annotation from alignment pair,
there are any orphaned annotations in the alignment pair, they are unaligned too.

Figure 18.4: Links View

Advanced Features
The options Align, Reset Selection and Remove Alignment are available by default. The
Align and the Reset Selection options appear when user wants to align new annotations.
The Remove Alignment option only appears when user right clicks on the already aligned
annotations. The ﬁrst two actions are available when there is at least one annotation selected
in the source language and another one is selected in the target language. Apart from these
three basic actions, the editor also allows adding more actions to the editor.
There are four diﬀerent types of actions: actions that should be taken before the user
starts aligning words (PreDisplayAction); actions that should be taken when the user
aligns annotations (AlignmentAction); the actions that should be taken when the user has

Tools for Alignment Tasks

423

Figure 18.5: Parallel View

completed aligning all the words in the given sentence pair (FinishedAlignmentAction)
and the actions to publish any data or statistics to the user. For example, to help users
in the alignment process by suggesting word alignments, one may want to wrap a pretrained statistical word alignment model as PreDisplayAction. Similarly, actions of the type
AlignmentAction can be used for submitting examples to the model in order for the model
to update itself. When all the words in a sentence pair are aligned, one may want to sign oﬀ
the pair and take actions such as comparing all the alignments in that sentence pair with the
alignments carried out by some other user for the same pair. Similarly, while collecting data
in the background, one might want to display some information to the user (e.g. statistics
for the collected data or some suggestions that help users in the alignment process).
When users click on the next or the previous button, the editor obtains the next or the
previous pair that needs to be shown from the data source. Before the pair is displayed in the
editor, the editor calls the registered instances of the PreDisplayAction and the current pair
object is passed onto the instances of PreDisplayAction. Please note that this only happens
when the pair is not already signed oﬀ. Once the instances of PreDisplayAction have been

424

Tools for Alignment Tasks

Figure 18.6: Matrix View

executed, the editor collects the alignment information from the compound document and
displays it in the editor.
As explained earlier, when users right click on units of alignment in the editor a popup menu
with default options (e.g. Align, Reset Selection and Remove Alignment) is shown. The
editor allows adding new actions to this menu. It is also possible that users may want to
take extra actions when they click on any of the Align or the Remove Alignment options.
The AlignmentAction makes it possible to achieve this. Below we list some of the parameters
of the AlignmenAction. The implementation is called depending on these parameters.

invokeForAlignedAnnotation - the action appears in the options menu when user
right clicks on the aligned annotation.
invokeForHighlightedUnalignedAnnotation - the action appears in the options
menu when user right clicks on a highlighted but unaligned annotation.

Tools for Alignment Tasks

425

invokeForUnhighlightedUnalignedAnnotation - the action appears in the options
menu when user right clicks on an unhighlighted and unaligned annotation.
invokeWithAlignAction - the action is executed whenever user aligns some annotations.
invokeWithRemoveAction - the action is executed whenever user removes any alignment.
caption - in case of the ﬁrst three options, the caption is used in the options menu.
In case of the fourth and the ﬁfth options, the caption appears as a check box under
the actions tab.

These methods can be used for, for example, building up a dictionary in the background
while aligning word pairs. Before users click on the next button, they are asked if the pair
they were aligning has been aligned completely (i.e. signed oﬀ for further alignment). If user
replies yes to it, the actions registered as FinishedAlignmentAction are executed one after
the other. This could be helpful, for instance, to write an alignment exporter that exports
alignment results in an appropriate format or to update the dictionary with new alignments.
Users can point the editor to a ﬁle that contains a list of actions and parameters needed to
initialize them. A conﬁguration ﬁle is a simple text ﬁle with fully-qualiﬁed class name, and
required parameters speciﬁed in it. Below we give an example of such a conﬁguration ﬁle.

gate.alignment.actions.AlignmentCache,$relpath$/align-cache.txt,root

The ﬁrst argument is the name of the class that implements one of the actions described
above. The second parameter is the name of the ﬁle in which the alignment cache should store
its results. Finally, the third argument instructs the alignment cache to store root forms of
the words in the dictionary so that diﬀerent forms of the same words can be matched easily.
All the parameters (comma separated) after the class name are passed to the action. The
relpath parameter is resolved at runtime.
AlignmentCache is one such example of FinishedAlignmentAction and the PreDisplayAction. This is an inbuilt alignment cache in the editor which collects alignment pairs that the
users annotate. The idea here is to cache such pairs and later, align them automatically if
they appear in subsequent pairs, thus reducing the eﬀorts of humans to annotate the same
pair again. By default the alignment cache is disabled. Users wishing to enable it should look
into the plugins/Alignment/resources/actions.conf and uncomment the appropriate line.
Users wishing to implement their own actions should refer to the implementation of the
AlignmentCache.

426

18.2.9

Tools for Alignment Tasks

Saving Files and Alignments

A compound document can have more than one member documents, the alignment information stored as a document feature and more than one alignment features. The framework
allows users to store the whole compound document in a single XML ﬁle. The XML ﬁle contains all the necessary information about the compound document to load it back in Gate
and bring it to the state the compound document was when saving the document as XML. It
contains XML produced for each and every member document of the compound document
and the details of the document features set on the compound document. XML for each
member document includes XML elements for its content; annotations sets and annotations;
document features set on individual member document and the id given to this document as
as member of the compound document. Having a single XML ﬁle makes it possible to port
the entire ﬁle from one destination to the others easily.
Apart from this, the framework has an alignment exporter. Using the alignment exporter, it
is possible to store the alignment information in a separate XML ﬁle. For example, once the
annotators have aligned documents at a word level, the alignment information about both
the unit and the parent of unit annotations can be exported to an XML ﬁle. Figure 18.7
shows an XML ﬁle with word alignment information in it.

Figure 18.7: Word Alignment XML File
When aligning words in sentences, it is possible to have one or more source sentences aligned
with one or more target sentences in a pair. This is achieved by having Source and Target
elements within the Pair element which can have one or more Sentence elements in each
of them. Each word or token within these sentences is marked with Token element. Every
Token element has a unique id assigned to it which is used when aligning words. It is
possible to have 1:1 or 1:many and many:1 alignments. The Alignment element is used for

Tools for Alignment Tasks

427

mentioning every alignment pair with source and target attributes that refer to one of the
source token ids and one of the target document ids respectively. For example, according
to the ﬁrst alignment entry, the source token markets with id 3 is aligned with the target
token bAzAr with id 3. The exporter does not export any entry for the unaligned words.

18.2.10

Section-by-Section Processing

In this section, we describe a component that allows processing documents section-by-section.
Processing documents this way is useful for many reasons:
For example, a patent document has several diﬀerent sections but user is interested in processing only the ‘claims’ section or the ‘technical details section’. This is also useful for
processing a large document where processing it as a single document is not possible and the
only alternative is to divide it in several small documents to process them independently.
However, doing so would need another process that merges all the small documents and their
annotations back into the original document. On the other hand, a webpage may contain
proﬁles of diﬀerent people. If the document has more than one person with similar names,
running the ‘Orthomatcher PR’ on such a document would produce incorrect coreference
chains.
All such problems can be solved by using a PR called ‘Segment Processing PR’. This PR
is distributed as part of the ‘Alignment’ plugin. User needs to provide the following four
parameters to run this PR.
1. document: This is the document to be processed.
2. controller: This is a corpus controller that needs to be used for processing the
segments of the document.
3. segmentAnnotationType: Sections of the documents (that need to be processed)
should be annotated with some annotation type and the type of such annotation should
be provided as the value to this parameter.
4. inputASName: This is the name of the annotation set that contains the segment annotations.
Given these parameters, each span in the document that is annotated as the type speciﬁed
by the segmentAnnotationType is processed independently.
Given a corpus of publications, if you just want to process the abstract section with the
ANNIE application, please follow the following steps. It is assumed that the boundaries of
abstracts in all these publications are already identiﬁed. If not, you would have to do some
processing to identify them prior to using the following steps. In the following example,

428

Tools for Alignment Tasks

we assume that the abstract boundaries have been annotated as ‘Abstract’ annotations and
stored under the ‘Original markups’ annotation set.
Steps:
1. Create a new corpus and populate it with a set of publications that you would like to
process with ANNIE.
2. Load the ANNIE application.
3. Load the ‘Alignment’ plugin.
4. Create an instance of the ‘Segment Processing PR’ by selecting it from the list of
processing resources.
5. Create a corpus pipeline.
6. Add the ‘Segment Processing PR’ into the pipeline and provide the following parameters:
(a) Provide the corpus with publication documents in it as a parameter to the corpus
controller.
(b) Select the ‘ANNIE’ controller for the ‘controller’ parameter.
(c) Type ‘Abstract’ in the ‘segmentAnnotationType’ parameter.
(d) Type ‘Original markups’ in the ‘inputASName’ parameter.
7. Run the application.
Now, you should see that the ANNIE application has only processed the text in each document that was annotated as ‘Abstract’.

Chapter 19
Combining GATE and UIMA
UIMA (Unstructured Information Management Architecture) is a platform for natural language processing, originally developed by IBM but now maintained by the Apache Software
Foundation. It has many similarities to the GATE architecture – it represents documents
as text plus annotations, and allows users to deﬁne pipelines of analysis engines that manipulate the document (or Common Analysis Structure in UIMA terminology) in much the
same way as processing resources do in GATE. The Apache UIMA SDK provides support
for building analysis components in Java and C++ and running them either locally on one
machine, or deploying them as services that can be accessed remotely. The SDK is available
for download from http://incubator.apache.org/uima/.
Clearly, it would be useful to be able to include UIMA components in GATE applications
and vice-versa, letting GATE users take advantage of UIMA’s ﬂexible deployment options
and UIMA users access JAPE and the many useful plugins already available in GATE. This
chapter describes the interoperability layer provided as part of GATE to support this. The
UIMA-GATE interoperability layer is based on Apache UIMA 2.2.2. GATE 5.0 and earlier
included an implementation based on version 1.2.3 of the pre-Apache IBM UIMA SDK.
The rest of this chapter assumes that you have at least a basic understanding of core UIMA
concepts, such as type systems, primitive and aggregate analysis engines (AEs), feature structures, the format of AE XML descriptors, etc. It will probably be helpful to refer to the
relevant sections of the UIMA SDK User’s Guide and Reference (supplied with the SDK)
alongside this document.
There are two main parts to the interoperability layer:
1. A wrapper to allow a UIMA Analysis Engine (AE), whether primitive or aggregate, to
be used within GATE as a Processing Resource (PR).
2. A wrapper to allow a GATE processing pipeline (speciﬁcally a CorpusController) to
be used within UIMA as an AE.
429

430

Combining GATE and UIMA

The two components operate in very similar ways. Given a document in the source form
(either a GATE Document or a UIMA CAS), a document in the target form is created with a
copy of the source document’s text. Some of the annotations from the source are transferred
to the target, according to a mapping deﬁned by the user, and the target component is then
run. Finally, some of the annotations on the updated target document are then transferred
back to the source, according to the user-deﬁned mapping.
The rest of this document describes this process in more detail. Section 19.1 describes the
GATE AE wrapper, and Section 19.2 describes the UIMA CorpusController wrapper.

19.1

Embedding a UIMA AE in GATE

Embedding a UIMA analysis engine in a GATE application is a two step process. First, you
must construct a mapping descriptor XML ﬁle to deﬁne how to map annotations between
the UIMA CAS and the GATE Document. This mapping ﬁle, along with the analysis engine
descriptor, is used to instantiate an AnalysisEnginePR which calls the analysis engine on an
appropriately initialized CAS. Examples of all the XML ﬁles discussed in this section are
available in examples/conf under the UIMA plugin directory.

19.1.1

Mapping File Format

Figure 19.1 shows the structure of a mapping descriptor. The inputs section deﬁnes how
annotations on the GATE document are transferred to the UIMA CAS. The outputs section deﬁnes how annotations which have been added, updated and removed by the AE are
transferred back to the GATE document.
Input Deﬁnitions
Each input deﬁnition takes the following form:
<uimaAnnotation type="uima.Type" gateType="GATEType" indexed="true|false">
<feature name="..." kind="string|int|float|fs">
<!-- element defining the feature value goes here -->
</feature>
...
</uimaAnnotation>

When a document is processed, this will create one UIMA annotation of type uima.Type in
the CAS for each GATE annotation of type GATEType in the input annotation set, covering
the same oﬀsets in the text. If indexed is true, GATE will keep a record of which GATE

Combining GATE and UIMA

<uimaGateMapping>
<inputs>
<uimaAnnotation type="..." gateType="..." indexed="true|false">
<feature name="..." kind="string|int|float|fs">
<!-- element defining the feature value goes here -->
</feature>
...
</uimaAnnotation>
</inputs>
<outputs>
<added>
<gateAnnotation type="..." uimaType="...">
<feature name="...">
<!-- element defining the feature value goes here -->
</feature>
...
</gateAnnotation>
</added>
<updated>
...
</updated>
<removed>
...
</removed>
</outputs>
</uimaGateMapping>

Figure 19.1: Structure of a mapping descriptor for an AE in GATE

431

432

Combining GATE and UIMA

annotation gave rise to which UIMA annotation. If you wish to be able to track updates
to this annotation’s features and transfer the updated values back into GATE, you must
specify indexed="true". The indexed attribute defaults to false if omitted.
Each contained feature element will cause the corresponding feature to be set on the generated annotation. UIMA features can be string, integer or ﬂoat valued, or can be a reference
to another feature structure, and this must be speciﬁed in the kind attribute. The feature’s
value is speciﬁed using a nested element, but exactly how this value is handled is determined
by the kind.
There are various options for setting feature values:
<string value="fixed string" /> The simplest case - a ﬁxed Java String.
<docFeatureValue name="featureName" /> The value of the given named feature of
the current GATE document.
<gateAnnotFeatureValue name="featureName" /> The value of a given feature on
the current GATE annotation (i.e. the one on which the oﬀsets of the UIMA annotation
are based).
<featureStructure type="uima.fs.Type">...</featureStructure> A feature structure of the given type. The featureStructure element can itself contain feature
elements recursively.
The value is assigned to the feature according to the feature’s kind:
string The value object’s toString() method is called, and the resulting String is set as
the string value of the feature.
int If the value object is a subclass of java.lang.Number, its intValue() method is
called, and the result is set as the integer value of the feature. If the value object is not a Number, it is toString()ed, and the resulting String is parsed using
Integer.parseInt(). If this succeeds, the integer result is used, if it fails the feature
is set to zero.
ﬂoat As for int, except that Numbers are converted by calling floatValue(), and nonNumbers are parsed using Float.parseFloat().
fs The value object is assumed to be a FeatureStructure, and is used as-is.
ClassCastException will result if the value object is not a FeatureStructure.

A

In particular, <featureStructure> value elements should only be used with features of kind
fs. While nothing will stop you using them with string features, the result will probably
not be what you expected.

Combining GATE and UIMA

433

Output Deﬁnitions
The output deﬁnitions take a similar form. There are three groups:
added Annotations which have been added by the AE, and for which corresponding new
annotations are to be created in the GATE document.
updated Annotations that were created by an input deﬁnition (with indexed="true")
whose feature values have been modiﬁed by the AE, and these values are to be transferred back to the original GATE annotations.
removed Annotations that were created by an input deﬁnition (with indexed="true")
which have been removed from the CAS1 and whose source annotations are to be
removed from the GATE document.
The deﬁnition elements for these three types all take the same form:
<gateAnnotation type="GATEType" uimaType="uima.Type">
<feature name="featureName">
<!-- element defining the feature value goes here -->
</feature>
...
</gateAnnotation>

For added annotations, this has the mirror-image eﬀect to the input deﬁnition – for each
UIMA annotation of the given type, create a GATE annotation at the same oﬀsets and set
its feature values as speciﬁed by feature elements. For a gateAnnotation the feature
elements do not have a kind, as features in GATE can have arbitrary Objects as values.
The possible feature value elements for a gateAnnotation are:
<string value="fixed string" /> A ﬁxed string, as before.
<uimaFSFeatureValue name="uima.Type:FeatureName" kind="string|int|float" />
The value of the given feature of the current UIMA annotation. The feature name must
be speciﬁed in fully-qualiﬁed form, including the type on which it is deﬁned. The kind
is used in a similar way as in input deﬁnitions:
string The Java String object returned as the string value of the feature is used.
int An Integer object is created from the integer value of the feature.
ﬂoat A Float object is created from the ﬂoat value of the feature.
1
Strictly speaking, removed from the annotation index, as feature structures cannot be removed from the
CAS entirely.

434

Combining GATE and UIMA

fs The UIMA FeatureStructure object is returned. Since FeatureStructure objects are not guaranteed to be valid once the CAS has been cleared, a downstream GATE component must extract the relevant information from the feature
structure before the next document is processed. You have been warned.
Feature names in uimaFSFeatureValue must be qualiﬁed with their type name, as the feature
may have been deﬁned on a supertype of the feature’s own type, rather than the type itself.
For example, consider the following:
<gateAnnotation type="Entity" uimaType="com.example.Entity">
<feature name="type">
<uimaFSFeatureValue name="com.example.Entity:Type" kind="string" />
</feature>
<feature name="startOffset">
<uimaFSFeatureValue name="uima.tcas.Annotation:begin" kind="int" />
</feature>
</gateAnnotation>

For updated annotations, there must have been an input deﬁnition with indexed="true"
with the same GATE and UIMA types. In this case, for each GATE annotation of the
appropriate type, the UIMA annotation that was created from it is found in the CAS. The
feature deﬁnitions are then used as in the added case, but here, the feature values are set on
the original GATE annotation, rather than on a newly created annotation.
For removed annotations, the feature deﬁnitions are ignored, and the annotation is removed
from GATE if the UIMA annotation which it gave rise to has been removed from the UIMA
annotation index.
A Complete Example
Figure 19.2 shows a complete example mapping descriptor for a simple UIMA AE that takes
tokens as input and adds a feature to each token giving the number of lower case letters in
the token’s string.2 In this case the UIMA feature that holds the number of lower case letters
is called LowerCaseLetters, but the GATE feature is called numLower. This demonstrates
that the feature names do not need to agree, so long as a mapping between them can be
deﬁned.

19.1.2

The UIMA Component Descriptor

As well as the mapping ﬁle, you must provide the UIMA component descriptor that deﬁnes
how to access the AE that is to be called. This could be a primitive or aggregate analysis
2
The Java code implementing this AE is in the examples directory of the UIMA plugin. The AE descriptor
and mapping ﬁle are in examples/conf.

Combining GATE and UIMA

435

<uimaGateMapping>
<inputs>
<uimaAnnotation type="gate.uima.cas.Token" gateType="Token" indexed="true">
<feature name="String" kind="string">
<gateAnnotFeatureValue name="string" />
</feature>
</uimaAnnotation>
</inputs>
<outputs>
<updated>
<gateAnnotation type="Token" uimaType="gate.uima.cas.Token">
<feature name="numLower">
<uimaFSFeatureValue name="gate.uima.cas.Token:LowerCaseLetters"
kind="int" />
</feature>
</gateAnnotation>
</updated>
</outputs>
</uimaGateMapping>

Figure 19.2: An example mapping descriptor

engine descriptor, or a URI speciﬁer giving the location of a remote Vinci or SOAP service.
It is up to the developer to ensure that the types and features used in the mapping descriptor
are compatible with the type system and capabilities of the AE, or a runtime error is likely
to occur.

19.1.3

Using the AnalysisEnginePR

To use a UIMA AE in GATE Developer, load the UIMA plugin and create a ‘UIMA Analysis
Engine’ processing resource. If using the GATE Embedded, rather than GATE Developer,
the class name is gate.uima.AnalysisEnginePR. The processing resource expects two parameters:
analysisEngineDescriptor The URL of the UIMA analysis engine descriptor (or URI
speciﬁer, for a remote AE service). This must be a file: URL, as UIMA needs a ﬁle
path against which to resolve imports.
mappingDescriptor The URL of the mapping descriptor ﬁle. This may be any kind of
URL (file:, http:, Class.getResource(), ServletContext.getResource(), etc.)
Any errors processing either of the descriptor ﬁles will cause an exception to be thrown.
Once instantiated, you can add the PR to a pipeline in the usual way. AnalysisEnginePR
implements LanguageAnalyser, so can be used in any of the standard GATE pipeline types.

436

Combining GATE and UIMA

The PR takes the following runtime parameter (in addition to the document parameter which
is set automatically by a CorpusController):
annotationSetName The annotation set to process. Any input mappings take annotations
from this set, and any output mappings place their new annotations in this set (added
outputs) or update the input annotations in this set (updated or removed). If not
speciﬁed, the default (unnamed) annotation set is used.
The Annotator implementation must be available for GATE to load. For an annotator
written in Java, this means that the JAR ﬁle containing the annotator class (and any other
classes it depends on) must be present in the GATE classloader. The easiest way to achieve
this is to put the JAR ﬁle or ﬁles in a new directory, and create a creole.xml ﬁle in the
same directory to reference the JARs:
<CREOLE-DIRECTORY>
<JAR>my-annotator.jar</JAR>
<JAR>classes-it-uses.jar</JAR>
</CREOLE-DIRECTORY>

This directory should then be loaded in GATE as a CREOLE plugin. Note that, due to the
complex mechanics of classloaders in Java, putting your JARs in GATE’s lib directory will
not work.
For annotators written in C++ you need to ensure that the C++ enabler libraries (available
separately from http://incubator.apache.org/uima/) and the shared library containing your
annotator are in a directory which is on the PATH (Windows) or LD LIBRARY PATH (Linux)
when GATE is run.

19.2

Embedding a GATE CorpusController in UIMA

The process of embedding a GATE controller in a UIMA application is more or less the
mirror image of the process detailed in the previous section. Again, the developer must
supply a mapping descriptor deﬁning how to map between UIMA and GATE annotations,
and pass this, plus the GATE controller deﬁnition, to an AE which performs the translation
and calls the GATE controller.

19.2.1

Mapping File Format

The mapping descriptor format is virtually identical to that described in Section 19.1.1,
except that the input deﬁnitions are <gateAnnotation> elements and the output deﬁnitions
are <uimaAnnotation> elements. The input and output deﬁnition elements support an

Combining GATE and UIMA

437

extra attribute, annotationSetName, which allows inputs to be taken from, and outputs
to be placed in, diﬀerent annotation sets. For example, the following hypothetical example
maps com.example.Person annotations into the default set and com.example.html.Anchor
annotations to ‘a’ tags in the ‘Original markups’ set.

<inputs>
<gateAnnotation type="Person" uimaType="com.example.Person">
<feature name="kind">
<uimaFSFeatureValue name="com.example.Person:Kind" kind="string"/>
</feature>
</gateAnnotation>
<gateAnnotation type="a" annotationSetName="Original markups"
uimaType="com.example.html.Anchor">
<feature name="href">
<uimaFSFeatureValue name="com.example.html.Anchor:hRef" kind="string" />
</feature>
</gateAnnotation>
</inputs>

Figure 19.3 shows a mapping descriptor for an application that takes tokens and sentences
produced by some UIMA component and runs the GATE part of speech tagger to tag them
with Penn TreeBank POS tags.3 In the example, no features are copied from the UIMA
tokens, but they are still indexed="true" as the POS feature must be copied back from
GATE.

19.2.2

The GATE Application Deﬁnition

The GATE application to embed is given as a standard ‘.gapp ﬁle’, as produced by saving the
state of an application in the GATE GUI. The .gapp ﬁle encodes the information necessary
to load the correct plugins and create the various CREOLE components that make up the
application. The .gapp ﬁle must be fully speciﬁed and able to be executed with no user
intervention other than pressing the Go button. In particular, all runtime parameters must
be set to their correct values before saving the application state. Also, since paths to things
like CREOLE plugin directories, resource ﬁles, etc. are stored relative to the .gapp ﬁle’s
location, you must not move the .gapp ﬁle to a diﬀerent directory unless you can keep
all the CREOLE plugins it depends on at the same relative locations. The ‘Export for
GATECloud.net’ option (section 3.8.4) may help you here.
3
The .gapp ﬁle implementing this example is in the test/conf directory under the UIMA plugin, along
with the mapping ﬁle and the AE descriptor that will run it.

438

Combining GATE and UIMA

<uimaGateMapping>
<inputs>
<gateAnnotation type="Token"
uimaType="com.ibm.uima.examples.tokenizer.Token"
indexed="true" />
<gateAnnotation type="Sentence"
uimaType="com.ibm.uima.examples.tokenizer.Sentence" />
</inputs>
<outputs>
<updated>
<uimaAnnotation type="com.ibm.uima.examples.tokenizer.Token"
gateType="Token">
<feature name="POS" kind="string">
<gateAnnotFeatureValue name="category" />
</feature>
</uimaAnnotation>
</updated>
</outputs>
</uimaGateMapping>

Figure 19.3: An example mapping descriptor for the GATE POS tagger

19.2.3

Conﬁguring the GATEApplicationAnnotator

GATEApplicationAnnotator is the UIMA annotator that handles mapping the CAS into a
GATE document and back again and calling the GATE controller. There is a template AE
descriptor XML ﬁle for the annotator provided in the conf directory. Most of the template
ﬁle can be used unchanged, but you will need to modify the type system deﬁnition and
input/output capabilities to match the types and features used in your mapping descriptor.
If the mapping descriptor references a type or feature that is not deﬁned in the type system,
a runtime error will occur.
The annotator requires two external resources:
GateApplication The .gapp ﬁle containing the saved application state.
MappingDescriptor The mapping descriptor XML ﬁle.
These must be bound to suitable URLs, either by editing the resourceManagerConfiguration
section of the primitive descriptor, or by supplying the binding in an aggregate descriptor
that includes the GATEApplicationAnnotator as one of its delegates.
In addition, you may need to set the following Java system properties:
uima.gate.conﬁgdir The path to the GATE conﬁg directory.

This defaults to

Combining GATE and UIMA

439

gate-config in the same directory as uima-gate.jar.
uima.gate.siteconﬁg The location of the sitewide gate.xml conﬁguration ﬁle. This defaults to gate.uima.configdir /site-gate.xml.
uima.gate.userconﬁg The location of the user-speciﬁc gate.xml conﬁguration ﬁle. This
defaults to gate.uima.configdir /user-gate.xml.
The default conﬁg ﬁles are deliberately simpliﬁed from the standard versions supplied with
GATE, in particular they do not load any plugins automatically (not even ANNIE). All
the plugins used by your application are speciﬁed in the .gapp ﬁle, and will be loaded
when the application is loaded, so it is best to avoid loading any others from gate.xml, to
avoid problems such as two diﬀerent versions of the same plugin being loaded from diﬀerent
locations.
Classpath Notes
In addition to the usual UIMA library JAR ﬁles, GATEApplicationAnnotator requires a
number of JAR ﬁles from the GATE distribution in order to function. In the ﬁrst instance, you should include gate.jar from GATE’s bin directory, and also all the JAR ﬁles
from GATE’s lib directory on the classpath. If you use the supplied Ant build ﬁle, ant
documentanalyser will run the document analyser with this classpath. Depending on exactly which GATE plugins your application uses, you may be able to exclude some of the
lib JAR ﬁles (for example, you will not need Weka if you do not use the machine learning
plugin), but it is safest to start with them all. GATE will load plugin JAR ﬁles through its
own classloader, so these do not need to be on the classpath.

440

Combining GATE and UIMA

Chapter 20
More (CREOLE) Plugins
For the previous reader was none other than myself. I had already read this book
long ago.
The old sickness has me in its grip again: amnesia in litteris, the total loss of
literary memory. I am overcome by a wave of resignation at the vanity of all
striving for knowledge, all striving of any kind. Why read at all? Why read
this book a second time, since I know that very soon not even a shadow of
a recollection will remain of it? Why do anything at all, when all things fall
apart? Why live, when one must die? And I clap the lovely book shut, stand
up, and slink back, vanquished, demolished, to place it again among the mass of
anonymous and forgotten volumes lined up on the shelf.
...
But perhaps - I think, to console myself - perhaps reading (like life) is not a
matter of being shunted on to some track or abruptly oﬀ it. Maybe reading is
an act by which consciousness is changed in such an imperceptible manner that
the reader is not even aware of it. The reader suﬀering from amnesia in litteris
is most deﬁnitely changed by his reading, but without noticing it, because as
he reads, those critical faculties of his brain that could tell him that change
is occurring are changing as well. And for one who is himself a writer, the
sickness may conceivably be a blessing, indeed a necessary precondition, since
it protects him against that crippling awe which every great work of literature
creates, and because it allows him to sustain a wholly uncomplicated relationship
to plagiarism, without which nothing original can be created.
Three Stories and a Reﬂection, Patrick Suskind, 1995 (pp. 82, 86).

This chapter describes additional CREOLE resources which do not form part of ANNIE,
and have not been covered in previous chapters.
441

442

20.1

More (CREOLE) Plugins

Verb Group Chunker

The rule-based verb chunker is based on a number of grammars of English [Cobuild 99,
Azar 89]. We have developed 68 rules for the identiﬁcation of non recursive verb groups.
The rules cover ﬁnite (’is investigating’), non-ﬁnite (’to investigate’), participles (’investigated’), and special verb constructs (’is going to investigate’). All the forms may include
adverbials and negatives. The rules have been implemented in JAPE. The ﬁnite state analyser produces an annotation of type ‘VG’ with features and values that encode syntactic
information (‘type’, ‘tense’, ‘voice’, ‘neg’, etc.). The rules use the output of the POS tagger
as well as information about the identity of the tokens (e.g. the token ‘might’ is used to
identify modals).
The grammar for verb group identiﬁcation can be loaded as a Jape grammar into the GATE
architecture and can be used in any application: the module is domain independent.

20.2

Noun Phrase Chunker

The NP Chunker application is a Java implementation of the Ramshaw and Marcus BaseNP
chunker (in fact the ﬁles in the resources directory are taken straight from their original
distribution) which attempts to insert brackets marking noun phrases in text which have
been marked with POS tags in the same format as the output of Eric Brill’s transformational
tagger. The output from this version should be identical to the output of the original
C++/Perl version released by Ramshaw and Marcus.
For more information about baseNP structures and the use of transformation-based learning
to derive them, see [Ramshaw & Marcus 95].

20.2.1

Diﬀerences from the Original

The major diﬀerence is the assumption is made that if a POS tag is not in the mapping
ﬁle then it is tagged as ‘I’. The original version simply failed if an unknown POS tag was
encountered. When using the GATE wrapper the chunk tag can be changed from ‘I’ to any
other legal tag (B or O) by setting the unknownTag parameter.

20.2.2

Using the Chunker

The Chunker requires the Creole plugin ‘Parser NP Chunking’ to be loaded. The two loadtime parameters are simply urls pointing at the POS tag dictionary and the rules ﬁle, which
should be set automatically. There are ﬁve runtime parameters which should be set prior to
executing the chunker.

More (CREOLE) Plugins

443

annotationName: name of the annotation the chunker should create to identify noun
phrases in the text.
inputASName: The chunker requires certain types of annotations (e.g. Tokens with
part of speech tags) for identifying noun chunks. This parameter tells the chunker
which annotation set to use to obtain such annotations from.
outputASName: This is where the results (i.e. new noun chunk annotations will be
stored).
posFeature: Name of the feature that holds POS tag information. ’
unknownTag: it works as speciﬁed in the previous section.
The chunker requires the following PRs to have been run ﬁrst: tokeniser, sentence splitter,
POS tagger.

20.3

TaggerFramework

The Tagger Framework is an extension of work originally developed in order to provide support for the TreeTagger plugin within GATE. Rather than focusing on providing support for
a single external tagger this plugin provides a generic wrapper that can easily be customised
(no Java code is required) to incorporate many diﬀerent taggers within GATE.
The plugin currently provides example applications (see plugins/Tagger Framework/resources)
for the following taggers: GENIA (a biomedical tagger), Hunpos (providing support for English and Hungarian), TreeTagger (supporting German, French, Spanish and Italian as well
as English), and the Stanford Tagger (supporting English, German and Arabic).
The basic idea behind this plugin is to allow the use of many external taggers. Providing
such a generic wrapper requires a few assumptions. Firstly we assume that the external
tagger will read from a ﬁle and that the contents of this ﬁle will be one annotation per line
(i.e. one token or sentence per line). Secondly we assume that the tagger will write it’s
response to stdout and that it will also be based on one annotation per line – although there
is no assumption that the input and output annotation types are the same.
An important issue with most external taggers is tokenisation: Generally, when using a
native GATE tagger in a pipeline, “Token” annotations are ﬁrst generated by a tokeniser,
and then processed by a POS tagger. Most external taggers, on the other hand, have built-in
code to perform their own tokenisation. In this case, there are generally two options: (1) use
the tokens generated by the external tagger and import them back into GATE (typically
into a “Token” annotation type). Or (2), if the tagger accepts pre-tokenised text, the Tagger
Framework can be conﬁgured to pass the annotations as generated by a GATE tokeniser
to the external tagger. For details on this, please refer to the ‘updateAnnotations’ runtime

444

More (CREOLE) Plugins

parameter described below. However, if the tokenisation strategies are signiﬁcantly diﬀerent,
this may lead to a degradation of the tagger’s performance.

Initialization Parameters
– preProcessURL: The URL of a JAPE grammar that should be run over each
document before running the tagger.
– postProcessURL: The URL of a JAPE grammar that should be run over each
document after running the tagger. This can be used, for example, to add chunk
annotations using IOB tags output by the tagger and stored as features on Token
annotations.
Runtime Parameters
– debug: if set to true then a whole heap of useful information will be printed to
the messages tab as the tagger runs. Defaults to false.
– encoding: this must be set to the encoding that the tagger expects the input/output ﬁles to use. If this is incorrectly set is highly likely that either the tagger will
fail or the results will be meaningless. Defaults to ISO-8859-1 as this seems to
be the most commonly required encoding.
– failOnUnmappableCharacter: What to do if a character is encountered in the
document which cannot be represented in the selected encoding. If the parameter is true (the default), unmappable characters cause the wrapper to throw
an exception and fail. If set to false, unmappable characters are replaced by
question marks when the document is passed to the tagger. This is useful if your
documents are largely OK but contain the odd character from outside the Latin-1
range.
– inputTemplate: template string describing how to build the line of input for the
tagger corresponding to a single annotation. The template contains placeholders
of the form ${feature} which will be replaced by the value of the corresponding
feature from the annotation. The default template is ${string}, which simply
passes the string feature of each annotation to the tagger. Typical variants would
be ${string}\t${category} for an entity tagger that requires the string and the
part of speech tag for each token, separated by a tab1 . If a particular annotation
does not have one of the speciﬁed features, the corresponding slot in the template
will be left blank (i.e. replaced by an empty string). It is only an error if a
particular annotation contains none of the features speciﬁed by the template.
– regex: this should be a Java regular expression that matches a single line in the
output from the tagger. Capturing groups should be used to deﬁne the sections
of the expression which match the useful output.
1

Java string escape sequences such as \t will be decoded before the template is expanded.

More (CREOLE) Plugins

445

– featureMapping: this is a mapping from feature name to capturing group in the
regular expression. Each feature will be added to the output annotations with a
value equal to the speciﬁed capturing group. For example, the TreeTagger uses a
regular expression (.+)\t(.+)\t(.+) to capture the three column output. This
is then combined with the feature mapping {string=1, category=2, lemma=3}
to add the appropriate feature/values to the output annotations.
– inputASName: the name of the annotation set which should be used for input.
If not speciﬁed the default (i.e. un-named) annotation set will be used.
– inputAnnotationType: the name of the annotation used as input to the tagger.
This will usually be Token. Note that the input annotations must contain a
string feature which will be used as input to the tagger. Tokens usually have
this feature but if, for example, you wish to use Sentence as the input annotation
then you will need to add the string feature. JAPE grammars for doing this are
provided in plugins/Tagger Framework/resources.
– outputASName: the name of the annotation set which should be used for
output. If not speciﬁed the default (i.e. un-named) annotation set will be used.
– outputAnnotationType: the name of the annotation to be provided as output.
This is usually Token.
– taggerBinary: a URL indicating the location of the external tagger. This is usually a shell script which may perform extra processing before executing the tagger. The plugins/Tagger Framework/resources directory contains example scripts
(where needed) for the supported taggers. These scripts may need editing (for
example, to set the installation directory of the tagger) before they can be used.
– taggerDir: the directory from which the tagger must be executed. This can be
left unspeciﬁed.
– taggerFlags: an ordered set of ﬂags that should be passed to the tagger as
command line options
– updateAnnotations: If set to true then the plugin will attempt to update existing output annotations. This can fail if the output from the tagger and the
existing annotations are created diﬀerently (i.e. the tagger does its own tokenization). Setting this option to false will make the plugin create new output annotations, removing any existing ones, to prevent the two sets getting out of sync.
This is also useful when the tagger is domain speciﬁc and may do a better job
than GATE. For example, the GENIA tagger is better at tokenising biomedical
text than the ANNIE tokeniser. Defaults to true.

By default the GenericTagger PR simply tries to execute the taggerBinary using the normal
Java Runtime.exec() mechanism. This works ﬁne on Unix-style platforms such as Linux or
Mac OS X, but on Windows it will only work if the taggerBinary is a .exe ﬁle. Attempting
to invoke other types of program fails on Windows with a rather cryptic “error=193”.

446

More (CREOLE) Plugins

To support other types of tagger programs such as shell scripts or Perl scripts, the GenericTagger PR supports a Java system property shell.path. If this property is set then
instead of invoking the taggerBinary directly the PR will invoke the program speciﬁed by
shell.path and pass the tagger binary as the ﬁrst command-line parameter.
If the tagger program is a shell script then you will need to install the appropriate interpreter, such as sh.exe from the cygwin tools, and set the shell.path system property to
point to sh.exe. For GATE Developer you can do this by adding the following line to
build.properties (see Section 2.3, and note the extra backslash before each backslash and
colon in the path):
run.shell.path: C\:\\cygwin\\bin\\sh.exe

Similarly, for Perl or Python scripts you should install a suitable interpreter and set
shell.path to point to that.
You can also run taggers that are invoked using a Windows batch ﬁle (.bat). To use
a batch ﬁle you do not need to use the shell.path system property, but instead set
the taggerBinary runtime parameter to point to C:\WINDOWS\system32\cmd.exe and
set the ﬁrst two taggerFlags entries to “/c” and the Windows-style path to the tagger batch ﬁle (e.g. C:\MyTagger\runTagger.bat). This will cause the PR to run
cmd.exe /c runTagger.bat which is the way to run batch ﬁles from Java.

20.3.1

TreeTagger - Multilingual POS Tagger

The TreeTagger is a language-independent part-of-speech tagger, which supports a number
of diﬀerent languages through parameter ﬁles, including English, French, German, Spanish,
Italian and Bulgarian. Originally made available in GATE through a dedicated wrapper,
it is now fully supported through the Tagger Framework. You must install the TreeTagger
separately from
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html
Avoid installing it in a directory that contains spaces in its path.

Tokenisation and Command Scripts. When running the TreeTagger through the Tagger Framework, you can choose between passing Tokens generated within GATE to the
TreeTagger for POS tagging or let the TreeTagger perform tokenisation as well, importing
the generated Tokens into GATE annotations. If you need to pass the Tokens generated
by GATE to the TreeTagger, it is important that you create your own command scripts to
skip the tokenisation step done by default in the TreeTagger command scripts (the ones in
the TreeTagger’s cmd directory). A few example scripts for passing GATE Tokens to the

More (CREOLE) Plugins

447

TreeTagger are available under plugins/Tagger_Framework/resources/TreeTagger, for
example, tree-tagger-german-gate runs the German parameter ﬁle with existing “Token”
annotations.
Note that you must set the paths in these command ﬁles to point to the location where you
installed the TreeTagger:
BIN=/usr/local/durmtools/TreeTagger/bin
CMD=/usr/local/durmtools/TreeTagger/cmd
LIB=/usr/local/durmtools/TreeTagger/lib

The Tagger Framework will run the TreeTagger on any platform that supports the TreeTagger tool, including Linux, Mac OS X and Windows, but the GATE-speciﬁc scripts
require a POSIX-style Bourne shell with the gawk, tr and grep commands, plus Perl
for the Spanish tagger. For Windows this means that you will need to install the appropriate parts of the Cygwin environment from http://www.cygwin.com and set the
system property treetagger.sh.path to contain the path to your sh.exe (typically
C:\cygwin\bin\sh.exe).
POS Tags. For English the POS tagset is a slightly modiﬁed version of the Penn Treebank
tagset, where the second letter of the tags for verbs distinguishes between ‘be’ verbs (B),
‘have’ verbs (H) and other verbs (V).

Figure 20.1: A French document processed by the TreeTagger through the Tagger Framework
The tagsets for other languages can be found on the TreeTagger web site. Figure 20.1 shows
a screenshot of a French document processed with the TreeTagger.

448

More (CREOLE) Plugins

20.4

Chemistry Tagger

This GATE module is designed to tag a number of chemistry items in running text. Currently
the tagger tags compound formulas (e.g. SO2, H2O, H2SO4 ...) ions (e.g. Fe3+, Cl-) and
element names and symbols (e.g. Sodium and Na). Limited support for compound names
is also provided (e.g. sulphur dioxide) but only when followed by a compound formula (in
parenthesis or commas).

20.4.1

Using the Tagger

The Tagger requires the Creole plugin ‘Tagger Chemistry’ to be loaded. It requires the
following PRs to have been run ﬁrst: tokeniser and sentence splitter (the annotation set containing the Tokens and Sentences can be set using the annotationSetName runtime parameter). There are four init parameters giving the locations of the two gazetteer list deﬁnitions,
the element mapping ﬁle and the JAPE grammar used by the tagger (in previous versions
of the tagger these ﬁles were ﬁxed and loaded from inside the ChemTagger.jar ﬁle). Unless
you know what you are doing you should accept the default values.
The annotations added to documents are ‘ChemicalCompound’, ‘ChemicalIon’ and ‘ChemicalElement’ (currently they are always placed in the default annotation set). By default
‘ChemicalElement’ annotations are removed if they make up part of a larger compound or
ion annotation. This behaviour can be changed by setting the removeElements parameter
to false so that all recognised chemical elements are annotated.

20.5

Annotating Numbers

The Tagger Numbers creole repository contains a number of processing resources which are
designed to annotate numbers appearing within documents. As well as annotating a given
span as being a number the PRs also determine the exact numeric value of the number and
add this as a feature of the annotation. This makes the annotations created by these PRs
ideal for building more complex annotations such as measurements or monetary units.
All the PRs in this plugin produce Number annotations with the following standard features

type: this describes the types of tokens that make up the number, e.g. roman, words,
numbers
value: this is the actual value (stored as a Double) of the number that has been
annotated

More (CREOLE) Plugins

449

String
Value
3ˆ2
9
101
101
3,000
3000
3.3e3
3300
1/4
0.25
9ˆ1/2
3
4x10ˆ3
4000
5.5*4ˆ5
5632
thirty one
31
three hundred
300
four thousand one hundred and two 4102
3 million
3000000
f¨nfundzwanzig
u
25
Table 20.1: Numbers Tagger Examples

Each PR might also create other features which are described, along with the PR, in the
following sections.

20.5.1

Numbers in Words and Numbers

The “Numbers Tagger” annotates numbers made up from numbers or numeric words. If that
wasn’t really clear enough then Table 20.1 shows numerous ways of representing numbers
that can all be annotated by this tagger (depending upon the conﬁguration ﬁles used).
To create an instance of the PR you will need to conﬁgure the following initialization time
parameters (sensible defaults are provided):
conﬁgURL: the URL of the conﬁguration ﬁle you wish to use (see below for details),
defaults to resources/languages/all.xml which currently provides support for English, French, German, Spanish and a variety of number related Unicode symbols.
If you want a single language the you can specify the appropriately named ﬁle, i.e.
resources/languages/english.xml.
encoding: the encoding of the conﬁguration ﬁle, defaults to UTF-8
postProcessURL: the URL of the JAPE grammar used for post-processing – don’t
change this unless you know what you are doing!
The conﬁguration ﬁle is an XML document that speciﬁes the words that can be used as
numbers or multipliers (such as hundred, thousand, ...) and conjunctions that can then be
used to combine sequences of numbers together. An example conﬁguration ﬁle can be seen in

450

More (CREOLE) Plugins

<config>
<description>Basic Example</description>
<imports>
<url encoding="UTF-8">symbols.xml</url>
</imports>
<words>
<word value="0">zero</word>
<word value="1">one</word>
<word value="2">two</word>
<word value="3">three</word>
<word value="4">four</word>
<word value="5">five</word>
<word value="6">six</word>
<word value="7">seven</word>
<word value="8">eight</word>
<word value="9">nine</word>
<word value="10">ten</word>
</words>
<multipliers>
<word value="2">hundred</word>
<word value="2">hundreds</word>
<word value="3">thousand</word>
<word value="3">thousands</word>
</multipliers>
<conjunctions>
<word whole="true">and</word>
</conjunctions>
<decimalSymbol>.</decimalSymbol>
<digitGroupingSymbol>,</digitGroupingSymbol>
</config>

Figure 20.2: Example Numbers Tagger Conﬁg File

More (CREOLE) Plugins

451

Figure 20.2. This conﬁguration ﬁle speciﬁes a handful of words and multipliers and a single
conjunction. It also imports another conﬁguration ﬁle (in the same format) deﬁning Unicode
symbols. Most of the conﬁguration ﬁle is self-explanatory, however, the conjunctions needs
further clariﬁcation. In English conjunctions are whole words, that is they require white
space on either side of them, e.g. three hundred and one. In other languages, however,
numbers can be joined into a single word using a conjunction. For example, in German the
conjunction ‘und’ can appear in a number without white space, e.g. twenty one is written as
einundzwanzig. If the conjunction is a whole word, as in English, then the whole attribute
should be set to true, but for conjunctions like ‘und’ the attribute should be set to false.
In order to support diﬀerent number formats the symbols used to group numbers and to
represent the decimal point can also be conﬁgured. These are optional elements in the XML
conﬁguration ﬁle which if not supplied default to a comma for the digit group symbol and
a full stop for the decimal point. Whilst these are appropriate for many languages if you
wanted, for example, to parse documents written in Bulgarian you would want to specify
that the decimal symbol was a command and the grouping symbol was a space in order to
recognise numbers such as 1 000 000,303.
Once created an instance of the PR can then be conﬁgured using the following runtime
parameters:

allowWithinWords: digits can often occur within words (for example part numbers,
chemical equations etc.) where they should not be interpreted as numbers. If this
parameter is set to true then these instances will also be annotated as numbers (useful
for annotating money and measurements where spaces are often omitted), however,
the parameter defaults to false.
annotationSetName: the annotation set to use as both input and output for this
PR (due to the way this PR works the two sets have to be the same)
failOnMissingInputAnnotations: if the input annotations (Tokens and Sentences)
are missing should this PR fail or just not do anything, defaults to true to allow obvious
mistakes in pipeline conﬁguration to be captured at an early stage.
useHintsFromOriginalMarkups: often the original markups will provide hints that
may be useful for correctly interpreting numbers within documents (i.e. numeric powers
may be in <sup></sup> tags), if this parameter is set to true then these hints will
be used to help parse the numbers, defaults to true.

There are no extra annotation features which are speciﬁc to this numbers PR. The type
feature can take one of three values based upon the text that is annotated; words, numbers,
wordsAndNumbers.

452

More (CREOLE) Plugins

20.5.2

Roman Numerals

The “Roman Numerals Tagger” annotates Roman numerals appearing in the document. The
tagger is conﬁgured using the following runtime parameters:
allowLowerCase: traditionally Roman numerals must be all in uppercase. Setting
this parameter to false, however, allows Roman numerals written in lowercase to also
be annotated. This parameter defaults to false.
maxTailLength: Roman numerals are often used in labelling sections, ﬁgures, tables
etc. and in such cases can be followed by additional information. For example, Table
IVa, Appendix IIIb. These characters are referred to as the tail of the number and this
parameter constrains the number of characters that can appear. The default value is
0 in which case strings such as ’IVa’ would not be annotated in any way.
outputASName: the name of the annotation set in which the Number annotations
should be created.
As well as the normal Number annotation features (the type feature will always take the
value ‘roman’ Roman numeral annotations also include the following features:
tail: contains the tail, if any, that appears after the Roman numeral.

20.6

Annotating Measurements

Measurements mentioned in text documents can be diﬃcult to accurately deal with. As well
as the numerous ways in which numeric values can be written each type of measurement
(distance, area, time etc.) can be written using a variety of diﬀerent units. For example,
lengths can be measured in metres, centimetres, inches, yards, miles, furlongs and chains, to
mention just a few. Whilst measurements may all have diﬀerent units and values they can, in
theory be compared to one another. Extracting, normalizing and comparing measurements
can be a useful IE process in many diﬀerent domains. The Measurement Tagger (which can
be found in the Tagger Measurements plugin) attempts to provide such annotations for use
within IE applications.
The Measurements Tagger uses a parser based upon a modiﬁed version of the Java port
of the GNU Units package. This allows us to not only recognise and annotation spans of
text as being a measurement but also to normalize the units to allow for easy comparison of
diﬀerent measurement values.
This PR actually produces two diﬀerent annotations; Measurement and Ratio.

More (CREOLE) Plugins

453

Measurement annotations represent measurements that involve a unit, e.g. 3mph, three
pints, 4 m3 . Single measurements (i.e. those not referring to a range or interval) are referred
to as scalar measurements and have the following features:
type: for scalar measurements is always scalar
unit: the unit as recognised from the text. Note that this won’t necessarily be the
annotated text. For example, an annotation spanning the text “three miles” would
have a unit feature of “mile”.
value: a Double holding the value of the measurement (this usually comes directly
from the value feature of a Number annotation).
dimension: the measurements dimension, e.g. speed, volume, area, length, time etc.
normalizedUnit: to enable measurements of the same dimension but speciﬁed in
diﬀerent units to be compared the PR reduces all units to their base form. A base
form usually consists of a combination of SI units. For example, centimetre, mm, and
kilometre are all normalized to m (for metre).
normalizedValue: a Double instance holding the normalized value, such that the combination of the normalized value and normalized unit represent the same measurement
as the original value and unit.
normalized: a String representing the normalized measurement (usually a simple space
separated concatenation of the normalized value and unit).
Annotations which represent an interval or range have a slightly diﬀerent set of features.
The type feature is set to interval, there is no normalized or unit feature and the value
features (included the normalized version) are replaced by the following features, the values
of which are simply copied from the Measurement annotations which mark the boundaries
of the interval.
normalizedMinValue: a Double representing the minimum normalized number that
forms part of the interval.
normalizedMaxValue: a Double representing the minimum normalized number that
forms part of the interval.
Interval annotations do not replace scalar measurements and so multiple Measurement annotations may well overlap. They can of course be distinguished by the type feature.
As well as Measurement annotations the tagger also adds Ratio annotations to documents.
Ratio annotations cover measurements that do not have a unit. Percentages are the most

454

More (CREOLE) Plugins

common ratios to be found in documents, but also amounts such as “300 parts per million”
are annotated.
A Ratio annotation has the following features:
value: a Double holding the actual value of the ratio. For example, 20% will have a
value of 0.2.
numerator: the numerator of the ratio. For example, 20% will have a numerator of
20.
denominator: the denominator of the ratio. For example, 20% will have a denominator
of 100.
An instance of the measurements tagger is created using the following initialization parameters:
commonURL: this ﬁle deﬁnes units that are also common words and so should not
be annotated as a measurement unless they form a compound unit involving two or
more unit symbols. For example, C is the accepted abbreviation for coulomb but often
appears in documents as part of a reference to a table or ﬁgure, i.e. Figure 3C, which
should not be annotated as a measurement. The default ﬁle was hand tuned over a
large patent corpus but may need to be edited when used with diﬀerent domains.
encoding: the encoding to use when reading both of the conﬁguration ﬁles, defaults
to UTF-8.
japeURL: the URL of the JAPE grammar that drives the measurement parser. Unless
you really know what you are doing, the value of this parameter should not be changed.
locale: the locale to use when parsing the units deﬁnition ﬁle, defaults to en GB.
unitsURL: the URL of the main unit deﬁnition ﬁle to use. This should be in the
same format as accepted by the GNU Units package.
The PR does not attempt to recognise or annotate numbers, instead it relies on Number
annotations being present in the document. Whilst these annotations could be generated by
any resource executed prior to the measurements tagger, we recommend using the Numbers
Tagger described in Section 20.5. If you choose to produce Number annotations in some
other way note that they must have a value feature containing a Double representing the
value of the number. An example GATE application, showing how to conﬁgure and use the
two PRs together, is provided with the measurements plugin.
Once created an instance of the tagger can be conﬁgured using the following runtime parameters:

More (CREOLE) Plugins

455

consumeNumberAnnotations: if true then Number annotations used to ﬁnd measurements will be consumed and removed from the document, defaults to true.
failOnMissingInputAnnotations: if the input annotations (Tokens) are missing
should this PR fail or just not do anything, defaults to true to allow obvious mistakes
in pipeline conﬁguration to be captured at an early stage.
ignoredAnnotations: a list of annotation types in which a measurement can never
occur, defaults to a set containing Date and Money.
inputASName: the annotation set used as input to this PR.
outputASName: the annotation set to which new annotations will be added.
The ability to prevent the tagger from annotating measurements which occur within other
annotations is a very useful feature. The runtime parameters, however, only allow you to
specify the names of annotations and not to restrict on feature values or any other information
you may know about the documents being processed. Internally ignoring sections of a
document is controlled by adding CannotBeAMeasurement annotations that span the text
to be ignored. If you need greater control over the process than the ignoredAnnotations
parameter allows then you can create CannotBeAMeasurement annotations prior to running
the measurement tagger, for example a JAPE grammar placed before the tagger in the
pipeline. Note that these annotations will be deleted by the measurements tagger once
processing has completed.

20.7

Annotating and Normalizing Dates

Many information extraction tasks beneﬁt from or require the extraction of accurate date
information. While ANNIE (Chapter 6) does produce Date annotations no attempt is made
to normalize these dates, i.e. to ﬁrmly ﬁx all dates, even partial or relative ones, to a timeline
using a common date representation. The PR in the Tagger DateNormalizer plugin attempts
to ﬁll this gap by normalizing dates against the date of the document (see below for details
on how this is determined) in order to tie each Date annotation to a speciﬁc date. This
includes normalizing dates such as April 1st, today, yesterday, and next Tuesday, as well as
converting fully speciﬁed dates (ones in which the day, month and year are speciﬁed) into a
common format.
Diﬀerent cultures/countries have diﬀerent conventions for writing dates, as well as diﬀerent
languages using diﬀerent words for the days of the week and the months of the year. The
parser underlying this PR makes use of the locale-speciﬁc information when parsing documents. When initializing an instance of the Date Normalizer you can specify the locale to
use using ISO language and country codes along with Java speciﬁc variants (for details of
these codes see the Java Locale documentation). So for example, to specify Britsh English

456

More (CREOLE) Plugins

(which means the day usually comes before the month in a date) use en GB, or for American
English (where the month usually appears before the day in a date) specify en US. If you
need to override the locale on a document basis then you can do this by setting a document
feature called locale to a string encoded as above. If neither the initialization parameter or
document feature are present or do not represent a valid locale then the default locale of the
JVM running GATE will be used.
Once initialized and added to a pipeline the Date Normalizer has the following runtime
parameters that can be used to control it’s behaviour.
dateFormat: the format that dates should be normalized to. The format of this
parameter is the same as that use by the Java SimpleDateFormat whose documentation
describes the full range of possible formats. This defaults to dd/MM/yyyy. Note that
this parameter is only required if the numericOuput parameter is set to false.
failOnMissingInputAnnotations: if the input annotations (Tokens) are missing
should this PR fail or just not do anything, defaults to true to allow obvious mistakes
in pipeline conﬁguration to be captured at an early stage.
inputASName: the annotation set used as input to this PR.
normalizedDocumentFeature: if set then the normalized version of the document
date will be stored in a document feature with this name. This parameter defaults
to normalized-date although it can be left blank to suppress storage of the document
date.
numericOutput: if true then instead of formatting the normalized dates as String
features of the Date annotations they are instead converted into a numeric representation. Speciﬁcally the ﬁrst converted to the form yyyyMMdd and then cast to a Double.
This is useful as dates can then be sorted numerical (which is fast) into order. If false
then the formatting string in the dateFormat parameter is used instead to create a
string representation. This defaults to false.
outputASName: the annotation set to which new annotations will be added.
sourceOfDocumentDate: this parameter is a list of the names of annotations, annotation features (encoded as Annotation.feature), and document features to inspect
when trying to determine the date of the document. The PR works through the list
getting the text of feature or under the annotation (if no feature is speciﬁed) and then
parsing this to ﬁnd a fully speciﬁed date, i.e. one where the day, month and year are
all present. Once a date is found processing of the list stops and the date is used as the
date of the document. If you specify an annotation that can occur multiple times in a
document then they are sorted based on a numeric priority feature (which defaults to
0) or their order within the document. The idea here is that there are multiple ways
in which to determine the date of a document but most are domain speciﬁc and this
allows previous PRs in an application to determine the document date. This defaults

More (CREOLE) Plugins

457

to an empty list which is taken to assume that the document was written on the day
it is being processed. The same assumption applies if no fully-speciﬁed date can be
found once the whole list has been processed.
It is important to note that rather this plugin creates new Date annotations and so if you run
it in the same pipeline as the ANNIE NE Transducer you will likely end up with overlapping
Date annotations. Depending on your needs it may be that you need a JAPE grammar to
delete ANNIE Date annotations before running this PR. In practice we have found that the
Date annotations added by ANNIE can be a good source of document dates and so a JAPE
grammar that uses ANNIE Dates to add new DocumentDate annotations and to delete other
Date annotations can be a useful step before running this PR.

20.8

ABNER - A Biomedical Named Entity Recogniser

ABNER is A Biomedical Named Entity Recogniser. It uses machine learning (linear-chain
conditional random ﬁelds, CRFs) to ﬁnd entities such as genes, cell types, and DNA in text.
Full details of ABNER can be found at http://pages.cs.wisc.edu/ bsettles/abner/
The ABNER plugin, called ‘Tagger Abner’, contains a single PR, called AbnerTagger, which
wraps ABNER. To use AbnerTagger, ﬁrst load the Tagger Abner plugin through the plugins
console, and then create a new AbnerTagger PR in the usual way. The AbnerTagger PR has
no loadtime parameters (apart from Name). It does not require any other PRs to be run
prior to execution.
The AbnerTagger has two runtime parameters:
abnerMode The Abner model that will be used for tagging. The plugin can use one
of two previously trained machine learning models for tagging text, as provided by
Abner:
– BIOCREATIVE trained on the BioCreative corpus
– NLPBA trained on the NLPBA corpus
outputASName The name of the output annotation set to which AbnerTagger output
will be written.
The AbnerTagger creates annotations of type ‘Tagger’ with a feature and value
‘source=abner’. Each annotation may also have features of ‘class’ and ‘type’ set by Abner to values such as:

458

More (CREOLE) Plugins

Protein
DNA
RNA
Cell Line
Cell Type
Gene

Abner does support training of models on other data, but this functionality is not, however,
supported by the GATE wrapper.
For
further
details
please
refer
http://pages.cs.wisc.edu/ bsettles/abner/

20.9

to

the

Abner

documentation

at

Snowball Based Stemmers

The stemmer plugin, ‘Stemmer Snowball’, consists of a set of stemmers PRs for the following 11 European languages: Danish, Dutch, English, Finnish, French, German, Italian,
Norwegian, Portuguese, Russian, Spanish and Swedish. These take the form of wrappers
for the Snowball stemmers freely available from http://snowball.tartarus.org. Each Token is
annotated with a new feature ‘stem’, with the stem for that word as its value. The stemmers
should be run as other PRs, on a document that has been tokenised.
There are three runtime parameters which should be set prior to executing the stemmer on
a document.

annotationType: This is the type of annotations that represent tokens in the document.
Default value is set to ‘Token’.
annotationFeature: This is the name of a feature that contains tokens’ strings. The
stemmer uses value of this feature as a string to be stemmed. Default value is set to
‘string’.
annotationSetName: This is where the stemmer expects the annotations of type as
speciﬁed in the annotationType parameter to be.

More (CREOLE) Plugins

20.9.1

459

Algorithms

The stemmers are based on the Porter stemmer for English [Porter 80], with rules implemented in Snowball e.g.
define Step_1a as
( [substring] among (
’sses’ (<-’ss’)
’ies’ (<-’i’)
’ss’ () ’s’ (delete)
)

20.10

GATE Morphological Analyzer

The Morphological Analyser PR can be found in the Tools plugin. It takes as input a
tokenized GATE document. Considering one token and its part of speech tag, one at a
time, it identiﬁes its lemma and an aﬃx. These values are than added as features on the
Token annotation. Morpher is based on certain regular expression rules. These rules were
originally implemented by Kevin Humphreys in GATE1 in a programming language called
Flex. Morpher has a capability to interpret these rules with an extension of allowing users
to add new rules or modify the existing ones based on their requirements. In order to allow
these operations with as little eﬀort as possible, we changed the way these rules are written.
More information on how to write these rules is explained later in Section 20.10.1.
Two types of parameters, Init-time and run-time, are required to instantiate and execute
the PR.
rulesFile (Init-time) The rule ﬁle has several regular expression patterns. Each pattern
has two parts, L.H.S. and R.H.S. L.H.S. deﬁnes the regular expression and R.H.S. the
function name to be called when the pattern matches with the word under consideration. Please see 20.10.1 for more information on rule ﬁle.
caseSensitive (init-time) By default, all tokens under consideration are converted into
lowercase to identify their lemma and aﬃx. If the user selects caseSensitive to be true,
words are no longer converted into lowercase.
document (run-time) Here the document must be an instance of a GATE document.
aﬃxFeatureName (run-time) Name of the feature that should hold the aﬃx value.
rootFeatureName (run-time) Name of the feature that should hold the root value.
annotationSetName (run-time) Name of the annotationSet that contains Tokens.

460

More (CREOLE) Plugins

considerPOSTag (run-time) Each rule in the rule ﬁle has a separate tag, which speciﬁes
which rule to consider with what part-of-speech tag. If this option is set to false, all
rules are considered and matched with all words. This option is very useful. For
example if the word under consideration is ”singing”. ”singing” can be used as a noun
as well as a verb. In the case where it is identiﬁed as a verb, the lemma of the same
would be ”sing” and the aﬃx ”ing”, but otherwise there would not be any aﬃx.
failOnMissingInputAnnotations (run-time) If set to true (the default) the PR will terminate with an Exception if none of the required input Annotations are found in a
document. If set to false the PR will not terminate and instead log a single warning
message per session and a debug message per document that has no input annotations.

20.10.1

Rule File

GATE provides a default rule ﬁle, called default.rul, which is available under the gate/plugins/Tools/morph/resources directory. The rule ﬁle has two sections.

1. Variables
2. Rules

Variables
The user can deﬁne various types of variables under the section deﬁneVars. These variables
can be used as part of the regular expressions in rules. There are three types of variables:

1. Range With this type of variable, the user can specify the range of characters. e.g. A
==> [-a-z0-9]
2. Set With this type of variable, user can also specify a set of characters, where one
character at a time from this set is used as a value for the given variable. When this
variable is used in any regular expression, all values are tried one by one to generate the string which is compared with the contents of the document. e.g. A ==>
[abcdqurs09123]
3. Strings Where in the two types explained above, variables can hold only one character
from the given set or range at a time, this allows specifying strings as possibilities for
the variable. e.g. A ==> ‘bb’ OR ‘cc’ OR ‘dd’

More (CREOLE) Plugins

461

Rules
All rules are declared under the section deﬁneRules. Every rule has two parts, LHS and
RHS. The LHS speciﬁes the regular expression and the RHS the function to be called when
the LHS matches with the given word. ‘==>’ is used as delimiter between the LHS and
RHS.
The LHS has the following syntax:
< ” ∗ ”—”verb”—”noun” >< regularexpression >.
User can specify which rule to be considered when the word is identiﬁed as ‘verb’ or ‘noun’.
‘*’ indicates that the rule should be considered for all part-of-speech tags. If the part-ofspeech should be used to decide if the rule should be considered or not can be enabled or
disabled by setting the value of considerPOSTags option. Combination of any string along
with any of the variables declared under the deﬁneVars section and also the Kleene operators,
‘+’ and ‘*’, can be used to generate the regular expressions. Below we give few examples of
L.H.S. expressions.
<verb>”bias”
<verb>”canvas”{ESEDING} ”ESEDING” is a variable deﬁned under the deﬁneVars
section. Note: variables are enclosed with ”{” and ”}”.
<noun>({A}*”metre”) ”A” is a variable followed by the Kleene operator ”*”, which
means ”A” can occur zero or more times.
<noun>({A}+”itis”) ”A” is a variable followed by the Kleene operator ”+”, which
means ”A” can occur one or more times.
< ∗ >”aches” ”< ∗ >” indicates that the rule should be considered for all part-ofspeech tags.
On the RHS of the rule, the user has to specify one of the functions from those listed
below. These rules are hard-coded in the Morph PR in GATE and are invoked if the regular
expression on the LHS matches with any particular word.
stem(n, string, aﬃx ) Here,
– n = number of characters to be truncated from the end of the string.
– string = the string that should be concatenated after the word to produce the
root.
– aﬃx = aﬃx of the word
irreg stem(root, aﬃx ) Here,

462

More (CREOLE) Plugins

– root = root of the word
– aﬃx = aﬃx of the word
– null stem() This means words are themselves the base forms and should not be
analyzed.
semi reg stem(n,string) semir reg stem function is used with the regular expressions
that end with any of the {EDING} or {ESEDING} variables deﬁned under the variable
section. If the regular expression matches with the given word, this function is invoked,
which returns the value of variable (i.e. {EDING} or {ESEDING}) as an aﬃx. To
ﬁnd a lemma of the word, it removes the n characters from the back of the word and
adds the string at the end of the word.

20.11

Flexible Exporter

The Flexible Exporter enables the user to save a document (or corpus) in its original format
with added annotations. The user can select the name of the annotation set from which these
annotations are to be found, which annotations from this set are to be included, whether
features are to be included, and various renaming options such as renaming the annotations
and the ﬁle.
At load time, the following parameters can be set for the ﬂexible exporter:
includeFeatures - if set to true, features are included with the annotations exported; if
false (the default status), they are not.
useSuﬃxForDumpFiles - if set to true (the default status), the output ﬁles have the
suﬃx deﬁned in suﬃxForDumpFiles; if false, no suﬃx is deﬁned, and the output ﬁle
simply overwrites the existing ﬁle (but see the outputFileUrl runtime parameter for an
alternative).
suﬃxForDumpFiles - this deﬁnes the suﬃx if useSuﬃxForDumpFiles is set to true. By
default the suﬃx is .gate.
useStandOﬀXML - if true then the format will be the GATE XML format that separates nodes and annotations inside the ﬁle which allows overlapping annotations to be
saved.
The following runtime parameters can also be set (after the ﬁle has been selected for the
application):
annotationSetName - this enables the user to specify the name of the annotation set
which contains the annotations to be exported. If no annotation set is deﬁned, it will
use the Default annotation set.

More (CREOLE) Plugins

463

annotationTypes - this contains a list of the annotations to be exported. By default it
is set to Person, Location and Date.
dumpTypes - this contains a list of names for the exported annotations. If the annotation name is to remain the same, this list should be identical to the list in annotationTypes. The list of annotation names must be in the same order as the corresponding
annotation types in annotationTypes.
outputDirectoryUrl - this enables the user to specify the export directory where the
ﬁle is exported with its original name and an extension (provided as a parameter)
appended at the end of ﬁlename. Note that you can also save a whole corpus in one
go. If not provided, use the temporary directory.

20.12

Annotation Set Transfer

The Annotation Set Transfer allows copying or moving annotations to a new annotation
set if they lie between the beginning and the end of an annotation of a particular type
(the covering annotation). For example, this can be used when a user only wants to run
a processing resource over a speciﬁc part of a document, such as the Body of an HTML
document. The user speciﬁes the name of the annotation set and the annotation which
covers the part of the document they wish to transfer, and the name of the new annotation
set. All the other annotations corresponding to the matched text will be transferred to
the new annotation set. For example, we might wish to perform named entity recognition
on the body of an HTML text, but not on the headers. After tokenising and performing
gazetteer lookup on the whole text, we would use the Annotation Set Transfer to transfer
those annotations (created by the tokeniser and gazetteer) into a new annotation set, and
then run the remaining NE resources, such as the semantic tagger and coreference modules,
on them.
The Annotation Set Transfer has no loadtime parameters. It has the following runtime
parameters:
inputASName - this deﬁnes the annotation set from which annotations will be transferred (copied or moved). If nothing is speciﬁed, the Default annotation set will be
used.
outputASName - this deﬁnes the annotation set to which the annotations will be transferred. This default value for this parameter is ‘Filtered’. If it is left blank the Default
annotation set will be used.
tagASName - this deﬁnes the annotation set which contains the annotation covering the
relevant part of the document to be transferred. This default value for this parameter
is ‘Original markups’. If it is left blank the Default annotation set will be used.

464

More (CREOLE) Plugins

textTagName - this deﬁnes the type of the annotation covering the annotations to be
transferred. The default value for this parameter is ‘BODY’. If this is left blank, then
all annotations from the inputASName annotation set will be transferred. If more
than one covering annotation is found, the annotation covered by each of them will
be transferred. If no covering annotation is found, the processing depends on the
copyAllUnlessFound parameter (see below).
copyAnnotations - this speciﬁes whether the annotations should be moved or copied.
The default value false will move annotations, removing them from the inputASName
annotation set. If set to true the annotations will be copied.
transferAllUnlessFound - this speciﬁes what should happen if no covering annotation
is found. The default value is true. In this case, all annotations will be copied or moved
(depending on the setting of parameter copyAnnotations) if no covering annotation
is found. If set to false, no annotation will be copied or moved.
annotationTypes - if annotation type names are speciﬁed for this list, only candidate
annotations of those types will be transferred or copied. If an entry in this list is speciﬁed in the form OldTypeName=NewTypeName, then annotations of type OldTypeName
will be selected for copying or transfer and renamed to NewTypeName in the output
annotation set.
For example, suppose we wish to perform named entity recognition on only the text covered
by the BODY annotation from the Original Markups annotation set in an HTML document.
We have to run the gazetteer and tokeniser on the entire document, because since these
resources do not depend on any other annotations, we cannot specify an input annotation
set for them to use. We therefore transfer these annotations to a new annotation set (Filtered)
and then perform the NE recognition over these annotations, by specifying this annotation
set as the input annotation set for all the following resources. In this example, we would
set the following parameters (assuming that the annotations from the tokenise and gazetteer
are initially placed in the Default annotation set).
inputASName: Default
outputASName: Filtered
tagASName: Original markups
textTagName: BODY
copyAnnotations: true or false (depending on whether we want to keep the Token and
Lookup annotations in the Default annotation set)
copyAllUnlessFound: true

More (CREOLE) Plugins

465

The AST PR makes a shallow copy of the feature map for each transferred annotation, i.e.
it creates a new feature map containing the same keys and values as the original. It does
not clone the feature values themselves, so if your annotations have a feature whose value
is a collection and you need to make a deep copy of the collection value then you will not
be able to use the AST PR to do this. Similarly if you are copying annotations and do in
fact want to share the same feature map between the source and target annotations then
the AST PR is not appropriate. In these sorts of cases a JAPE grammar or Groovy script
would be a better choice.

20.13

Schema Enforcer

One common use of the Annotation Set Transfer (AST) PR (see Section 20.12) is to create
a ‘clean’ or ﬁnal annotation set for a GATE application, i.e. an annotation set containing
only those annotations which are required by the application without any temporary or
intermediate annotations which may also have been created. Whilst really useful the AST
suﬀers from two problems 1) it can be complex to conﬁgure and 2) it oﬀers no support for
modifying or removing features of the annotations it copies.
Many GATE applications are developed through a process which starts with experts manually annotating documents in order for the application developer to understand what is
required and which can later be used for testing and evaluation. This is usually done using
either GATE Teamware or within GATE Developer using the Schema Annotation Editor
(Section 3.4.6). Either approach requires that each of the annotation types being created
is described by an XML based Annotation Schema. The Schema Enforcer (part of the
Schema Tools plugin) uses these same schemas to create an annotation set, the contents of
which, strictly matches the provided schemas.
The Schema Enforcer will copy an annotation if and only if....
the type of the annotation matches one of the supplied schemas
all required features are present and valid (i.e. meet the requirements for being copied
to the ’clean’ annotation)
Each feature of an annotation is copied to the new annotation if and only if....
the feature name matches a feature in the schema describing the annotation
the value of the feature is of the same type as speciﬁed in the schema
if the feature is deﬁned, in the schema, as an enumerated type then the value must
match one of the permitted values

466

More (CREOLE) Plugins

The Schema Enforcer has no initialization parameters and is conﬁgured via the following
runtime parameters:
inputASName - - this deﬁnes the annotation set from which annotations will be copied.
If nothing is speciﬁed, the default annotation set will be used.
outputASName - this deﬁnes the annotation set to which the annotations will be transferred. This must be an empty or non-existent annotation set.
schmeas - a list of schemas that will be enforced when duplicating the input annotation
set.
useDefaults - if true then the default value for required features (speciﬁed using the
value attribute in the XML schema) will be used to help complete an otherwise invalid
annotation, defaults to false.
Whilst this PR makes the creation of a clean output set easy (given the schemas) it is worth
noting that schemas can only deﬁne features which have basic types; string, integer, boolean,
ﬂoat, double, short, and byte. This means that you cannot deﬁne a feature which has an
object as it’s value. For example, this prevents you deﬁning a feature as a list of numbers.
If this is an issue then it is trivial to write JAPE to copy extra features not speciﬁed in the
schemas as the annotations have the same ID in both the input and output annotation sets.
An example JAPE ﬁle for copying the matches feature created by the Orthomatcher PR
(see Section 6.8) is provided.

20.14

Information Retrieval in GATE

GATE comes with a full-featured Information Retrieval (IR) subsystem that allows queries to
be performed against GATE corpora. This combination of IE and IR means that documents
can be retrieved from the corpora not only based on their textual content but also according
to their features or annotations. For example, a search over the Person annotations for
‘Bush’ will return documents with higher relevance, compared to a search in the content for
the string ‘bush’. The current implementation is based on the most popular open source
full-text search engine - Lucene (available at http://jakarta.apache.org/lucene/) but other
implementations may be added in the future.
An Information Retrieval system is most often considered a system that accepts as input a
set of documents (corpus) and a query (combination of search terms) and returns as input
only those documents from the corpus which are considered as relevant according to the
query. Usually, in addition to the documents, a proper relevance measure (score) is returned
for each document. There exist many relevance metrics, but usually documents which are
considered more relevant, according to the query, are scored higher.

More (CREOLE) Plugins

467

Figure 20.3: Documents with scores, returned from a search over a corpus

doc1
doc2
...
...
docn

term1
w1,1
w2,1
...
...
wn , 1

term2
w1,2
w2,1
...
...
wn,2

...
...
...
...
...
...

...
...
...
...
...
...

termk
w1,k
w2,k
...
...
wn,k

Table 20.2: An information retrieval document-term matrix

468

More (CREOLE) Plugins

Figure 20.3 shows the results from running a query against an indexed corpus in GATE.
Information Retrieval systems usually perform some preprocessing one the input corpus in
order to create the document-term matrix for the corpus. A document-term matrix is usually
presented as in Table 20.2, where doci is a document from the corpus, termj is a word that is
considered as important and representative for the document and wi, j is the weight assigned
to the term in the document. There are many ways to deﬁne the term weight functions,
but most often it depends on the term frequency in the document and in the whole corpus
(i.e. the local and the global frequency). Note that the machine learning plugin described in
Chapter 17 can produce such document-term matrix (for detailed description of the matrix
produced, see Section 17.2.4).
Note that not all of the words appearing in the document are considered terms. There
are many words (called ‘stop-words’) which are ignored, since they are observed too often
and are not representative enough. Such words are articles, conjunctions, etc. During the
preprocessing phase which identiﬁes such words, usually a form of stemming is performed in
order to minimize the number of terms and to improve the retrieval recall. Various forms
of the same word (e.g. ‘play’, ‘playing’ and ‘played’) are considered identical and multiple
occurrences of the same term (probably ‘play’) will be observed.
It is recommended that the user reads the relevant Information Retrieval literature for a
detailed explanation of stop words, stemming and term weighting.
IR systems, in a way similar to IE systems, are evaluated with the help of the precision and
recall measures (see Section 10.1 for more details).

20.14.1

Using the IR Functionality in GATE

In order to run queries against a corpus, the latter should be ‘indexed’. The indexing process
ﬁrst processes the documents in order to identify the terms and their weights (stemming is
performed too) and then creates the proper structures on the local ﬁle system. These ﬁle
structures contain indexes that will be used by Lucene (the underlying IR engine) for the
retrieval.
Once the corpus is indexed, queries may be run against it. Subsequently the index may be
removed and then the structures on the local ﬁle system are removed too. Once the index
is removed, queries cannot be run against the corpus.

Indexing the Corpus
In order to index a corpus, the latter should be stored in a serial datastore. In other words,
the IR functionality is unavailable for corpora that are transient or stored in a RDBMS
datastores (though support for the latter may be added in the future).

More (CREOLE) Plugins

469

To index the corpus, follow these steps:
Select the corpus from the resource tree (top-left pane) and from the context menu
(right button click) choose ‘Index Corpus’. A dialogue appears that allows you to
specify the index properties.
In the index properties dialogue, specify the underlying IR system to be used (only
Lucene is supported at present), the directory that will contain the index structures,
and the set of properties that will be indexed such as document features, content, etc
(the same properties will be indexed for each document in the corpus).
Once the corpus in indexed, you may start running queries against it. Note that the
directory speciﬁed for the index data should exist and be empty. Otherwise an error
will occur during the index creation.

Figure 20.4: Indexing a corpus by specifying the index location and indexed features (and
content)

Querying the Corpus
To query the corpus, follow these steps:
Create a SearchPR processing resource. All the parameters of SearchPR are runtime
so they are set later.

470

More (CREOLE) Plugins

Create a “pipeline” application (not a “corpus pipeline”) containing the SearchPR.
Set the following SearchPR parameters:
– The corpus that will be queried.
– The query that will be executed.
– The maximum number of documents returned.
A query looks like the following:
{+/-}field1:term1 {+/-}field2:term2 ? {+/-}fieldN:termN
where field is the name of a index ﬁeld, such as the one speciﬁed at index creation
(the document content ﬁeld is body) and term is a term that should appear in the
ﬁeld.
For example the query:
+body:government +author:CNN
will inspect the document content for the term ‘government’ (together with variations
such as ‘governments’ etc.) and the index ﬁeld named ‘author’ for the term ‘CNN’.
The ‘author’ ﬁeld is speciﬁed at index creation time, and is either a document feature
or another document property.
After the SearchPR is initialized, running the application executes the speciﬁed query
over the speciﬁed corpus.
Finally, the results are displayed (see ﬁg.1) after a double-click on the SearchPR processing resource.

Removing the Index
An index for a corpus may be removed at any time from the ‘Remove Index’ option of the
context menu for the indexed corpus (right button click).

20.14.2

Using the IR API

The IR API within GATE Embedded makes it possible for corpora to be indexed, queried and
results returned from any Java application, without using GATE Developer. The following
sample indexes a corpus, runs a query against it and then removes the index.

More (CREOLE) Plugins

471

1
2
3
4
5
6

// open a serial datastore
SerialDataStore sds =
Factory . openDataStore ( " gate . persist . SerialDataStore " ,
" / tmp / datastore1 " );
sds . open ();

7
8
9
10

//set an AUTHOR feature for the test document
Document doc0 = Factory . newDocument ( new URL ( " / tmp / documents / doc0 . html " ));
doc0 . getFeatures (). put ( " author " ," John Smith " );

11
12
13

Corpus corp0 = Factory . newCorpus ( " TestCorpus " );
corp0 . add ( doc0 );

14
15
16
17

//store the corpus in the serial datastore
Corpus serialCorpus = ( Corpus ) sds . adopt ( corp0 , null );
sds . sync ( serialCorpus );

18
19

//index the corpus - the content and the AUTHOR feature

20
21

IndexedCorpus indexedCorpus = ( IndexedCorpus ) serialCorpus ;

22
23
24
25
26
27
28
29
30

De fa ul t I n d e x D e f i n i t i o n did = new D e f a u l t I n d e x D e f i n i t i o n ();
did . setIrEngineClassName (
gate . creole . ir . lucene . LuceneIREngine . class . getName ());
did . setIndexLocation ( " / tmp / index1 " );
did . addIndexField ( new IndexField ( " content " ,
new D o c u m e n t C o n t e n t R e a d e r () , false ));
did . addIndexField ( new IndexField ( " author " , null , false ));
indexedCorpus . se tI nd ex De fi ni ti on ( did );

31
32
33

indexedCorpus . getIndexManager (). createIndex ();
//the corpus is now indexed

34
35
36
37

//search the corpus
Search search = new LuceneSearch ();
search . setCorpus ( ic );

38
39

QueryResultList res = search . search ( " + content : government + author : John " );

40
41
42
43
44
45
46
47

//get the results
Iterator it = res . getQueryResults ();
while ( it . hasNext ()) {
QueryResult qr = ( QueryResult ) it . next ();
System . out . println ( " DOCUMENT_ID = " + qr . getDocumentID ()
+ ",
score = " + qr . getScore ());
}

472

More (CREOLE) Plugins

20.15

Websphinx Web Crawler

The ‘Web Crawler Websphinx’ plugin enables GATE to build a corpus from a web crawl. It
is based on Websphinx, a JAVA-based, customizable, multi-threaded web crawler.
Note: if you are using this plugin via an IDE, you may need to make sure that the websphinx.jar ﬁle is on the IDE’s classpath, or add it to the IDE’s lib directory.
The basic idea is to specify a source URL (or set of documents created from web URLs) and
a depth and maximum number of documents to build the initial corpus upon which further
processing could be done. The PR itself provides a number of other parameters to regulate
the crawl.
This PR now uses the HTTP Content-Type headers to determine each web page’s encoding
and MIME type before creating a GATE Document from it. It also adds to each document a Date feature (with a java.util.Date value) based on the HTTP Last-Modified
header (if available) or the current timestamp, an originalMimeType feature taken from
the Content-Type header, and an originalLength feature indicating the size in bytes of the
downloaded document.

20.15.1

Using the Crawler PR

In order to use the processing resource you need to load the plugin using the plugin manager,
create an instance of the crawl PR from the list of processing resources, and create a corpus
in which to store crawled documents. In order to use the crawler, create a simple pipeline
(not a corpus pipeline) and add the crawl PR to the pipeline.
Once the crawl PR is created there will be a number of parameters that can be set based on
the PR required (see also Figure 20.5).
depth The depth (integer) to which the crawl should proceed.
dfs A boolean:
true the crawler visits links with a depth-ﬁrst strategy;
false the crawler visits links with a breadth-ﬁrst strategy;
domain An enum value, presented as a pull-down list in the GUI:
SUBTREE The crawler visits only the descendents of the pages speciﬁed as the roots
for the crawl.
WEB The crawler can visit any pages on the web.
SERVER The crawler can visit only pages that are present on the server where the
root pages are located.

More (CREOLE) Plugins

473

Figure 20.5: Crawler parameters

max The maximum number (integer) of pages to be kept: the crawler will stop when it has
stored this number of documents in the output corpus. Use −1 to ignore this limit.
maxPageSize The maximum page size in kB; pages over this limit will be ignored—even
as roots of the crawl—and their links will not be crawled. If your crawl does not add
any documents (even the seeds) to the output corpus, try increasing this value. (A 0
or negative value here means “no limit”.)
stopAfter The maximum number (integer) of pages to be fetched: the crawler will stop
when it has visited this number of pages. Use −1 to ignore this limit. If max >
stopAfter > 0 then the crawl will store at most stopAfter (not max ) documents.
root A string containing one URL to start the crawl.
source A corpus that contains the documents whose gate.sourceURL features will be used
to start the crawl. If you use both root and source parameters, both the root value
and the URLs collected from the source documents will seed the crawl.
outputCorpus The corpus in which the fetched documents will be stored.

474

More (CREOLE) Plugins

keywords A List<String> for matching against crawled documents. If this list is empty
or null, all documents fetched will be kept. Otherwise, only documents that contain
one of these strings will be stored in the output corpus. (Documents that are fetched
but not kept are still scanned for further links.)
keywordsCaseSensitive This boolean determines whether keyword matching is casesensitive or not.
convertXmlTypes GATE’s XmlDocumentFormat only accepts certain MIME types.
If this parameter is true, the crawl PR converts other XML types (such as
application/atom+xml.xml) to text/xml before trying to instantiate the GATE document (this allows GATE to handle RSS feeds, for example).
userAgent If this parameter is blank, the crawler will use the default Websphinx user-agent
header. Set this parameter to spoof the header.
Once the parameters are set, the crawl can be run and the documents fetched (and matched
to the keywords, if that list is in use) are added to the speciﬁed corpus. Documents that are
fetched but not matched are discarded after scanning them for further links.
Note that you must use a simple Pipeline, and not a Corpus Pipeline. In order to process
the corpus of crawled documents, you need to build a separate Corpus Pipeline and run it
after crawling. You could combine the two functions by carefully developing a Scriptable
Controller (see section 7.16.3 for details).

20.16

Google Plugin

This plugin is no longer operational because the functionality, provided by Google, on which
it depends, is no longer available.

20.17

Yahoo Plugin

The Yahoo API is now integrated with GATE, and can be used as a PR-based plugin. This
plugin, ‘Web Search Yahoo’, allows the user to query Yahoo and build a document corpus
that contains the search results returned by Yahoo for the query. For more information
about the Yahoo API please refer to http://developer.yahoo.com/search/. In order to use
the Yahoo PR, you need to obtain an application ID.
The Yahoo PR can be used for a number of diﬀerent application scenarios. For example,
one use case is where a user wants to ﬁnd the diﬀerent named entities that can be associated
with a particular individual. In this example, the user could build a collection of documents
by querying Yahoo with the individual’s name and then running ANNIE over the collection.

More (CREOLE) Plugins

475

This would annotate the results and show the diﬀerent Organization, Location and other
entities that are associated with the query.

20.17.1

Using the YahooPR

In order to use the PR, you ﬁrst need to load the plugin using the GATE Developer plugin
manager. Once the PR is loaded, it can be initialized by creating an instance of a new PR.
Here you need to specify the Yahoo Application ID. Please use the license key assigned to
you by registering with Yahoo.
Once the Yahoo PR is initialized, it can be placed in a pipeline or a conditional pipeline
application. This pipeline would contain the instance of the Yahoo PR just initialized as
above. There are a number of parameters to be set at runtime:
corpus: The corpus used by the plugin to add or append documents from the Web.
corpusAppendMode: If set to true, will append documents to the corpus. If set to
false, will remove preexisting documents from the corpus, before adding the documents newly fetched by the PR
limit: A limit on the results returned by the search. Default set to 10.
pagesToExclude: This is an optional parameter. It is a list with URLs not to be
included in the search.
query: The query sent to Yahoo. It is in the format accepted by Yahoo.
Once the required parameters are set we can run the pipeline. This will then download all
the URLs in the results and create a document for each. These documents would be added
to the corpus.

20.18

Google Translator PR

The Google Translator PR allows users to translate their documents into many other
languages using the Google translation service.
It is based on the library called
google-translate-api-java which is distributed under the LGPL licence and is available
to download from http://code.google.com/p/google-api-translate-java/.
The PR is included in the plugin called Web Translate Google and depends on the Alignment
plugin. (chapter 18).
If a user wants to translate an English document into French using the Google Translator
PR. The ﬁrst thing user needs to do is to create an instance of CompoundDocument with

476

More (CREOLE) Plugins

the English document as a member of it. The CompoundDocument in GATE provides a
convenient way to group parallel documents that are translations of one other (see chapter
18 for more information). The idea is to use text from one of the members of the provided
compound document, translate it using the google translation service and create another
member with the translated text. In the process, the PR also aligns the chunks of parallel
texts. Here, a chunk could be a sentence, paragraph, section or the entire document.
siteReferrer is the only init-time parameter required to instantiate the PR. It has to be
a valid website address. The value of this parameter is required to inform Google about the
users using their service. There are seven run-time parameters:
document - an instance of the compound document with a member document containing source text.
sourceDocumentId - id of the source member document that needs to be translated.
targetDocumentId - id of the target member document. This document is created by
the PR and contains the translated text.
sourceLanguage - the language of the source document.
targetLanguage - the language into which the source document should be translated.
unitOfTranslation - annotation type used for identifying chunks of texts to be translated and aligned.
inputASName - name of the annotation set which contains unit of translations.
alignmentFeatureName - name of the alignment feature used for storing the alignment
information. The alignment feature is a document feature stored on the compound
document.

20.19

WordNet in GATE

GATE currently supports versions 1.6 and newer of WordNet, so in order to use WordNet in
GATE, you must ﬁrst install a compatible version of WordNet on your computer. WordNet
is available at http://wordnet.princeton.edu/. The next step is to conﬁgure GATE to work
with your local WordNet installation. Since GATE relies on the Java WordNet Library
(JWNL) for WordNet access, this step consists of providing one special xml ﬁle that is used
internally by JWNL. This ﬁle describes the location of your local copy of the WordNet index
ﬁles. An example of this wn-conﬁg.xml ﬁle is shown below:

<?xml version="1.0" encoding="UTF-8"?>

More (CREOLE) Plugins

477

Figure 20.6: WordNet in GATE – results for ‘bank’

<jwnl_properties language="en">
<version publisher="Princeton" number="3.0" language="en"/>
<dictionary class="net.didion.jwnl.dictionary.FileBackedDictionary">
<param name="morphological_processor"
value="net.didion.jwnl.dictionary.morph.DefaultMorphologicalProcessor">
<param name="operations">
<param value=
"net.didion.jwnl.dictionary.morph.LookupExceptionsOperation"/>
<param value="net.didion.jwnl.dictionary.morph.DetachSuffixesOperation">
<param name="noun"
value="|s=|ses=s|xes=x|zes=z|ches=ch|shes=sh|men=man|ies=y|"/>
<param name="verb"
value="|s=|ies=y|es=e|es=|ed=e|ed=|ing=e|ing=|"/>
<param name="adjective"
value="|er=|est=|er=e|est=e|"/>
<param name="operations">

478

More (CREOLE) Plugins

Figure 20.7: WordNet in GATE

More (CREOLE) Plugins

479

<param
value="net.didion.jwnl.dictionary.morph.LookupIndexWordOperation"/>
<param
value="net.didion.jwnl.dictionary.morph.LookupExceptionsOperation"/>
</param>
</param>
<param value="net.didion.jwnl.dictionary.morph.TokenizerOperation">
<param name="delimiters">
<param value=" "/>
<param value="-"/>
</param>
<param name="token_operations">
<param
value="net.didion.jwnl.dictionary.morph.LookupIndexWordOperation"/>
<param
value="net.didion.jwnl.dictionary.morph.LookupExceptionsOperation"/>
<param
value="net.didion.jwnl.dictionary.morph.DetachSuffixesOperation">
<param name="noun"
value="|s=|ses=s|xes=x|zes=z|ches=ch|shes=sh|men=man|ies=y|"/>
<param name="verb"
value="|s=|ies=y|es=e|es=|ed=e|ed=|ing=e|ing=|"/>
<param name="adjective" value="|er=|est=|er=e|est=e|"/>
<param name="operations">
<param value=
"net.didion.jwnl.dictionary.morph.LookupIndexWordOperation"/>
<param value=
"net.didion.jwnl.dictionary.morph.LookupExceptionsOperation"/>
</param>
</param>
</param>
</param>
</param>
</param>
<param name="dictionary_element_factory" value=
"net.didion.jwnl.princeton.data.PrincetonWN17FileDictionaryElementFactory"/>
<param name="file_manager" value=
"net.didion.jwnl.dictionary.file_manager.FileManagerImpl">
<param name="file_type" value=
"net.didion.jwnl.princeton.file.PrincetonRandomAccessDictionaryFile"/>
<param name="dictionary_path" value="/home/mark/WordNet-3.0/dict/"/>
</param>
</dictionary>
<resource class="PrincetonResource"/>
</jwnl_properties>

480

More (CREOLE) Plugins

There are three things in this ﬁle which you need to conﬁgure based upon the version of
WordNet you wish to use. Firstly change the number attribute of the version element to
match the version of WordNet you are using. Then edit the value of the dictionary_path
parameter to point to your local installation of WordNet. Finally, if you want to use version 1.6 of WordNet then you allso need to alter the dictionary_element_factory to
use net.didion.jwnl.princeton.data.PrincetonWN16FileDictionaryElementFactory.
For full details of the format of the conﬁguration ﬁle see the JWNL documentation
at http://sourceforge.net/projects/jwordnet.
After conﬁguring GATE to use WordNet, you can start using the built-in WordNet browser
or API. In GATE Developer, load the WordNet plugin via the Plugin Management Console.
Then load WordNet by selecting it from the set of available language resources. Set the
value of the parameter to the path of the xml properties ﬁle which describes the WordNet
location (wn-conﬁg).
Once WordNet is loaded in GATE Developer, the well-known interface of WordNet will appear. You can search Word Net by typing a word in the box next to to the label ‘SearchWord”
and then pressing ‘Search’. All the senses of the word will be displayed in the window below.
Buttons for the possible parts of speech for this word will also be activated at this point.
For instance, for the word ‘play’, the buttons ‘Noun’, ‘Verb’ and ‘Adjective’ are activated.
Pressing one of these buttons will activate a menu with hyponyms, hypernyms, meronyms
for nouns or verb groups, and cause for verbs, etc. Selecting an item from the menu will
display the results in the window below.
To upgrade any existing GATE applications to use this improved WordNet plugin simply
replace your existing conﬁguration ﬁle with the example above and conﬁgure for WordNet
1.6. This will then give results identical to the previous version – unfortunately it was not
possible to provide a transparent upgrade procedure.
More information about WordNet can be found at http://wordnet.princeton.edu/
More information about the JWNL library can be found at http://sourceforge.net/
projects/jwordnet
An example of using the WordNet API in GATE is available on the GATE examples page
at http://gate.ac.uk/wiki/code-repository/index.html.

20.19.1

The WordNet API

GATE Embedded oﬀers a set of classes that can be used to access the WordNet Lexical
Database. The implementation of the GATE API for WordNet is based on Java WordNet
Library (JWNL). There are just a few basic classes, as shown in Figure 20.8. Details about
the properties and methods of the interfaces/classes comprising the API can be obtained
from the JavaDoc. Below is a brief overview of the interfaces:

More (CREOLE) Plugins

481

WordNet: the main WordNet class. Provides methods for getting the synsets of a
lemma, for accessing the unique beginners, etc.
Word: oﬀers access to the word’s lemma and senses
WordSense: gives access to the synset, the word, POS and lexical relations.
Synset: gives access to the word senses (synonyms) in the synset, the semantic relations, POS etc.
Verb: gives access to the verb frames (not working properly at present)
Adjective: gives access to the adj. position (attributive, predicative, etc.).
Relation: abstract relation such as type, symbol, inverse relation, set of POS tags,
etc. to which it is applicable.
LexicalRelation
SemanticRelation
VerbFrame

Figure 20.8: The Wordnet API

482

20.20

More (CREOLE) Plugins

Kea - Automatic Keyphrase Detection

Kea is a tool for automatic detection of key phrases developed at the University of Waikato in
New Zealand. The home page of the project can be found at http://www.nzdl.org/Kea/.
This user guide section only deals with the aspects relating to the integration of Kea in
GATE. For the inner workings of Kea, please visit the Kea web site and/or contact its
authors.
In order to use Kea in GATE Developer, the ‘Keyphrase Extraction Algorithm’ plugin needs
to be loaded using the plugins management console. After doing that, two new resource types
are available for creation: the ‘KEA Keyphrase Extractor’ (a processing resource) and the
‘KEA Corpus Importer’ (a visual resource associated with the PR).

20.20.1

Using the ‘KEA Keyphrase Extractor’ PR

Kea is based on machine learning and it needs to be trained before it can be used to extract
keyphrases. In order to do this, a corpus is required where the documents are annotated
with keyphrases. Corpora in the Kea format (where the text and keyphrases are in separate
ﬁles with the same name but diﬀerent extensions) can be imported into GATE using the
‘KEA Corpus Importer’ tool. The usage of this tool is presented in a subsection below.
Once an annotated corpus is obtained, the ‘KEA Keyphrase Extractor’ PR can be used to
build a model:
1. load a ‘KEA Keyphrase Extractor’
2. create a new ‘Corpus Pipeline’ controller.
3. set the corpus for the controller
4. set the ‘trainingMode’ parameter for the PR to ‘true’
5. run the application.
After these steps, the Kea PR contains a trained model. This can be used immediately by
switching the ‘trainingMode’ parameter to ‘false’ and running the PR over the documents
that need to be annotated with keyphrases. Another possibility is to save the model for later
use, by right-clicking on the PR name in the right hand side tree and choosing the ‘Save
model’ option.
When a previously built model is available, the training procedure does not need to be
repeated, the existing model can be loaded in memory by selecting the ‘Load model’ option
in the PR’s context menu.

More (CREOLE) Plugins

483

Figure 20.9: Parameters used by the Kea PR

The Kea PR uses several parameters as seen in Figure 20.9:
document The document to be processed.
inputAS The input annotation set. This parameter is only relevant when the PR is running in training mode and it speciﬁes the annotation set containing the keyphrase
annotations.
outputAS The output annotation set. This parameter is only relevant when the PR is
running in application mode (i.e. when the ‘trainingMode’ parameter is set to false)
and it speciﬁes the annotation set where the generated keyphrase annotations will be
saved.
minPhraseLength the minimum length (in number of words) for a keyphrase.
minNumOccur the minimum number of occurrences of a phrase for it to be a keyphrase.
maxPhraseLength the maximum length of a keyphrase.
phrasesToExtract how many diﬀerent keyphrases should be generated.
keyphraseAnnotationType the type of annotations used for keyphrases.
dissallowInternalPeriods should internal periods be disallowed.

484

More (CREOLE) Plugins

Figure 20.10: Options for the ‘KEA Corpus Importer’

trainingMode if ‘true’ the PR is running in training mode; otherwise it is running in
application mode.
useKFrequency should the K-frequency be used.

20.20.2

Using Kea Corpora

The authors of Kea provide on the project web page a few manually annotated corpora that
can be used for training Kea. In order to do this from within GATE, these corpora need to
be converted to the format used in GATE (i.e. GATE documents with annotations). This
is possible using the ‘KEA Corpus Importer’ tool which is available as a visual resource
associated with the Kea PR. The importer tool can be made visible by double-clicking on
the Kea PR’s name in the resources tree and then selecting the ‘KEA Corpus Importer’ tab,
see Figure 20.10.
The tool will read ﬁles from a given directory, converting the text ones into GATE documents
and the ones containing keyphrases into annotations over the documents.
The user needs to specify a few values:
Source Directory the directory containing the text and key ﬁles. This can be typed in or
selected by pressing the folder button next to the text ﬁeld.

More (CREOLE) Plugins

485

Extension for text ﬁles the extension used for text ﬁelds (by default .txt).
Extension for keyphrase ﬁles the extension for the ﬁles listing keyphrases.
Encoding for input ﬁles the encoding to be used when reading the ﬁles.
Corpus name the name for the GATE corpus that will be created.
Output annotation set the name for the annotation set that will contain the keyphrases
read from the input ﬁles.
Keyphrase annotation type the type for the generated annotations.

20.21

Ontotext JapeC Compiler

Note: the JapeC compiler does not currently support the new JAPE language features introduced in July–September 2008. If you need to use negation, the @length and @string
accessors, the contextual operators within and contains, or any comparison operators other
than ==, then you will need to use the standard JAPE transducer instead of JapeC.
JapeC is an alternative implementation of the JAPE language which works by compiling
JAPE grammars into Java code. Compared to the standard implementation, these compiled
grammars can be several times faster to run. At Ontotext, a modiﬁed version of the ANNIE
sentence splitter using compiled grammars has been found to run up to ﬁve times as fast
as the standard version. The compiler can be invoked manually from the command line, or
used through the ‘Ontotext Japec Compiler’ PR in the Jape Compiler plugin.
The ‘Ontotext Japec Transducer’ (com.ontotext.gate.japec.JapecTransducer) is a processing
resource that is designed to be an alternative to the original Jape Transducer. You can
simply replace gate.creole.Transducer with com.ontotext.gate.japec.JapecTransducer in your
gate application and it should work as expected.
The Japec transducer takes the same parameters as the standard JAPE transducer:
grammarURL the URL from which the grammar is to be loaded. Note that the Japec
Transducer will only work on file: URLs. Also, the alternative binaryGrammarURL
parameter of the standard transducer is not supported.
encoding the character encoding used to load the grammars.
ontology the ontology used for ontolog-aware transduction.
Its runtime parameters are likewise the same as those of the standard transducer:
document the document to process.

486

More (CREOLE) Plugins

inputASName name of the AnnotationSet from which input annotations to the transducer
are read.
outputASName name of the AnnotationSet to which output annotations from the transducer are written.
The Japec compiler itself is written in Haskell. Compiled binaries are provided for Windows,
Linux (x86) and Mac OS X (PowerPC), so no Haskell interpreter is required to run Japec
on these platforms. For other platforms, or if you make changes to the compiler source code,
you can build the compiler yourself using the Ant build ﬁle in the Jape Compiler plugin
directory. You will need to install the latest version of the Glasgow Haskell Compiler2 and
associated libraries. The japec compiler can then be built by running:
../../bin/ant japec.clean japec
from the Jape Compiler plugin directory.

20.22

Annotation Merging Plugin

If we have annotations about the same subject on the same document from diﬀerent annotators, we may need to merge the annotations.
This plugin implements two approaches for annotation merging.
MajorityVoting takes a parameter numMinK and selects the annotation on which at least
numMinK annotators agree. If two or more merged annotations have the same span, then
the annotation with the most supporters is kept and other annotations with the same span
are discarded.
MergingByAnnotatorNum selects one annotation from those annotations with the same span,
which the majority of the annotators support. Note that if one annotator did not create the
annotation with the particular span, we count it as one non-support of the annotation with
the span. If it turns out that the majority of the annotators did not support the annotation
with that span, then no annotation with the span would be put into the merged annotations.
The annotation merging methods are available via the Annotation Merging plugin. The
plugin can be used as a PR in a pipeline or corpus pipeline. To use the PR, each document
in the pipeline or the corpus pipeline should have the annotation sets for merging. The
annotation merging PR has no loading parameters but has several run-time parameters,
explained further below.
The annotation merging methods are implemented in the GATE API, and are available in
GATE Embedded as described in Section 7.18.
2

GHC version 6.4.1 was used to build the supplied binaries for Windows, Linux and Mac

More (CREOLE) Plugins

487

Parameters
annSetOutput: the annotation set in the current document for storing the merged
annotations. You should not use an existing annotation set, as the contents may be
deleted or overwritten.
annSetsForMerging: the annotation sets in the document for merging. It is an optional
parameter. If it is not assigned with any value, the annotation sets for merging would be
all the annotation sets in the document except the default annotation set. If speciﬁed,
it is a sequence of the names of the annotation sets for merging, separated by ‘;’. For
example, the value ‘a-1;a-2;a-3’ represents three annotation set, ‘a-1’, ‘a-2’ and ‘a-3’.
annTypeAndFeats: the annotation types in the annotation set for merging. It is
an optional parameter. It speciﬁes the annotation types in the annotation sets
for merging. For each type speciﬁed, it may also specify an annotation feature
of the type. The parameter is a sequence of names of annotation types, separated by ‘;’. A single annotation feature can be speciﬁed immediately following
the annotation type’s name, separated by ‘->’ in the sequence. For example, the
value ‘SENT->senRel;OPINION OPR;OPINION SRC->type’ speciﬁes three annotation types, ‘SENT’, ‘OPINION OPR’ and ‘OPINION SRC’ and speciﬁes the annotation feature ‘senRel’ and ‘type’ for the two types SENT and OPINION SRC, respectively but does not specify any feature for the type OPINION OPR. If the annTypeAndFeats parameter is not set, the annotation types for merging are all the types in
the annotation sets for merging, and no annotation feature for each type is speciﬁed.
keepSourceForMergedAnnotations: should source annotations be kept in the annSetsForMerging annotation sets when merged? True by default.
mergingMethod: speciﬁes the method used for merging. Possible values are MajorityVoting and MergingByAnnotatorNum, referring to the two merging methods described above, respectively.
minimalAnnNum: speciﬁes the minimal number of annotators who agree on one annotation in order to put the annotation into merged set, which is needed by the merging
method MergingByAnnotatorNum. If the value of the parameter is smaller than 1, the
parameter is taken as 1. If the value is bigger than total number of annotation sets
for merging, it is taken to be total number of annotation sets. If no value is assigned,
a default value of 1 is used. Note that the parameter does not have any eﬀect on the
other merging method MajorityVoting.

20.23

Copying Annotations between Documents

Sometimes a document has two copies, each of which was annotated by diﬀerent annotators
for the same task. We may want to copy the annotations in one copy to the other copy of the

488

More (CREOLE) Plugins

document. This could be in order to use less resources, or so that we can process them with
some other plugin, such as annotation merging or IAA. The Copy Annots Between Docs
plugin does exactly this.
The plugin is available with the GATE distribution. When loading the plugin into GATE,
it is represented as a processing resource, Copy Anns to Another Doc PR. You need
to put the PR into a Corpus Pipeline to use it. The plugin does not have any initialisation
parameters. It has several run-time parameters, which specify the annotations to be copied,
the source documents and target documents. In detail, the run-time parameters are:
sourceFilesURL speciﬁes a directory in which the source documents are in. The
source documents must be GATE xml documents. The plugin copies the annotations
from these source documents to target documents.
inputASName speciﬁes the name of the annotation set in the source documents.
Whole annotations or parts of annotations in the annotation set will be copied.
annotationTypes speciﬁes one or more annotation types in the annotation set inputASName which will be copied into target documents. If no value is given, the plugin
will copy all annotations in the annotation set.
outputASName speciﬁes the name of the annotation set in the target documents,
into which the annotations will be copied. If there is no such annotation set in the
target documents, the annotation set will be created automatically.
The Corpus parameter of the Corpus Pipeline application containing the plugin speciﬁes a
corpus which contains the target documents. Given one (target) document in the corpus,
the plugin tries to ﬁnd a source document in the source directory speciﬁed by the parameter
sourceFilesURL, according to the similarity of the names of the source and target documents.
The similarity of two ﬁle names is calculated by comparing the two strings of names from the
start to the end of the strings. Two names have greater similarity if they share more characters from the beginning of the strings. For example, suppose two target documents have the
names aabcc.xml and abcab.xml and three source ﬁles have names abacc.xml, abcbb.xml and
aacc.xml, respectively. Then the target document aabcc.xml has the corresponding source
document aacc.xml, and abcab.xml has the corresponding source document abcbb.xml.

20.24

OpenCalais Plugin

OpenCalais provides a web service for semantic annotation of text. The user submits a
document to the web service, which returns entity and relations annotations in RDF, JSON
or some other format. Typically, users integrate OpenCalais annotation of their web pages
to provide additional links and ‘semantic functionality’. OpenCalais can be found at http:
//www.opencalais.com

More (CREOLE) Plugins

489

The GATE OpenCalais PR submits a GATE document to the OpenCalais web service,
and adds the annotations from the OpenCalais response as GATE annotations in the GATE
document. It therefore provides OpenCalais semantic annotation functionality within GATE,
for use by other PRs.
The PR only supports OpenCalais entities, not relations - although this should be straightforward for a competent Java programmer to add. Each OpenCalais entity is represented in
GATE as an OpenCalais annotation, with features as given in the OpenCalais documentation.
The PR can be loaded with the CREOLE plugin manager dialog, from the creole directory
in the gate distribution, gate/plugins/Tagger OpenCalais. In order to use the PR, you will
need to have an OpenCalais account, and request an OpenCalais service key. You can do this
from the OpenCalais web site at http://www.opencalais.com. Provide your service key
as an initialisation parameter when you create a new OpenCalais PR in GATE. OpenCalais
make restrictions on the the number of requests you can make to their web service. See the
OpenCalais web page for details.
Initialisation parameters are:
openCalaisURL This is the URL of the OpenCalais REST service, and should not
need to be changed - unless OpenCalais moves it!
licenseID Your OpenCalais service key. This has to be requested from OpenCalais
and is speciﬁc to you.
Various runtime parameters are available from the OpenCalais API, and are named the same
as in that API. See the OpenCalais documentation for further details.

20.25

LingPipe Plugin

LingPipe is a suite of Java libraries for the linguistic analysis of human language3 . We
have provided a plugin called ‘LingPipe’ with wrappers for some of the resources available
in the LingPipe library. In order to use these resources, please load the ‘LingPipe’ plugin.
Currently, we have integrated the following ﬁve processing resources.
LingPipe Tokenizer PR
LingPipe Sentence Splitter PR
LingPipe POS Tagger PR
3

see http://alias-i.com/lingpipe/

490

More (CREOLE) Plugins

LingPipe NER PR
LingPipe Language Identiﬁer PR
Please note that most of the resources in the LingPipe library allow learning of new models.
However, in this version of the GATE plugin for LingPipe, we have only integrated the
application functionality. You will need to learn new models with Lingpipe outside of GATE.
We have provided some example models under the ‘resources’ folder which were downloaded
from LingPipe’s website. For more information on licensing issues related to the use of these
models, please refer to the licensing terms under the LingPipe plugin directory.
The LingPipe system can be loaded from the GATE GUI by simply selecting the ‘Load
LingPipe System’ menu item under the ‘File’ menu. This is similar to loading the ANNIE
application with default values.

20.25.1

LingPipe Tokenizer PR

As the name suggests this PR tokenizes document text and identiﬁes the boundaries of
tokens. Each token is annotated with an annotation of type ‘Token’. Every annotation has a
feature called ‘length’ that gives a length of the word in number of characters. There are no
initialization parameters for this PR. The user needs to provide the name of the annotation
set where the PR should output Token annotations.

20.25.2

LingPipe Sentence Splitter PR

As the name suggests, this PR splits document text in sentences. It identiﬁes sentence
boundaries and annotates each sentence with an annotation of type ‘Sentence’. There are
no initialization parameters for this PR. The user needs to provide name of the annotation
set where the PR should output Sentence annotations.

20.25.3

LingPipe POS Tagger PR

The LingPipe POS Tagger PR is useful for tagging individual tokens with their respective
part of speech tags. Each document must already have been processed with a tokenizer and
a sentence splitter (any kinds in GATE, not necessarily the LingPipe ones) since this PR
has Token and Sentence annotations as prerequisites. This PR adds a category feature to
each token.
This PR requires a model (dataset from training the tagger on a tagged corpus), which
must be provided as an initialization parameter. Several models are included in this plugin’s

More (CREOLE) Plugins

491

resources directory. Additional models can be downloaded from the LingPipe website4 or
trained according to LingPipe’s instructions5 .
Two models for Bulgarian are now available in GATE: bulgarian-full.model and
bulgarian-simpliﬁed.model, trained on a transformed version of the BulTreeBankDP [Osenova & Simov 04, Simov & Osenova 03, Simov et al. 02, Simov et al. 04a]. The full
model uses the complete tagset [Simov et al. 04b] whereas the simpliﬁed model uses tags
truncated before any hyphens (for example, Pca--p, Pca--s-f, Pca--s-m, Pca--s-n, and
Pce-as-m are all merged to Pca) to improve performance. This reduces the set from 573 to
249 tags and saves memory.
This PR has the following run-time parameters.
inputASName The name of the annotation set with Token and Sentence annotations.
applicationMode The POS tagger can be applied on the text in three diﬀerent modes.
FIRSTBEST The tagger produces one tag for each token (the one that it calculates
is best) and stores it as a simple String in the category feature.
CONFIDENCE The tagger produces the best ﬁve tags for each token, with conﬁdence scores, and stores them as a Map<String, Double> in the category feature.
This application mode requires more memory than the others.
NBEST The tagger produces the ﬁve best taggings for the whole document and
then stores one to ﬁve tags for each token (with document-based scores) as a
Map<String, List<Double>> in the category feature. This application mode is
noticeably slower than the others.

20.25.4

LingPipe NER PR

The LingPipe NER PR is used for named entity recognition. The PR recognizes entities
such as Persons, Organizations and Locations in the text. This PR requires a model which
it then uses to classify text as diﬀerent entity types. An example model is provided under
the ‘resources’ folder of this plugin. It must be provided at initialization time. Similar to
other PRs, this PR expects users to provide name of the annotation set where the PR should
output annotations.

20.25.5

LingPipe Language Identiﬁer PR

As the name suggests, this PR is useful for identifying the language of a document or span
of text. This PR uses a model ﬁle to identify the language of a text. A model is provided
4
5

http://alias-i.com/lingpipe/web/models.html
http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html

492

More (CREOLE) Plugins

in this plugin’s resources/models subdirectory and as the default value of this required
initialization parameter. The PR has the following runtime parameters.
annotationType If this is supplied, the PR classiﬁes the text underlying each annotation
of the speciﬁed type and stores the result as a feature on that annotation. If this is
left blank (null or empty), the PR classiﬁes the text of each document and stores the
result as a document feature.
annotationSetName The annotation set used for input and output; ignored if annotationType is blank.
languageIdFeatureName The name of the document or annotation feature used to store
the results.
Unlike most other PRs (which produce annotations), this one adds either document features
or annotation features. (To classify both whole documents and spans within them, use
two instances of this PR.) Note that classiﬁcation accuracy is better over long spans of
text (paragraphs rather than sentences, for example). More information on the languages
supported can be found in the LingPipe documentation.

20.26

OpenNLP Plugin

OpenNLP provides java-based tools for sentence detection, tokenization, pos-tagging, chunking and parsing, named-entity detection, and coreference. See the OpenNLP website for
details: http://opennlp.sourceforge.net/. The tools use the Maxent machine learning
package. See http://maxent.sourceforge.net/ for details.
In order to use these tools as GATE processing resources, load the ‘OpenNLP’ plugin via
the Plugin Management Console. Alternatively, the OpenNLP system can be loaded from
the GATE GUI by simply selecting the ‘Load OpenNLP System’ menu item under the ‘File’
menu. The OpenNLP PRs will be loaded, together with a pre-conﬁgured corpus pipeline
application containing these PRs. This is similar to loading the ANNIE application with
default values.
We have integrated the following ﬁve processing resources:
OpenNlpTokenizer Tokenizer PR
OpenNlpSentenceSplit Sentence Splitter PR
OpenNlpPOS POS Tagger PR
OpenNlpNameFinder NER PR

More (CREOLE) Plugins

493

OpenNlpChunker Chunker PR
Note that in general, these PRs can be mixed with other PRs of similar types. For example,
you could create a pipeline that uses the OpenNLP Tokenizer, and the ANNIE POS Tagger.
You may occasionally have problems with some combinations. Notes on compatibility and
PR prerequisites are given for each PR in Section 20.26.2.
Note also that some of the OpenNLP tools use quite large machine learning models, which
the PRs need to load into memory. You may ﬁnd that you have to give additional memory
to GATE in order to use the OpenNLP PRs comfortably. See Section 2.7.5 for an example
of how to do this.
Below, we describe the parameters common to all of the OpenNLP PRs. This is followed by
a section which gives brief details of each PR. For more details on each, see the OpenNLP
website, http://opennlp.sourceforge.net/.

20.26.1

Parameters common to all PRs

Load-time Parameters
All OpenNLP PRs have a ‘model’ parameter, which takes a URL. The URL should reference
a valid Maxent model, or in the case of the Name Finder a directory containing a set of models
for the diﬀerent types of name sought. Default models can be found in the ‘models/english’
directory. In addition, the OpenNlpPOS POS Tagger PR has a ‘dictionary’ parameter, which
also takes a URL, and a ‘dictionaryEncoding’ parameter giving the character encoding of
the dictionary ﬁle. The default can be found in the ‘models/english’ directory.
For details of training new models (outside of the GATE framework), see Section 20.26.3
Run-time Parameters
The OpenNLP PRs have runtime parameters to specify the annotation sets they should use
for input and/or output. These are detailed below in the description of each PR, but all
PRs will use the default unnamed annotation set unless told otherwise.

20.26.2

OpenNLP PRs

OpenNlpTokenizer - Tokenizer PR
This PR adds Token annotations to the annotation set speciﬁed by the annotationSetName
parameter.

494

More (CREOLE) Plugins

This PR does not require any other PR to be run beforehand. It creates annotations of
type Token, with a feature and value ‘source=openNLP’ and a string feature that takes the
underlying string as its value.

OpenNlpSentenceSplit - Sentence Splitter PR
This PR adds Sentence annotations to the annotation set speciﬁed by the annotationSetName
parameter.
This PR does not require any other PR to be run beforehand. It creates annotations of type
Sentence, with a feature and value ‘source=openNLP’. If the OpenNLP sentence splitter
returns no output (i.e. fails to ﬁnd any sentences), then GATE will add a single sentence
annotation from oﬀset 0 to the end of the document. This is to prevent downstream PRs
that require sentence annotations from failing.

OpenNlpPOS - POS Tagger PR
This PR adds a feature for Part Of Speech to Token annotations.
This PR requires Sentence and Token annotations to be present in the annotation set speciﬁed by its annotationSetName parameter before it will work. These Sentence and Token
annotations do not have to be from another OpenNLP PR. They could, for example, be
from the ANNIE PRs. This PR adds a ‘category’ feature to each Token, with the predicted
Part Of Speech as value.

OpenNlpNameFinder - NER PR
This PR ﬁnds standard Named Entities, adding them as annotations named for each named
Entity type.
This PR requires Sentence and Token annotations to be present in the annotation set speciﬁed by its inputASName parameter before it will work. These Sentence and Token annotations do not have to be from another OpenNLP PR. They could, for example, be from
the ANNIE PRs. You may ﬁnd, however, that not all pairings of Tokenizer and Sentence
Splitter will work successfully.
The Token annotations do not need to have a ‘category’ feature. In other words, you do not
need to run a POS Tagger before using this PR.
This PR creates annotations for each named entity in the annotation set speciﬁed by the
outputASName parameter, with a feature and value ‘source=openNLP’. It also adds a feature
of ‘type’ to each annotation, with the same values as the annotation type. Types include:

More (CREOLE) Plugins

495

person
organization
location
date
time
percentage
For full details of all types, see the OpenNLP website, http://opennlp.sourceforge.net/.

OpenNlpChunker - Chunker PR
This PR ﬁnds noun, verb, and other chunks, adding their position as features Token annotations.
This PR requires Sentence and Token annotations to be present in the annotation set speciﬁed by the annotationSetName parameter before it will work. The Token annotations need
to have a ‘category’ feature. In other words, you also need to run a POS Tagger before using
this PR. The Sentence and Token annotations (and ‘category’ POS tag features) do not have
to be from another OpenNLP PR. They could, for example, be from the ANNIE PRs.
This PR creates features of type ‘chunk’ for each token. The value of this feature deﬁne
whether the token is at the beginning of a chunk, inside a chunk, or outside a chunk, using
the standard BIO model. Example values and their interpretations are:
B-NP Token is the ﬁrst (beginning) token of a Noun Phrase
I-NP Token is inside a Noun Phrase
B-VP Token is the ﬁrst (beginning) token of a Verb Phrase
I-VP Token is inside a Verb Phrase
O Token is outside any phrase
B-PP Token is the ﬁrst (beginning) token of a Prepositional Phrase
B-ADVP Token is the ﬁrst (beginning) token of an Adverbial Phrase
For full details of all chunk values, see the OpenNLP website, http://opennlp.
sourceforge.net/.

496

20.26.3

More (CREOLE) Plugins

Training new models

Within the OpenNLP framework, new models can be trained for each of the tools. By default,
the GATE PRs use the standard Maxent models for English which can be found in the
plugin’s ‘models/english’ directory. The models are copies of those in the ”models” module
in the OpenNLP CVS repository at http://opennlp.cvs.sourceforge.net. If you need to
train a diﬀerent model for an OpenNLP PR, you will have to do this outside of GATE, and
then use the ﬁle URL of your new model as a value for the ‘model’ parameter of the PR. For
details on how to train models, see the Maxent website http://maxent.sourceforge.net/.

20.27

Tagger MetaMap Plugin

MetaMap, from the National Library of Medicine (NLM), maps biomedical text to the
UMLS Metathesaurus and allows Metathesaurus concepts to be discovered in a text
corpus.
The Tagger MetaMap plugin for GATE wraps the MetaMap Java API client to allow GATE
to communicate with a remote (or local) MetaMap PrologBeans mmserver10 and MetaMap
distribution. This allows the content of speciﬁed annotations (or the entire document content) to be processed by MetaMap and the results converted to GATE annotations and
features.
To use this plugin, you will need access to a remote MetaMap server, or install one locally
by downloading and installing the complete distribution:
http://metamap.nlm.nih.gov/
and Java PrologBeans mmserver
http://metamap.nlm.nih.gov/README_javaapi.html
The default mmserver10 location and port locations are localhost and 8066. To use a
diﬀerent server location and/or port, see the above API documentation and specify the
--metamap server host and --metamap server port options within the metaMapOptions run-time parameter.

20.27.1

Run-time parameters

1. annotateNegEx: set this to true to add NegEx features to annotations (NegExType
and NegExTrigger). See http://code.google.com/p/negex/ for more information
on NegEx
2. annotatePhrases: set to true to output MetaMap phrase-level annotations (generally

More (CREOLE) Plugins

497

noun-phrase chunks). Only phrases containing a MetaMap mapping will be annotated.
Can be useful for post-coordination of phrase-level terms that do not exist in a precoordinated form in UMLS.
3. inputASName: input Annotation Set name.
Use in conjunction with inputASTypes: (see below). Unless speciﬁed, the entire document content will be
sent to MetaMap.
4. inputASTypes: only send the content of these annotations within inputASName
to MetaMap and add new MetaMap annotations inside each. Unless speciﬁed, the
entire document content will be sent to MetaMap.
5. inputASTypeFeature: send the content of this feature within inputASTypes
to MetaMap and wrap a new MetaMap annotation around each annotation in inputASTypes. If the feature is empty or does not exist, then the annotation content is
sent instead.
6. metaMapOptions: set parameter-less MetaMap options here. Default is -Xdt (truncate Candidates mappings, disallow derivational variants and do not use full text
parsing). See http://metamap.nlm.nih.gov/README_javaapi.html for more details.
NB: only set the -y parameter (word-sense disambiguation) if wsdserverctl is running.
7. outputASName: output Annotation Set name.
8. outputASType: output annotation name to be used for all MetaMap annotations
9. outputMode: determines which mappings are output as annotations in the GATE
document, for each phrase:
AllCandidatesAndMappings: annotate both Candidate and ﬁnal mappings.
This will usually result in multiple, overlapping annotations for each term/phrase
AllMappings: annotate all the ﬁnal MetaMap Mappings for each phrase. This
will result in fewer annotations with higher precision (e.g. for ’lung cancer’ only
the complete phrase will be annotated as Neoplastic Process [neop])
HighestMappingOnly: annotate only the highest scoring MetaMap Mapping
for each phrase. If two Mappings have the same score, the ﬁrst returned by
MetaMap is output.
AllCandidates: annotate all Candidate mappings and not the ﬁnal Mappings.
This will result in more annotations with less precision (e.g. for ’lung cancer’ both
’lung’ (bpoc) and ’lung cancer’ (neop) will be annotated).
10. taggerMode: determines whether all term instances are processed by MetaMap, the
ﬁrst instance only, or the ﬁrst instance with coreference annotations added. Only used
if the inputASTypes parameter has been set.

498

More (CREOLE) Plugins

FirstOccurrenceOnly: only process and annotate the ﬁrst instance of each term
in the document
CoReference: process and annotate the ﬁrst instance and coreference following
instances
AllOccurrences: process and annotate all term instances independently

20.28

Content Detection Using Boilerpipe

When working in a closed domain it is often possible to craft a few JAPE rules to separate
real document content from the boilerplate headers, footers, menus, etc. that often appear,
especially when dealing with web documents. As the number of document sources increases,
however, it becomes diﬃcult to separate content from boilerplate using hand crafted rules
and a more general approach is required.
The ‘Tagger Boilerpipe’ plugin contains a PR that can be used to apply the boilerpipe
library (see http://code.google.com/p/boilerpipe/) to GATE documents in order to annotate the content sections. The boilerpipe library is based upon work reported in
[Kohlsch¨tter et al. 10], although it has seen a number of improvements since then. Due
u
to the way in which the library works not all features are currently available through the
GATE PR.
The PR is conﬁgured using the following runtime parameters:
allContent: this parameter deﬁnes how the mime type parameter should be interpreted and if documents should, instead of being processed, by assumed to contain
nothing but actual content. defaults to ‘If Mime Type is NOT Listed’ which means
that any document with a mime type not listed is assumed to be all content.
annotateBoilerplate: should we annotate the boilerplate sections of the document,
defaults to false.
annotateContent: should we annotate the main content of the document, defaults
to true.
boilerplateAnnotationName: the name of the annotation type to annotate sections determined to be boilerplate, defaults to ‘Boilerplate’. Whilst this parameter is
optional is must be speciﬁed if annotateBoilerplate is set to true.
contentAnnotationName: the name of the annotation type to annotate sections
determined to be content, defaults to ‘Content’. Whilst this parameter is optional is
must be speciﬁed if annotateContent is set to true.
debug: if true then annotations created by the PR will contain debugging info, defaults
to false.

More (CREOLE) Plugins

499

extractor: speciﬁes the boilerpipe extractor to use, defaults to the default extractor.
failOnMissingInputAnnotations: if the input annotations (Tokens) are missing
should this PR fail or just not do anything, defaults to true to allow obvious mistakes
in pipeline conﬁguration to be captured at an early stage.
inputASName: the name of the input annotation set
mimeTypes: a set of mime types that control document processing, defaults to text/html. The exact behaviour of the PR is dependent upon both this parameter and
the value of the allContent parameter.
ouputASName: the name of the output annotation set
useHintsFromOriginalMarkups: often the original markups will provide hints that
may be useful for correctly identifying the main content of the document. If true, useful
markup (currently the title, body, and anchor tags) will be used by the PR to help
detect content, defaults to true.

20.29

Inter Annotator Agreement

The IAA plugin, “Inter Annotator Agreement”, computes interannotator agreement measures for various tasks. For named entity annotations, it computes the F-measures, namely
Precision, Recall and F1, for two or more annotation sets. For text classiﬁcation tasks, it
computes Cohen’s kappa and some other IAA measures which are more suitable than the
F-measures for the task. This plugin is fully documented in Section 10.5. Chapter 10 introduces various measures of interannotator agreement and describes a range of tools provided
in GATE for calculating them.

20.30

Schema Annotation Editor

The plugin ‘Schema Annotation Editor’ constrains the annotation editor to permitted types.
See Section 3.4.6 for more information.

500

More (CREOLE) Plugins

Part IV
The GATE Family: Cloud, MIMIR,
Teamware

501

Chapter 21
GATE Cloud
The growth of unstructured content on the internet has resulted in an increased need for researchers in diverse ﬁelds to run language processing and text mining on large-scale datasets,
many of which are impossible to process in reasonable time on standard desktops. However,
in order to take advantage of the on-demand compute power and data storage on the cloud,
NLP researchers currently have to re-write/adapt their algorithms.
Therefore, we have now adapted the GATE infrastructure (and its JAPE rule-based and
machine learning engines) to the cloud and thus enabled researchers to run their GATE
applications without a signiﬁcant overhead. In addition to lowering the barrier to entry,
GATE Cloud also reduces the time required to carry out large-scale NLP experiments by
allowing researchers to harness the on-demand compute power of the cloud.
Cloud computing means many things in many contexts. On GATECloud.net it means:

zero ﬁxed costs: you don’t buy software licences or server hardware, just pay for the
compute time that you use
near zero startup time: in a matter of minutes you can specify, provision and deploy
the type of computation that used to take months of planning
easy in, easy out: if you try it and don’t like it, go elsewhere! you can even take the
software with you, it’s all open source
someone else takes the admin load: - the GATE team from the University of
Sheﬃeld make sure you’re running the best of breed technology for text, search and
semantics
cloud providers’ data center managers (we use Amazon Inc.) make sure the hardware
and operating platform for your work is scaleable, reliable and cheap
503

504

GATE Cloud

GATE is (and always will be) free, but machine time, training, dedicated support and
bespoke development is not. Using GATECloud you can rent cloud time to process large
batches of documents on vast server farms, or academic clusters. You can push a terabyte
of annotated data into an index server and replicate the data across the world. Or just
purchase training services and support for the various tools in the GATE family.

21.1

GATE Cloud services: an overview

At the time of writing, there are several kinds of services, oﬀerred from GATE Cloud, but
they will be growing signiﬁcantly over the course of the next six months, so for an up-to-date
list see https://gatecloud.net/shopfront.
GATE Annotation Services: these allow you to run a GATE application, on the
cloud, over large document collections. The GATE application can be created by the
user in GATE Developer and uploaded on the cloud, or users can use some pre-packaged
applications, e.g., ANNIE, ANNIE with Number and Measurement add-ons.
GATE Teamware (Chapter 22): a web-based collaborative annotation tool, that supports distributed teams of manual annotators and data managers/curators to produce
gold-standard corpora for evaluation and training.
GATE MIMIR (Chapter 23): a multi-paradigm information management index and
repository which can be used to index and search over text, annotations, semantic
schemas (ontologies), and semantic meta-data (instance data). It allows queries that
arbitrarily mix full- text, structural, linguistic and semantic queries and that can scale
to terabytes of text.

21.2

Comparison with other systems

There are several other text-analysis-as-a-service systems out there that do some of what we
do. Here are some diﬀerences:
We’re the only open source solution.
We’re the only customisable solution we support a bring-your-own-annotator option
– a GATE pipeline – as well as pre-packaged entity annotation services like other
systems.
We’re the only end-to-end full lifecycle solution. We don’t just do entity extraction
– we do data preparation, inter-annotator agreement, quality assurance and control,
data visualisation, indexing and search of full text/annotation graph/ontology/instance
store, etc. etc. etc.

GATE Cloud

505

Bulk upload of documents to process, no need to use programming APIs.
No recurring monthly costs, pay-per-use, billed per hour.
No daily limit on number of documents to process.
No limit on document size.
Costs of processing dependent on overall data size, not number of documents.
Web-based collaborative annotation tool to correct mistakes and create training
and evaluation data (see Chapter 22).
Speed: other systems price per document (we price on processing time) – this makes
it impossible to compare like with like (do you really want to compare the processing
of individual tweets against 200 page technical reports?!). GATECloud is also heavily
optimised for high volumes – if you want to do low volumes, you can do them on
your netbook.
Community: we’ve been here for more than 15 years, and our community of developers, users, third party suppliers and so on is second to none.

21.3

How to buy services

Before you can buy any of our cloud based oﬀerings you need to create an account on
GATECloud.net, use the Register link at the top right of any page and follow the instructions.
Once registered and logged in you can browse through the shop and decided on the services
you wish to purchase.
The shop does not handle money but works instead with vouchers bought from the University
of Sheﬃeld’s on-line shop. Vouchers are available in multiples of 5, the amount you need to
purchase will depend upon the services you wish to use. Once you are ready to buy time on
GATECloud.net create an account with the University shop and then buy the appropriate
amount credit vouchers. Be sure to use the same email address when buying vouchers as
when registering for a GATECloud.net account so that credit you purchase can automatically
be added to your GATECloud.net account.
Once you have enough credit you can click through to the checkout where you can review
your basket before ﬁnalizing your order.
Annotation job purchases should appear instantly within your dashboard. Teamware servers
take a little longer to create and we will e-mail you when the server is ready for use.
All past purchases can be monitored and controlled via your dashboard.

506

GATE Cloud

Figure 21.1: Shopping stages

21.4

Pricing and discounts

We run your jobs in the cloud and we pass on the cloud costs, plus a small premium. We do
not have our own private cloud, so each job we run costs us money. Therefore we can’t run
a zero cost service, but we do supply discounts and freebies for people wanting to try the
service. To get a discount:
create an account
use the GATE Cloud contact page to send us your user name and request for discount
we apply a pricing rule to your account
you then shop in the normal manner, as described in Section 21.3 above.
A last word on pricing: the underlying software is all open source, so there’s nothing to stop
you rolling your own if you can’t aﬀord the cloud costs.

GATE Cloud

21.5

507

Annotation Jobs on GATECloud.net

GATECloud.net annotation jobs provide a way to quickly process large numbers of documents using a GATE application, with the results exported to ﬁles in GATE XML or XCES
format and/or sent to a Mimir server for indexing. Annotation jobs are optimized for the
processing of large batches of documents (tens of thousands or more) rather than processing
a small number of documents on the ﬂy (GATE Developer is best suited for the latter).
To submit an annotation job you ﬁrst choose which GATE application you want to run.
GATECloud.net provides some standard pre-packaged applications (e.g., ANNIE), or you
can provide your own application (see Section 21.6). You then upload the documents you
wish to process packaged up into ZIP or (optionally compressed) TAR archives, or ARC
ﬁles (as produced by the Heritrix web crawler), and decide which annotations you would like
returned as output, and in what format.
When the job is started, GATECloud.net takes the document archives you provided and
divides them up into manageable-sized batches of up to 15,000 documents. Each batch is
then processed using the GATE paralleliser and the generated output ﬁles are packaged up
and made available for you to download from the GATECloud.net site when the job has
completed.

21.5.1

The Annotation Service Charges Explained

GATECloud.net annotation jobs run on a public commercial cloud, which charges us per
hour for the processing time we consume. As GATECloud.net allows you to run your own
GATE application, and diﬀerent GATE applications can process radically diﬀerent numbers
of documents in a given amount of time (depending on the complexity of the application)
we cannot adopt the ” x per thousand documents” pricing structure used by other similar
services. Instead, GATECloud.net passes on to you, the user, the per-hour charges we pay
to the cloud provider plus a small mark-up to cover our own costs.
For a given annotation job, we add up the total amount of compute time taken to process
all the individual batches of documents that make up your job (counted in seconds), round
this number up to the next full hour and multiply this by the hourly price for the particular
job type to get the total cost of the job. For example, if your annotation job was priced at
1 per hour and split into three batches that each took 56 minutes of compute time then
the total cost of the job would be 3 (178 minutes of compute time, rounded up to 3 hours).
However, if each batch took 62 minutes to process then the total cost would be 4 (184
minutes, rounded up to 4 hours).
While the job is running, we apply charges to your account whenever a job has consumed
ten CPU hours since the last charge (which takes considerably less than ten real hours as
several batches will typically execute in parallel). If your GATECloud.net account runs out

508

GATE Cloud

of funds at any time, all your currently-executing annotation jobs will be suspended. You
will be able to resume the suspended jobs once you have topped up your account to clear the
negative balance. Note that it is not possible to download the result ﬁles from completed
jobs if your GATECloud.net account is overdrawn.

21.5.2

Annotation Job Execution in Detail

Each annotation job on GATECloud.net consists of a number of individual tasks:
First a single ”split” task which takes the initial document archives that were provided
when the job was conﬁgured and splits them into manageable batches for processing.
– ARC ﬁles are not currently split - each complete ARC ﬁle will be processed as a
single processing task.
– ZIP ﬁles that are smaller than 50MB will not be split, and will be processed as a
single processing task.
– ZIP ﬁles larger than 50MB, and all TAR ﬁles, will be split into chunks of maximum
size 50MB (compressed size) or 15,000 documents, whichever is the smaller. Each
chunk will be processed as a separate processing task.
One or more processing tasks, as determined by the split task described above. Each
processing task will run the GATE application over the documents from its input chunk
as deﬁned by the input speciﬁcation, and save any output ﬁles in ZIP archives of no
more than 100MB, which will be available to download once the job is complete.
A ﬁnal ”join” task to collate the execution logs from the processing tasks and produce
an overall summary report.
Note that because ZIP and TAR input ﬁles may be split into chunks, it is important that
each input document in the archive should be self-contained, for example XML ﬁles should
not refer to a DTD stored elsewhere in the ZIP ﬁle. If your documents do have external
dependencies such as DTDs then you have two choices, you can either (a) use GATE Developer to load your original documents and re-save them as GATE XML format (which is self
contained), or (b) use a custom annotation job (see below) and include the additional ﬁles
in your application ZIP, and refer to them using absolute paths.

21.6

Running Custom Annotation Jobs on GATECloud.net

GATECloud.net provides a way for you to run pretty much any GATE application on the
cloud. You develop your application in the usual way using GATE Developer and then save

GATE Cloud

509

it as a single self-contained ZIP ﬁle, typically using the ”Export for GATECloud.net” option.
This section tells you what you need to know to ensure that your application will run on
GATECloud.net.

21.6.1

Preparing Your Application: The Basics

You supply your GATE application to GATECloud.net as a single ZIP ﬁle, which is expected
to contain a saved application state in the usual ”.xgapp” format, along with all the GATE
plugins, JAPE grammars and other resources that the application requires. The saved application state must be named application.xgapp and must be located at the ’root directory’
of the zip ﬁle (i.e. when the ZIP is unpacked it must leave a ﬁle named application.xgapp
in the directory where the ZIP is unpacked and not in a sub-directory). All URL paths used
by the application should be relative paths that do not contain any ’..’ components, so they
will point to ﬁles in the same directory as application.xgapp or a sub-directory under this
location.

Figure 21.2: Application ZIP structure
The easiest way to build such a package is simply to save your application in GATE Developer
using the ”Export for GATECloud.net” option, which produces a ZIP ﬁle containing an
application.xgapp and all its required resources in one click.

21.6.2

The GATECloud.net environment

For many GATE applications that just use the standard pure-Java ANNIE components, the
basic information above is all you need to know to run your application on GATECloud.net.
But for more advanced applications that involve custom PRs, platform-speciﬁc native helpers
(such as an external tagger), or other components that need to know the path where they
are installed, you will need to know a little more about the environment in which your
application will be running.

510

GATE Cloud

Hardware and software
GATECloud.net annotation jobs are executed on virtual 64-bit (x86 64) Linux servers in the
cloud, speciﬁcally Ubuntu 10.10 (Maverick Meerkat). The GATE application is run using
the open-source GCP tool1 on Sun Java 6 (1.6.0 21). The current oﬀering uses the Amazon
EC2 cloud, and runs jobs on their ’m1.xlarge’ machines which provide 4 virtual CPU cores
and 15GB of memory, of which 13GB is available to the GCP process.
The GCP (GATE Cloud Paralleliser) process is conﬁgured for ’headless’ operation
(-Djava.awt.headless=true), and your code should not assume that a GUI display is
available.
GCP loads one copy of your application.xgapp in the usual way using the PersistenceManager. It then uses the GATE duplication mechanism to make a further 5 independent copies of the loaded application, and runs 6 parallel threads to process your documents. For most PRs this duplication process is essentially equivalent to loading the original
application.xgapp 6 times but if you are writing a custom PR you may wish to consider
implementing a custom duplication strategy.

Directories
The application ZIP ﬁle will always be unpacked in a directory named /gatecloud/application
on the cloud server. Thus the application ﬁle will always be
/gatecloud/application/application.xgapp and if any of your components need to
know the absolute path to their resource ﬁles you can work this out by prepending /gatecloud/application/ to the path of the entry inside your ZIP package.
The user account that runs the GCP process has full read and write access in the
/gatecloud/application directory, so if any of your components need to create temporary
ﬁles then this is a good place to put them. Any ﬁles created under /gatecloud/application
will be lost when the current batch of documents has been processed.
The directory /gatecloud/batch/output is where GCP will write any output ﬁles speciﬁed
by the output deﬁnitions you supply when running an annotation job. All ﬁles created
under this directory will be packaged up into ZIP ﬁles when the batch of documents has
been processed and made available for download when the job has completed. Thus, any
additional output ﬁles that your application creates and that need to be returned to the user
should be placed under /gatecloud/batch/output.
Your code should not assume it has permission to read and write any ﬁles outside these two
locations.
1
Source code is available in the subversion repository at
https://gate.svn.sourceforge.net/svnroot/gate/gcp/trunk

GATE Cloud

511

Native code components
Many PRs are simply wrappers around non-Java tools, for example third-party taggers of
various kinds. If your application requires the use of any non-Java components you must
ensure that the version you include in your ZIP package is the one that will run on Linux
x86 64, and in particular on Ubuntu 10.10. The cloud processing servers have a reasonable
set of packages installed by default, including a basic install of Perl and Python, sed, awk
and bash. To request additional packages please contact GATE Cloud support with your
requirements. If you want to be sure your code will work on GATECloud.net then the
best approach is to sign up for your own account at Amazon Web Services, run your own
instance of the same machine image that GATECloud.net uses and test the software yourself.
As Amazon charges by the hour with no up-front fees this should cost you very little.
As your code will be running in a Linux environment, remember that any native executable
or script that your application needs to call must be marked with execute permission on
the ﬁlesystem. GATECloud.net uses the standard Info-ZIP ”unzip” tool to unpack the
application ZIP package, which respects permission settings speciﬁed in the ZIP ﬁle, so if
you build your package using the corresponding ”zip” tool the permissions will be preserved.
However, many ZIP ﬁle creation tools (including GATE’s ”Export for GATECloud.net”) do
not preserve permissions in this way. Therefore GATECloud.net also supports an alternative
mechanism to mark ﬁles as executable.
Once the application ZIP has been unpacked, we look through the resulting directory tree
for ﬁles named .executables. If any such ﬁle is found, we treat each line in the ﬁle as
a relative path, and set the execute ﬂag on the corresponding ﬁle in the ﬁle system. For
example, imagine the following structure:
application.xgapp
plugins
- MyTagger
- resources
- tagger.sh
- postprocessor.pl

Here, tagger.sh and postprocessor.pl are scripts that need to be marked as executable, so we
could create a ﬁle plugins/MyTagger/.executables containing the two lines:
resources/tagger.sh
resources/postprocessor.pl

or equivalently, create plugins/MyTagger/resources/.executables containing
tagger.sh
postprocessor.pl

512

GATE Cloud

Either way, the eﬀect would be to make the GATECloud.net processing machine mark the
relevant ﬁles as executable before running your application.
Security and privacy
GATECloud.net does not run a separate machine for each annotation job. Instead it splits
each annotation job up into manageable pieces (referred to as tasks), puts these tasks into a
queue, and runs a collection of processing machines (referred to as ”nodes”) that simply take
the next task from the queue whenever they have ﬁnished processing their previous task.
While a task is running it has exclusive use of that particular node - we never run more than
one task on the same node at the same time - but once the task is complete the same node
will then run another task (which may or may not be part of the same annotation job).
To ensure the security and privacy of your code and data, the node takes the following
precautions:
All GCP processes are run as an unprivileged user account which only has write permission in a restricted area of the ﬁlesystem (see above).
At the end of every task, all processes running under that user ID are forcibly terminated (so there’s no risk of a stray or malicious background process started by a
previous task being able to read your data).
The /gatecloud/application and /gatecloud/batch directories are completely
deleted at the end of every task (whether the task completed successfully or failed)
so your data will not be left for the following task to see.

Chapter 22
GATE Teamware: A Web-based
Collaborative Corpus Annotation Tool
Current tools demonstrate that text annotation projects can be approached successfully in
a collaborative fashion. However, we believe that this can be improved further by providing
a uniﬁed environment that provides a multi-role methodological framework to support the
diﬀerent phases and actors in the annotation process. The multi-role support is particularly
important, as it enables the most eﬃcient use of the skills of the diﬀerent people and lowers
overall annotation costs through having simple and eﬃcient annotation web-based UIs for
non-specialist annotators. In this paper we present Teamware, a novel web-based collaborative annotation environment which enables users to carry out complex corpus annotation
projects, involving less skilled, cheaper annotators working remotely from within their web
browsers. It has been evaluated by us through the creation of several gold standard corpora,
as well as through external evaluation in commercial annotation projects.
For technical and user interface details not covered in this chapter, please refer to the
Teamware User Guide.
GATE Teamware is open-source software, released under the GNU Aﬀero General Public
Licence version 3. Commercial licences are available from the University of Sheﬃeld. The
source code is available from the subversion repository at
https://gate.svn.sourceforge.net/svnroot/gate/teamware/trunk

22.1

Introduction

For the past ten years, NLP development frameworks such as OpenNLP, GATE, and UIMA
have been providing tool support and facilitating NLP researchers with the task of implementing new algorithms, sharing, and reusing them. At the same time, Information
513

514

GATE Teamware: Web-based Annotation Tool

Extraction (IE) research and computational linguistics in general has been driven forward
by the growing volume of annotated corpora, produced by research projects and through
evaluation initiatives such as MUC [Marsh & Perzanowski 98], ACE1 , DUC [DUC 01], and
CoNLL shared tasks. Some of the NLP frameworks (e.g., AGTK [Maeda & Strassel 04],
GATE [Cunningham et al. 02]) even provide text annotation user interfaces. However, much
more is needed in order to produce high quality annotated corpora: a stringent methodology,
annotation guidelines, inter-annotator agreement measures, and in some cases, annotation
adjudication (or data curation) to reconcile diﬀerences between annotators.
Current tools demonstrate that annotation projects can be approached in a collaborative
fashion successfully. However, we believe that this can be improved further by providing
a uniﬁed environment that provides a multi-role methodological framework to support the
diﬀerent phases and actors in the annotation process. The multi-role support is particularly
important, as it enables the most eﬃcient use of the skills of the diﬀerent people and lowers
overall annotation costs through having simple and eﬃcient annotation web-based UIs for
non-specialist annotators. This also enables role-based security, project management and
performance measurement of annotators, which are all particularly important in corporate
environments.
This chapter presents Teamware, a web-based software suite and a methodology for the
implementation and support of complex annotation projects. In addition to its research
uses, it has also been tested as a framework for cost-eﬀective commercial annotation services,
supplied either as in-house units or as outsourced specialist activities.
In comparison to previous work Teamware is a novel general purpose, web-based annotation
framework, which:

structures the roles of the diﬀerent actors involved in large-scale corpus annotation
(e.g., annotators, editors, managers) and supports their interactions in an uniﬁed environment;
provides a set of general purpose text annotation tools, tailored to the diﬀerent user
roles, e.g., a curator management tool with inter-annotator agreement metrics and
adjudication facilities and a web-based document tool for in-experienced annotators;
supports complex annotation workﬂows and provides a management console with business process statistics, such as time spent per document by each of its annotators,
percentage of completed documents, etc;
oﬀers methodological support, to complement the diverse technological tool support.
1

http://www.ldc.upenn.edu/Projects/ACE/

GATE Teamware: Web-based Annotation Tool

22.2

515

Requirements for Multi-Role Collaborative Annotation Environments

As discussed above, collaborative corpus annotation is a complex process, which involves
diﬀerent kinds of actors (e.g., annotators, editors, managers) and also requires a diverse
range of pre-processing, a user interface, and evaluation tools. Here we structure all these
into a coherent set of key requirements, which arise from our goal to provide cost-eﬀective
corpus annotation.
Firstly, due to the multiple actors involved and their complex interactions, a collaborative environment needs to support these diﬀerent roles through user groups, access privileges,
and corresponding user interfaces. Secondly, since many annotation projects manipulate
hundreds of documents, there needs to be a remote, eﬃcient data storage. Thirdly, signiﬁcant cost savings can be achieved through pre-annotating corpora automatically, which
in turns requires support for automatic annotation services and their ﬂexible conﬁguration. Last, but not least, a ﬂexible workﬂow engine is required to capture the complex
requirements and interactions.
Next we discuss the four high-level requirements in ﬁner-grained details.

22.2.1

Typical Division of Labour

Due to annotation projects having diﬀerent sizes and complexity, in some cases the same
person might perform more than one role or new roles might be needed. For example, in
small projects it is common that the person who deﬁnes and manages the project is also the
one who carries out quality assurance and adjudication. Nevertheless these are two distinct
roles (manager vs editor), involving diﬀerent tasks and requiring diﬀerent tool support.
Annotators are given a set of annotation guidelines and often work on the same document
independently. This is needed in order to get more reliable results and/or measure how well
humans perform the annotation task (see more on Inter-Annotator Agreement (IAA) below).
Consequently, manual annotation is a slow and error-prone task, which makes overall corpus
production very expensive. In order to allow the involvement of less-specialised annotators,
the manual annotation user interface needs to be simple to learn and use. In addition, there
needs to be an automatic training mode for annotators where their performance is compared
against a known gold standard and all mistakes are identiﬁed and explained to the annotator,
until they have mastered the guidelines.
Since the annotators and the corpus editors are most likely working at diﬀerent locations,
there needs to be a communication channel between them, e.g., instant messaging. If an
editor/manager is not available, an annotator should also be able to mark an annotation
as requiring discussion and then all such annotations should be shown automatically in
the editor console. In addition, the annotation environment needs to restrict annotators

516

GATE Teamware: Web-based Annotation Tool

to working on a maximum of n documents (given as a number or percentage), in order to
prevent an over-zealous annotator from taking over a project and introducing individual
bias. Annotators also need to be able to save their work and, if they close the annotation
tool, the same document must be presented to them for completion the next time they log
in.
From the user interface perspective, there needs to be support for annotating document-level
metadata (e.g., language identiﬁcation), word-level annotations (e.g., named entities, POS
tags), and relations and trees (e.g., co-reference, syntax trees). Ideally, the interface should
oﬀer some generic components for all these, which can be customised with the project-speciﬁc
tags and values via an XML schema or other similar declarative mechanism. The UI also
needs to be extensible, so specialised UIs can easily be plugged in, if required.
Editors or curators are responsible for measuring Inter-Annotator Agreement (IAA), annotation adjudication, gold-standard production, and annotator training. They also need
to communicate with annotators when questions arise. Therefore, they need to have wider
privileges in the system. In addition to the standard annotation interfaces, they need to
have access to the actual corpus and its documents and run IAA metrics. They also need
a specialised adjudication interface which helps them identify and reconcile diﬀerences in
multiply annotated documents. For some annotation projects, they also need to be able to
send a problematic document back for re-annotation.
Project managers are typically in charge of deﬁning new corpus annotation projects and
their workﬂows, monitoring their progress, and dealing with performance issues. Depending
on project speciﬁcs, they may work together with the curators and deﬁne the annotation
guidelines, the associated schemas (or set of tags), and prepare and upload the corpus to
be annotated. They also make methodological choices: whether to have multiple annotators
per document; how many; which automatic NLP services need to be used to pre-process the
data; and what is the overall workﬂow of annotation, quality assurance, adjudication, and
corpus delivery.
Managers need a project monitoring tool where they can see:
Whether a corpus is currently assigned to a project or, what annotation projects have
been run on the corpus with links to these projects or their archive reports (if no longer
active). Also links to the the annotation schemas for all annotation types currently in
the corpus.
Project completion status (e.g., 80% manually annotated, 20% adjudicated).
Annotator statistics within and across projects: which annotator worked on each of
the documents, what schemas they used, how long they took, and what was their IAA
(if measured).
The ability to lock a corpus from further editing, either during or after a project.

GATE Teamware: Web-based Annotation Tool

517

Ability to archive project reports, so projects can be deleted from the active list.
Archives should preserve information on what was done and by whom, how long it
took, etc.

22.2.2

Remote, Scalable Data Storage

Given the multiple user roles and the fact that several annotation projects may need to be
running at the same time, possibly involving diﬀerent, remotely located teams, the data
storage layer needs to scale to accommodate large, distributed corpora and have the necessary security in place through authentication and ﬁne-grained user/group access control.
Data security is paramount and needs to be enforced as data is being sent over the web
to the remote annotators. Support for diverse document input and output formats is also
necessary, especially stand-oﬀ ones when it is not possible to modify the original content.
Since multiple users can be working concurrently on the same document, there needs to be
an appropriate locking mechanism to support that. The data storage layer also needs to
provide facilities for storing annotation guidelines, annotation schemas, and, if applicable,
ontologies. Last, but not least, a corpus search functionality is often required, at least one
based on traditional keyword-based search, but ideally also including document metadata
and linguistic annotations.

22.2.3

Automatic annotation services

Automatic annotation services can reduce signiﬁcantly annotation costs (e.g., annotation of
named entities), but unfortunately they also tend to be domain or application speciﬁc. Also,
several might be needed in order to bootstrap all types that need to be annotated, e.g.,
named entities, co-reference, and relation annotation modules. Therefore, the architecture
needs to be open so that new services can be added easily. Such services can encapsulate
diﬀerent IE modules and take as input one or more documents (or an entire corpus). The
automatic services also need to be scalable, in order to minimise their impact on the overall
project completion time. The project manager should also be able to choose services based
on their accuracy on a given corpus.
Machine Learning (ML) IE modules can be regarded as a speciﬁc kind of automatic service.
A mixed initiative system [Day et al. 97] can be set up by the project manager and used to
facilitate manual annotation behind the scenes. This means that once a document has been
annotated manually, it will be sent to train the ML service which internally generates an
ML model. This model will then be applied by the service on any new document, so that
this document will be partially pre-annotated. The human annotator then only needs to
validate or correct the annotations provided by the ML system, which makes the annotation
task signiﬁcantly faster [Day et al. 97].

518

22.2.4

GATE Teamware: Web-based Annotation Tool

Workﬂow Support

In order to have an open, ﬂexible model of corpus annotation processes, we need a powerful
workﬂow engine which supports asynchronous execution and arbitrary mix of automatic and
manual steps. For example, manual annotation and adjudication tasks are asynchronous.
Resilience to failures is essential and workﬂows need to save intermediary results from time to
time, especially after operations that are very expensive to re-run (e.g. manual annotation,
adjudication). The workﬂow engine also needs to have status persistence, action logging,
and activity monitoring, which is the basis for the project monitoring tools.
In a workﬂow it should be possible for more than one annotator to work on the same
document at the same time, however, during adjudication by editors, all aﬀected annotations
need to be locked to prevent concurrent modiﬁcations. For separation of concerns, it is also
often useful if the same corpus can have more than one active project. Similarly, the same
annotator needs to be able to work on several annotation projects.

Figure 22.1: Teamware Architecture Diagram

22.3

Teamware:
Examples

Architecture, Implementation, and

Teamware is a web-based collaborative annotation and curation environment, which allows
unskilled annotators to be trained and then used to lower the cost of corpus annotation
projects. Further cost reductions are achieved by bootstrapping with relevant automatic
annotation services, where these exist, and/or through mixed initiative learning methods. It
has a service-based architecture which is parallel, distributed, and also scalable (via service
replication) (see Figure 22.1).
As shown in Figure 22.1, the Teamware architecture consists of SOAP web services for
data storage, a set of web-based user interfaces (UI Layer), and an executive layer in the
middle where the workﬂows of the speciﬁc annotation projects are deﬁned. The UI Layer
is connected with the Executive Layer for exchanging command and control messages (such
as requesting the ID for document that needs to be annotated next), and also it connects

GATE Teamware: Web-based Annotation Tool

519

Figure 22.2: Dynamic Workﬂow Conﬁguration: Example

directly to the services layer for data-intensive communication (such as downloading the
actual document data, and uploading back the annotations produced).

22.3.1

Data Storage Service

The storage service provides a distributed data store for corpora, documents, and annotation
schemas. Input documents can be in all major formats (e.g. plain text, XML, HTML, PDF),
based on GATE’s comprehensive support. In all cases, when a document is created/imported
in Teamware, the format is analysed and converted into GATE’s single uniﬁed, graph-based
model of annotation. Then this internal annotation format is used for data exchange between
the service layer, the executive layer and the UI layer. Diﬀerent processes within Teamware
can add and remove annotation data within the same document concurrently, as long as two
processes do not attempt to manipulate the same subset of the data at the same time. A
locking mechanism is used to ensure this and prevent data corruption. The main export format for annotations is currently stand-oﬀ XML, including XCES [Ide et al. 00]. Document
text is represented internally using Unicode and data exchange uses the UTF-8 character
encoding, so Teamware supports documents written in any natural language supported by
the Unicode standard (and the Java platform).

520

GATE Teamware: Web-based Annotation Tool

Figure 22.3: Dynamic Workﬂow Conﬁguration: Example

22.3.2

Annotation Services

The Annotation Services (GAS) provide distribution of compute-intensive NLP tasks over
multiple processors. It is transparent to the external user how many machines are actually
used to execute a particular service. GAS provides a straightforward mechanism for running
applications, created with the GATE framework, as web services that carry out various NLP
tasks. In practical applications we have tested a wide range of services such as named entity
recognition (based on the freely-available ANNIE system [Cunningham et al. 02]), ontology population [Maynard et al. 09], patent processing [Agatonovic et al. 08], and automatic
adjudication of multiple annotation layers in corpora.
The GAS architecture is itself layered, with a separation between the web service endpoint
that accepts requests from clients and queues them for processing, and one or more workers
that take the queued requests and process them. The queueing mechanism used to communicate between the two sides is the Java Messaging System (JMS)2 , a standard framework
for reliable messaging between Java components, and the conﬁguration and wiring together
of all the components is handled using the Spring Framework 3 .
The endpoint, message queue and worker(s) are conceptually and logically separate, and
2
3

http://java.sun.com/products/jms/
http://www.springsource.org/

GATE Teamware: Web-based Annotation Tool

521

may be physically hosted within the same Java Virtual Machine (VM), within separate VMs
on the same physical host, or on separate hosts connected over a network. When a service
is ﬁrst deployed it will typically be as a single worker which resides in the same VM as
the service endpoint. This may be adequate for simple or lightly-loaded services but for
more heavily-loaded services additional workers may be added dynamically without shutting
down the web service, and similarly workers may be removed when no longer required. All
workers that are conﬁgured to consume jobs from the same endpoint will transparently share
the load. Multiple workers also provide fault-tolerance – if a worker fails its in-progress jobs
will be returned to the queue and will be picked up and handled by other workers.

22.3.3

The Executive Layer

Firstly, the executive layer implements authentication and user management, including role
deﬁnition and assignment. In addition, administrators can deﬁne here which UI components
are made accessible to which user roles (the defaults are shown in Figure 22.1).
The second major part is the workﬂow manager, which is based on JBoss jBPM4 and has
been developed to meet most of the requirements discussed in Section 22.2.4 above. Firstly,
it provides dynamic workﬂow management: create, read, update, delete (CRUD) workﬂow
deﬁnitions, and workﬂow actions. Secondly, it supports business process monitoring, i.e.,
measures how long annotators take, how good they are at annotating, as well as reporting
the overall progress and costs. Thirdly, there is a workﬂow execution engine which runs the
actual annotation projects. As part of the execution process, the project manager selects
the number of annotators per document; the annotation schemas; the set of annotators and
curator(s) involved in the project; and the corpus to be annotated.
Figure 22.3 shows an example workﬂow template. The diagram on the right shows the choice
points in workﬂow templates - whether to do automatic annotation or manual or both; which
automatic annotation services to execute and in what sequence; and for manual annotation
– what schemas to use, how may annotators per document, whether they can reject annotating a document, etc. The left-hand side shows the actual selections made for this particular
workﬂow, i.e., use both automatic and manual annotation; annotate measurements, references, and sections; and have one annotator per document. Once this template is saved by
the project manager, then it can be executed by the workﬂow engine on a chosen corpus and
list of annotators and curators. The workﬂow engine will ﬁrst call the automatic annotation
service to bootstrap and then its results will be corrected by human annotators.
The rationale behind having an executive layer rather than deﬁning authentication and
workﬂow management as services similar to the storage and ontology ones comes from the
fact that Teamware services are all SOAP web services, whereas elements of the executive
layer are only in part implemented as SOAP services with the rest being browser based.
Conceptually also the workﬂow manager acts like a middleman that ties together all the
4

http://www.jboss.com/products/jbpm/

522

GATE Teamware: Web-based Annotation Tool

Figure 22.4: The Schema-based Annotator UI

diﬀerent services and communicates with the user interfaces.

22.3.4

The User Interfaces

The Teamware user interfaces are web-based and do not require prior installation. They
either rendered natively in the web browser or, for more complex UIs, a Java Web Start
wrapper is provided around some Swing-based GATE editors (e.g., the document editor and
the ANNIC viewer [Aswani et al. 05]). After the user logs in, the system checks their role(s)
and access privileges to determine which interface elements they are allowed to access.

Annotator User Interface
When manual annotators log into Teamware, they see a very simple web page with one link
to their user proﬁle data and another one – to start annotating documents. The generic
schema-based annotator UI is shown in Figure 22.4 and it is a visual component in GATE,
which is reused here via Java Web Start5 . This removes the need to install GATE on
5

http://java.sun.com/javase/technologies/desktop/javawebstart/index.jsp

GATE Teamware: Web-based Annotation Tool

523

Figure 22.5: Part of the Adjudication UI

the annotator machines and instead they just click on a link to download and start a web
application.
The annotation editor dialog shows the annotation types (or tags) valid for the current
project and optionally their features (or attributes). These are generated automatically from
the annotation schemas assigned to the project by its manager. The annotation editor also
supports the modiﬁcation of annotation boundaries, as well as the use of regular expressions
to annotate multiple matching strings simultaneously. To add a new annotation, one selects
the text with the mouse (e.g., “Bank of England”) and then clicks on the desired annotation
type in the dialog (e.g., Organization). Existing annotations are edited by hovering over
them, which shows their current type and features in the editor dialog.
The toolbar at the top of Figure 22.4 shows all other actions which can be performed. The
ﬁrst button requests a new document to be annotated. When pressed, a request is sent to the
workﬂow manager which checks if there are any pending documents which can be assigned
to this annotator. The second button signals task completion, which saves the annotated
document as completed on the data storage layer and enables the annotator to ask for a new
one (via the ﬁrst button). The third (save) button stores the document without marking it
as completed in the workﬂow. This can be used for saving intermediary annotation results
or if an annotator needs to log oﬀ prior to completing a document. The next time they login
and request a new task, they will be given this document to complete ﬁrst.

Curator User Interface
As discussed above, curators (or editors) carry out quality assurance tasks. In Teamware
the curation tools cover IAA metrics (e.g. precision/recall and kappa) to identify if there
are diﬀerences between annotators; a visual annotation comparison tool to see quickly where
the diﬀerences are per annotation type [Cunningham et al. 02]; and an editor to edit and
reconcile annotations manually (i.e., adjudication) or by using external automatic services.

524

GATE Teamware: Web-based Annotation Tool

The key part of the manual adjudication UI is shown in Figure 22.5: the complete UI shows
also the full document text above the adjudication panel, as well as lists all annotation types
on the right, so the curator can select which one they want to work on. In our example, the
curator has chosen to adjudicate Date annotations created by two annotators and to store
the results in a new consensus annotation set. The adjudication panel has on top arrows
that allow curators to jump from one diﬀerence to the next, thus reducing the required
eﬀort. The relevant text snippet is shown and below it are shown the annotations of the two
annotators. The curator can easily see the diﬀerences and correct them, e.g., by dragging
the correct annotation into the consensus set.

Project Manager Interface
The project manager web UI is the most powerful and multi-functional one. It provides the
front-end to the executive layer (see Section 22.3.3 and Figure 22.3). In a nutshell, managers
upload documents and corpora, deﬁne the annotation schemas, choose and conﬁgure the
workﬂows and execute them on a chosen corpus. The management console also provides
project monitoring facilities, e.g., number of annotated documents, number in progress, and
yet to be completed (see Figure 22.6). Per annotator statistics are also available – time spent
per document, overall time worked, average IAA, etc. These requirements were discussed in
further detail in Section 22.2.1 above.

Figure 22.6: Project Progress Monitoring UI

22.4

Practical Applications

Teamware has already been used in practice in over 10 corpus annotation projects of varying
complexity and size – due to space limitations, here we focus on three representative ones.
Firstly, we tested the robustness of the data layer and the workﬂow manager in the face
of simultaneous concurrent access. For this we annotated 100 documents, 2 annotators per
document, with 60 active annotators requesting documents to annotate and saving their
results on the server. There were no latency or concurrency issues reported.
Once the current version was considered stable, we ran several corpus annotation projects to
produce gold standards for IE evaluation in three domains: business intelligence, ﬁsheries,

GATE Teamware: Web-based Annotation Tool

525

and bio-informatics. The latter involved 10 bio-informatics students which were ﬁrst given a
brief training session and were then allowed to work from home. The project had 2 annotators
per document, working with 6 entity types and their features. Overall, 109 Medline abstracts
of around 200-300 words each were annotated with average annotation speed of 9 minutes per
abstract. This project revealed several shortcomings of Teamware which will be addressed
in the forthcoming version 2:
IAA is calculated per document, but there is no easy way to see how it changes across
the entire corpus.
The datastore layer can sometimes leave the data in an inconsistent state following an
error, due to the underlying binary Java serialisation format. A move towards XML
ﬁle-based storage is being investigated.
There needs to be a limit on the proportion of documents which any given annotator is
allowed to work on, since one over-zealous annotator ended up introducing a signiﬁcant
bias by annotating more than 80% of all documents.
The most versatile and still ongoing practical use of Teamware has been in a commercial
context, where a company has two teams of 5 annotators each (one in China and one in
the Philippines). The annotation projects are being deﬁned and overseen by managers in
the USA, who also act occasionally as curators. They have found that the standard doubleannotated agreement-based approach is a good foundation for their commercial needs (e.g.,
in the early stages of the project and continuously for gold standard production), while they
also use very simple workﬂows where the results of automatic services are being corrected
by annotators, working only one per document to maximise volume and lower the costs. In
the past few months they have annotated over 1,400 documents, many of which according to
multiple schemas and annotation guidelines. For instance, 400 patent documents were doubly
annotated both with measurements (IAA achieved 80-95%) and bio-informatics entities, and
then curated and adjudicated to create a gold standard. They also annotated 1000 Medline
abstracts with species information where they measured average speed of 5-7 minutes per
document. The initial annotator training in Teamware was between 30 minutes and one hour,
following which they ran several small-scale experimental projects to train the annotators
in the particular annotation guidelines (e.g., measurements in patents). Annotation speed
also improved over time, as the annotators became more proﬁcient with the guidelines – the
Teamware annotator statistics registered improvements of between 15 and 20%. Annotation
quality (measured through inter-annotator agreement) remained high, even when annotators
have worked on many documents over time.

526

GATE Teamware: Web-based Annotation Tool

Chapter 23
GATE M´
ımir
M´
ımir 1 is a multi-paradigm information management index and repository which can be
used to index and search over text, annotations, semantic schemas (ontologies), and semantic meta-data (instance data). It allows queries that arbitrarily mix full-text, structural,
linguistic and semantic queries and that can scale to terabytes of text.
Full details on how to build and use M´
ımir can be found in its own user guide. GATE M´
ımir
is open-source software, released under the GNU Aﬀero General Public Licence version
3. Commercial licences are available from the University of Sheﬃeld. The source code is
available from the subversion repository at
https://gate.svn.sourceforge.net/svnroot/gate/mimir/trunk

1

Old Norse “The rememberer, the wise one”.

527

528

GATE M´
ımir

Appendix A
Change Log
This chapter lists major changes to GATE in roughly chronological order by release. Changes
in the documentation are also referenced here.

A.1

April 2011

Three new optional attributes can be speciﬁed in <GATECONFIG> element of gate.xml or
local conﬁguration ﬁle:
addNamespaceFeatures - set to ”true” to deserialize namespace preﬁx and URI
information as features.
namespaceURI - The feature name to use that will hold the namespace URI of the
element, e.g. ”namespace”
namespacePreﬁx - The feature name to use that will hold the namespace preﬁx of
the element, e.g. ”preﬁx”
Setting these attributes will alter GATE’s default namespace deserialization behaviour to
remove the namespace preﬁx and add it as a feature, along with the namespace URI. This
allows namespace-preﬁxed elements in the Original markups annotation set to be matched
with JAPE expressions, and also allows namespace scope to be added to new annotations
when serialized to XML. See 5.5.2 for details.
Searchable Serial Datastores (Lucene-based) are now portable and can be moved across
diﬀerent systems. Also, several GUI improvements have been made to ease the creation of
Lucene datastores. See 9 for details.
The Websphinx Crawler PR (section 20.15) has new runtime parameters for controlling the
maximum page size and spooﬁng the user-agent.
529

530

Change Log

A.2

March 2011

A new creole repository, Teamware Tools, contains a new PR called QA Summariser for
Teamware. When documents are annotated using Teamware, this PR can be used for generating a summary of agreements among annotators. It does this by pairing individual
annotators. It also compares each individual annotator’s annotations with those available in
the consensus annotation set in the respective documents. See Section 10.7 for full details.
A new creole repository, Tagger Measurements, contains a new PR for annotating and normalizing measurements within a document. See Section 20.6 for full details.
A new creole repository, Tagger DateNormalizer, contains a new PR for annotating and
normalizing dates within a document. See Section 20.7 for full details.
The populate method that allowed populating corpus from a trecweb ﬁle has been made
more generic to accept a tag. The method extracts content between the start and end of
this tag to create new documents. In GATE Developer, right-clicking on an instance of the
Corpus and choosing the option “Populate from Single Concatenated File” allows users to
populate the corpus using this functionality. See Section 7.4.5 for more details.

A.3

February 2011

GATE now requires Java 6 or above.
Fixed a regression in the JAPE parser that prevented the use of RHS macros that refer to
a LHS label (named blocks :label { ... } and assignments :label.Type = {}
A new creole repository, Tagger Numbers, containing a number of PRs for annotating numbers with their numeric value. See Section 20.5 for full details.
A new creole repository, Tagger Boilerpipe, which contains a boilerpipe1 based PR for performing content detection. See Section 20.28 for full details.
The Tagger MetaMap plugin has been rewritten to make use of the new MetaMap Java API
features. There are numerous performance enhancements and a bug ﬁx where changes to
the metaMapOptions run-time parameter were previously not enacted. The previous version
of the plugin has been moved to plugins/Obsolete.
Please note that the updated Tagger MetaMap plugin is not parameter-compatible with
the previous version, so your application pipelines will need to be updated. Specifically, the following parameters have been removed: mmServerHost , mmServerPort ,
mmServerTimeout , excludeSemanticTypes, restrictSemanticTypes, scoreThreshold.
1

http://code.google.com/p/boilerpipe/

Change Log

531

These can now be speciﬁed using the --metamap server host, --metamap server port,
--metamap server timeout, -k, -J and -r options in the metaMapOptions run-time parameter string.
outputASType has been made a run-time parameter.
useNegEx has been renamed to annotateNegEx
The following changes have been made to outputMode
MappingsOnly has been renamed to AllMappings
CandidatesOnly has been renamed to AllCandidates
CandidatesAndMappings has been renamed to AllCandidatesAndMappings
In addition, new parameters have been added. See Section 20.27 for full details.

A.4

January 2011

Added a new Schema Enforcer PR that can be used to create a ‘clean’ output annotation
set based on a set of annotation schemas. See Section 20.13 for full details.
Enhanced the Groovy scriptable controller with some features inspired by the realtime controller, in particular the ability to ignore exceptions thrown by PRs and the ability to limit
the running time of certain PRs. See section 7.16.3 for details.
A few bug ﬁxes and improvements to the “recover” logic of the packagegapp Ant task (see
section E.2).

A.5

December 2010

Added support for handling controller events to JAPE by making it possible to deﬁne
ControllerStarted, ControllerFinished, and ControllerAborted code blocks in a JAPE
ﬁle (see section 8.6.5).
JAPE Java right-hand-side code can now access an ActionContext object through the predeﬁned ﬁeld ctx which allows access to the corpus LR and the transducer PR and their
features (see section 8.6.5).

532

A.6
A.6.1

Change Log

Version 6.0 (November 2010)
Major new features

Added an annotation tool for the document editor: the Relation Annotation Tool (RAT). It
is designed to annotate a document with ontology instances and to create relations between
annotations with ontology object properties. It is close and compatible with the Ontology
Annotation Tool (OAT) but focus on relations between annotations. See section 14.7 for
details.
Added a new scriptable controller to the Groovy plugin, whose execution strategy is controlled by a simple Groovy DSL. This supports more powerful conditional execution than
is possible with the standard conditional controllers (for example, based on the presence or
absence of a particular annotation, or a combination of several document feature values),
rich ﬂow control using Groovy loops, etc. See section 7.16.3 for details.
A new version of Alignment Editor has been added to the GATE distribution. It consists
of several new features such as the new alignment viewer, ability to create alignment tasks
and store in xml ﬁles, three diﬀerent views to align the text (links view and matrix view
- suitable for character, word and phrase alignments, parallel view - suitable for sentence
or long text alignment), an alignment exporter and many more. See chapter 18 for more
information.
MetaMap, from the National Library of Medicine (NLM), maps biomedical text to the
UMLS Metathesaurus and allows Metathesaurus concepts to be discovered in a text corpus. The Tagger MetaMap plugin for GATE wraps the MetaMap Java API client to allow
GATE to communicate with a remote (or local) MetaMap PrologBeans mmserver and
MetaMap distribution. This allows the content of speciﬁed annotations (or the entire document content) to be processed by MetaMap and the results converted to GATE annotations
and features. See section 20.27 for details.
A new plugin called Web Translate Google has been added with a PR called Google Translator PR in it. It allows users to translate text using the Google translation services. See
section 20.18 for more information.
New Gazetteer Editor for ANNIE Gazetteer that can be used instead of Gaze. It uses tables
instead of text area to display the gazetteer deﬁnition and lists, allows sorting on any column,
ﬁltering of the lists, reloading a list, etc. See section 13.2.2.

A.6.2

Breaking changes

This release contains a few small changes that are not backwards-compatible:

Change Log

533

Changed the semantics of the ontology-aware matching mode in JAPE to take account of the default namespace in an ontology. Now class feature values that are
not complete URIs will be treated as naming classes within the default namespace of
the target ontology only, and not (as previously) any class whose URI ends with the
speciﬁed name. This is more consistent with the way OWL normally works, as well as
being much more eﬃcient to execute. See section 14.10 for more details.
Updated the WordNet plugin to support more recent releases of WordNet than 1.6.
The format of the conﬁguration ﬁle has changed, if you are using the previous WordNet
1.6 support you will need to update your conﬁguration. See section 20.19 for details.
The deprecated Tagger TreeTagger plugin has been removed, applications that used it
will need to be updated to use the Tagger Framework plugin instead. See section 20.3
for details of how to do this.

A.6.3

Other new features and bugﬁxes

The concept of templates has been introduced to JAPE. This is a way to declare named
“variables” in a JAPE grammar that can contain placeholders that are ﬁlled in when the
template is referenced. See section 8.1.5 for full details.
Added a JAPE operator to get the string covered by a left-hand-side label and assign it to
a feature of a new annotation on the right hand side (see section 8.1.4).
Added a new API to the CREOLE registry to permit plugins that live entirely on the
classpath. CreoleRegister.registerComponent instructs the registry to scan a single java
Class for annotations, adding it to the set of registered plugins. See section 7.3 for details.
Maven artifacts for GATE are now published to the central Maven repository. See section 2.5.1 for details.
Bugﬁx: DocumentImpl no longer changes its stringContent parameter value whenever the
document’s content changes. Among other things, this means that saved application states
will no longer contain the full text of the documents in their corpus, and documents containing XML or HTML tags that were originally created from string content (rather than a
URL) can now safely be stored in saved application states and the GATE Developer saved
session.
A processing resource called Quality Assurance PR has been added in the Tools plugin. The
PR wraps the functionality of the Quality Assurance Tool (section 10.3).
A new section for using the Corpus Quality Assurance from GATE Embedded has been
written. See section 10.3.
The Generic Tagger PR (in the Tagger Framework plugin) now allows more ﬂexible speciﬁcation of the input to the tagger, and is no longer limited to passing just the “string” feature

534

Change Log

from the input annotations. See section 20.3 for details.
Added new parameters and options to the LingPipe Language Identiﬁer PR. (section 20.25.5), and corrected the documentation for the LingPipe POS Tagger (section 20.25.3).
In the document editor, ﬁxed several exceptions to make editing text with annotations
highlighted working. So you should now be able to edit the text and the annotations should
behave correctly that is to say move, expand or disappear according to the text insertions
and deletions.
Options for document editor: read-only and insert append/prepend have been moved from
the options dialogue to the document editor toolbar at the top right on the triangle icon
that display a menu with the options. See section 3.2.
Added new parameters and options to the Crawl PR and document features to its output;
see section 20.15 for details.
Fixed a bug where ontology-aware JAPE rules worked correctly when the target annotation’s
class was a subclass of the class speciﬁed in the rule, but failed when the two class names
matched exactly.
Improved support for conditional pipelines containing non-LanguageAnalyser processing resources.
Added the current Corpus to the script binding for the Groovy Script PR, allowing a Groovy
script to access and set corpus-level features. Also added callbacks that a Groovy script
can implement to do additional pre- or post-processing before the ﬁrst and after the last
document in a corpus. See section 7.16 for details.

A.7

Version 5.2.1 (May 2010)

This is a bugﬁx release to resolve several bugs that were reported shortly after the release of
version 5.2:
Fixed some bugs with the automatic “create instance” feature in OAT (the ontology
annotation tool) when used with the new Ontology plugin.
Added validation to datatype property values of the date, time and datetime types.
Fixed a bug with Gazetteer LKB that prevented it working when the dictionaryPath
contained spaces.
Added a utility class to handle common cases of encoding URIs for use in ontologies,
and ﬁxed the example code to show how to make use of this. See chapter 14 for details.

Change Log

535

The annotation set transfer PR now copies the feature map of each annotation it
transfers, rather than re-using the same FeatureMap (this means that when used to
copy annotations rather than move them, the copied annotation is independent from
the original and modifying the features of one does not modify the other). See section 20.12 for details.
The Log4J log ﬁles are now created by default in the .gate directory under the user’s
home directory, rather than being created in the current directory when GATE starts,
to be more friendly when GATE is installed in a shared location where the user does
not have write permission.

This release also ﬁxes some shortcomings in the Groovy support added by 5.2, in particular:

The corpora variable in the console now includes persistent corpora (loaded from a
datastore) as well as transient corpora.
The subscript notation for annotation sets works with long values as well as ints, so
someAS[annotation.start()..annotation.end()] works as expected.

A.8
A.8.1

Version 5.2 (April 2010)
JAPE and JAPE-related

Introduced a utility class gate.Utils containing static utility methods for frequently-used
idioms such as getting the string covered by an annotation, ﬁnding the start and end oﬀsets
of annotations and sets, etc. This class is particularly useful on the right hand side of JAPE
rules (section 8.6.5).
Added type parameters to the bindings map available on the RHS of JAPE rules, so you can
now do AnnotationSet as = bindings.get("label") without a cast (see section 8.6.5).
Fixed a bug with JAPE’s handling of features called “class” in non-ontology-aware mode.
Previously JAPE would always match such features using an equality test, even if a different operator was used in the grammar, i.e. {SomeType.class != "foo"} was matched
as {SomeType.class == "foo"}. The correct operator is now used. Note that this does
not aﬀect the ontology-aware behaviour: when an ontology parameter is speciﬁed, “class”
features are always matched using ontology subsumption.
Custom JAPE operators and annotation accessors can now be loaded from plugins as well
as from the lib directory (see section 8.2.2).

536

A.8.2

Change Log

Other Changes

Added a mechanism to allow plugins to contribute menu items to the “Tools” menu in GATE
Developer. See section 4.8 for details.
Enhanced Groovy support in GATE: the Groovy console and Groovy Script PR (in the
Groovy plugin) now import many GATE classes by default, and a number of utility methods
are mixed in to some of the core GATE API classes to make them more natural to use in
Groovy. See section 7.16 for details.
Modiﬁed the batch learning PR (in the Learning plugin) to make it safe to use several
instances in APPLICATION mode with the same conﬁguration ﬁle and the same learned
model at the same time (e.g. in a multithreaded application). The other modes (including
training and evaluation) are unchanged, and thus are still not safe for use in this way. Also
ﬁxed a bug that prevented APPLICATION mode from working anywhere other than as the
last PR in a pipeline when running over a corpus in a datastore.
Introduced a simple way to create duplicate copies of an existing resource instance, with a
way for individual resource types to override the default duplication algorithm if they know
a better way to deal with duplicating themselves. See section 7.7.
Enhanced the Spring support in GATE to provide easy access to the new duplication API,
and to simplify the conﬁguration of the built-in Spring pooling mechanisms when writing
multi-threaded Spring-based applications. See section 7.14.
The GAPP packager Ant task now respects the ordering of mapping hints, with earlier hints
taking precedence over later ones (see section E.2.3).
Bug ﬁx in the UIMA plugin from Roland Cornelissen - AnalysisEnginePR now properly
shuts down the wrapped AnalysisEngine when the PR is deleted.
Patch from Matt Nathan to allow several instances of a gazetteer PR in an embedded application to share a single copy of their internal data structures, saving considerable memory
compared to loading several complete copies of the same gazetteer lists (see section 13.11).
In the corpus quality assurance, measures for classiﬁcation tasks have been added. You can
also now set the beta for the fscore. This tool has been optimised to work with datastores
so that it doesn’t need to read all the documents before comparing them.

A.9

Version 5.1 (December 2009)

Version 5.1 is a major increment with lots of new features and integration of a number of
important systems from 3rd parties (e.g. LingPipe, OpenNLP, OpenCalais, a revised UIMA
connector). We’ve stuck with the 5 series (instead of jumping to 6.0) because the core

Change Log

537

remains stable and backwards compatible.
Other highlights include:
an entirely new ontology API from Johann Petrak of OFAI (the old one is still available
but as a plugin)
new benchmarking facilities for JAPE from Andrew Borthwick and colleagues at Intelius
new quality assurance tools from Thomas Heitz and colleagues at Ontotext and
Sheﬃeld
a generic tagger integration framework from Ren´ Witte of Concordia University
e
several new code contributions from Ontotext, including a large knowledge-based
gazetteer and various plugin wrappers from Marin Nozchev, Georgi Georgiev and colleagues
a revised and reordered user guide, amalgamated with the programmers’ guide and
other materials
Groovy support, application composition, section-by-section processing and lots of
other bits and pieces

A.9.1

New Features

LingPipe Support
LingPipe is a suite of Java libraries for the linguistic analysis of human language. We have
provided a plugin called ‘LingPipe’ with wrappers for some of the resources available in the
LingPipe library. For more details, see the section 20.25.

OpenNLP Support
OpenNLP provides tools for sentence detection, tokenization, pos-tagging, chunking and
parsing, named-entity detection, and coreference. The tools use Maximum Entropy modelling. We have provided a plugin called ‘OpenNLP’ with wrappers for some of the resources
available in the OpenNLP Tools library. For more details, see section 20.26.

538

Change Log

OpenCalais Support
We added a new PR called ‘OpenCalais PR’. This will process a document through the
OpenCalais service, and add OpenCalais entity annotations to the document. For more
details, see Section 20.24.

Ontology API
The ontology API (package gate.creole.ontology has been changed, the existing ontology
implementation based on Sesame1 and OWLIM2 (package gate.creole.ontology.owlim)
has been moved into the plugin Ontology_OWLIM2. An upgraded implementation based on
Sesame2 and OWLIM3 that also provides a number of new features has been added as plugin
Ontology. See Section 14.13 for a detailed description of all changes.

Benchmarking Improvements
A number of improvements to the benchmarking support in GATE. JAPE transducers now
log the time spent in individual phases of a multi-phase grammar and by individual rules
within each phase. Other PRs that use JAPE grammars internally (the pronominal coreferencer, English tokeniser) log the time taken by their internal transducers. A reporting
tool, called ‘Proﬁling Reports’ under the ‘Tools’ menu makes summary information easily
available. For more details, see chapter 11.

GUI improvements
To deal with quality assurance of annotations, one component has been updated and two new
components have been added. The annotation diﬀ tool has a new mode to copy annotations
to a consensus set, see section 10.2.1. An annotation stack view has been added in the
document editor and it allows to copy annotations to a consensus set, see section 3.4.3. A
corpus view has been added for all corpus to get statistics like precision, recall and F-measure,
see section 10.3.
An annotation stack view has been added in the document editor to make easier to see
overlapping annotations, see section 3.4.3.

ABNER Support
ABNER is A Biomedical Named Entity Recogniser, for ﬁnding entities such as genes in
text. We have provided a plugin called ‘AbnerTagger’ with a wrapper for ABNER. For more
details, see section 20.8.

Change Log

539

Generic Tagger Support
A new plugin has been added to provide an easy route to integrate taggers with GATE. The
Tagger Framework plugin provides examples of incorporating a number of external taggers
which should serve as a starting point for using other taggers. See Section 20.3 for more
details.

Section-by-Section Processing
We have added a new PR called ‘Segment Processing PR’. As the name suggests this PR
allows processing individual segments of a document independently of one other. For more
details, please look at the section 18.2.10.

Application Composition
The gate.Controller implementations provided with the main GATE distribution now also
implement the gate.ProcessingResource interface. This means that an application can
now contain another application as one of its components.

Groovy Support
Groovy is a dynamic programming language based on Java. You can now use it as a scripting
language for GATE, via the Groovy Console. For more details, see Section 7.16.

A.9.2

JAPE improvements

GATE now produces a warning when any Java right-hand-sides in JAPE rules make use of
the deprecated annotations parameter. All bundled JAPE grammars have been updated
to use the replacement inputAS and outputAS parameters as appropriate.
The new Imports: statement at the beginning of a JAPE grammar ﬁle can now be used to
make additional Java import statements available to the Java RHS code, see 8.6.5.
The JAPE debugger has been removed. Debugging of JAPE has been made easier as stack
traces now refer to the JAPE source ﬁle and line numbers instead of the generated Java
source code.
The Montreal Transducer has been made obsolete.

540

Change Log

A.9.3

Other improvements and bug ﬁxes

Plugin names have been rationalised. Mappings exist so that existing applications will continue to work, but the new names should be used in the future. Plugin name mappings are
given in Appendix B. Also, the Segmenter Chinese plugin (used to be known as chineseSegmenter plugin) is now part of the Lang Chinese plugin.
The User Guide has been amalgamated with the Programmer’s Guide; all material can now
be found in the User Guide. The ‘How-To’ chapter has been converted into separate chapters
for installation, GATE Developer and GATE Embedded. Other material has been relocated
to the appropriate specialist chapter.
Made Mac OS launcher 64-bit compatible. See section 2.2.1 for details.
The UIMA integration layer (Chapter 19) has been upgraded to work with Apache UIMA
2.2.2.
Oracle and PostGreSQL are no longer supported.
The MIAKT Natural Language Generation plugin has been removed.
The Minorthird plugin has been removed. Minorthird has changed signiﬁcantly since this
plugin was written. We will consider writing an up-to-date Minorthird plugin in the future.
A new gazetteer, Large KB Gazetteer (in the plugin ‘Gazetteer LKB’) has been added, see
Section 13.10 for details.
gate.creole.tokeniser.chinesetokeniser.ChineseTokeniser and related resources under the plugins/ANNIE/tokeniser/chinesetokeniser folder have been removed. Please refer to the
Lang Chinese plugin for resources related to the Chinese language in GATE.
Added an isInitialised() method to gate.Gate().
Added a parameter to the chemistry tagger PR (section 20.4) to allow it to operate on
annotation sets other than the default one.
Plus many more smaller bugﬁxes...

A.10

Version 5.0 (May 2009)

Note: existing users – if you delete your user conﬁguration ﬁle for any reason
you will ﬁnd that GATE Developer no longer loads the ANNIE plugin by default.
You will need to manually select ‘load always’ in the plugin manager to get the
old behaviour.

Change Log

A.10.1

541

Major New Features

JAPE Language Improvements
Several new extensions to the JAPE language to support more ﬂexible pattern matching.
Full details are in Chapter 8 but brieﬂy:
Negative constraints, that prevent a rule from matching if certain other annotations
are present (Section 8.1.10).
Additional matching operators for feature values, so you can now look for
{Token.length < 5}, {Lookup.minorType != "ignore"}, etc. as well as simple
equality (Section 8.2.2).
‘Meta-property’ accessors, see Section 8.1.4 to permit access to the string covered by
an annotation, the length of the annotation, etc., e.g. {Lookup@length > 4}.
Contextual operators, allowing you to search for one annotation contained within (or
containing) another, e.g. {Sentence contains {Lookup.majorType == "location"}}
(see Section 8.2.2).
Additional Kleene operator for ranges, e.g. ({Token})[2,5] matches between 2 and
5 consecutive tokens, see Section 8.2.1.
Additional operators can be added via runtime conﬁguration (see Section 8.2.2).
Some of these extensions are similar to, but not the same as, those provided by the Montreal
Transducer plugin. If you are already familiar with the Montreal Transducer, you should
ﬁrst look at Section 8.10 which summarises the diﬀerences.
Resource Conﬁguration via Java 5 Annotations
Introduced an alternative style for supplying resource conﬁguration information via Java 5
annotations rather than in creole.xml. The previous approach is still fully supported as
well, and the two styles can be freely mixed. See Section 4.7 for full details.
Ontology-Based Gazetteer
Added a new plugin ‘Gazetteer Ontology Based’, which contains OntoRoot Gazetteer – a
dynamically created gazetteer which is, in combination with few other generic resources,
capable of producing ontology-aware annotations over the given content with regards to the
given ontology. For more details see Section 13.9.

542

Change Log

Inter-Annotator Agreement and Merging
New plugins to support tasks involving several annotators working on the same annotation task on the same documents. The plugin ‘Inter Annotator Agreement’ (Section 10.5) computes inter-annotator agreement scores between the annotators, the
‘Copy Annots Between Docs’ plugin (Section 20.23) copies annotations from several parallel documents into a single master document, and the ‘Annotation Merging’ plugin (Section 20.22) merges annotations from multiple annotators into a single ‘consensus’ annotation
set.

Packaging Self-Contained Applications for GATE Teamware
Added a mechanism to assemble a saved GATE application along with all the resource ﬁles
it uses into a single self-contained package to run on another machine (e.g. as a service in
GATE Teamware). This is available as a menu option (Section 3.8.4) which will work for
most common cases, but for complex cases you can use the underlying Ant task described
in Section E.2.

GUI Improvements
A new schema-driven tool to streamline manual annotation tasks (see Section 3.4.6).
Context-sensitive help on elements in the resource tree and when pressing F1 key.
Search in mailing list from the Help menu. Help is displayed in your browser or in a
Java browser if you don’t have one.
Improved search function inside documents with a regular expression builder. Search
and replace annotation function in all annotation editors.
Remember for each resource type the last path used when loading/saving a resource.
Remember the last annotations selected in the annotation set view when you shift click
on the annotation set view button.
Improved context menu and when possible added drag and drop in: resource tree,
annotation set view, annotation list view, corpus view, controller view. Context menu
key can be now used if you have Java 1.6.
New dialog box for error messages with user oriented messages, optional display of the
conﬁguration and proposing some useful actions. This will progressively replace the
old stack trace dump into the message panel which is still here for the moment but
should be hide by default in the future.
Add read-only document mode that can be enable from the Options menu.

Change Log

543

Add a selection ﬁlter in the status bar of the annotations list table to easily select rows
based on the text you enter.
Add the last ﬁve applications loaded/saved in the context menu of the language resources in the resources tree.
Display more informations on what going’s on in the waiting dialog box when running
an application. The goal is to improve it to get a global progress bar and estimated
time.

A.10.2

Other New Features and Improvements

New parser plugins: A new plugin for the Stanford Parser (see Section 16.4) and a
rewritten plugin for the RASP NLP tools (Section 16.2).
A new sentence splitter, based on regular expressions, has been added to the ANNIE
plugin. More details in Section 6.5.
‘Real-time’ corpus controller (Section 4.4), which terminates processing of a document
if it takes longer than a conﬁgurable timeout..
Major update to Annie OrthoMatcher coreference engine. Now correctly matches the
sequence ‘David Jones ... David ... David Smith ... David’ as referring to two people.
Also handles nicknames (David = Dave) via a new nickname list. Added optional
parameter ‘highPrecisionOrgs’, which if set to true turns oﬀ riskier org matching rules.
Many misc. bug ﬁxes.
Improved alignment editor (Chapter 18) with several advanced features and an API
for adding your own actions to the editor.
A new plugin for Chinese word segmentation, which is based on our work using machine
learning algorithms for the Sighan-05 Chinese word segmentation task. It can learn a
model from manually segmented text, and apply a learned model to segment Chinese
text. In addition several learned models are available with the plugin, which can be
used to segment text. For details about the plugin and those learned models see Section
15.5.1.
New features in the ML API to produce an n-gram based language model from a corpus
and a so-called ‘document-term matrix’ (see Section 20.14). Also introduced features to
support active learning, a new learning algorithm (PAUM) and various optimisations
including the ability to use an external executable for SVM training. Full details in
Chapter 17.
A new plugin to compute BDM scores for an ontology. The BDM score can be used
to evaluate ontology based information extraction and classiﬁcation. For details about
the plugin see Section 10.6.

544

Change Log

Added new ‘getCovering’ method to AnnotationSet. This method returns annotations
that completely span the provided range. An optional annotation type parameter can
be provided to further limit the returned set.
Complete redesign of ANNIC GUI. More details in Section 9.

A.10.3

Speciﬁc Bug Fixes

HTML document format parser: several bugs ﬁxed, including a null pointer exception
if the document contained certain characters illegal in HTML (#1754749). Also, the
HTML parser now respects the ‘Add space on markup unpack’ conﬁguration option –
previously it would always add space, even if the option was set to false.
Fixed a severe performance bug in the Annie Pronominal Coreferencer resulting in a
50X speed improvement.
JAPE did not always correctly handle the case when the input and output annotation
sets for a transducer were diﬀerent. This has now been ﬁxed.
‘Save Preserving Format’ was not correctly escaping ampersands and less than signs
when two HTML entities are close together. Only the ﬁrst one was replaced: A & B
& C was output as A &amp; B & C instead of A &amp; B &amp; C. This has now
been ﬁxed, and the ﬁx is also valid for the ﬂexible exporter but only if the standoﬀ
annotations parameter is set to false.

Plus many more minor bug ﬁxes

A.11

Version 4.0 (July 2007)

A.11.1

Major New Features

ANNIC
ANNotations In Context: a full-featured annotation indexing and retrieval system designed
to support corpus querying and JAPE rule authoring. It is provided as part of an extension
of the Serial Datastores, called Searchable Serial Datastore (SSD). See Section 9 for more
details.

Change Log

545

New Machine Learning API
A brand new machine learning layer speciﬁcally targetted at NLP tasks including text classiﬁcation, chunk learning (e.g. for named entity recognition) and relation learning. See
Chapter 17 for more details.
Ontology API
A new ontology API, based on OWL In Memory (OWLIM), which oﬀers a better API, revised
ontology event model and an improved ontology editor to name but few. See Chapter 14 for
more details.
OCAT
Ontology-based Corpus Annotation Tool to help annotators to manually annotate documents
using ontologies. For more details please see Section 14.6.
Alignment Tools
A new set of components (e.g. CompoundDocument, AlignmentEditor etc.) that help in
building alignment tools and in carrying out cross-document processing. See Chapter 18 for
more details.
New HTML Parser
A new HTML document format parser, based on Andy Clark’s NekoHTML. This parser is
much better than the old one at handling modern HTML and XHTML constructs, JavaScript
blocks, etc., though the old parser is still available for existing applications that depend on
its behaviour.
Java 5.0 Support
GATE now requires Java 5.0 or later to compile and run. This brings a number of beneﬁts:
Java 5.0 syntax is now available on the right hand side of JAPE rules with the default
Eclipse compiler. See Section D.5 for details.
enum types are now supported for resource parameters. see Section 7.11 for details on
deﬁning the parameters of a resource.

546

Change Log

AnnotationSet and the CreoleRegister take advantage of generic types. The
AnnotationSet interface is now an extension of Set<Annotation> rather than just
Set, which should make for cleaner and more type-safe code when programming to the
API, and the CreoleRegister now uses parameterized types, which are backwardscompatible but provide better type-safety for new code.

A.11.2

Other New Features and Improvements

Hiding the view for a particular resource (by right clicking on its tab and selecting
‘Hide this view’) will now completely close the associated viewers and dispose them.
Re-selecting the same resource at a later time will lead to re-creating the necessary
viewers and displaying them. This has two advantages: ﬁrstly it oﬀers a mechanism
for disposing views that are not needed any more without actually closing the resource
and secondly it provides a way to refresh the view of a resource in the situations where
it becomes corrupted.
The DataStore viewer now allows multiple selections. This lets users load or delete an
arbitrarily large number of resources in one operation.
The Corpus editor has been completely overhauled. It now allows re-ordering of documents as well as sorting the document list by either index or document name.
Support has been added for resource parameters of type gate.FeatureMap, and it is
also possible to specify a default value for parameters whose type is Collection, List
or Set. See Section 7.3 for details.
(Feature Request #1446642) After several requests, a mechanism has been added to
allow overriding of GATE’s document format detection routine. A new creation-time
parameter mimeType has been added to the standard document implementation, which
forces a document to be interpreted as a speciﬁc MIME type and prevents the usual
detection based on ﬁle name extension and other information. See Section 5.5.1 for
details.
A capability has been added to specify arbitrary sets of additional features on individual
gazetteer entries. These features are passed forward into the Lookup annotations
generated by the gazetteer. See Section 6.3 for details.
As an alternative to the Google plugin, a new plugin called yahoo has been added to
to allow users to submit their query to the Yahoo search engine and to load the found
pages as GATE documents. See Section 20.17 for more details.
It is now easier to run a corpus pipeline over a single document in the GATE Developer
GUI – documents now provide a right-click menu item to create a singleton corpus
containing just this document. See Section 3.3 for details.

Change Log

547

A new interface has been added that lets PRs receive notiﬁcation at the start and
end of execution of their containing controller. This is useful for PRs that need to do
cleanup or other processing after a whole corpus has been processed. See Section 4.4
for details.
The GATE Developer GUI does not call System.exit() any more when it is closed.
Instead an eﬀort is made to stop all active threads and to release all GUI resources,
which leads to the JVM exiting gracefully. This is particularly useful when GATE is
embedded in other systems as closing the main GATE window will not kill the JVM
process any more.
The set of AnnotationSchemas that used to be included in the core gate.jar and loaded
as builtins have now been moved to the ANNIE plugin. When the plugin is loaded,
the default annotation schemas are instantiated automatically and are available when
doing manual annotation.
There is now support in creole.xml ﬁles for automatically creating instances of a resource that are hidden (i.e. do not show in the GUI). One example of this can be seen
in the creole.xml ﬁle of the ANNIE plugin where the default annotation schemas are
deﬁned.
A couple of helper classes have been added to assist in using GATE within a Spring
application. Section 7.14 explains the details.
Improvements have been made to the thread-safety of some internal components, which
mean that it is now safe to create resources in multiple threads (though it is not safe
to use the same resource instance in more than one thread). This is a big advantage
when using GATE in a multithreaded environment, such as a web application. See
Section 7.13 for details.
Plugins can now provide custom icons for their PRs and LRs in the plugin JAR ﬁle.
See Section 7.11 for details.
It is now possible to override the default location for the saved session ﬁle using a
system property. See Section 2.3 for details.
The TreeTagger plugin (‘Tagger TreeTagger’) supports a system property to specify
the location of the shell interpreter used for the tagger shell script. In combination
with Cygwin this makes it much easier to use the tagger on Windows.
The Buchart plugin has been removed. It is superseded by SUPPLE, and instructions on how to upgrade your applications from Buchart to SUPPLE are given in
Section 16.3. The probability ﬁnder plugin has also been removed, as it is no longer
maintained.
The bootstrap wizard now creates a basic plugin that builds with Ant. Since a Unixstyle make command is no longer required this means that the generated plugin will
build on Windows without needing Cygwin or MinGW.

548

Change Log

The GATE source code has moved from CVS into Subversion. See Section 2.2.3 for
details of how to check out the code from the new repository.
An optional parameter, keepOriginalMarkupsAS, has been added to the DocumentReset PR which allows users to decide whether to keep the Original Markups AS or not
while reseting the document. See Section 6.1 for more details.

A.11.3

Bug Fixes and Optimizations

The Morphological Analyser has been optimized. A new FSM based, although with
minor alteration to the basic FSM algorithm, has been implemented to optimize the
Morphological Analyser. The previous proﬁling ﬁgures show that the morpher when integrated with ANNIE application used to take upto 60% of the overall processing time.
The optimized version only takes 7.6% of the total processing time. See Section 20.10
for more details on the morpher.
The ANNIE Sentence Splitter was optimised. The new version is about twice as fast
as the previous one. The actual speed increase varies widely depending on the nature
of the document.
The imlementation of the OrthoMatcher component has been improved. This resources
takes signiﬁcantly less time on large documents.
The implementation of AnnotationSets has been improved. GATE now requires
up to 40% less memory to run and is also 20% faster on average. The get methods of AnnotationSet return instances of ImmutableAnnotationSet. Any attempt
at modifying the content of these objects will trigger an Exception. An empty
ImmutableAnnotationSet is returned instead of null.
The Chemistry tagger (Section 20.4) has been updated with a number of bugﬁxes and
improvements.
The Document user interface has been optimised to deal better with large bursts of
events which tend to occur when the document that is currently displayed gets modiﬁed. The main advantages brought by this new implementation are:
– The document UI refreshes faster than before.
– The presence of the GUI for a document induces a smaller performance penalty
than it used to. Due to a better threading implementation, machines beneﬁting from multiple CPUs (e.g. dual CPU, dual core or hyperthreading machines)
should only see a negligible increase in processing time when a document is displayed compared to the situations where the document view is not shown. In the
previous version, displaying a document while it was processed used to increase
execution time by an order of magnitude.

Change Log

549

– The GUI is more responsive now when a large number of annotations are displayed, hidden or deleted.
– The strange exceptions that used to occur occasionally while working with the
document GUI should not happen any more.
And as always there are many smaller bugﬁxes too numerous to list here...

A.12

Version 3.1 (April 2006)

A.12.1

Major New Features

Support for UIMA
UIMA (http://www.research.ibm.com/UIMA/) is a language processing framework developed by IBM. UIMA and GATE share some functionality but are complementary in most
respects. GATE now provides an interoperability layer to allow UIMA applications to include
GATE components in their processing and vice-versa. For full information, see Chapter19.
New Ontology API
The ontology layer has been rewritten in order to provide an abstraction layer between the
model representation and the tools used for input and output of the various representation
formats. An implementation that uses Jena 2 (http://jena.sourceforge.net/ontology) for
reading and writing OWL and RDF(S) is provided.
Ontotext Japec Compiler
Japec is a compiler for JAPE grammars developed by Ontotext Lab. It has some limitations
compared to the standard JAPE transducer implementation, but can run JAPE grammars
up to ﬁve times as fast. By default, GATE still uses the stable JAPE implementation, but
if you want to experiment with Japec, see Section 20.21.

A.12.2

Other New Features and Improvements

Addition of a new JAPE matching style ‘all’. This is similar to Brill, but once all rules
from a given start point have matched, the matching will continue from the next oﬀset
to the current one, rather than from the position in the document where the longest
match ﬁnishes. More details can be found in Section 8.4.

550

Change Log

Limited support for loading PDF and Microsoft Word document formats. Only the
text is extracted from the documents, no formatting information is preserved.
The Buchart parser has been deprecated and replaced by a new plugin called SUPPLE the Sheﬃeld University Prolog Parser for Language Engineering. Full details, including
information on how to move your application from Buchart to SUPPLE, is in Section
16.3.
The Hepple POS Tagger is now open-source. The source code has been included in the
GATE Developer/Embedded distribution, under src/hepple/postag. More information
about the POS Tagger can be found in Section 6.6.
Minipar is now supported on Windows. minipar-windows.exe, a modiﬁed version of
pdemo.cpp is added under the gate/plugins/Parser Minipar directory to allow users
to run Minipar on windows platform. While using Minipar on Windows, this binary
should be provided as a value for miniparBinary parameter. For full information on
Minipar in GATE, see Section 16.1.
The XmlGateFormat writer(Save As Xml from GATE Developer GUI, gate.Document.toXml()
from GATE Embedded API) and reader have been modiﬁed to write and read GATE
annotation IDs. For backward compatibility reasons the old reader has been kept.
This change ﬁxes a bug which manifested in the following situation: If a GATE document had annotations carrying features of which values were numbers representing
other GATE annotation IDs, after a save and a reload of the document to and from
XML, the former values of the features could have become invalid by pointing to other
annotations. By saving and restoring the GATE annotation ID, the former consistency
of the GATE document is maintained. For more information, see Section 5.5.2.
The NP chunker and chemistry tagger plugins have been updated. Mark A. Greenwood
has relicenced them under the LGPL, so their source code has been moved into the
GATE Developer/Embedded distribution. See Sections 20.2 and 20.4 for details.
The Tree Tagger wrapper has been updated with an option to be less strict when
characters that cannot be represented in the tagger’s encoding are encountered in the
document.
JAPE Transducers can be serialized into binary ﬁles. The option to load serialized
version of JAPE Transducer (an init-time parameter binaryGrammarURL) is also implemented which can be used as an alternative to the parameter grammarURL. More
information can be found in Section 8.9.
On Mac OS, GATE Developer now behaves more ‘naturally’. The application menu
items and keyboard shortcuts for About and Preferences now do what you would expect,
and exiting GATE Developer with command-Q or the Quit menu item properly saves
your options and current session.
Updated versions of Weka(3.4.6) and Maxent(2.4.0).

Change Log

551

Optimisation in gate.creole.ml: the conversion of AnnotationSet into ML examples is
now faster.
It is now possible to create your own implementation of Annotation, and have
GATE use this instead of the default implementation. See AnnotationFactory and
AnnotationSetImpl in the gate.annotation package for details.

A.12.3

Bug Fixes

The Tree Tagger wrapper has been updated in order to run under Windows.
The SUPPLE parser has been made more user-friendly. It now produces more helpful
error messages if things go wrong. Note that you will need to update any saved
applications that include SUPPLE to work with this version - see Section 16.3 for
details.
Miscellaneous ﬁxes in the Ontotext JapeC compiler.
Optimization : the creation of a Document is much faster.
Google plugin: The optional pagesToExclude parameter was causing a NullPointerException when left empty at run time. Full details about the plugin functionality can
be found in Section 20.16.
Minipar, SUPPLE, TreeTagger: These plugins that call external processes have been
ﬁxed to cope better with path names that contain spaces. Note that some of the
external tools themselves still have problems handling spaces in ﬁle names, but these
are beyond our control to ﬁx. If you want to use any of these plugins, be sure to read
the documentation to see if they have any such restrictions.
When using a non-default location for GATE conﬁguration ﬁles, the conﬁguration data
is saved back to the correct location when GATE exits. Previously the default locations
were always used.
Jape Debugger: ConcurrentModiﬁcationException in JAPE debugger. The JAPE
debugger was generating a ConcurrentModiﬁcationException during an attempt to
run ANNIE. There is no exception when running without the debugger enabled. As
result of ﬁxing one unnecessary and incorrect callback to debugger was removed from
SinglePhaseTransducer class.
Plus many other small bugﬁxes...

552

Change Log

A.13

January 2005

Release of version 3.
New plugins for processing in various languages (see 15). These are not full IE systems but
are designed as starting points for further development (French, German, Spanish, etc.), or
as sample or toy applications (Cebuano, Hindi, etc.).
Other new plugins:
Chemistry Tagger 20.4
Montreal Transducer (since retired)
RASP Parser 16.2
MiniPar 16.1
Buchart Parser 16.3
MinorThird (Version 5.1: removed)
NP Chunker 20.2
Stemmer 20.9
TreeTagger
Probability Finder
Crawler 20.15
Google PR 20.16
Support for SVM Light, a support vector machine implementation, has been added to the
machine learning plugin ‘Learning’ (see section 17.3.5).

A.14

December 2004

GATE no longer depends on the Sun Java compiler to run, which means it will now work
on any Java runtime environment of at least version 1.4. JAPE grammars are now compiled
using the Eclipse JDT Java compiler by default.
A welcome side-eﬀect of this change is that it is now much easier to integrate GATE-based
processing into web applications in Tomcat. See Section 7.15 for details.

Change Log

A.15

553

September 2004

GATE applications are now saved in XML format using the XStream library, rather than
by using native java serialization. On loading an application, GATE will automatically
detect whether it is in the old or the new format, and so applications in both formats
can be loaded. However, older versions of GATE will be unable to load applications saved
in the XML format. (A java.io.StreamCorruptedException: invalid stream header
exception will occcur.) It is possible to get new versions of GATE to use the old format by
setting a ﬂag in the source code. (See the Gate.java ﬁle for details.) This change has been
made because it allows the details of an application to be viewed and edited in a text editor,
which is sometimes easier than loading the application into GATE.

A.16

Version 3 Beta 1 (August 2004)

Version 3 incorporates a lot of new functionality and some reorganisation of existing components.
Note that Beta 1 is feature-complete but needs further debugging (please send us bug reports!).
Highlights include: completely rewritten document viewer/editor; extensive ontology support; a new plugin management system; separate .jar ﬁles and a Tomcat classloading ﬁx;
lots more CREOLE components (and some more to come soon).
Almost all the changes are backwards-compatible; some recent classes have been renamed
(particularly the ontologies support classes) and a few events added (see below); datastores
created by version 3 will probably not read properly in version 2. If you have problems use
the mailing list and we’ll help you ﬁx your code!
The gorey details:
Anonymous CVS is now available. See Section 2.2.3 for details.
CREOLE repositories and the components they contain are now managed as plugins.
You can select the plugins the system knows about (and add new ones) by going to
‘Manage CREOLE Plugins’ on the ﬁle menu.
The gate.jar ﬁle no longer contains all the subsidiary libraries and CREOLE component resources. This makes it easier to replace library versions and/or not load them
when not required (libraries used by CREOLE builtins will now not be loaded unless
you ask for them from the plugins manager console).
ANNIE and other bundled components now have their resource ﬁles (e.g. pattern ﬁles,
gazetteer lists) in a separate directory in the distribution – gate/plugins.

554

Change Log

Some testing with Sun’s JDK 1.5 pre-releases has been done and no problems reported.
The gate:// URL system used to load CREOLE and ANNIE resources in past releases
is no longer needed. This means that loading in systems like Tomcat is now much easier.
MAC OS X is now properly supported by the installed and the runtime.
An Ontology-based Corpus Annotation Tool (OCAT) has been implemented as a plugin. Documentation of its functionality is in Section 14.6.
The NLG Lexical tools from the MIAKT project have now been released.
The Features viewer/editor has been completely updated – see Section 3.4.5 for details.
The Document editor has been completely rewritten – see Section 3.2 for more information.
The datastore viewer is now a full-size VR – see Section 3.8.2 for more information.

A.17

July 2004

GATE documents now ﬁre events when the document content is edited. This was added in
order to support the new facility of editing documents from the GUI. This change will break
backwards compatibility by requiring all DocumentListener implementations to implement
a new method:
public void contentEdited(DocumentEvent e);

A.18

June 2004

A new algorithm has been implemented for the AnnotationDiﬀ function. A new, more
usable, GUI is included, and an ‘Export to HTML’ option added. More details about the
AnnotationDiﬀ tool are in Section 10.2.1.
A new build process, based on ANT (http://ant.apache.org/) is now available. The old build
process, based on make, is now unsupported. See Section 2.5 for details of the new build
process.
A Jape Debugger from Ontos AG has been integrated. You can turn integration ON with
command line option ‘-j’. If you run GATE Developer with this option, the new menu item
for Jape Debugger GUI will appear in the Tools menu. The default value of integration is
OFF. We are currently awaiting documentation for this.

Change Log

555

NOTE! Keep in mind there is ClassCastException if you try to debug ConditionalCorpusPipeline. Jape Debugger is designed for Corpus Pipeline only. The Ontos code needs to be
changed to allow debugging of ConditionalCorpusPipeline.

A.19

April 2004

There are now two alternative strategies for ontology-aware grammar transduction:
using the [ontology] feature both in grammars and annotations; with the default Transducer.
using the ontology aware transducer – passing an ontology LR to a new subsume
method in the SimpleFeatureMapImpl. the latter strategy does not check for ontology
features (this will make the writing of grammars easier – no need to specify ontology).
The changes are in:
SinglePhaseTransducer (always call subsume with ontology – if null then the ordinary
subsumption takes place)
SimpleFeatureMapImpl (new subsume method using an ontology LR)
More information about the ontology-aware transducer can be found in Section 14.10.
A morphological analyser PR has been added. This ﬁnds the root and aﬃx values of a token
and adds them as features to that token.
A ﬂexible gazetteer PR has been added. This performs lookup over a document based on
the values of an arbitrary feature of an arbitrary annotation type, by using an externally
provided gazetteer. See 13.7 for details.

A.20

March 2004

Support was added for the MAXENT machine learning library. (See 17.3.4 for details.)

A.21

Version 2.2 – August 2003

Note that GATE 2.2 works with JDK 1.4.0 or above. Version 1.4.2 is recommended, and is
the one included with the latest installers.

556

Change Log

GATE has been adapted to work with Postgres 7.3. The compatibility with PostgreSQL 7.2
has been preserved.
Note that as of Version 5.1 PostgreSQL is no longer supported.
New library version – Lucene 1.3 (rc1)
A bug in gate.util.Javac has been ﬁxed in order to account for situations when String literals
require an encoding diﬀerent from the platform default.
Temporary .java ﬁles used to compile JAPE RHS actions are now saved using UTF-8 and
the ‘-encoding UTF-8’ option is passed to the javac compiler.
A custom tools.jar is no longer necessary
Minor changes have been made to the look and feel of GATE Developer to improve its
appearance with JDK 1.4.2

A.22

Version 2.1 – February 2003

Integration of Machine Learning PR and WEKA wrapper (see Section 17.3).
Addition of DAML+OIL exporter.
Integration of WordNet (see Section 20.19).
The syntax tree viewer has been updated to ﬁx some bugs.

A.23

June 2002

Conditional versions of the controllers are now available (see Section 3.7.2). These allow
processing resources to be run conditionally on document features.
PostgreSQL Datastores are now supported.
These store data into a PostgreSQL RDBMS.
(As of Version 5.1 PostgreSQL is no longer supported.)
Addition of OntoGazetteer (see Section 13.4), an interface which makes ontologies visible
within GATE Developer, and supports basic methods for hierarchy management and traversal.
Integration of Prot´g´, so that people with developed Prot´g´ ontologies can use them within
e e
e e

Change Log

557

GATE.
Addition of IR facilities in GATE (see Section 20.14).
Modiﬁcation of the corpus benchmark tool (see Section 10.4.3), which now takes an application as a parameter.
See also for details of other recent bug ﬁxes.

558

Change Log

Appendix B
Version 5.1 Plugins Name Map
In version 5.1 we attempted to impose order on chaos by further deﬁning the plugin naming
convention (see Section 12.3.2) and renaming those existing plugins that did not conform to
it. Below, you will ﬁnd a mapping of old plugin names to new.

559

560
Old Name
abner
alignment
annotationMerging
arabic
bdmComputation
cebuano
Chemistry Tagger
chinese
chineseSegmenter
copyAS2AnoDoc
crawl
french
german
google
hindi
iaaPlugin
italian
Kea
learning
lkb gazetteer
Minipar
NP Chunking
Ontology Based Gazetteer
OpenCalais
openNLP
rasp
romanian
Stanford
Stemmer
SUPPLE
TaggerFramework
TreeTagger
uima
yahoo

Version 5.1 Plugins Name Map
New Name
Tagger Abner
Alignment
Annotation Merging
Lang Arabic
Ontology BDM Computation
Lang Cebuano
Tagger Chemistry
Lang Chinese
Lang Chinese
Copy Annots Between Docs
Web Crawler Websphinx
Lang French
Lang German
Web Search Google
Lang Hindi
Inter Annotator Agreement
Lang Italian
Keyphrase Extraction Algorithm
Learning
Gazetteer LKB
Parser Minipar
Tagger NP Chunking
Gazetteer Ontology Based
Tagger OpenCalais
OpenNLP
Parser RASP
Lang Romanian
Parser Stanford
Stemmer Snowball
Parser SUPPLE
Tagger Framework
Tagger TreeTagger
UIMA
Web Search Yahoo

Appendix C
Design Notes
Why has the pleasure of slowness disappeared? Ah, where have they gone, the
amblers of yesteryear? Where have they gone, those loaﬁng heroes of folk song,
those vagabonds who roam from one mill to another and bed down under the
stars? Have they vanished along with footpaths, with grasslands and clearings,
with nature? There is a Czech proverb that describes their easy indolence by
a metaphor: ‘they are gazing at God’s windows.’ A person gazing at God’s
windows is not bored; he is happy. In our world, indolence has turned into having
nothing to do, which is a completely diﬀerent thing: a person with nothing to do
is frustrated, bored, is constantly searching for an activity he lacks.
Slowness, Milan Kundera, 1995 (pp. 4-5).
GATE is a backplane into which specialised Java Beans plug. These beans are loose-coupled
with respect to each other - they communicate entirely by means of the GATE framework.
Inter-component communication is handled by model components - LanguageResources, and
events.
Components are deﬁned by conformance to various interfaces (e.g. LanguageResource),
ensuring separation of interface and implementation.
The reason for adding to the normal bean initialisation mech is that LRs, PRs and VRs all
have characteristic parameterisation phases; the GATE resources/components model makes
explicit these phases.

C.1

Patterns

GATE is structured around a number of what we might call principles, or patterns, or
alternatively, clever ideas stolen from better minds than mine. These patterns are:
561

562

Design Notes

modelling most things as extensible sets of components (cf. Section C.1.1);
separating components into model, view, or controller (cf. Section C.1.2) types;
hiding implementation behind interfaces (cf. Section C.1.3).
Four interfaces in the top-level package describe the GATE view of components: Resource,
ProcessingResource, LanguageResource and VisualResource.

C.1.1

Components

Architectural Principle
Wherever users of the architecture may wish to extend the set of a particular type of entity,
those types should be expressed as components.
Another way to express this is to say that the architecture is based on agents. I’ve avoided
this in the past because of an association between this term and the idea of bits of code
moving around between machines of their own volition. I take this to be somewhat pointless,
and probably the result of an anthropomorphic obsession with mobility as a correlate of
intelligence. If we drop this connotation, however, we can say that GATE is an agent-based
architecture. If we want to, that is.

Framework Expression
Many of the classes in the framework are components, by which we mean classes that conform
to an interface with certain standard properties. In our case these properties are based on the
Java Beans component architecture, with the addition of component metadata, automated
loading and standardised storage, threading and distribution.
All components inherit from Resource, via one of the three sub-interfaces LanguageResource
(LR), VisualResource (VR) or ProcessingResource (PR) VisualResources (VRs) are straightforward – they represent visualisation and editing components that participate in GUIs –
but the distinction between language and processing resources merits further discussion.
Like other software, LE programs consist of data and algorithms. The current orthodoxy in
software development is to model both data and algorithms together, as objects1 . Systems
that adopt the new approach are referred to as Object-Oriented (OO), and there are good
reasons to believe that OO software is easier to build and maintain than other varieties
[Booch 94, Yourdon 96].
1
Older development methods like Jackson Structured Design [Jackson 75] or Structured Analysis
[Yourdon 89] kept them largely separate.

Design Notes

563

In the domain of human language processing R&D, however, the terminology is a little
more complex. Language data, in various forms, is of such signiﬁcance in the ﬁeld that it
is frequently worked on independently of the algorithms that process it. For example: a
treebank2 can be developed independently of the parsers that may later be trained from
it; a thesaurus can be developed independently of the query expansion or sense tagging
mechanisms that may later come to use it. This type of data has come to have its own
term, Language Resources (LRs) [LREC-1 98], covering many data sources, from lexicons to
corpora.
In recognition of this distinction, we will adopt the following terminology:
Language Resource (LR): refers to data-only resources such as lexicons, corpora, thesauri or ontologies. Some LRs come with software (e.g. Wordnet has both a user
query interface and C and Prolog APIs), but where this is only a means of accessing
the underlying data we will still deﬁne such resources as LRs.
Processing Resource (PR): refers to resources whose character is principally programmatic or algorithmic, such as lemmatisers, generators, translators, parsers or speech
recognisers. For example, a part-of-speech tagger is best characterised by reference to
the process it performs on text. PRs typically include LRs, e.g. a tagger often has a
lexicon; a word sense disambiguator uses a dictionary or thesaurus.
Additional terminology worthy of note in this context: language data refers to LRs which are
at their core examples of language in practice, or ‘performance data’, e.g. corpora of texts or
speech recordings (possibly including added descriptive information as markup); data about
language refers to LRs which are purely descriptive, such as a grammar or lexicon.
PRs can be viewed as algorithms that map between diﬀerent types of LR, and which typically
use LRs in the mapping process. An MT engine, for example, maps a monolingual corpus
into a multilingual aligned corpus using lexicons, grammars, etc.3
Further support for the PR/LR terminology may be gleaned from the argument in favour of
declarative data structures for grammars, knowledge bases, etc. This argument was current
in the late 1980s and early 1990s [Gazdar & Mellish 89], partly as a response to what has
been seen as the overly procedural nature of previous techniques such as augmented transition
networks. Declarative structures represent a separation between data about language and
the algorithms that use the data to perform language processing tasks; a similar separation
to that used in GATE.
Adopting the PR/LR distinction is a matter of conforming to established domain practice
and terminology. It does not imply that we cannot model the domain (or build software
to support it) in an Object-Oriented manner; indeed the models in GATE are themselves
Object-Oriented.
2
3

A corpus of texts annotated with syntactic analyses.
This point is due to Wim Peters.

564

Design Notes

C.1.2

Model, view, controller

According to Buschmann et al (Pattern-Oriented Software Architecture, 1996), the ModelView-Controller (MVC) pattern
...divides an interactive application into three components. The model contains the core functionality and data. Views display information to the user.
Controllers handle user input. Views and controllers together comprise the user
interface. A change-propagation mechanism ensures consistency between the user
interface and the model. [p.125]
A variant of MVC, the Document-View pattern,
...relaxes the separation of view and controller... The View component of
Document-View combines the responsibilities of controller and view in MVC,
and implements the user interface of the system.
A beneﬁt of both arrangements is that
...loose coupling of the document and view components enables multiple simultaneous synchronized but diﬀerent views of the same document.
Geary (Graphic Java 2, 3rd Edtn., 1999) gives a slightly diﬀerent view:
MVC separates applications into three types of objects:
Models: Maintain data and provide data accessor methods
Views: Paint a visual representation of some or all of a model’s data
Controllers: Handle events ... By encapsulating what other architectures
intertwine, MVC applications are much more ﬂexible and reusable than their
traditional counterparts.
[pp. 71, 75]
Swing, the Java user interface framework, uses
a specialised version of the classic MVC meant to support pluggable look and
feel instead of applications in general. [p. 75]
GATE may be regarded as an MVC architecture in two ways:

Design Notes

565

directly, because we use the Swing toolkit for the GUIs;
by analogy, where LRs are models, VRs are views and PRs are controllers. Of these,
the latter sits least easily with the MVC scheme, as PRs may indeed be controllers but
may also not be.

C.1.3

Interfaces

Architectural Principle
The implementation of types should generally be hidden from the clients of the architecture.

Framework Expression
With a few exceptions (such as for utility classes), clients of the framework work with the
gate.* package. This package is mostly composed of interface deﬁnitions. Instantiations of
these interfaces are obtained via the Factory class.
The subsidiary packages of GATE provide the implementations of the gate.* interfaces
that are accessed via the factory. They themselves avoid directly constructing classes from
other packages (with a few exceptions, such as JAPE’s need for unattached annotation sets).
Instead they use the factory.

C.2

Exception Handling

When and how to use exceptions? Borrowing from Bill Venners, here are some guidelines
(with examples):
1. Exceptions exist to refer problem conditions up the call stack to a level at which they
may be dealt with. "If your method encounters an abnormal condition that it can’t
handle, it should throw an exception." If the method can handle the problem rationally, it should catch the exception and deal with it.
Example:
If the creation of a resource such as a document requires a URL as a parameter, the
method that does the creation needs to construct the URL and read from it. If there is
an exception during this process, the GATE method should abort by throwing its own
exception. The exception will be dealt with higher up the food chain, e.g. by asking
the user to input another URL, or by aborting a batch script.

566

Design Notes

2. All GATE exceptions should inherit from gate.util.GateException (a descendant of
java.lang.Exception, hence a checked exception) or gate.util.GateRuntimeException
(a descendant of java.lang.RuntimeException, hence an unchecked exception). This
rule means that clients of GATE code can catch all sorts of exceptions thrown by the
system with only two catch statements. (This rule may be broken by methods that
are not public, so long as their callers catch the non-GATE exceptions and deal with
them or convert them to GateException/GateRuntimeException.) Almost all exceptions thrown by GATE should be checked exceptions: the point of an exception is
that clients of your code get to know about it, so use a checked exception to make the
compiler force them to deal with it. Except:
Example:
With reference to the previous example, a problem using the URL will be signalled by
something like an UnknownHostException or an IOException. These should be caught
and re-thrown as descendants of GateException.
3. In a situation where an exceptional condition is an indication of a bug in the GATE
library, or in the implementation of some other library, then it is permissible to throw
an unchecked exception.
Example:
If a method is creating annotations on a document, and before creating the annotations
it checks that their start and end points are valid ranges in relation to the content of
the document (i.e. they fall within the oﬀset space of the document, and the end
is after the start), then if the method receives an InvalidOﬀsetException from the
AnnotationSet.add call, something is seriously wrong. In such cases it may be best to
throw a GateRuntimeException.
4. Where you are inheriting from a non-GATE class and therefore have the exception
signatures ﬁxed for you, you may add a new exception deriving from a non-GATE
class.
Example:
The SAX XML parser API uses SaxException. Implementing a SAX parser for a
document type involves overriding methods that throw this exception. Where you
want to have a subtype for some problem which is speciﬁc to GATE processing, you
could use GateSaxException which extends SaxException.
5. Test code is diﬀerent: in the JUnit test cases it is ﬁne just to declare that each method
throws Exception and leave it at that. The JUnit test runner will pick up the exceptions and report them to you. Test methods should, however, try and ensure that the
exceptions thrown are meaningful. For example, avoid null pointer exceptions in the
test code itself, e.g. by using assertNonNull.

Design Notes

567

Example:
1
2
3
4
5
6
7
8
9
10
11

public void testComments () throws Exception {
ResourceData docRd = ( ResourceData ) reg . get ( " gate . Document " );
assertNotNull (
" testComments : couldn ’t find document res data " , docRd
);
String comment = docRd . getComment ();
assert (
" testComments : incorrect or missing COMMENT on document " ,
comment != null && comment . equals ( " GATE document " )
);
} // testComments()

See also the testing notes.
6. "Throw a diﬀerent exception type for each abnormal condition." You can go too far on
this one - a hundred exception types per package would certainly be too much - but in
general you should create a new exception type for each diﬀerent sort of problem you
encounter.
Example:
The gate.creole package has a ResourceInstantiationException - this deals with all
problems to do with creating resources. We could have had "ResourceUrlProblem"
and "ResourceParameterProblem" but that would probably have ended up with too
many. On the other hand, just throwing everything as GateException is too coarse
(Hamish take note!).
7. Put exceptions in the package that they’re thrown from (unless they’re used in many
packages, in which case they can go in gate.util). This makes it easier to ﬁnd them in
the documentation and prevents name clashes.
Example:
gate.jape.ParserException is correctly placed; if it was in gate.util it might clash with,
for example, gate.xml.ParserException if there was such.

568

Design Notes

Appendix D
JAPE: Implementation
The annual Diagram prize for the oddest book title of the year has been awarded
to Gerard Forlin’s Butterworths Corporate Manslaughter Service, a hefty law
tome providing guidance and analysis on corporate liability for deaths in the
workplace.
The book, not published until January, was up against ﬁve other shortlisted titles:
Fancy Coﬃns to Make Yourself; The Flat-Footed Flies of Europe; Lightweight
Sandwich Construction; Tea Bag Folding; and The Art and Craft of Pounding
Flowers: No Paint, No Ink, Just a Hammer! The shortlist was thrown open to
readers of the literary trade magazine The Bookseller, who chose the winner by
voting on the magazine’s website. Butterworths Corporate Manslaughter Service,
a snip at 375, emerged as the overall victor with 35% of the vote.
The Diagram prize has been a regular on the award circuit since 1978, when
Proceedings of the Second International Workshop on Nude Mice carried oﬀ
the inaugural award. Since then, titles such as American Bottom Archaeology
and last year’s winner, High-Performance Stiﬀened Structures (an engineering
publication), have received unwonted publicity through the prize. This year’s
winner is perhaps most notable for its lack of entendre.
Manslaughter Service kills oﬀ competition in battle of strange titles, Emma Yates,
The Guardian, November 30, 2001.

This appendix gives implementation details and formal deﬁnitions of the JAPE annotation
patterns language. Section D.1 gives a more formal deﬁnition of the JAPE grammar, and
some examples of its use. Section D.2 describes JAPE’s relation to CPSL. Section D.3
describes the initialisation of a JAPE grammar, Section D.4 talks about the execution of
JAPE grammars, and the ﬁnal section explains how to switch the Java compiler used for
JAPE.
569

570

D.1

JAPE: Implementation

Formal Description of the JAPE Grammar

JAPE is similar to CPSL (a Common Pattern Speciﬁcation Language, developed in the
TIPSTER programme by Doug Appelt and others), with a few exceptions. Figure D.1 gives
a BNF (Backus-Naur Format) description of the grammar.
An example rule LHS:
Rule: KiloAmount
( ({Token.kind == "containsDigitAndComma"}):number
{Token.string == "kilograms"} ):whole
A basic constraint speciﬁcation appears between curly braces, and gives a conjunction of
annotation/attribute/value speciﬁers which have to match at a particular point in the annotation graph. A complex constraint speciﬁcation appears within round brackets, and may
be bound to a label with the ‘:’ operator; the label then becomes available in the RHS for
access to the annotations matched by the complex constraint. Complex constraints can also
have Kleene operators (*, +, ?) applied to them. A sequence of constraints represents a
sequential conjunction; disjunction is represented by separating constraints with ‘|’.
Converted to the format accepted by the JavaCC LL parser generator, the most signiﬁcant
fragment of the CPSL grammar (as described by Appelt, based on an original speciﬁcation
from a TIPSTER working group chaired by Boyan Onyshkevych) goes like this:
constraintGroup -->
(patternElement)+ ("|" (patternElement)+ )*
patternElement -->
"{" constraint ("," constraint)* "}"
|
"(" constraintGroup ")" (kleeneOp)? (binding)?
Here the ﬁrst line of patternElement is a basic constraint, the second a complex one.

JAPE: Implementation

571

MultiPhaseTransducer ::=
( <multiphase> <ident> )?
( ( ( JavaImportBlock )
( ( ControllerStartedBlock )
| ( ControllerFinishedBlock )
| ( ControllerAbortedBlock )
)*
( SinglePhaseTransducer )+ ) |
( <phases> ( <path> )+ ) )
<EOF>
SinglePhaseTransducer ::=
<phase> <ident>
( ( <input> ( <ident> )* ) |
( <option> ( <ident> <assign> ( <ident> | <bool> ) )* ) )*
( ( Rule ) | MacroDef | TemplateDef )*
JavaImportBlock ::= ( <javaimport> <leftBrace> ConsumeBlock )?
ControllerStartedBlock ::= ( <controllerstarted> <leftBrace> ConsumeBlock )
ControllerFinishedBlock ::= ( <controllerfinished> <leftBrace> ConsumeBlock )
ControllerAbortedBlock ::= ( <controlleraborted> <leftBrace> ConsumeBlock )
Rule ::=
<rule> <ident>
( <priority> <integer> )?
LeftHandSide "-->" RightHandSide
MacroDef ::= <macro> <ident> ( PatternElement | Action )
TemplateDef ::= <template> <ident> <assign> AttrVal
LeftHandSide ::= ConstraintGroup
ConstraintGroup ::= ( PatternElement )+ ( <bar> ( PatternElement )+ )*
PatternElement ::= ( <ident> | BasicPatternElement | ComplexPatternElement )
BasicPatternElement ::=
( ( <leftBrace> Constraint ( <comma> Constraint )* <rightBrace> ) |
( <string> ) )
ComplexPatternElement ::=
<leftBracket> ConstraintGroup <rightBracket>
( KleeneOperator )?
( <colon> ( <ident> | <integer> ) )?
KleeneOperator ::=
( <kleeneOp> ) |
( <leftSquare> ( <integer> ( <comma> <integer> )? ) <rightSquare> )
Constraint ::=
( <pling> )? <ident>
( ( FeatureAccessor <attrOp> AttrVal )
| ( <metaPropOp> <ident> <attrOp> AttrVal )
| ( <ident> ( ( <leftBrace> Constraint <rightBrace> ) | ( Constraint ) ) )
)?
FeatureAccessor ::= ( <period> <ident> )
AttrVal ::= ( ( <string> | <ident> | <integer> | <floatingPoint> | <bool> ) )
| ( TemplateCall )
TemplateCall ::= <leftSquare> <ident>
( <ident> <assign> AttrVal ( <comma> )? )*
<rightSquare>
RightHandSide ::= Action ( <comma> Action )*
Action ::= ( NamedJavaBlock | AnonymousJavaBlock | AssignmentExpression | <ident> )
NamedJavaBlock ::= <colon> <ident> <leftBrace> ConsumeBlock
AnonymousJavaBlock ::= <leftBrace> ConsumeBlock
AssignmentExpression ::=
( <colon> | <colonplus> ) <ident> <period> <ident> <assign>
<leftBrace>
( <ident> <assign>
( AttrVal |
( <colon>
<ident>
( ( <period> <ident> ( <period> | <metaPropOp> ) <ident> ) |
( <metaPropOp> <ident> )
)
)
)
( <comma> )?
)*
<rightBrace>
appendSpecials ::= java code
ConsumeBlock ::= java code

Figure D.1: BNF of JAPE’s grammar

572

JAPE: Implementation

An example of a complete rule:
Rule: NumbersAndUnit
( ( {Token.kind == "number"} )+:numbers {Token.kind == "unit"} )
-->
:numbers.Name = { rule = "NumbersAndUnit" }
This says ‘match sequences of numbers followed by a unit; create a Name annotation across
the span of the numbers, and attribute rule with value NumbersAndUnit’.

D.2

Relation to CPSL

We diﬀer from the CPSL spec in various ways:

1. No pre- or post-ﬁx context is allowed on the LHS.
2. No function calls on the LHS.
3. No string shorthand on the LHS.
4. We have multiple rule application algorithms (see Section 8.4).
5. Expressions relating to labels unbound on the LHS are not evaluated on the RHS. (In
TextPro they evaluate to ‘false’.)
6. JAPE allows arbitrary Java code on the RHS.
7. JAPE has a diﬀerent macro syntax, and allows macros for both the RHS and LHS.
8. JAPE grammars are compiled and can be stored as serialised Java objects.

Apart from this, it is a full implementation of CPSL, and the formal power of the languages
is the same (except that a JAPE RHS can delete annotations, which straight CPSL cannot).
The rule LHS is a regular language over annotations; the rule RHS can perform arbitrary
transformations on annotations, but the RHS is only ﬁred after the LHS been evaluated, and
the eﬀects of a rule application can only be referenced after the phase in which it occurs, so
the recognition power is no more than regular.

JAPE: Implementation

D.3

573

Initialisation of a JAPE Grammar

When a JAPE grammar is loaded in GATE, each phase is converted into a ﬁnite state
machine, a process that has several stages. Each rule is treated as a regular expression using
annotation-based patterns as input symbols. A JAPE phase is a disjunction of rules, so it is
also a regular expression. The ﬁrst stage of building the associated FSM for a JAPE phase
is the construction of a non-deterministic ﬁnite-state automaton, following the algorithm
described in [Aho et al. 86].
Additional to standard regular expressions, JAPE rules also contain bindings (labels associated to pattern segments). These are intended to be associated to the matched input
symbols (i.e. annotations) during the matching process, and are used while executing the
actions caused by the rule ﬁring. Upon creating the equivalent FSM for a given JAPE rule,
bindings are associated with the FSM transitions. This changes the semantics of a transition
– besides moving the state machine into a new current state, a transition may also bind the
consumed annotation(s) with one or more labels.
In order to optimise the execution time during matching (at the expense of storage space),
NFAs are usually converted to Deterministic Finite State Automata (DFAs) using e.g. the
subset algorithm [Aho et al. 86]. In the case of JAPE this transformation is not possible
due to the binding labels: two or more transitions from the NFA that match the same
annotation pattern cannot be compacted into a single transition in the DFA if they have
diﬀerent bindings. Because of this, JAPE grammars are represented as non-deterministic
ﬁnite state machines. A partial optimisation that eliminates the ￿-transitions from the NFA
is however performed.
The actions represented on the right hand side of JAPE rules are converted to compiled Java
classes and are associated with ﬁnal states in the FSM. The ﬁnal in-memory representation of
a JAPE grammar thus consists of a non-deterministic ﬁnite state machine, with transitions
that use annotation-based patterns as input symbols, additionally marked with bindings
information and for which the ﬁnal states are associated with actions.
Starting from the following two JAPE rules:
Rule: PersonPrefix
(
({Token})+ {Person}
):pers
--> {...}
Rule: OrganisationPrefix (
({Token})+ {Organisation}
):org --> {...}
the associated NFA is constructed, as illustrated in Figure D.2. Note that due to the fact that

574

JAPE: Implementation

the ﬁnal states are associated with diﬀerent actions, they cannot be joined into a single one
and are kept separate. This automaton is then optimised by eliminating the ￿-transitions,
resulting in the NFA presented in Figure D.3. For the sake of simplicity, the annotation
patterns used are the most basic ones, depending solely on annotation type. In the graphical
representation, the transitions are marked with the type of annotation that they match and
the associated binding in square brackets.

Figure D.2: Example of a non-deterministic ﬁnite state machine compiled from JAPE rules.

Figure D.3: Example of a ﬁnite state machine compiled from JAPE rules, with ￿-transitions
removed.
It can be observed in Figure D.3 that there are two transitions starting from state 1 (leading
to states 2, respectively 4) that both consume annotations of type Token, thus even the
optimised ﬁnite state machine is still non-deterministic.
Once a JAPE grammar is converted to the equivalent ﬁnite state automaton, the initialisation
phase is complete.

JAPE: Implementation

D.4

575

Execution of JAPE Grammars

The execution of a JAPE grammar can be described in simple terms as ﬁnding a path through
an annotation graph where all the annotations traversed form a sequence that is accepted by
the ﬁnite state machine built during the initialisation phase. The actual process is somewhat
more complex than that, as it also needs to take into account the various matching modes,
the ﬁltering of input annotation types, to deal with the assignment of matched annotation
to bindings, and to manage the execution of actions whenever successful matches occur.
Executing a JAPE grammar involves simulating the execution of a non-deterministic ﬁnite
state automaton (NFA) while using an annotation graph as input. At each step we start
from a document position (initially zero) and a ﬁnite state machine in a given state (initially
the start state). Annotations found at the given document position are compared with the
restrictions encoded in the NFA transitions; if they match, the annotations are consumed
and the state machine moves to a new state. Ambiguities are possible at each step both
in terms of input (several matching annotations can start at the same oﬀset) and in terms
of available NFA transitions (the state machine is non-deterministic, so multiple transitions
with the same restrictions can be present). When such ambiguities are encountered, the
current state machine is cloned to create as many copies as necessary, and each such copy
continues the matching process independently. The JAPE executor thus needs to keep track
of a family of state machines that are running in parallel – henceforth we shall call these
FSM instances.
Whenever one of the active FSM instances is moved to a new state, a test is performed to
check if the new state is a ﬁnal one. If that is the case, the FSM instance is said to be in an
accepting state, and a copy of its state is saved for later usage.
When none of the active FSM instances can advance any further, the stored accepting FSM
instances are used to execute JAPE actions, according to the declared matching style of the
current grammar.
A high-level view1 of the algorithm used during the execution of a JAPE grammar is presented in Listing D.1, in a Java-inspired pseudo-code.
The next paragraphs contain some more detailed comments, indexed using the line numbers
in the listing:
line 1 The annotations present in the document are ﬁltered according to the Input declaration in the JAPE code, if one was present. This causes the JAPE executor to
completely ignore annotations that are not listed as valid input.
lines 2–4 The matching process is initialised by setting the document position to 0, and
creating empty lists of active and accepting FSM instances.
1
The view of the algorithm presented here is greatly simpliﬁed, for the sake of clarity. The actual
implementation consists of a few thousand lines of Java code.

576

JAPE: Implementation
Listing D.1: JAPE matching algorithm

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

processIn p ut f i lt e r s ();
currentDocPosition = 0;
activeFSMInstances = new List < FSMInstance >();
acc epti ng F S M I n s t a n c e s = new List < FSMInstance >();
while ( cu rr en tD oc Po si ti on < document . length ()){
//create an initial FSM instance, starting from
//the current document position
activeFSMInstances . add (
new FSMInstance ( cu rr en tD ocP os it io n ));
//advance all FSM instances,
//until no further advance is possible
while (! ac ti ve FS MI ns ta nc es . isEmpty ()){
FSMInstance aFSM = a ct iv eFSMInstances . remove (0);
//advance aFSM, consuming annotations, linking used
//annotations to binding labels, as required;

16

//create cloned copies as necessary and add them to
//activeFSMInstances;

17
18
19

//save any accepting state of aFSM
//into acceptingFSMInstances;

20
21

}
if (! ac c e p t i n g F S M I n s t a n c e s . isEmpty ()){
//execute the action(s)
}

22
23
24
25
26

//move to the new document position, in accordance
//with the matching style.

27
28
29

}

lines 5–29 The matching continues until all the document text is exhausted.
line 8 Each step starts from the current document position with a single FSM instance.
lines 12–22 While there are still active FSM instances, they are advanced as far as possible.
Whenever ambiguities are encountered, cloned copies are created and added to the list
of active FSM instances. Whenever an FSM instance reaches a ﬁnal state during its
advancing, a copy of its state is saved to the list of accepting FSM instances.
lines 23–25 This segment of code is reached when there are no more active FSM instances
– all active instances were advanced as far as possible and either saved to the accepting
list (if they reached a ﬁnal state during that process) or simply discarded (if they could
advance no further but they still have not reached a ﬁnal state).
At this point, any successful matches that occurred need to be acted upon, so the list
of accepting FSM instances is inspected. If there are any, their associated actions are
now executed, according to the desired matching style. For instance if the matching
style used is Appelt, then only the accepting FSM instance that has covered the most

JAPE: Implementation

577

input will be executed; conversely, if the matching style is Brill, then all accepting
FSM instances will have their actions executed, etc.
line 27 When this point is reached, all possible matches from the current document position were found and the required action executed. The next step is to move to the
next starting position in the document, and re-start the matching process from there.
Depending on the matching style selected, the new document position is either the
oldP osition + 1, in the case of All, or matchingEndP osition + 1 in all other cases.

D.5

Using a Diﬀerent Java Compiler

GATE allows you to choose which Java compiler is used to compile the action classes generated from JAPE rules. The preferred compiler is speciﬁed by the Compiler type option
in gate.xml. At present the supported values are:
Sun The Java compiler supplied with the JDK. Although the option is called Sun, it supports
any JDK that supplies com.sun.tools.javac.Main in a standard location, including
the IBM JDK (all platforms) and the Apple JDK for Mac OS X.
Eclipse The Eclipse compiler, from the Java Development Tools of the Eclipse project2 .
Currently we use the compiler from Eclipse 3.2, which supports Java 5.0.
By default, the Eclipse compiler is used. It compiles faster than the Sun compiler, and loads
dependencies via the GATE ClassLoader, which means that Java code on the right hand
side of JAPE rules can refer to classes that were loaded from a plugin JAR ﬁle. The Sun
compiler can only load classes from the system classpath, so it will not work if GATE is
loaded from a subsidiary classloader, e.g. a Tomcat web application. You should generally
use the Eclipse compiler unless you have a compelling reason not to.
Support for other compilers can be added, but this is not documented here - if you’re in a
position to do this, you won’t mind reading the source code...

2

http://www.eclipse.org/jdt

578

JAPE: Implementation

Appendix E
Ant Tasks for GATE
This chapter describes the Ant tasks provided by GATE that you can use in your own build
ﬁles. The tasks require Ant 1.7 or later.

E.1

Declaring the Tasks

To use the GATE Ant tasks in your build ﬁle you must include the following <typedef>
(where ${gate.home} is the location of your GATE installation):
<typedef resource="gate/util/ant/antlib.xml">
<classpath>
<pathelement location="${gate.home}/bin/gate.jar" />
<fileset dir="${gate.home}/lib" includes="*.jar" />
</classpath>
</typedef>

If you have problems with library conﬂicts you should be able to reduce the JAR ﬁles included
from the lib directory to just jdom, xstream and jaxen.

E.2

E.2.1

The packagegapp task - bundling an application
with its dependencies
Introduction

GATE saved application states (GAPP ﬁles) are an XML representation of the state of a
GATE application. One of the features of a GAPP ﬁle is that it holds references to the
579

580

Ant Tasks for GATE

external resource ﬁles used by the application as paths relative to the location of the GAPP
ﬁle itself (or relative to the location of the GATE home directory where appropriate). This
is useful in many cases but if you want to package up a copy of an application to send to
a third party or to use in a web application, etc., then you need to be very careful to save
the ﬁle in a directory above all its resources, and package the resources up with the GAPP
ﬁle at the same relative paths. If the application refers to resources outside its own ﬁle tree
(i.e. with relative paths that include ..) then you must either maintain this structure or
manually edit the XML to move the resource references around and copy the ﬁles to the
right places to match. This can be quite tedious and error-prone...
The packagegapp Ant task aims to automate this process. It extracts all the relative paths
from a GAPP ﬁle, writes a modiﬁed version of the ﬁle with these paths rewritten to point
to locations below the new GAPP ﬁle location (i.e. with no .. path segments) and copies
the referenced ﬁles to their rewritten locations. The result is a directory structure that can
be easily packaged into a zip ﬁle or similar and moved around as a self-contained unit.
This Ant task is the underlying driver for the ‘Export for GATECloud.net’ option described
in Section 3.8.4. Export for GATECloud.net does the equivalent of:
<packagegapp src="sourceFile.gapp"
destfile="{tempdir}/application.xgapp"
copyPlugins="yes"
copyResourceDirs="yes"
onUnresolved="recover" />

followed by packaging the temporary directory into a zip ﬁle. These options are explained
in detail below.
The packagegapp task requires Ant 1.7 or later.

E.2.2

Basic Usage

In many cases, the following simple invocation will do what you want:
<packagegapp src="original.xgapp"
gatehome="/path/to/GATE"
destfile="package/target.xgapp" />

Note that the parent directory of the destfile (in this case package) must already exist. It
will not be created automatically. The value for the gatehome attribute should be the path
to your GATE installation (the directory containing build.xml, the bin, lib and plugins
directories, etc.). If you know that the gapp ﬁle you want to package does not reference any
resources relative to the GATE home directory1 then this attribute may be omitted.
1

You can check this by searching for the string “ gatehome ” in the XML

Ant Tasks for GATE

581

This will perform the following steps:
1. Read in the original.xgapp ﬁle and extract all the relative paths it contains.
2. For each plugin referred to by a relative path, foo/bar/MyPlugin, rewrite the plugin
location to be plugins/MyPlugin (relative to the location of the destfile). If the
application refers to two plugins in diﬀerent original locations with the same name,
one of them will be renamed to avoid a name clash. If one plugin is a subdirectory of
another plugin, this nesting will be maintained in the relocated directory structure.
3. For each resource ﬁle referred to by the gapp, see if it lives under the original location
of one of the plugins moved in the previous step. If so, rewrite its location relative to
the new location of the plugin.
4. If there are any relative resource paths that are not accounted for by the above rule
(i.e. they do not live inside a referenced plugin), the build fails (see Section E.2.3 for
how to change this behaviour).
5. Write out the modiﬁed GAPP to the destfile.
6. Recursively copy the whole content of each of the plugins from step 2 to their new
locations2 .
This means that the all the relative paths in the new GAPP ﬁle (package/target.xgapp)
will point to plugins/Something. You can now bundle up the whole package directory and
take it elsewhere.

E.2.3

Handling Non-Plugin Resources

By default, the task only handles relative resource paths that point within one of the plugins
that the GAPP refers to. However, many applications refer to resources that live outside the
plugin directories, for example custom JAPE grammars, gazetteer lists, etc. The task provides two approaches to support this: it can handle the unresolved references automatically,
or you can provide your own ‘hints’ to augment the default plugin-based ones.

Resolving Unresolved Resources
By default, the build will fail if there are any relative paths that cannot be accounted for by
the plugins (or the explicit hints, see Section E.2.3). However, this is conﬁgurable using the
onUnresolved attribute, which can take the following values:
2

This is done with an Ant copy task and so is subject to the normal defaultexcludes

582

Ant Tasks for GATE

fail (default) the build fails if an unresolved relative path is found.
absolute unresolved relative paths are left pointing to the same location as in the original
ﬁle, but as an absolute rather than a relative URL. The same ﬁle will be used even if
you move the GAPP ﬁle to a diﬀerent directory. This option is useful if the resource
in question is visible at the same absolute location on the machine where you will be
putting the packaged ﬁle (for example a very large dictionary or ontology held on a
network share).
recover attempt to recover gracefully (see below).
With onUnresolved="recover", unresolved resources are relocated to a directory named
application-resources under the target GAPP ﬁle location. Resources in the same original directory are copied to the same subdirectory of application-resources, ﬁles from
diﬀerent original directories are copied to diﬀerent subdirectories. Typically, for a resource
whose original location was .../myresources/grammar/clever.jape the target location
would be application-resources/grammar/clever.jape but if the application also referred to (say) .../otherresources/grammar/clean.jape then this would be mapped into
application-resources/grammar-2 to avoid a name clash.
As with plugins, if one unresolved resource is contained in a subdirectory of a
directory containing another unresolved resource, the relative path will be preserved, i.e.
if the application refers to .../dictionaries/main.txt and also
.../dictionaries/specialist/medical.txt then the latter will be relocated to
application-resources/dictionaries/specialist rather than simply creating another
top-level application-resources/specialist directory. This is particularly relevant when
using the copyResourceDirs option described below.
Example:
<packagegapp src="original.xgapp" destfile="package/target.xgapp"
onUnresolved="recover" />

Providing Mapping Hints
By default, the task knows how to handle resources that live inside plugins. You can think
of this as a ‘hint’ /foo/bar/MyPlugin -> plugins/MyPlugin, saying that whenever the
mapper ﬁnds a resource path of the form /foo/bar/MyPlugin/X , it will relocate it to
plugins/MyPlugin/X relative to the output GAPP ﬁle. You can specify your own hints
which will be used the same way.
<packagegapp src="original.xgapp" destfile="package/target.xgapp">
<hint from="${user.home}/my-app-v1" to="resources/my-app" />
<hint from="/share/data/bigfiles" absolute="yes" />
</packagegapp>

Ant Tasks for GATE

583

In this example, ~/my-app-v1/grammar/main.jape would be mapped to the location
resources/my-app/grammar/main.jape (as always, relative to the output GAPP ﬁle). You
can also hint that certain resources should be converted to absolute paths rather than being
packaged with the application, using absolute="yes". The from and to values refer to
directories - you cannot hint a single ﬁle, nor put two ﬁles from the same original directory
into diﬀerent directories in the packaged GAPP.
Explicit hints override the default plugin-based hints. For example given the hint
from="${gate.home}/plugins/ANNIE/resources" to="resources/ANNIE", resources in
the ANNIE plugin would be mapped into resources/ANNIE, but the plugin creole.xml
itself would still be mapped into plugins/ANNIE.
As well as providing the hints inline in the build ﬁle you can also read them from a ﬁle in
the normal Java Properties format3 , using

<hint file="hints.properties" />
The keys in the property ﬁle are the from paths (in this case, relative paths are resolved
against the project base directory, as with the location attribute of a property task) and
the values are the to paths relative to the output ﬁle location.
The order of the <hint> elements is signiﬁcant – if more than one hint could apply to the
same resource ﬁle, the one deﬁned ﬁrst is used. For example, given the hints

<hint from="${gate.home}/plugins/ANNIE/resources/tokeniser" to="tokeniser" />
<hint from="${gate.home}/plugins/ANNIE/resources" to="annie" />

the resource plugins/ANNIE/resources/tokeniser/DefaultTokeniser.rules would be
mapped into the tokeniser directory, but if the hints were reversed it would instead be
mapped into annie/tokeniser. Note, however, that this does not necessarily extend to
hints loaded from property ﬁles, as the order in which hints from a single property ﬁle are
applied is not speciﬁed. Given

<hint file="file1.proeprties" />
<hint file="file2.properties" />

the relative precedence of two hints from ﬁle1 is not ﬁxed, but it is the case that all hints in
ﬁle1 will be applied before those in ﬁle2.
3
the hint tag supports all the attributes of the standard Ant property tag so can load the hints from a
ﬁle on disk or from a resource in a JAR ﬁle

584

Ant Tasks for GATE

E.2.4

Streamlining your Plugins

By default, the task will recursively copy the whole content of every plugin into the target
directory. In most cases this is OK but it may be the case that your plugins contain many
extraneous resources that are not used by your application. In this case you can specify
copyPlugins="no":
<packagegapp src="original.xgapp" destfile="package/target.xgapp"
copyPlugins="no" />

In this mode, the packager task will copy only the following ﬁles from each plugin:
creole.xml
any JAR ﬁles referenced from <JAR> elements in creole.xml4
In addition it will of course copy any ﬁles directly referenced by the GAPP, but not ﬁles
referenced indirectly (the classic examples being .lst ﬁles used by a gazetteer .def, or
the individual phases of a multiphase JAPE grammar) or ﬁles that are referenced by the
creole.xml itself as AUTOINSTANCE parameters (e.g. the annotation schemas in ANNIE).
You will need to name these extra ﬁles explicitly as extra resources (see the next section).

E.2.5

Bundling Extra Resources

Apart from plugins (when you don’t use copyPlugins="no"), the only ﬁles copied into the
target directory are those that are referenced directly from the GAPP ﬁle. This is often but
not always suﬃcient, for example if your application contains a multiphase JAPE transducer
then packagegapp will include the main JAPE ﬁle but not the individual phase ﬁles. The
task provides two ways to include extra ﬁles in the package:
If you set the attribute copyResourceDirs="yes" on the packagegapp task then whenever the task packages a referenced resource ﬁle it will also recursively include the whole
contents of the directory containing that ﬁle in the output package. You probably don’t
want to use this option if you have resource ﬁles in a directory shared with other ﬁles
(e.g. your home directory...).
To include speciﬁc extra resources you can use an <extraresourcespath> (see below).
4
When loading a plugin, the classloader inspects the Class-Path attribute in each JAR ﬁle’s manifest
and also loads the JARs that this references. However the packager task does not do this, so if you use the
manifest mechanism with your plugins you will need to explicitly reference the additional JAR ﬁles using an
extraresourcespath.

Ant Tasks for GATE

585

The <extraresourcespath> allows you to specify speciﬁc extra ﬁles that should be included
in the package:
<packagegapp src="original.xgapp" destfile="package/target.xgapp">
<extraresourcespath>
<pathelement location="${user.home}/common-files/README" />
<fileset dir="${user.home}/my-app-v1" includes="grammar/*.jape" />
</extraresourcespath>
</packagegapp>

As the name suggests, this is a path-like structure and supports all the usual elements and
attributes of an Ant <path>, including multiple nested fileset, filelist, pathelement
and other path elements. For speciﬁc types of indirect references, there are helper elements that can be included under extraresourcespath. Currently the only one of these is
gazetteerlists, which takes the path to a gazetteer deﬁnition ﬁle and returns the set of
.lst ﬁles the deﬁnition uses:
<gazetteerlists definition="my/resources/lists.def" encoding="UTF-8" />
Other helpers (e.g. for multiphase JAPE) may be implemented in future.
You can also refer to a path deﬁned elsewhere in the usual way:
<path id="extra.files">
...
</path>
<packagegapp ...>
<extraresourcespath refid="extra.files" />
</packagegapp>

Resources declared in the extraresourcespath and directories included using copyResourceDirs
are treated exactly the same as resources that are referenced by the GAPP ﬁle - their target
locations in the package are determined by the mapping hints, default plugin-based hints,
and the onUnresolved setting as above. If you want to put extra resource ﬁles at speciﬁc
locations in the package tree, independent of the mapping hints mechanism, you should do
this with a separate <copy> task after the <packagegapp> task has done its work.

E.3

The expandcreoles Task - Merging AnnotationDriven Conﬁg into creole.xml

The expandcreoles task processes a number of creole.xml ﬁles from plugins, processes
any @CreoleResource and @CreoleParameter annotations on the declared resource classes,

586

Ant Tasks for GATE

and merges this conﬁguration with the original XML conﬁguration into a new copy of the
creole.xml. It is not necessary to do this in the normal use of GATE, and this task is
documented here simply for completeness. It is intended simply for use with non-GATE
tools that can process the creole.xml ﬁle format to extract information about plugins (the
prime use case for this is to generate the GATE plugins information page automatically from
the plugin deﬁnitions).
The typical usage of this task (taken from the GATE build.xml) is:
<expandcreoles todir="build/plugins" gatehome="${basedir}">
<fileset dir="plugins" includes="*/creole.xml" />
</expandcreoles>

This will initialise GATE with the given GATE HOME directory, then read each ﬁle from
the nested ﬁleset, parse it as a creole.xml, expand it from any annotation conﬁguration, and
write it out to a ﬁle under build/plugins. Each output ﬁle will be generated at the same
location relative to the todir as the original ﬁle was relative to the dir of its fileset.

Appendix F
Named-Entity State Machine Patterns
There are, it seems to me, two basic reasons why minds aren’t computers... The
ﬁrst... is that human beings are organisms. Because of this we have all sorts of
needs - for food, shelter, clothing, sex etc - and capacities - for locomotion, manipulation, articulate speech etc, and so on - to which there are no real analogies
in computers. These needs and capacities underlie and interact with our mental activities. This is important, not simply because we can’t understand how
humans behave except in the light of these needs and capacities, but because
any historical explanation of how human mental life developed can only do so
by looking at how this process interacted with the evolution of these needs and
capacities in successive species of hominids.
...
The second reason... is that... brains don’t work like computers.
Minds, Machines and Evolution, Alex Callinicos, 1997 (ISJ 74, p.103).
This chapter describes the individual grammars used in GATE for Named Entity Recognition, and how they are combined together. It relates to the default NE grammar for
ANNIE, but should also provide guidelines for those adapting or creating new grammars.
For documentation about speciﬁc grammars other than this core set, use this document in
combination with the comments in the relevant grammar ﬁles. chapter 8 also provides information about designing new grammar rules and tips for ensuring maximum processing
speed.

F.1

Main.jape

This ﬁle contains a list of the grammars to be used, in the correct processing order. The
ordering of the grammars is crucial, because they are processed in series, and later grammars
587

588

Named-Entity State Machine Patterns

may depend on annotations produced by earlier grammars.
The default grammar consists of the following phases:
ﬁrst.jape
ﬁrstname.jape
name.jape
name post.jape
date pre.jape
date.jape
reldate.jape
number.jape
address.jape
url.jape
identiﬁer.jape
jobtitle.jape
ﬁnal.jape
unknown.jape
name context.jape
org context.jape
loc context.jape
clean.jape

F.2

ﬁrst.jape

This grammar must always be processed ﬁrst. It can contain any general macros needed
for the whole grammar set. This should consist of a macro deﬁning how space and control
characters are to be processed (and may consequently be diﬀerent for each grammar set,
depending on the text type). Because this is deﬁned ﬁrst of all, it is not necessary to restate
this in later grammars. This has a big advantage – it means that default grammars can be
used for specialised grammar sets, without having to be adapted to deal with e.g. diﬀerent

Named-Entity State Machine Patterns

589

treatment of spaces and control characters. In this way, only the ﬁrst.jape ﬁle needs to be
changed for each grammar set, rather than every individual grammar.
The ﬁrst.jape grammar also has a dummy rule in. This is never intended to ﬁre – it is simply
added because every grammar set must contain rules, but there are no speciﬁc rules we wish
to add here. Even if the rule were to match the pattern deﬁned, it is designed not to produce
any output (due to the empty RHS).

F.3

ﬁrstname.jape

This grammar contains rules to identify ﬁrst names and titles via the gazetteer lists. It
adds a gender feature where appropriate from the gazetteer list. This gender feature is used
later in order to improve co-reference between names and pronouns. The grammar creates
separate annotations of type FirstPerson and Title.

F.4

name.jape

This grammar contains initial rules for organization, location and person entities. These rules
all create temporary annotations, some of which will be discarded later, but the majority of
which will be converted into ﬁnal annotations in later grammars. Rules beginning with ‘Not’
are negative rules – this means that we detect something and give it a special annotation
(or no annotation at all) in order to prevent it being recognised as a name. This is because
we have no negative operator (we have ‘=’ but not ‘!=’).

F.4.1

Person

We ﬁrst deﬁne macros for initials, ﬁrst names, surnames, and endings. We then use these
to recognise combinations of ﬁrst names from the previous phase, and surnames from their
POS tags or case information. Persons get marked with the annotation ‘TempPerson’. We
also percolate feature information about the gender from the previous annotations if known.

F.4.2

Location

The rules for Location are fairly straightforward, but we deﬁne them in this grammar so that
any ambiguity can be resolved at the top level. Locations are often combined with other
entity types, such as Organisations. This is dealt with by annotating the two entity types
separately, and them combining them in a later phase. Locations are recognised mainly by

590

Named-Entity State Machine Patterns

gazetteer lookup, using not only lists of known places, but also key words such as mountain,
lake, river, city etc. Locations are annotated as TempLocation in this phase.

F.4.3

Organization

Organizations tend to be deﬁned either by straight lookup from the gazetteer lists, or,
for the majority, by a combination of POS or case information and key words such as
‘company’, ‘bank’, ‘Services’ ‘Ltd.’ etc. Many organizations are also identiﬁed by contextual
information in the later phase org context.jape. In this phase, organizations are annotated
as TempOrganization.

F.4.4

Ambiguities

Some ambiguities are resolved immediately in this grammar, while others are left until later
phases. For example, a Christian name followed by a possible Location is resolved by default
to a person rather than a Location (e.g. ‘Ken London’). On the other hand, a Christian name
followed by a possible organisation ending is resolved to an Organisation (e.g. ‘Alexandra
Pottery’), though this is a slightly less sure rule.

F.4.5

Contextual information

Although most of the rules involving contextual information are invoked in a much later
phase, there are a few which are invoked here, such as ‘X joined Y’ where X is annotated as
a Person and Y as an Organization. This is so that both annotations types can be handled
at once.

F.5

name post.jape

This grammar runs after the name grammar to ﬁx some erroneous annotations that may
have been created. Of course, a more elegant solution would be not to create the problem
in the ﬁrst instance, but this is a workaround. For example, if the surname of a Person
contains certain stop words, e.g. ‘Mary And’ then only the ﬁrst name should be recognised
as a Person. However, it might be that the ﬁrstname is also an Organization (and has been
tagged with TempOrganization already), e.g. ‘U.N.’ If this is the case, then the annotation
is left untouched, because this is correct.

Named-Entity State Machine Patterns

F.6

591

date pre.jape

This grammar precedes the date phase, because it includes extra context to prevent dates
being recognised erroneously in the middle of longer expressions. It mainly treats the case
where an expression is already tagged as a Person, but could also be tagged as a date (e.g.
16th Jan).

F.7

date.jape

This grammar contains the base rules for recognising times and dates. Given the complexity
of potential patterns representing such expressions, there are a large number of rules and
macros.
Although times and dates can be mutually ambiguous, we try to distinguish between them
as early as possible. Dates, times and years are generally tagged separately (as TempDate,
TempTime and TempYear respectively) and then recombined to form a ﬁnal Date annotation in a later phase. This is because dates, times and years can be combined together in
many diﬀerent ways, and also because there can be much ambiguity between the three. For
example, 1312 could be a time or a year, while 9-10 could be a span of time or date, or a
ﬁxed time or date.

F.8

reldate.jape

This grammar handles relative rather than absolute date and time sequences, such as ‘yesterday morning’, ‘2 hours ago’, ‘the ﬁrst 9 months of the ﬁnancial year’etc. It uses mainly
explicit key words such as ‘ago’ and items from the gazetteer lists.

F.9

number.jape

This grammar covers rules concerning money and percentages. The rules are fairly straightforward, using keywords from the gazetteer lists, and there is little ambiguity here, except
for example where ‘Pound’ can be money or weight, or where there is no explicit currency
denominator.

592

F.10

Named-Entity State Machine Patterns

address.jape

Rules for Address cover ip addresses, phone and fax numbers, and postal addresses. In
general, these are not highly ambiguous, and can be covered with simple pattern matching,
although phone numbers can require use of contextual information. Currently only UK
formats are really handled, though handling of foreign zipcodes and phone number formats
is envisaged in future. The annotations produced are of type Email, Phone etc. and are
then replaced in a later phase with ﬁnal Address annotations with ‘phone’ etc. as features.

F.11

url.jape

Rules for email addresses and Urls are in a separate grammar from the other address types,
for the simple reason that SpaceTokens need to be identiﬁed for these rules to operate,
whereas this is not necessary for the other Address types. For speed of processing, we place
them in separate grammars so that SpaceTokens can be eliminated from the Input when
they are not required.

F.12

identiﬁer.jape

This grammar identiﬁes ‘Identiﬁers’ which basically means any combination of numbers and
letters acting as an ID, reference number etc. not recognised as any other entity type.

F.13

jobtitle.jape

This grammar simply identiﬁes Jobtitles from the gazetteer lists, and adds a JobTitle annotation, which is used in later phases to aid recognition of other entity types such as Person
and Organization. It may then be discarded in the Clean phase if not required as a ﬁnal
annotation type.

F.14

ﬁnal.jape

This grammar uses the temporary annotations previously assigned in the earlier phases, and
converts them into ﬁnal annotations. The reason for this is that we need to be able to resolve
ambiguities between diﬀerent entity types, so we need to have all the diﬀerent entity types
handled in a single grammar somewhere. Ambiguities can be resolved using prioritisation

Named-Entity State Machine Patterns

593

techniques. Also, we may need to combine previously annotated elements, such as dates and
times, into a single entity.
The rules in this grammar use Java code on the RHS to remove the existing temporary
annotations, and replace them with new annotations. This is because we want to retain the
features associated with the temporary annotations. For example, we might need to keep
track of whether a person is male or female, or whether a location is a city or country. It
also enables us to keep track of which rules have been used, for debugging purposes.
For the sake of obfuscation, although this phase is called ﬁnal, it is not the ﬁnal phase!

F.15

unknown.jape

This short grammar ﬁnds proper nouns not previously recognised, and gives them an Unknown annotation. This is then used by the namematcher – if an Unknown annotation can
be matched with a previously categorised entity, its annotation is changed to that of the
matched entity. Any remaining Unknown annotations are useful for debugging purposes,
and can also be used as input for additional grammars or processing resources.

F.16

name context.jape

This grammar looks for Unknown annotations occurring in certain contexts which indicate
they might belong to Person. This is a typical example of a grammar that would beneﬁt
from learning or automatic context generation, because useful contexts are (a) hard to ﬁnd
manually and may require large volumes of training data, and (b) often very domain–speciﬁc.
In this core grammar, we conﬁne the use of contexts to fairly general uses, since this grammar
should not be domain–dependent.

F.17

org context.jape

This grammar operates on a similar principle to name context.jape. It is slightly oriented
towards business texts, so does not quite fulﬁl the generality criteria of the previous grammar.
It does, however, provide some insight into more detailed use of contexts.¡/p¿

594

F.18

Named-Entity State Machine Patterns

loc context.jape

This grammar also operates in a similar manner to the preceding two, using general context
such as coordinated pairs of locations, and hyponymic types of information.

F.19

clean.jape

This grammar comes last of all, and simply aims to clean up (remove) some of the temporary
annotations that may not have been deleted along the way.

Appendix G
Part-of-Speech Tags used in the
Hepple Tagger
CC - coordinating conjunction: ‘and’, ‘but’, ‘nor’, ‘or’, ‘yet’, plus, minus, less, times (multiplication), over (division). Also ‘for’ (because) and ‘so’ (i.e., ‘so that’).
CD - cardinal number
DT - determiner: Articles including ‘a’, ‘an’, ‘every’, ‘no’, ‘the’, ‘another’, ‘any’, ‘some’,
‘those’.
EX - existential there: Unstressed ‘there’ that triggers inversion of the inﬂected verb and the
logical subject; ‘There was a party in progress’.
FW - foreign word
IN - preposition or subordinating conjunction
JJ - adjective: Hyphenated compounds that are used as modiﬁers; happy-go-lucky.
JJR - adjective - comparative: Adjectives with the comparative ending ‘-er’ and a comparative meaning. Sometimes ‘more’ and ‘less’.
JJS - adjective - superlative: Adjectives with the superlative ending ‘-est’ (and ‘worst’).
Sometimes ‘most’ and ‘least’.
JJSS - -unknown-, but probably a variant of JJS
-LRB- - -unknownLS - list item marker: Numbers and letters used as identiﬁers of items in a list.
MD - modal: All verbs that don’t take an ‘-s’ ending in the third person singular present:
‘can’, ‘could’, ‘dare’, ‘may’, ‘might’, ‘must’, ‘ought’, ‘shall’, ‘should’, ‘will’, ‘would’.
NN - noun - singular or mass
NNP - proper noun - singular: All words in names usually are capitalized but titles might
not be.
NNPS - proper noun - plural: All words in names usually are capitalized but titles might
not be.
NNS - noun - plural
NP - proper noun - singular
NPS - proper noun - plural
595

596

References

PDT - predeterminer: Determiner like elements preceding an article or possessive pronoun;
‘all/PDT his marbles’, ‘quite/PDT a mess’.
POS - possessive ending: Nouns ending in ‘’s’ or ‘’’.
PP - personal pronoun
PRPR - unknown-, but probably possessive pronoun
PRP - unknown-, but probably possessive pronoun
PRP - unknown, but probably possessive pronoun,such as ‘my’, ‘your’, ‘his’, ‘his’, ‘its’,
‘one’s’, ‘our’, and ‘their’.
RB - adverb: most words ending in ‘-ly’. Also ‘quite’, ‘too’, ‘very’, ‘enough’, ‘indeed’, ‘not’,
‘-n’t’, and ‘never’.
RBR - adverb - comparative: adverbs ending with ‘-er’ with a comparative meaning.
RBS - adverb - superlative
RP - particle: Mostly monosyllabic words that also double as directional adverbs.
STAART - start state marker (used internally)
SYM - symbol: technical symbols or expressions that aren’t English words.
TO - literal to
UH - interjection: Such as ‘my’, ‘oh’, ‘please’, ‘uh’, ‘well’, ‘yes’.
VBD - verb - past tense: includes conditional form of the verb ‘to be’; ‘If I were/VBD rich...’.
VBG - verb - gerund or present participle
VBN - verb - past participle
VBP - verb - non-3rd person singular present
VB - verb - base form: subsumes imperatives, inﬁnitives and subjunctives.
VBZ - verb - 3rd person singular present
WDT - wh-determiner
WP - possessive wh-pronoun: includes ‘whose’
WP - wh-pronoun: includes ‘what’, ‘who’, and ‘whom’.
WRB - wh-adverb: includes ‘how’, ‘where’, ‘why’. Includes ‘when’ when used in a temporal
sense.
:: - literal colon
, - literal comma
- literal dollar sign
- - literal double-dash
- literal double quotes
- literal grave
( - literal left parenthesis
. - literal period
# - literal pound sign
) - literal right parenthesis
- literal single quote or apostrophe

References
[Agatonovic et al. 08]
M. Agatonovic, N. Aswani, K. Bontcheva, H. Cunningham, T. Heitz, Y. Li,
I. Roberts, and V. Tablan. Large-scale, parallel automatic patent annotation.
In Proc. of 1st International CIKM Workshop on Patent Information Retrieval PaIR’08, Napa Valley, California, USA, October 30 2008.
[Aho et al. 86]
A. V. Aho, R. Sethi, and J. D. Ullman. Compilers Principles, Techniques, and
Tools. Addison-Wesley, Reading, Massachusetts, 1986.
[Aswani & Gaizauskas 09]
N. Aswani and R. Gaizauskas. Evolving a General Framework for Text Alignment:
Case Studies with Two South Asian Languages. In Proceedings of the International
Conference on Machine Translation: Twenty-Five Years On, Cranﬁeld, Bedfordshire, UK, November 2009.
[Aswani & Gaizauskas 10]
N. Aswani and R. Gaizauskas. Developing Morphological Analysers for South Asian
Languages: Experimenting with the Hindi and Gujarati Languages. In 7th Language Resources and Evaluation Conference (LREC), La Valletta, Malta, May 2010.
ELRA.
[Aswani et al. 05]
N. Aswani, V. Tablan, K. Bontcheva, and H. Cunningham. Indexing and Querying
Linguistic Metadata and Document Content. In Proceedings of Fifth International
Conference on Recent Advances in Natural Language Processing (RANLP2005),
Borovets, Bulgaria, 2005.
[Aswani et al. 06]
N. Aswani, K. Bontcheva, and H. Cunningham. Mining information for instance
uniﬁcation. In 5th International Semantic Web Conference (ISWC2006), Athens,
Georgia, USA, 2006.
[Azar 89]
S. Azar. Understanding and Using English Grammar. Prentice Hall Regents, 1989.
597

598

References

[Baker et al. 02]
P. Baker, A. Hardie, T. McEnery, H. Cunningham, and R. Gaizauskas. EMILLE,
A 67-Million Word Corpus of Indic Languages: Data Collection, Mark-up and Harmonisation. In Proceedings of 3rd Language Resources and Evaluation Conference
(LREC’2002), pages 819–825, 2002.
[Bird & Liberman 99]
S. Bird and M. Liberman. A Formal Framework for Linguistic Annotation. Technical
Report MS-CIS-99-01, Department of Computer and Information Science, University
of Pennsylvania, 1999. http://xxx.lanl.gov/abs/cs.CL/9903003.
[Bontcheva & Sabou 06]
K. Bontcheva and M. Sabou. Learning Ontologies from Software Artifacts: Exploring
and Combining Multiple Sources. In Workshop on Semantic Web Enabled Software
Engineering (SWESE), Athens, G.A., USA, November 2006.
[Bontcheva 04]
K. Bontcheva. Open-source Tools for Creation, Maintenance, and Storage of Lexical
Resources for Language Generation from Ontologies. In Proceedings of 4th Language
Resources and Evaluation Conference (LREC’04), 2004.
[Bontcheva 05]
K. Bontcheva. Generating Tailored Textual Summaries from Ontologies. In Second
European Semantic Web Conference (ESWC’2005), 2005.
[Bontcheva et al. 00]
K. Bontcheva, H. Brugman, A. Russel, P. Wittenburg, and H. Cunningham. An
Experiment in Unifying Audio-Visual and Textual Infrastructures for Language Processing R&D. In Proceedings of the Workshop on Using Toolsets and Architectures
To Build NLP Systems at COLING-2000, Luxembourg, 2000. http://gate.ac.uk/.
[Bontcheva et al. 02a]
K. Bontcheva, H. Cunningham, V. Tablan, D. Maynard, and O. Hamza. Using GATE as an Environment for Teaching NLP.
In Proceedings of the
ACL Workshop on Eﬀective Tools and Methodologies in Teaching NLP, 2002.
http://gate.ac.uk/sale/acl02/gate4teaching.pdf.
[Bontcheva et al. 02b]
K. Bontcheva, H. Cunningham, V. Tablan, D. Maynard, and H. Saggion. Developing
Reusable and Robust Language Processing Components for Information Systems using GATE. In Proceedings of the 3rd International Workshop on Natural Language
and Information Systems (NLIS’2002), Aix-en-Provence, France, 2002. IEEE Computer Society Press. http://gate.ac.uk/sale/nlis/nlis.ps.
[Bontcheva et al. 02c]
K. Bontcheva, M. Dimitrov, D. Maynard, V. Tablan, and H. Cunningham.
Shallow Methods for Named Entity Coreference Resolution. In Chaˆnes de
ı

References

599

r´f´rences et r´solveurs d’anaphores, workshop TALN 2002, Nancy, France, 2002.
ee
e
http://gate.ac.uk/sale/taln02/taln-ws-coref.pdf.
[Bontcheva et al. 03]
K. Bontcheva, A. Kiryakov, H. Cunningham, B. Popov, and M. Dimitrov. Semantic web enabled, open source language technology. In EACL workshop on Language Technology and the Semantic Web: NLP and XML, Budapest, Hungary, 2003.
http://gate.ac.uk/sale/eacl03-semweb/bontcheva-etal-final.pdf.
[Bontcheva et al. 04]
K. Bontcheva, V. Tablan, D. Maynard, and H. Cunningham. Evolving GATE to
Meet New Challenges in Language Engineering. Natural Language Engineering,
10(3/4):349—373, 2004.
[Bontcheva et al. 06a]
K. Bontcheva, H. Cunningham, A. Kiryakov, and V. Tablan. Semantic Annotation
and Human Language Technology. In J. Davies, R. Studer, and P. Warren, editors,
Semantic Web Technology: Trends and Research. John Wiley and Sons, 2006.
[Bontcheva et al. 06b]
K. Bontcheva, J. Davies, A. Duke, T. Glover, N. Kings, and I. Thurlow. Semantic
Information Access. In J. Davies, R. Studer, and P. Warren, editors, Semantic Web
Technologies. John Wiley and Sons, 2006.
[Bontcheva et al. 09]
K. Bontcheva, B. Davis, A. Funk, Y. Li, and T. Wang. Human Language Technologies. In J. Davies, M. Grobelnik, and D. Mladenic, editors, Semantic Knowledge
Management, pages 37–49. 2009.
[Bontcheva et al. 10]
K. Bontcheva, H. Cunningham, I. Roberts, and V. Tablan. Web-based collaborative
corpus annotation: Requirements and a framework implementation. In Proceedings
of the Workshop on New Challenges for NLP Frameworks, Valletta, Malta, May
2010.
[Booch 94]
G. Booch. Object-Oriented Analysis and Design 2nd Edn. Benjamin/Cummings,
1994.
[Brugman et al. 99]
H. Brugman, K. Bontcheva, P. Wittenburg, and H. Cunningham. Integrating Multimedia and Textual Software Architectures for Language Technology. Technical
report MPI-TG-99-1, Max-Planck Institute for Psycholinguistics, Nijmegen, Netherlands, 1999.
[Carletta 96]
J. Carletta. Assessing agreement on classiﬁcation tasks: the Kappa statistic. Computational Linguistics, 22(2):249–254, 1996.

600

References

[CC001]
LIBSVM: a library for support vector machines, 2001. Software available at http:
//www.csie.ntu.edu.tw/~cjlin/libsvm.
[Chinchor 92]
N. Chinchor. MUC-4 Evaluation Metrics. In Proceedings of the Fourth Message
Understanding Conference, pages 22–29, 1992.
[Cimiano et al. 03]
P. Cimiano, S.Staab, and J. Tane. Automatic Acquisition of Taxonomies from Text:
FCA meets NLP. In Proceedings of the ECML/PKDD Workshop on Adaptive Text
Extraction and Mining, pages 10–17, Cavtat-Dubrovnik, Croatia, 2003.
[Cobuild 99]
C. Cobuild, editor. English Grammar. Harper Collins, 1999.
[Cunningham & Bontcheva 05]
H. Cunningham and K. Bontcheva. Computational Language Systems, Architectures. Encyclopedia of Language and Linguistics, 2nd Edition, pages 733–752, 2005.
[Cunningham & Scott 04a]
H. Cunningham and D. Scott. Introduction to the Special Issue on Software
Architecture for Language Engineering. Natural Language Engineering, 2004.
http://gate.ac.uk/sale/jnle-sale/intro/intro-main.pdf.
[Cunningham & Scott 04b]
H. Cunningham and D. Scott, editors. Special Issue of Natural Language Engineering
on Software Architecture for Language Engineering. Cambridge University Press,
2004.
[Cunningham 94]
H. Cunningham. Support Software for Language Engineering Research. Technical
Report 94/05, Centre for Computational Linguistics, UMIST, Manchester, 1994.
[Cunningham 99a]
H. Cunningham. A Deﬁnition and Short History of Language Engineering. Journal
of Natural Language Engineering, 5(1):1–16, 1999.
[Cunningham 99b]
H. Cunningham. JAPE: a Java Annotation Patterns Engine. Research Memorandum
CS–99–06, Department of Computer Science, University of Sheﬃeld, May 1999.
[Cunningham 00]
H. Cunningham. Software Architecture for Language Engineering. Unpublished PhD
thesis, University of Sheﬃeld, 2000. http://gate.ac.uk/sale/thesis/.
[Cunningham 02]
H. Cunningham. GATE, a General Architecture for Text Engineering. Computers
and the Humanities, 36:223–254, 2002.

References

601

[Cunningham 05]
H. Cunningham. Information Extraction, Automatic. Encyclopedia of Language and
Linguistics, 2nd Edition, pages 665–677, 2005.
[Cunningham et al. 94]
H. Cunningham, M. Freeman, and W. Black. Software Reuse, Object-Oriented
Frameworks and Natural Language Processing. In New Methods in Language Processing (NeMLaP-1), September 1994, Manchester, 1994. (Re-published in book form
1997 by UCL Press).
[Cunningham et al. 95]
H. Cunningham, R. Gaizauskas, and Y. Wilks. A General Architecture for Text
Engineering (GATE) – a new approach to Language Engineering R&D. Technical
Report CS–95–21, Department of Computer Science, University of Sheﬃeld, 1995.
http://xxx.lanl.gov/abs/cs.CL/9601009.
[Cunningham et al. 96a]
H. Cunningham, K. Humphreys, R. Gaizauskas, and M. Stower. CREOLE Developer’s Manual. Technical report, Department of Computer Science, University of
Sheﬃeld, 1996. http://www.dcs.shef.ac.uk/nlp/gate.
[Cunningham et al. 96b]
H. Cunningham, K. Humphreys, R. Gaizauskas, and Y. Wilks. TIPSTERCompatible Projects at Sheﬃeld. In Advances in Text Processing, TIPSTER Program Phase II. DARPA, Morgan Kaufmann, California, 1996.
[Cunningham et al. 96c]
H. Cunningham, Y. Wilks, and R. Gaizauskas.
GATE – a General
Architecture for Text Engineering.
In Proceedings of the 16th Conference on Computational Linguistics (COLING-96), Copenhagen, August 1996.
ftp://ftp.dcs.shef.ac.uk/home/hamish/auto papers/Cun96b.ps.
[Cunningham et al. 96d]
H. Cunningham, Y. Wilks, and R. Gaizauskas. Software Infrastructure for Language
Engineering. In Proceedings of the AISB Workshop on Language Engineering for
Document Analysis and Recognition, Brighton, U.K., April 1996.
[Cunningham et al. 96e]
H. Cunningham, Y. Wilks, and R. Gaizauskas. New Methods, Current Trends and
Software Infrastructure for NLP. In Proceedings of the Conference on New Methods
in Natural Language Processing (NeMLaP-2), Bilkent University, Turkey, September
1996. ftp://ftp.dcs.shef.ac.uk/home/hamish/auto papers/Cun96c.ps.
[Cunningham et al. 97a]
H. Cunningham, K. Humphreys, R. Gaizauskas, and Y. Wilks. GATE – a TIPSTERbased General Architecture for Text Engineering. In Proceedings of the TIPSTER
Text Program (Phase III) 6 Month Workshop. DARPA, Morgan Kaufmann, California, May 1997. ftp://ftp.dcs.shef.ac.uk/home/hamish/auto papers/Cun97e.ps.

602

References

[Cunningham et al. 97b]
H. Cunningham, K. Humphreys, R. Gaizauskas, and Y. Wilks.
Software
Infrastructure for Natural Language Processing.
In Proceedings of the 5th
Conference on Applied Natural Language Processing (ANLP-97), March 1997.
ftp://ftp.dcs.shef.ac.uk/home/hamish/auto papers/Cun97a.ps.gz.
[Cunningham et al. 98a]
H. Cunningham, W. Peters, C. McCauley, K. Bontcheva, and Y. Wilks. A Level
Playing Field for Language Resource Evaluation. In Workshop on Distributing
and Accessing Lexical Resources at Conference on Language Resources Evaluation,
Granada, Spain, 1998. http://www.dcs.shef.ac.uk/ hamish/dalr.
[Cunningham et al. 98b]
H. Cunningham, M. Stevenson, and Y. Wilks. Implementing a Sense Tagger within a
General Architecture for Language Engineering. In Proceedings of the Third Conference on New Methods in Language Engineering (NeMLaP-3), pages 59–72, Sydney,
Australia, 1998.
[Cunningham et al. 99]
H. Cunningham, R. Gaizauskas, K. Humphreys, and Y. Wilks. Experience with a
Language Engineering Architecture: Three Years of GATE. In Proceedings of the
AISB’99 Workshop on Reference Architectures and Data Standards for NLP, Edinburgh, April 1999. The Society for the Study of Artiﬁcial Intelligence and Simulation
of Behaviour. http://www.dcs.shef.ac.uk/ hamish/GateAisb99.html.
[Cunningham et al. 00a]
H. Cunningham, K. Bontcheva, W. Peters, and Y. Wilks.
Uniform language resource access and distribution in the context of a General Architecture for Text Engineering (GATE). In Proceedings of the Workshop on Ontologies and Language Resources (OntoLex’2000), Sozopol, Bulgaria, September 2000.
http://gate.ac.uk/sale/ontolex/ontolex.ps.
[Cunningham et al. 00b]
H. Cunningham, K. Bontcheva, V. Tablan, and Y. Wilks. Software Infrastructure
for Language Resources: a Taxonomy of Previous Work and a Requirements Analysis. In Proceedings of the 2nd International Conference on Language Resources and
Evaluation (LREC-2), Athens, 2000. http://gate.ac.uk/.
[Cunningham et al. 00c]
H. Cunningham, D. Maynard, K. Bontcheva, V. Tablan, and Y. Wilks. Experience
of using GATE for NLP R&D. In Proceedings of the Workshop on Using Toolsets
and Architectures To Build NLP Systems at COLING-2000, Luxembourg, 2000.
http://gate.ac.uk/.
[Cunningham et al. 00d]
H. Cunningham, D. Maynard, and V. Tablan. JAPE: a Java Annotation Patterns

References

603

Engine (Second Edition). Research Memorandum CS–00–10, Department of Computer Science, University of Sheﬃeld, November 2000.
[Cunningham et al. 02]
H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. GATE: A Framework
and Graphical Development Environment for Robust NLP Tools and Applications.
In Proceedings of the 40th Anniversary Meeting of the Association for Computational
Linguistics (ACL’02), 2002.
[Cunningham et al. 03]
H. Cunningham, V. Tablan, K. Bontcheva, and M. Dimitrov.
guage Engineering Tools for Collaborative Corpus Annotation.
Proceedings
of
Corpus
Linguistics
2003,
Lancaster,
UK,
http://gate.ac.uk/sale/cl03/distrib-ollie-cl03.doc.

LanIn
2003.

[Damljanovic & Bontcheva 08]
D. Damljanovic and K. Bontcheva. Enhanced Semantic Access to Software Artefacts.
In Workshop on Semantic Web Enabled Software Engineering (SWESE), Karlsruhe,
Germany, October 2008.
[Damljanovic 10]
D. Damljanovic. Towards Portable Controlled Natural Languages for Querying Ontologies. In M. Rosner and N. Fuchs, editors, Proceedings of the 2nd Workshop on
Controlled Natural Language, Lecture Notes in Computer Science. Springer Berlin /
Heidelberg, Marettimo Island, Sicily, September 2010.
[Damljanovic et al. 08]
D. Damljanovic, V. Tablan, and K. Bontcheva. A text-based query interface to
owl ontologies. In 6th Language Resources and Evaluation Conference (LREC),
Marrakech, Morocco, May 2008. ELRA.
[Damljanovic et al. 09]
D. Damljanovic, F. Amardeilh, and K. Bontcheva. CA Manager Framework:
Creating Customised Workﬂows for Ontology Population and Semantic Annotation. In Proceedings of The Fifth International Conference on Knowledge Capture
(KCAP’09), California, USA, September 2009.
[Davies & Fleiss 82]
M. Davies and J. Fleiss. Measuring Agreement for Multinomial Data. Biometrics,
38:1047–1051, 1982.
[Davis et al. 06]
B. Davis, S. Handschuh, H. Cunningham, and V. Tablan. Further use of Controlled
Natural Language for Semantic Annotation of Wikis. In Proceedings of the 1st Semantic Authoring and Annotation Workshop at ISWC2006, Athens, Georgia, USA,
November 2006.

604

References

[Day et al. 97]
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, P. Robinson, and M. Vilain. MixedInitiative Development of Language Processing Systems. In Proceedings of the 5th
Conference on Applied Natural Language Processing (ANLP-97), 1997.
[Della Valle et al. 08]
E. Della Valle, D. Cerizza, I. Celino, A. Turati, H. Lausen, N. Steinmetz, M. Erdmann, and A. Funk. Realizing Service-Finder: Web service discovery at web scale.
In European Semantic Technology Conference (ESTC), Vienna, September 2008.
[Dimitrov 02a]
M. Dimitrov.
A Light-weight Approach to Coreference Resolution for
Named Entities in Text.
MSc Thesis, University of Soﬁa, Bulgaria, 2002.
http://www.ontotext.com/ie/thesis-m.pdf.
[Dimitrov 02b]
M. Dimitrov.
A Light-weight Approach to Coreference Resolution for
Named Entities in Text.
MSc Thesis, University of Soﬁa, Bulgaria, 2002.
http://www.ontotext.com/ie/thesis-m.pdf.
[Dimitrov et al. 02]
M. Dimitrov, K. Bontcheva, H. Cunningham, and D. Maynard. A Light-weight
Approach to Coreference Resolution for Named Entities in Text. In Proceedings
of the Fourth Discourse Anaphora and Anaphor Resolution Colloquium (DAARC),
Lisbon, 2002.
[Dimitrov et al. 04]
M. Dimitrov, K. Bontcheva, H. Cunningham, and D. Maynard. A Light-weight
Approach to Coreference Resolution for Named Entities in Text. In A. Branco,
T. McEnery, and R. Mitkov, editors, Anaphora Processing: Linguistic, Cognitive
and Computational Modelling. John Benjamins, 2004.
[Dowman et al. 05a]
M. Dowman, V. Tablan, H. Cunningham, and B. Popov.
Content augmentation for mixed-mode news broadcasts.
In Proceedings of the 3rd
European Conference on Interactive Television:
User Centred ITV Systems, Programmes and Applications, Aalborg University, Denmark, 2005.

http://gate.ac.uk/sale/euro-itv-2005/content-augmentation-for-mixed-mode-news-broadcast

[Dowman et al. 05b]
M. Dowman, V. Tablan, H. Cunningham, and B. Popov. Web-assisted annotation, semantic indexing and search of television and radio news. In Proceedings of the 14th International World Wide Web Conference, Chiba, Japan, 2005.
http://gate.ac.uk/sale/www05/web-assisted-annotation.pdf.
[Dowman et al. 05c]
M. Dowman, V. Tablan, H. Cunningham, C. Ursu, and B. Popov. Semantically

References

605

enhanced television news through web and video integration. In Second European
Semantic Web Conference (ESWC’2005), 2005.
[DUC 01]
NIST. Proceedings of the Document Understanding Conference, September 13 2001.
[Eugenio & Glass 04]
B. D. Eugenio and M. Glass. The kappa statistic: a second look. Computational
Linguistics, 1(30), 2004. (squib).
[Fleiss 75]
J. L. Fleiss. Measuring agreement between two judges on the presence or absence of
a trait. Biometrics, 31:651–659, 1975.
[Frakes & Baeza-Yates 92]
W. Frakes and R. Baeza-Yates, editors. Information retrieval, data structures and
algorithms. Prentice Hall, New York, Englewood Cliﬀs, N.J., 1992.
[Funk et al. 07a]
A. Funk, D. Maynard, H. Saggion, and K. Bontcheva. Ontological integration of information extracted from multiple sources. In Multi-source Multilingual Information
Extraction and Summarization (MMIES) workshop at Recent Advances in Natural
Language Processing (RANLP07), pages 9–15, Borovets, Bulgaria, September 2007.
[Funk et al. 07b]
A. Funk, V. Tablan, K. Bontcheva, H. Cunningham, B. Davis, and S. Handschuh.
CLOnE: Controlled Language for Ontology Editing. In Proceedings of the 6th International Semantic Web Conference (ISWC 2007), Busan, Korea, November 2007.
[Gaizauskas et al. 95]
R. Gaizauskas, T. Wakao, K. Humphreys, H. Cunningham, and Y. Wilks. Description of the LaSIE system as used for MUC-6. In Proceedings of the Sixth Message
Understanding Conference (MUC-6). Morgan Kaufmann, California, 1995.
[Gaizauskas et al. 96a]
R. Gaizauskas, P. Rodgers, H. Cunningham, and K. Humphreys. GATE User Guide.
http://www.dcs.shef.ac.uk/nlp/gate, 1996.
[Gaizauskas et al. 96b]
R. Gaizauskas, H. Cunningham, Y. Wilks, P. Rodgers, and K. Humphreys.
GATE – an Environment to Support Research and Development in Natural Language Engineering. In Proceedings of the 8th IEEE International Conference on
Tools with Artiﬁcial Intelligence (ICTAI-96), Toulouse, France, October 1996.
ftp://ftp.dcs.shef.ac.uk/home/robertg/ictai96.ps.
[Gaizauskas et al. 03]
R. Gaizauskas, M. A. Greenwood, M. Hepple, I. Roberts, H. Saggion, and M. Sargaison. The University of Sheﬃeld’s TREC 2003 Q&A Experiments. In In Proceedings
of the 12th Text REtrieval Conference, 2003.

606

References

[Gaizauskas et al. 04]
R. Gaizauskas, M. A. Greenwood, M. Hepple, I. Roberts, H. Saggion, and M. Sargaison. The University of Sheﬃeld’s TREC 2004 Q&A Experiments. In In Proceedings
of the 13th Text REtrieval Conference, 2004.
[Gaizauskas et al. 05]
R. Gaizauskas, M. A. Greenwood, M. Hepple, H. Harkema, H. Saggion, and
A. Sanka. The University of Sheﬃelds TREC 2005 Q&A Experiments. In In Proceedings of the 11th Text REtrieval Conference, 2005.
[Gamb¨ck & Olsson 00]
a
B. Gamb¨ck and F. Olsson. Experiences of Language Engineering Algorithm Reuse.
a
In Second International Conference on Language Resources and Evaluation (LREC),
pages 155–160, Athens, Greece, 2000.
[Gazdar & Mellish 89]
G. Gazdar and C. Mellish. Natural Language Processing in Prolog. Addison-Wesley,
Reading, MA, 1989.
[Greenwood et al. 02]
M. A. Greenwood, I. Roberts, and R. Gaizauskas. The University of Sheﬃelds TREC
2002 Q&A Experiments. In In Proceedings of the 11th Text REtrieval Conference,
2002.
[Grishman 97]
R. Grishman. TIPSTER Architecture Design Document Version 2.3. Technical report, DARPA, 1997. http://www.itl.nist.gov/div894/894.02/related projects/tipster/.
[Hepple 00]
M. Hepple. Independence and commitment: Assumptions for rapid training and
execution of rule-based POS taggers. In Proceedings of the 38th Annual Meeting
of the Association for Computational Linguistics (ACL-2000), Hong Kong, October
2000.
[Hripcsak & Heitjan 02]
G. Hripcsak and D. Heitjan. Measuring agreement in medical informatics reliability
studies. Journal of Biomedical Informatics, 35:99–110, 2002.
[Hripcsak & Rothschild 05]
G. Hripcsak and A. S. Rothschild. Agreement, the F-measure, and Reliability in
Information Retrieval. Journal of the American Medical Informatics Association,
12(3):296–298, 2005.
[Humphreys et al. 96]
K. Humphreys, R. Gaizauskas, H. Cunningham, and S. Azzam. CREOLE Module
Speciﬁcations. http://www.dcs.shef.ac.uk/nlp/gate/, 1996.

References

607

[Humphreys et al. 98]
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. Cunningham, and Y. Wilks. Description of the LaSIE system as used for MUC7. In Proceedings of the Seventh Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/muc/index.html, 1998.
[Humphreys et al. 99]
K. Humphreys, R. Gaizauskas, M. Hepple, and M. Sanderson. The University of
Sheﬃeld TREC-8 Q&A System. In In Proceedings of the 8th Text REtrieval Conference, 1999.
[Ide et al. 00]
N. Ide, P. Bonhomme, and L. Romary. XCES: An XML-based Standard for Linguistic Corpora. In Proceedings of the Second International Language Resources and
Evaluation Conference (LREC), pages 825–830, Athens, Greece, 2000.
[Jackson 75]
M. Jackson. Principles of Program Design. Academic Press, London, 1975.
[Kiryakov 03]
A. Kiryakov. Ontology and Reasoning in MUMIS: Towards the Semantic Web. Technical Report CS–03–03, Department of Computer Science, University of Sheﬃeld,
2003. http://gate.ac.uk/gate/doc/papers.html.
[Kohlsch¨tter et al. 10]
u
C. Kohlsch¨tter, P. Fankhauser, and W. Nejdl. Boilerplate Detection using Shallow
u
Text Features. In Proceedings of the Third ACM International Conference on Web
Search and Data Mining, 2010.
[Laclavik & Maynard 09]
M. Laclavik and D. Maynard. Motivating intelligent email in business: an investigation into current trends for email processing and communication research. In
Proceedings of Workshop on Emails in e-Commerce and Enterprise Context, 11th
IEEE Conference on Commerce and Enterprise Computing, Vienna, Austria, 2009.
[Lal & Ruger 02]
P. Lal and S. Ruger. Extract-based summarization with simpliﬁcation. In Proceedings of the ACL 2002 Automatic Summarization / DUC 2002 Workshop, 2002.
http://www.doc.ic.ac.uk/ srueger/pr-p.lal-2002/duc02-final.pdf.
[Lal 02]
P. Lal. Text summarisation. Unpublished M.Sc. thesis, Imperial College, London,
2002.
[Li & Bontcheva 08]
Y. Li and K. Bontcheva. Adapting support vector machines for f-term-based classiﬁcation of patents. ACM Transactions on Asian Language Information Processing,
7(2):7:1–7:19, 2008.

608

References

[Li & Cunningham 08]
Y. Li and H. Cunningham. Geometric and Quantum Methods for Information Retrieval. SIGIR Forum, 42(2):22–32, 2008.
[Li & Shawe-Taylor 03]
Y. Li and J. Shawe-Taylor. The SVM with Uneven Margins and Chinese Document
Categorization. In Proceedings of The 17th Paciﬁc Asia Conference on Language,
Information and Computation (PACLIC17), Singapore, Oct. 2003.
[Li & Shawe-Taylor 06]
Y. Li and J. Shawe-Taylor. Using KCCA for Japanese-English Cross-language Information Retrieval and Document Classiﬁcation. Journal of Intelligent Information
Systems, 27(2):117–133, 2006.
[Li & Shawe-Taylor 07]
Y. Li and J. Shawe-Taylor. Advanced Learning Algorithms for Cross-language Patent
Retrieval and Classiﬁcation. Information Processing and Management, 43(5):1183–
1199, 2007.
[Li et al. 02]
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. The Perceptron
Algorithm with Uneven Margins. In Proceedings of the 9th International Conference
on Machine Learning (ICML-2002), pages 379–386, 2002.
[Li et al. 04]
Y. Li, K. Bontcheva, and H. Cunningham. An SVM Based Learning Algorithm for Information Extraction. Machine Learning Workshop, Sheﬃeld, 2004.
http://gate.ac.uk/sale/ml-ws04/mlw2004.pdf.
[Li et al. 05a]
Y. Li, K. Bontcheva, and H. Cunningham. SVM Based Learning System For Information Extraction. In M. N. J. Winkler and N. Lawerence, editors, Deterministic
and Statistical Methods in Machine Learning, LNAI 3635, pages 319–339. Springer
Verlag, 2005.
[Li et al. 05b]
Y. Li, K. Bontcheva, and H. Cunningham. Using Uneven Margins SVM and Perceptron for Information Extraction. In Proceedings of Ninth Conference on Computational Natural Language Learning (CoNLL-2005), 2005.
[Li et al. 05c]
Y. Li, C. Miao, K. Bontcheva, and H. Cunningham. Perceptron Learning for Chinese Word Segmentation. In Proceedings of Fourth SIGHAN Workshop on Chinese
Language processing (Sighan-05), pages 154–157, Korea, 2005.
[Li et al. 07a]
Y. Li, K. Bontcheva, and H. Cunningham. Hierarchical, Perceptron-like Learning

References

609

for Ontology Based Information Extraction. In 16th International World Wide Web
Conference (WWW2007), pages 777–786, May 2007.
[Li et al. 07b]
Y. Li, K. Bontcheva, and H. Cunningham. Cost Sensitive Evaluation Measures for
F-term Patent Classiﬁcation. In The First International Workshop on Evaluating
Information Access (EVIA 2007), pages 44–53, May 2007.
[Li et al. 07c]
Y. Li, K. Bontcheva, and H. Cunningham. Experiments of opinion analysis on the
corpora MPQA and NTCIR-6. In Proceedings of the Sixth NTCIR Workshop Meeting
on Evaluation of Information Access Technologies: Information Retrieval, Question
Answering and Cross-Lingual Information Access, pages 323–329, May 2007.
[Li et al. 07d]
Y. Li, K. Bontcheva, and H. Cunningham. SVM Based Learning System for Fterm Patent Classiﬁcation. In Proceedings of the Sixth NTCIR Workshop Meeting
on Evaluation of Information Access Technologies: Information Retrieval, Question
Answering and Cross-Lingual Information Access, pages 396–402, May 2007.
[Li et al. 09]
Y. Li, K. Bontcheva, and H. Cunningham. Adapting SVM for Data Sparseness and
Imbalance: A Case Study on Information Extraction. Natural Language Engineering,
15(2):241–271, 2009.
[Lombard et al. 02]
M. Lombard, J. Snyder-Duch, and C. C. Bracken. Content analysis in mass communication: Assessment and reporting of intercoder reliability. Human Communication
Research, 28:587–604, 2002.
[LREC-1 98]
Conference on Language Resources Evaluation (LREC-1), Granada, Spain, 1998.
[LREC-2 00]
Second Conference on Language Resources Evaluation (LREC-2), Athens, 2000.
[Maeda & Strassel 04]
K. Maeda and S. Strassel. Annotation Tools for Large-Scale Corpus Development:
Using AGTK at the Linguistic Data Consortium. In Proceedings of 4th Language
Resources and Evaluation Conference (LREC’2004), 2004.
[Manning & Sch¨tze 99]
u
C. Manning and H. Sch¨tze. Foundations of Statistical Natural Language Prou
cessing. MIT press, Cambridge, MA, 1999. Supporting materials available at
http://www.sultry.arts.usyd.edu.au/fsnlp/ .

610

References

[Manov et al. 03]
D. Manov, A. Kiryakov, B. Popov, K. Bontcheva, and D. Maynard. Experiments with geographic knowledge for information extraction. In Workshop on
Analysis of Geographic References, HLT/NAACL’03, Edmonton, Canada, 2003.
http://gate.ac.uk/sale/hlt03/paper03.pdf.
[Marsh & Perzanowski 98]
E. Marsh and D. Perzanowski. Muc-7 evaluation of ie technology: Overview of
results. In Proceedings of the Seventh Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related projects/muc/index.html, 1998.
[Maynard 05]
D. Maynard. Benchmarking ontology-based annotation tools for the semantic web.
In UK e-Science Programme All Hands Meeting (AHM2005) Workshop on Text
Mining, e-Research and Grid-enabled Language Technology, Nottingham, UK, 2005.
[Maynard 08]
D. Maynard. Benchmarking textual annotation tools for the semantic web. In Proc.
of 6th International Conference on Language Resources and Evaluation (LREC),
Marrakech, Morocco, 2008.
[Maynard et al. 00]
D. Maynard, H. Cunningham, K. Bontcheva, R. Catizone, G. Demetriou,
R. Gaizauskas, O. Hamza, M. Hepple, P. Herring, B. Mitchell, M. Oakes, W. Peters,
A. Setzer, M. Stevenson, V. Tablan, C. Ursu, and Y. Wilks. A Survey of Uses of
GATE. Technical Report CS–00–06, Department of Computer Science, University
of Sheﬃeld, 2000.
[Maynard et al. 01]
D. Maynard, V. Tablan, C. Ursu, H. Cunningham, and Y. Wilks. Named Entity
Recognition from Diverse Text Types. In Recent Advances in Natural Language
Processing 2001 Conference, pages 257–274, Tzigov Chark, Bulgaria, 2001.
[Maynard et al. 02a]
D. Maynard, K. Bontcheva, H. Saggion, H. Cunningham, and O. Hamza. Using a
Text Engineering Framework to Build an Extendable and Portable IE-based Summarisation System. In Proceedings of the ACL Workshop on Text Summarisation,
pages 19–26, Phildadelphia, Pennsylvania, 2002. ACM.
[Maynard et al. 02b]
D. Maynard, H. Cunningham, K. Bontcheva, and M. Dimitrov. Adapting a robust
multi-genre NE system for automatic content extraction. In Proceedings of the 10th
International Conference on Artiﬁcial Intelligence: Methodology, Systems, Applications (AIMSA’02), Varna, Bulgaria, Sep 2002.
[Maynard et al. 02c]
D. Maynard, H. Cunningham, K. Bontcheva, and M. Dimitrov. Adapting A Robust

References

611

Multi-Genre NE System for Automatic Content Extraction. In Proceedings of the
Tenth International Conference on Artiﬁcial Intelligence: Methodology, Systems,
Applications (AIMSA 2002), 2002.
[Maynard et al. 02d]
D. Maynard, H. Cunningham, and R. Gaizauskas. Named entity recognition at
sheﬃeld university. In H. Holmboe, editor, Nordic Language Technology – Arbog for
Nordisk Sprogtechnologisk Forskningsprogram 2002-2004, pages 141–145. Museum
Tusculanums Forlag, 2002.
[Maynard et al. 02e]
D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Saggion, K. Bontcheva, and
Y. Wilks. Architectural Elements of Language Engineering Robustness. Journal
of Natural Language Engineering – Special Issue on Robust Methods in Analysis of
Natural Language Data, 8(2/3):257–274, 2002.
[Maynard et al. 03a]
D. Maynard, K. Bontcheva, and H. Cunningham. From information extraction to
content extraction. Submitted to EACL’2003, 2003.
[Maynard et al. 03b]
D. Maynard, K. Bontcheva, and H. Cunningham. Towards a semantic extraction of named entities. In G. Angelova, K. Bontcheva, R. Mitkov, N. Nicolov, and N. Nikolov, editors, Proceedings of Recent Advances in Natural Language Processing (RANLP’03), pages 255–261, Borovets, Bulgaria, Sep 2003.
http://gate.ac.uk/sale/ranlp03/ranlp03.pdf.
[Maynard et al. 03c]
D. Maynard, K. Bontcheva, and H. Cunningham. Towards a semantic extraction
of Named Entities. In Recent Advances in Natural Language Processing, Bulgaria,
2003.
[Maynard et al. 03d]
D. Maynard, V. Tablan, K. Bontcheva, and H. Cunningham. Rapid customisation
of an Information Extraction system for surprise languages. Special issue of ACM
Transactions on Asian Language Information Processing: Rapid Development of
Language Capabilities: The Surprise Languages, 2:295–300, 2003.
[Maynard et al. 03e]
D. Maynard, V. Tablan, and H. Cunningham. NE recognition without training
data on a language you don’t speak. In ACL Workshop on Multilingual and Mixedlanguage Named Entity Recognition: Combining Statistical and Symbolic Models,
Sapporo, Japan, 2003.
[Maynard et al. 04a]
D. Maynard, K. Bontcheva, and H. Cunningham. Automatic Language-Independent
Induction of Gazetteer Lists. In Proceedings of 4th Language Resources and Evaluation Conference (LREC’04), Lisbon, Portugal, 2004. ELRA.

612

References

[Maynard et al. 04b]
D. Maynard, H. Cunningham, A. Kourakis, and A. Kokossis. Ontology-Based Information Extraction in hTechSight. In First European Semantic Web Symposium
(ESWS 2004), Heraklion, Crete, 2004.
[Maynard et al. 04c]
D. Maynard, M. Yankova, N. Aswani, and H. Cunningham. Automatic Creation and
Monitoring of Semantic Metadata in a Dynamic Knowledge Portal. In Proceedings of
the 11th International Conference on Artiﬁcial Intelligence: Methodology, Systems,
Applications (AIMSA 2004), Varna, Bulgaria, 2004.
[Maynard et al. 06]
D. Maynard, W. Peters, and Y. Li. Metrics for evaluation of ontology-based information extraction. In WWW 2006 Workshop on Evaluation of Ontologies for the
Web (EON), Edinburgh, Scotland, 2006.
[Maynard et al. 07a]
D. Maynard, W. Peters, M. d’Aquin, and M. Sabou. Change management for metadata evolution. In ESWC International Workshop on Ontology Dynamics (IWOD),
Innsbruck, Austria, June 2007.
[Maynard et al. 07b]
D. Maynard, H. Saggion, M. Yankova, K. Bontcheva, and W. Peters. Natural Language Technology for Information Integration in Business Intelligence. In 10th International Conference on Business Information Systems (BIS-07), Poznan, Poland,
25-27 April 2007.
[Maynard et al. 08a]
D. Maynard, W. Peters, and Y. Li. Evaluating evaluation metrics for ontologybased applications: Inﬁnite reﬂection. In Proc. of 6th International Conference on
Language Resources and Evaluation (LREC), Marrakech, Morocco, 2008.
[Maynard et al. 08b]
D. Maynard, Y. Li, and W. Peters. NLP Techniques for Term Extraction and Ontology Population. In P. Buitelaar and P. Cimiano, editors, Bridging the Gap between
Text and Knowledge - Selected Contributions to Ontology Learning and Population
from Text. IOS Press, 2008.
[Maynard et al. 09]
D. Maynard, A. Funk, and W. Peters. SPRAT: a tool for automatic semantic patternbased ontology population. In International Conference for Digital Libraries and the
Semantic Web, Trento, Italy, September 2009.
[McEnery et al. 00]
A. McEnery, P. Baker, R. Gaizauskas, and H. Cunningham. EMILLE: Building a
Corpus of South Asian Languages. Vivek, A Quarterly in Artiﬁcial Intelligence,
13(3):23–32, 2000.

References

613

[Osenova & Simov 04]
P. Osenova and K. Simov. BulTreeBank stylebook. Technical Report BTB-TR05,
BulTreeBank Project, May 2004.
[Pastra et al. 02]
K. Pastra, D. Maynard, H. Cunningham, O. Hamza, and Y. Wilks. How
feasible is the reuse of grammars for named entity recognition?
In Proceedings of the 3rd Language Resources and Evaluation Conference, 2002.
http://gate.ac.uk/sale/lrec2002/reusability.ps.
[Peters et al. 98]
W. Peters, H. Cunningham, C. McCauley, K. Bontcheva, and Y. Wilks. Uniform Language Resource Access and Distribution. In Workshop on Distributing
and Accessing Lexical Resources at Conference on Language Resources Evaluation,
Granada, Spain, 1998.
[Polajnar et al. 05]
T. Polajnar, V. Tablan, and H. Cunningham. User-friendly ontology authoring
using a controlled language. Technical Report CS Report No. CS-05-10, University
of Sheﬃeld, Sheﬃeld, UK, 2005.
[Porter 80]
M. Porter. An algorithm for suﬃx stripping. Program, 14(3):130–137, 1980.
[Ramshaw & Marcus 95]
L. Ramshaw and M. Marcus. Text Chunking Using Transformation-Based Learning.
In Proceedings of the Third ACL Workshop on Very Large Corpora, 1995.
[Saggion & Funk 09]
H. Saggion and A. Funk. Extracting opinions and facts for business intelligence.
RNTI Journal, E(17):119–146, November 2009.
[Saggion & Gaizauskas 04a]
H. Saggion and R. Gaizauskas. Mining on-line sources for deﬁnition knowledge.
In Proceedings of the 17th FLAIRS 2004, Miami Bearch, Florida, USA, May 17-19
2004. AAAI.
[Saggion & Gaizauskas 04b]
H. Saggion and R. Gaizauskas. Multi-document summarization by cluster/proﬁle
relevance and redundancy removal. In Proceedings of the Document Understanding
Conference 2004. NIST, 2004.
[Saggion & Gaizauskas 05]
H. Saggion and R. Gaizauskas. Experiments on statistical and pattern-based biographical summarization. In Proceedings of EPIA 2005, pages 611–621, 2005.

614

References

[Saggion 04]
H. Saggion. Identifying deﬁnitions in text collections for question answering. lrec.
In Proceedings of Language Resources and Evaluation Conference. ELDA, 2004.
[Saggion 06]
H. Saggion. Multilingual Multidocument Summarization Tools and Evaluation. In
Proceedings of LREC 2006, 2006.
[Saggion 07]
H. Saggion. Shef: Semantic tagging and summarization techniques applied to crossdocument coreference. In Proceedings of SemEval 2007, Assocciation for Computational Linguistics, pages 292–295, June 2007.
[Saggion et al. 02a]
H. Saggion, H. Cunningham, K. Bontcheva, D. Maynard, C. Ursu, O. Hamza, and
Y. Wilks. Access to Multimedia Information through Multisource and Multilanguage
Information Extraction. In Proceedings of the 7th Workshop on Applications of
Natural Language to Information Systems (NLDB 2002), Stockholm, Sweden, 2002.
[Saggion et al. 02b]
H. Saggion, H. Cunningham, D. Maynard, K. Bontcheva, O. Hamza, C. Ursu, and
Y. Wilks. Extracting Information for Information Indexing of Multimedia Material.
In Proceedings of 3rd Language Resources and Evaluation Conference (LREC’2002),
2002. http://gate.ac.uk/sale/lrec2002/mumis lrec2002.ps.
[Saggion et al. 03a]
H. Saggion, K. Bontcheva, and H. Cunningham. Robust Generic and Query-based
Summarisation. In Proceedings of the European Chapter of Computational Linguistics (EACL), Research Notes and Demos, 2003.
[Saggion et al. 03b]
H. Saggion, H. Cunningham, K. Bontcheva, D. Maynard, O. Hamza, and Y. Wilks.
Multimedia Indexing through Multisource and Multilingual Information Extraction;
the MUMIS project. Data and Knowledge Engineering, 48:247–264, 2003.
[Saggion et al. 03c]
H. Saggion, J. Kuper, H. Cunningham, T. Declerck, P. Wittenburg, M. Puts, F. DeJong, and Y. Wilks. Event-coreference across Multiple, Multi-lingual Sources in the
Mumis Project. In Proceedings of the European Chapter of Computational Linguistics (EACL), Research Notes and Demos, 2003.
[Saggion et al. 07]
H. Saggion, A. Funk, D. Maynard, and K. Bontcheva. Ontology-based information
extraction for business applications. In Proceedings of the 6th International Semantic
Web Conference (ISWC 2007), Busan, Korea, November 2007.

References

615

[Scott & Gaizauskas. 00]
S. Scott and R. Gaizauskas. The University of Sheﬃeld TREC-9 Q&A System. In
In Proceedings of the 9th Text REtrieval Conference, 2000.
[Shaw & Garlan 96]
M. Shaw and D. Garlan. Software Architecture. Prentice Hall, New York, 1996.
[Simov & Osenova 03]
K. Simov and P. Osenova. Practical annotation scheme for an HPSG treebank
of Bulgarian. In Proceedings of the 4th International Workshop on Linguistically
Interpreteted Corpora (LINC-2003), Budapest, Hungary, 2003.
[Simov et al. 02]
K. Simov, G. Popova, and P. Osenova. HPSG-based syntactic treebank of Bulgarian
(BulTreeBank). In A. Wilson, P. Rayson, and T. McEnery, editors, A Rainbow
of Corpora: Corpus Linguistics and the Languages of the World, pages 135–142.
Lincom-Europa, Munich, 2002.
[Simov et al. 04a]
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. Design and implementation of
the Bulgarian HPSG-based treebank. Journal of Research on Language and Computation, 2(4):495–522, December 2004.
[Simov et al. 04b]
K. Simov, P. Osenova, and M. Slavcheva. BulTreeBank morphosyntactic tagset.
Technical Report BTB-TR03, BulTreeBank Project, March 2004.
[Stevenson et al. 98]
M. Stevenson, H. Cunningham, and Y. Wilks. Sense tagging and language engineering. In Proceedings of the 13th European Conference on Artiﬁcial Intelligence
(ECAI-98), pages 185–189, Brighton, U.K., 1998.
[Tablan et al. 02]
V. Tablan, C. Ursu, K. Bontcheva, H. Cunningham, D. Maynard, O. Hamza,
T. McEnery, P. Baker, and M. Leisher. A Unicode-based Environment for
Creation and Use of Language Resources. In 3rd Language Resources and
Evaluation Conference, Las Palmas, Canary Islands – Spain, 2002. ELRA.
http://gate.ac.uk/sale/iesl03/iesl03.pdf.
[Tablan et al. 03]
V. Tablan, K. Bontcheva, D. Maynard, and H. Cunningham. Ollie: on-line learning
for information extraction. In SEALTS ’03: Proceedings of the HLT-NAACL 2003
workshop on Software engineering and architecture of language technology systems,
volume 8, pages 17–24, Morristown, NJ, USA, 2003. Association for Computational
Linguistics. http://gate.ac.uk/sale/hlt03/ollie-sealts.pdf.

616

References

[Tablan et al. 06a]
V. Tablan, W. Peters, D. Maynard, H. Cunningham, and K. Bontcheva. Creating tools for morphological analysis of sumerian. In 5th Language Resources and
Evaluation Conference (LREC), Genoa, Italy, May 2006. ELRA.
[Tablan et al. 06b]
V. Tablan, T. Polajnar, H. Cunningham, and K. Bontcheva. User-friendly ontology
authoring using a controlled language. In 5th Language Resources and Evaluation
Conference (LREC), Genoa, Italy, May 2006. ELRA.
[Tablan et al. 08]
V. Tablan, D. Damljanovic, and K. Bontcheva. A natural language query interface to
structured information. In Proceedings of the 5h European Semantic Web Conference
(ESWC 2008), volume 5021 of Lecture Notes in Computer Science, pages 361–375,
Tenerife, Spain, June 2008. Springer-Verlag New York Inc.
[Ursu et al. 05]
C. Ursu, T. Tablan, H. Cunningham, and B. Popav. Digital media preservation and
access through semantically enhanced web-annotation. In Proceedings of the 2nd
European Workshop on the Integration of Knowledge, Semantic and Digital Media
Technologies (EWIMT 2005), London, UK, December 01 2005.
[van Rijsbergen 79]
C. van Rijsbergen. Information Retrieval. Butterworths, London, 1979.
[Wang et al. 05]
T. Wang, D. Maynard, W. Peters, K. Bontcheva, and H. Cunningham. Extracting
a domain ontology from linguistic resource based on relatedness measurements. In
Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2005), pages 345–351, Compiegne, France, Septmeber 2005.
[Wang et al. 06]
T. Wang, Y. Li, K. Bontcheva, H. Cunningham, and J. Wang. Automatic Extraction
of Hierarchical Relations from Text. In Proceedings of the Third European Semantic
Web Conference (ESWC 2006), Budva, Montenegro, 2006.
[Witten & Frank 99]
I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and
Techniques with Java Implementations. Morgan Kaufmann, 1999.
[Wood et al. 03]
M. M. Wood, S. J. Lydon, V. Tablan, D. Maynard, and H. Cunningham. Using
parallel texts to improve recall in IE. In Recent Advances in Natural Language
Processing, Bulgaria, 2003.
[Wood et al. 04]
M. Wood, S. Lydon, V. Tablan, D. Maynard, and H. Cunningham. Populating

References

617

a Database from Parallel Texts using Ontology-based Information Extraction. In
Proceedings of NLDB 2004, 2004. http://gate.ac.uk/sale/nldb2004/NLDB.pdf.
[Yourdon 89]
E. Yourdon. Modern Structured Analysis. Prentice Hall, New York, 1989.
[Yourdon 96]
E. Yourdon. The Rise and Resurrection of the American Programmer. Prentice Hall,
New York, 1996.

618

References

Colophon
Formal semantics (henceforth FS), at least as it relates to computational language understanding, is in one way rather like connectionism, though without
the crucial prop Sejnowski’s work (1986) is widely believed to give to the latter:
both are old doctrines returned, like the Bourbons, having learned nothing and
forgotten nothing. But FS has nothing to show as a showpiece of success after
all the intellectual groaning and eﬀort.
On Keeping Logic in its Place (in Theoretical Issues in Natural Language Processing, ed. Wilks), Yorick Wilks, 1989 (p.130).
A
We used L TEX to produce this document, along with TeX4HT for the HTML production.
Thank you Don Knuth, Leslie Lamport and Eitan Gurari.

619

Boston University Computer Science Technical Report No. 2005-12

Real Time Eye Tracking and Blink Detection with USB Cameras
Michael Chau and Margrit Betke
Computer Science Department
Boston University
Boston, MA 02215, USA
{mikechau, betke@cs.bu.edu}
May 12, 2005

Abstract

1

Introduction

A human-computer interface (HCI) system designed for use
by people with severe disabilities is presented. People that
are severely paralyzed or afﬂicted with diseases such as
ALS (Lou Gehrig’s disease) or multiple sclerosis are unable to move or control any parts of their bodies except for
their eyes. The system presented here detects the user’s eye
blinks and analyzes the pattern and duration of the blinks,
using them to provide input to the computer in the form of
a mouse click. After the automatic initialization of the system occurs from the processing of the user’s involuntary eye
blinks in the ﬁrst few seconds of use, the eye is tracked in
real time using correlation with an online template. If the
user’s depth changes signiﬁcantly or rapid head movement
occurs, the system is automatically reinitialized. There are
no lighting requirements nor ofﬂine templates needed for
the proper functioning of the system. The system works with
inexpensive USB cameras and runs at a frame rate of 30
frames per second. Extensive experiments were conducted
to determine both the system’s accuracy in classifying voluntary and involuntary blinks, as well as the system’s ﬁtness
in varying environment conditions, such as alternative camera placements and different lighting conditions. These experiments on eight test subjects yielded an overall detection
accuracy of 95.3%.

A great deal of computer vision research is dedicated to
the implementation of systems designed to detect user
movements and facial gestures [1, 2, 4, 5, 6, 15, 16]. In
many cases, such systems are created with the speciﬁc goal
of providing a way for people with disabilities or limited
motor skills to be able to use computer systems, albeit in
much simpler applications [1, 15, 16]. The motivation for
the system proposed here is to provide an inexpensive,
unobtrusive means for disabled people to interact with
simple computer applications in a meaningful way that
requires minimal effort.
This goal is accomplished using a robust algorithm
based on the work by Grauman et al. [11, 12]. Some of
these methods are implemented here, while some have been
enhanced or modiﬁed to the end of simpliﬁed initialization
and more efﬁcient maintenance of the real time tracking.
The automatic initialization phase is triggered by the
analysis of the involuntary blinking of the current user of
the system, which creates an online template of the eye
to be used for tracking. This phase occurs each time the
current correlation score of the tracked eye falls below a
deﬁned threshold in order to allow the system to recover
and regain its accuracy in detecting the blinks. This system
can be utilized by users that are able to voluntarily blink
and have a use for applications that require mouse clicks as
input (e.g. switch and scanning programs/games [22]).
A thorough survey on work related to eye and blink
detection methods is presented by Grauman et al., as well
as Magee et al. [12, 16]. Since the implementation of the
1

BlinkLink blink detection system by Grauman et al., a
number of signiﬁcant contributions and advancements have
been made in the HCI ﬁeld. Gorodnichy and Roth present
communication interfaces that operate using eye blinks
[8, 9, 10]. Motion analysis methods and frame differencing
techniques used to locate the eyes are used Bhaskar et al.
and Gorodnichy [3, 8, 9]. Detecting eye blinking in the
presence of spontaneous movements as well as occlusion
and out-of-plane motion is discussed by Moriyama et al.
[19]. Methods for locating eyes using gradients and luminance and color information with templates are presented
by Rurainsky and Eisert [21]. Miglietta et al. present
results of a study involving the use of an eyeglass frame
worn by the patients in an Intenstive Care Unit that detects
eye blinks to operate a switch system [18]. There still have
not been many blink detection related systems designed to
work with inexpensive USB webcams [7, 8]. There have,
however, been a number of other feature detection systems
that use more expensive and less portable alternatives, such
as digital and IR cameras for video input [3, 19, 21, 23].
Aside from the portability concerns, these systems are
also typically unable to achieve the desirable higher frame
rates of approximately 30 fps that are common with USB
cameras.

image differencing

tracker
lost

thresholding

detect and analyze
blinking

opening

track eye

label connected
components

create eye
template

filter out
infeasible pairs

return location of best
candidate for eye pair

Figure 1: Overview of the main stages in the system.

2.1 Initialization
Naturally, the ﬁrst step in analyzing the blinking of the user
is to locate the eyes. To accomplish this, the difference
image of each frame and the previous frame is created and
then thresholded, resulting in a binary image showing the
regions of movement that occurred between the two frames.

The main contribution of this paper is to provide a
robust reimplementation of the system described by Grauman et al. [11] that is able to run in real time at 30 frames
per second on readily available and affordable webcams.
As mentioned, most systems dealing with motion analysis
required the use of rather expensive equipment and highend video cameras. However, in recent years, inexpensive
webcams manufactured by companies such as Logitech
have become ubiquitous, facilitating the incorporation of
these motion analysis systems on a more widespread basis.
The system described here is an accurate and useful tool
to give handicapped people another alternative to interface
with computer systems.

Next, a 3x3 star-shaped convolution kernel is passed
over the binary difference image in an Opening morphological operation [14]. This functions to eliminate a great
deal of noise and naturally-occurring jitter that is present
around the user in the frame due to the lighting conditions
and the camera resolution, as well as the possibility of
background movement. In addition, this Opening operation
also produces fewer and larger connected components in
the vicinity of the eyes (when a blink happens to occur),
which is crucial for the efﬁciency and accuracy of the next
phase (see Figure 2).

2 Methods
The algorithm used by the system for detecting and analyzing blinks is initialized automatically, dependent only upon
the inevitability of the involuntary blinking of the user.
Motion analysis techniques are used in this stage, followed
by online creation of a template of the open eye to be used
for the subsequent tracking and template matching that is
carried out at each frame. A ﬂow chart depicting the main
stages of the system is shown in Figure 1.

A recursive labeling procedure is applied next to recover
the number of connected components in the resultant binary
image. Under the circumstances in which this system was
optimally designed to function, in which the users are
for the most part paralyzed, this procedure yields only a
few connected components, with the ideal number being
two (the left eye and the right eye). In the case that other
movement has occurred, producing a much larger number
of components, the system discards the current binary
2

2.2 Template Creation

A

B

C

If the previous stage results in a pair of components that
passes the set of ﬁlters, then it is a good indication that the
user’s eyes have been successfully located. At this point,
the location of the larger of the two components is chosen
for creation of the template. Since the size of the template
that is to be created is directly proportional to the size of
the chosen component, the larger one is chosen for the
purpose of having more brightness information, which will
result in more accurate tracking and correlation scores (see
Figure 3).

D

Since the system will be tracking the user’s open eye,
it would be a mistake to create the template at the instant
that the eye was located, since the user was blinking at this
moment. Thus, once the eye is believed to be located, a
timer is triggered. After a small number of frames elapse,
which is judged to be the approximate time needed for the
user’s eye to become open again after an involuntary blink,
the template of the user’s open eye is created. Therefore,
during initialization, the user is assumed to be blinking at a
normal rate of one involuntary blink every few moments.
Again, no ofﬂine templates are necessary and the creation
of this online template is completely independent of any
past templates that may have been created during the run of
the system.

Figure 2: Motion analysis phase: (A) User at frame f .
(B) User at frame f + 1, having just blinked. (C) Initial
difference of the two frames f and f +1. Note the great deal
of noise in the background due to the lighting conditions
and camera properties. (D) Difference image used to locate
the eyes after performing the Opening operation.
image and waits to process the next involuntary blink in
order to maintain efﬁciency and accuracy in locating the
eyes.
Given an image with a small number of connected
components output from the previous processing steps,
the system is able to proceed efﬁciently by considering
each pair of components as a possible match for the user’s
left and right eyes. The ﬁltering of unlikely eye pair
matches is based on the computation of six parameters
for each component pair: the width and height of each
of the two components and the horizontal and vertical
distance between the centroids of the two components. A
number of experimentally-derived heuristics are applied to
these statistics to pinpoint the exact pair that most likely
represents the user’s eyes. For example, if there is a large
difference in either the width or height of each of the two
components, then they likely are not the user’s eyes. As an
additional example of one of these many ﬁlters, if there is
a large vertical distance between the centroids of the two
components, then they are also not likely to be the user’s
eyes, since such a property would not be humanly possible.
Such observations not only lead to accurate detection of
the user’s eyes, but also speed up the search greatly by
eliminating unlikely components immediately.

Figure 3: Open eye templates: Note the diversity in the appearance of some of the open templates that were used during user experiments. Working templates range from very
small to large in overall size, as well very tight around the
eye to a larger area surrounding the eye, including the eyebrow.

2.3 Eye Tracking
As noted by Grauman et al., the use of template matching
is necessary for the desired accuracy in analyzing the user’s
blinking since it allows the user some freedom to move
around slightly [11]. Though the primary purpose of such
a system is to serve people with paralysis, it is a desirable
3

feature to allow for some slight movement by the user or
the camera that would not be feasible if motion analysis
were used alone.
The normalized correlation coefﬁcient, also implemented in the system proposed by Grauman et al., is used
to accomplish the tracking [11]. This measure is computed
at each frame using the following formula:
x,y
x,y

A

B

C

D

¯
¯
[f (x,y)−fu,v ][t(x−u,y−v)−t]

¯
[f (x,y)−fu,v ]2

x,y

¯
[t(x−u,y−v)−t]2

where f (x, y) is the brightness of the video frame at
¯
the point (x, y), fu,v is the average value of the video
frame in the current search region, t(x, y) is the brightness
¯
of the template image at the point (x, y), and t is the
average value of the template image. The result of this
computation is a correlation score between -1 and 1 that
indicates the similarity between the open eye template and
all points in the search region of the video frame. Scores
closer to 0 indicate a low level of similarity, while scores
closer to 1 indicate a probable match for the open eye
template. A major beneﬁt of using this similarity measure
to perform the tracking is that it is insensitive to constant
changes in ambient lighting conditions. The Results section
shows that the eye tracking and blink detection works just
as well in the presence of both very dark and bright lighting.

Figure 4: Sample frames of a typical session: (A) The system is in this state during the motion analysis phase. The red
rectangle represents the region that is considered during the
frame differencing and labeling of connected components.
(B) The system enters this state once the eye is located and
remains this way as long as the eye is not believed to be
lost. The green rectangle represents the region at which the
open eye template was selected and the red rectangle now
represents the drastically reduced search space for performing the correlation. (C) User at frame f , with eyes already
closed for the deﬁned voluntary blink duration and (D) user
at frame f + 1, opening his eyes, with a yellow dot being
drawn on the eye to indicate that a voluntary blink just occurred.

Since this method requires an extensive amount of
computation and is performed 30 times per second, the
search region is restricted to a small area around the user’s
eye (see Figure 4). This reduced search space allows the
system to remain running smoothly in real time since it
drastically reduces the computation needed to perform the
correlation search at each frame.

Close examination of the correlation scores over time for a
number of different users of the system reveals rather clear
boundaries that allow for the detection of the blinks. As the
user’s eye is in the normal open state, very high correlation
scores of about 0.85 to 1.0 are reported. As the user blinks,
the scores fall to values of about 0.5 to 0.55. Finally, a very
important range to note is the one containing scores below
about 0.45. Scores in this range normally indicate that the
tracker has lost the location of the eye. In such cases, the
system must be reinitialized to relocate and track the new
position of the eye.

2.4 Blink Detection
The detection of blinking and the analysis of blink duration
are based solely on observation of the correlation scores
generated by the tracking at the previous step using the
online template of the user’s eye. As the user’s eye closes
during the process of a blink, its similarity to the open
eye template decreases. Likewise, it regains its similarity to the template as the blink ends and the user’s eye
becomes fully open again. This decrease and increase in
similarity corresponds directly to the correlation scores
returned by the template matching procedure (see Figure 5).

Given these ranges of correlation scores and knowledge of what they signify derived from experimentation
and observation across a number of test subjects, the system
detects voluntary blinks by using a timer that is triggered
each time the correlation scores fall below the threshold of
scores that represent an open eye. If the correlation scores

4

correlation scores over time

1.2

correlation score

1
1.0

0.8
0.8

0.6
0.6

0.4
0.4

0.2
0.2

0.0
0
0 10

1

30

60

90

120

150

180

210

240

270

19 28 37 46 55 64 73 82 91 100 109 118 127 136 145 154 163 172 181 190 199 208 217 226 235 244 253 262 271 280

time (in frames)

Figure 6: System interface: Notable features include the
ability for the user to deﬁne the voluntary blink length, the
ability to reset the tracking at any time, should a poor or
unexpected template location be chosen, and the ability to
save the session to a video ﬁle.

Figure 5: Correlation scores for the open eye template plotted over time (in frames). The scores form a clear waveform, as noted by Grauman et al., which is useful in deriving a threshold to be used for classifying the user’s eyes as
being open or closed at each frame [11]. In this example,
there were three short blinks followed by three long blinks,
three short blinks again, and ﬁnally one more long blink.

detector accuracy should yield correspondingly high
accuracy results for the usability tests, subject to the user’s
understanding and capabilities in carrying out the given
tasks, such as simple reaction time and matching games, as
described by Grauman et al. [11].

remain below this threshold and above the threshold that
results in reinitialization of the system for a deﬁned number
of frames that can be set by the user, then a voluntary blink
is judged to have occurred, causing a mouse click to be
issued to the operating system.

Therefore, the experiments conducted for this system
were more focused on detector accuracy, since this is
a more standard measure of the overall accuracy of the
system across a broad range of users. In order to measure
the detection accuracy, test subjects were seated in front of
the computer, approximately 2 feet away from the camera.
Subjects were instructed to act naturally, but were asked not
to turn their heads or move too abruptly, since this could
potentially lead to repeated reinitialization of the system,
making it difﬁcult to test the accuracy. In addition, this
constraint allowed for a closer simulation of the system’s
target audience of handicapped users.

3 Experiments
The system was primarily developed and tested on a Windows XP PC with an Intel Pentium IV 2.8 GHz processor
and 1 GB RAM. Video was captured with a Logitech
Quickcam Pro 4000 webcam at 30 frames per second.
All video was processed as grayscale images of 320 x
240 pixels using various utilities from the Intel OpenCV
and Image Processing libraries, as well as the Microsoft
DirectX SDK [13, 20, 17]. Figure 6 shows the interface for
the system. The experiments were conducted with eight
test subjects at two different locations (see Figure 7).

Similar to the tests done by Grauman et al., subjects
were also asked to blink random test patterns that were
determined prior to the start of the session [11]. For example, subjects were asked to blink two short blinks followed
by a long (voluntary) blink, or were asked to blink twice
voluntarily followed by a short (involuntary) blink. These
test results serve to show how well the system distinguishes
between the voluntary and involuntary blinks, which is the
crux of the problem. Tests involving the voluntary blink
length parameter were also conducted, with values ranging
from 5 to 20 frames (1/6 of a second to 2/3 of a second).

Reviewing the work done by Grauman et al., it is apparent
that similar results were obtained with experiments based
on testing the accuracy of the system and experiments
based on testing the usability of the system as a switch
input device [11]. Intuitively, this makes sense as good

In addition, as a further contribution, numerous other

5

Summary of results

experiments were also conducted to determine the ﬁtness of
the system under varying circumstances, such as alternative
camera placements, lighting conditions, and distance to the
camera. Such considerations are crucial when ruminating
on the possible deployment of such a system in a clinical
setting. As mentioned in the Introduction, an eye blink
detection device based on the use of infrared goggles
has been tested with a switch program in a hospital [18],
where a number of potential problems could arise with this
system, such as the wide range of possible orientations
of the user and distances to the camera. Some of the
experiments conducted aim to simulate these conditions in
order to gain insight into the plausibility of utilizing this
system for a diverse population of handicapped users.

- total blinks analyzed
- overall system measures
total missed blinks
total false positives
detector accuracy
- experimental system measures
total missed blinks
total false positives
detector accuracy
voluntary blink length
missed blinks
false positives

2288
43
64
95.3%
125
173
87.4%

5
10
20
1.09% 1.01% 2.53%
1.49% 1.44% 2.80%

Figure 8: The experimental system measures include the experiments involving the adjustments in the voluntary blink
length parameter, while the overall system measures disregard these outliers, which are detailed in the table.

Video of each test session was captured online and
post-processed to determine how well the system performed. The number of voluntary and involuntary blinks
detected by the system were written to a log ﬁle during
the session. Afterwards, the actual number of times the
user blinked voluntarily and involuntarily were counted
manually by reviewing the video of the session. False
positives and missed blinks were also noted.

double the number of blinks were missed (58), and nearly
double the number of false positives were detected (64).
This leads to the choice of the word “natural” to describe
the default blink length of 10 frames (1/3 of a second). The
test subjects found this to be the most intuitive length of
time to consider as the prolonged blink, with lower values
being too close to the involuntary length, and with higher
values such as 20 frames (2/3 of a second) producing an
unnatural feeling that was too long to be useful as a switch
input. This feeling was well-founded, as this longer blink
length lead to a severe degradation in the detector accuracy.
Nearly all of the misses and false positives in these sessions
were caused by users not holding their voluntary blinks
long enough for the system to correctly classify them.

4 Results
A large volume of data was collected in order to assess the
system accuracy. Compared to the 204 blinks provided
in the sequences by Grauman et al. [11], a total of 2,288
true blinks by the eight test subjects were analyzed in the
experiments for this system. Disregarding the sessions involving the testing of the voluntary blink length parameter
for reasons to be discussed later, there were 43 missed
blinks and 64 false positives, for an overall accuracy rate of
95.3%. Incorporating all sessions and experiments, there
were 125 missed blinks and 173 false positives, for an
accuracy rate of 87.4%. See Figure 8 for a summary of the
main results of the experiments.

In fact, the other experiments, designed to test how
well the system would fair in an environment whose
conditions are not known a priori, only resulted in 20
missed blinks and 31 false positives (see Figures 9 and 10).
Thus, the vast majority of missed blinks and false positives
across all experiments can be attributed to poor choices in
the voluntary blink length, which should not be considered
a problem for the accuracy of the system since these trials
were purely experimental and a length of approximately
10 frames (1/3 of a second) is known to be ideal for high
performance.

The ﬁrst rate of 95.3% should be considered as the overall
accuracy measure of the system because of the nature of
some of the extended experiments that inherently function
to reduce the accuracy rate. For example, in sessions tested
with the default, most natural voluntary blink length of 10
frames (1/3 of a second), there were only 23 missed blinks
and 33 false positives out of 1,242 blinks. On the other
hand, in sessions tested with a voluntary blink length of 20
frames (2/3 of a second), out of 504 such blinks, more than

6

Figure 7: Sample frames from sessions for each of the eight test subjects.
7

Figure 9: Sample frames from sessions testing alternate positions of the camera. The system still works accurately
with the camera placed well below the user’s face, as well
as with the camera rotated as much as about 45 degrees.

Figure 10: Sample frames from sessions testing varying
lighting conditions. The system still works accurately in
exceedingly bright and dark environments.

5 Discussion and Conclusions

Another improvement is this system’s compatibility
with inexpensive USB cameras, as opposed to the highresolution Sony EVI-D30 color video CCD camera used
by Grauman et al. [11]. These Logitech USB cameras are
more affordable and portable, and perhaps most importantly, support a higher real-time frame rate of 30 frames
per second.

The system proposed in this paper provides a binary switch
input alternative for people with disabilities similar to the
one presented by Grauman et al. [11]. However, some
signiﬁcant improvements and contributions were made
over such predecessor systems.

The reliability of the system has been shown with the
high accuracy results reported in the previous section.
In addition to the extensive testing that was conducted
to retrieve these results, additional considerations and
circumstances that are important for such a system were
tested that were not treated experimentally by Grauman
et al. [11]. One such consideration is the performance of
the system under different lighting conditions (see Figure
10). The experiments indicate that the system performs
equally well in extreme lighting conditions (i.e. with all
lights turned off, leaving the computer monitor as the only
light source, and with a lamp aimed directly at the video
camera). The accuracy percentages in these cases were
approximately the same as those that were retrieved in
normal lighting conditions.

The automatic initialization phase (involving the motion analysis work) is greatly simpliﬁed in this system,
with no loss of accuracy in locating the user’s eyes and
choosing a suitable open eye template. Given the reasonable assumption that the user is positioned anywhere
from about 1 to 2 feet away from the camera, the eyes
are detected within moments. As the distance increases
beyond this amount, the eyes can still be detected in
some cases, but it may take a longer time to occur since
the candidate pairs are much smaller and start to fail
the tests designed to pick out the likely components that
represent the user’s eyes. In all of the experiments in
which the subjects were seated between 1 and 2 feet
from the camera, it never took more than three involuntary
blinks by the user before the eyes were located successfully.
8

Another important consideration is the placement and
orientation of the camera with respect to the user (see
Figure 9). This was tested carefully to determine how
much freedom is available when setting up the camera,
a potentially crucial point when considering a clinical
environment, especially an Intensive Care Unit, which is
a prime setting that would beneﬁt from this system [18].
Aside from horizontal offset and orientation of the camera,
another issue of concern is the vertical offset of the camera
in relation to the user’s eyes. The experiments showed
that placing the camera below the user’s head resulted in
desirable functioning of the system. However, if the camera
is placed too high above the user’s head, in such a way
that it is aiming down at the user at a signiﬁcant angle, the
blink detection is no longer as accurate. This is caused by
the very small amount of variation in correlation scores
as the user blinks, since nearly all that is visible to the
camera is the eyelid of the user. Thus, when positioning
the camera, it is beneﬁcial to the detection accuracy to
maximize the degree of variation between the open and
closed eye images of the user. Finally, with respect to the
clinical environment, this system provides an unobtrusive
alternative to the one tested by Miglietta et al., which
required the user to wear a set of eyeglass frames for blink
detection [18]. This is an important point, considering the
additional discomfort that such an apparatus may bring to
the patients.

Figure 11: Experiment with a user wearing glasses. In some
cases, overwhelming glare from the computer monitor prevented the eyes from being located (left). With just the right
maneuvering by the user, the system was sometimes able to
ﬁnd and track the eye (right).

Acknowledgments
The work was supported by the National Science Foundation with grants IIS-0093367, IIS-0308213, IIS-0329009,
and EIA-0202067.

References
[1] M. Betke, J. Gips, and P. Fleming. The camera mouse:
Visual tracking of body features to provide computer
access for people with severe disabilities. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 10:1, pages 1–10, March 2002.

Some tests were also conducted with users wearing
glasses (see Figure 11), which exposed somewhat of a limitation with the system. In some situations, glare from the
computer monitor prevented the eyes from being located
in the motion analysis phase. Users were sometimes able
to maneuver their heads and position their eyes in such a
way that the glare was minimized, resulting in successful
location of the eyes, but this is not a reasonable expectation
for severely disabled people that may be operating with the
system.

[2] M. Betke, W. Mullally, and J. Magee. Active detection
of eye scleras in real time. Proceedings of the IEEE
CVPR Workshop on Human Modeling, Analysis and
Synthesis (HMAS 2000), Hilton Head Island, SC, June
2000.
[3] T.N. Bhaskar, F.T. Keat, S. Ranganath, and Y.V.
Venkatesh. Blink detection and eye tracking for eye
localization. Proceedings of the Conference on Convergent Technologies for Asia-Paciﬁc Region (TENCON 2003), pages 821–824, Bangalore, Inda, October
15-17 2003.

With the rapid advancement of technology and hardware in
use by modern computers, the proposed system could potentially be utilized not just by handicapped people, but by
the general population as an additional binary input. Higher
frame rates and ﬁner camera resolutions could lead to more
robust eye detection that is less restrictive on the user, while
increased processing power could be used to enhance the
tracking algorithm to more accurately follow the user’s eye
and recover more gracefully when it is lost. The ease of
use and potential for rapid input that this system provides
could be used to enhance productivity by incorporating it to
generate input for a task in any general software program.

[4] R.L. Cloud, M. Betke, and J. Gips. Experiments with a
camera-based human-computer interface system. Proceedings of the 7th ERCIM Workshop, User Interfaces
For All (UI4ALL 2002), pages 103–110, Paris, France,
October 2002.
[5] S. Crampton and M. Betke. Counting ﬁngers in real
time: A webcam-based human-computer interface

9

with game applications. Proceedings of the Conference on Universal Access in Human-Computer Interaction (afﬁliated with HCI International 2003), pages
1357–1361, Crete, Greece, June 2003.

[15] J. Lombardi and M. Betke. A camera-based eyebrow
tracker for hands-free computer control via a binary
switch. Proceedings of the 7th ERCIM Workshop,
User Interfaces For All (UI4ALL 2002), pages 199–
200, Paris, France, October 2002.

[6] C. Fagiani, M. Betke, and J. Gips. Evaluation of tracking methods for human-computer interaction. Proceedings of the IEEE Workshop on Applications in
Computer Vision (WACV 2002), pages 121–126, Orlando, Florida, December 2002.

[16] J.J. Magee, M.R. Scott, B.N. Waber, and M. Betke.
Eyekeys: A real-time vision interface based on gaze
detection from a low-grade video camera. Proceedings of the IEEE Workshop on Real-Time Vision for
Human-Computer Interaction (RTV4HCI), Washington, D.C., July 2004.

[7] D.O. Gorodnichy. On importance of nose for face
tracking. Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition
(FG 2002), pages 188–196, Washington, D.C., May
20-21 2002.

[17] Microsoft directx 8 sdk.
http://www.microsoft.com/downloads.
[18] M.A. Miglietta, G. Bochicchio, and T.M. Scalea.
Computer-assisted communication for criticcally ill
patients: a pilot study. The Journal of TRAUMA Injury, Infection, and Critical Care, Vol. 57, pages 488–
493, September 2004.

[8] D.O. Gorodnichy. Second order change detection,
and its application to blink-controlled perceptual interfaces. Proceedings of the IASTED Conference on
Visualization, Imaging and Image Processing (VIIP
2003), pages 140–145, Benalmadena, Spain, September 8-10 2003.

[19] T. Moriyama, T. Kanade, J.F. Cohn, J. Xiao, Z. Ambadar, J. Gao, and H. Imamura. Automatic recognition
of eye blinking in spontaneously occurring behavior.
Proceedings of the International Conference on Pattern Recognition (ICPR 2002), Vol. IV, pages 78–81,
Quebec City, Canada, 2002.

[9] D.O. Gorodnichy. Towards automatic retrieval of
blink-based lexicon for persons suffered from brainstem injury using video cameras. Proceedings of the
CVPR Workshop on Face Processing in Video (FPIV
2004), Washington, D.C., June 28 2004.

[20] Opencv library.
http://sourceforge.net/projects/opencvlibrary.

[10] D.O. Gorodnichy and G. Roth. Nouse use your nose
as a mouse perceptual vision technology for handsfree games and interfaces. Proceedings of the International Conference on Vision Interface (VI 2002), Calgary, Canada, May 27-29 2002.

[21] J. Rurainsky and P. Eisert. Eye center localization
using adaptive templates. Proceedings of the CVPR
Workshop on Face Processing in Video (FPIV 2004),
Washington, D.C., June 28 2004.

[11] K. Grauman, M. Betke, J. Gips, and G. Bradski.
Communication via eye blinks - detection and duration analysis in real time. Proceedings of the IEEE
Computer Vision and Pattern Recognition Conference (CVPR 2001), Vol. 2, pages 1010–1017, Kauai,
Hawaii, December 2001.

[22] Simtech publications.
http://hsj.com/products.html.
[23] X. Wei, Z. Zhu, L. Yin, and Q. Ji. A real-time face
tracking and animation system. Proceedings of the
CVPR Workshop on Face Processing in Video (FPIV
2004), Washington, D.C., June 28 2004.

[12] K. Grauman, M. Betke, J. Lombardi, J. Gips, and
G. Bradski. Communication via eye blinks and eyebrow raises: Video-based human-computer interaces.
Universal Access In The Information Society, 2(4),
pages 359–373, November 2003.
[13] Intel image processing library (ipl).
http://developer.intel.com/software/products/perﬂib/ijl.
[14] R. Jain, R. Kasturi, and B.G. Schunck. Machine Vision. Mc-Graw Hill, New York, 1995.

10

Syntactic probabilities affect pronunciation
variation in spontaneous speech

1
2
3

HARRY TILY, SUSANNE GAHL, INBAL ARNON, NEAL SNIDER,
ANUBHA KOTHARI, AND JOAN BRESNAN*

4
5
6

Stanford University
University of California, Berkeley

7
8
9
10
11
12

Abstract

13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

Speakers frequently have a choice among multiple ways of expressing one
and the same thought. When choosing between syntactic constructions for
expressing a given meaning, speakers are sensitive to probabilistic tendencies for syntactic, semantic or contextual properties of an utterance to favor
one construction or another. Taken together, such tendencies may align to
make one construction overwhelmingly more probable, marginally more
probable, or no more probable than another. Here, we present evidence
that acoustic features of spontaneous speech reﬂect these probabilities:
when speakers choose a less probable construction, they are more likely to
be disﬂuent, and their ﬂuent words are likely to have a relatively longer duration. Conversely, words in more probable constructions are shorter and
spoken more ﬂuently. Our ﬁndings suggest that the di¤ering probabilities
of a syntactic construction in context are not epiphenomenal, but reﬂect a
part of a speakers’ knowledge of their language.

28
29
30
31

Keywords
pronunciation variation, gradience, disﬂuency, ditransitive, word duration,
speech production, syntactic alternation

32
33
34
35
36
37
38
39
40
41
42

1.

Introduction

Empirical methods have become ubiquitous in all subﬁelds of Linguistics.
For example, the 2003 meeting of the Linguistic Society of America featured a symposium on ‘‘Probability theory and Linguistics’’, but only a
* Correspondence address: Harry Tily, Linguistics, Margaret Jacks Hall, Stanford University, CA 94305, USA. E-mails: hjt@stanford.edu; gahl@berkeley.edu. This work was
supported by NSF grant BCS-9818077. We thank Dan Jurafsky, Tom Wasow, and the
audience at the 2007 AMLaP conference for useful comments and suggestions.
Language and Cognition 1–2 (2009), 147–164
DOI 10.1515/LANGCOG.2009.008

1866–9808/09/0001–0147
6 Walter de Gruyter

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 147)

148
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

H. Tily et al.

single regular session on psycholinguistics and none on corpus linguistics.
By contrast, the 2008 meeting had several sessions devoted to psycholinguistics and corpus linguistics, and, moreover, featured corpus-based
and experimental psycholinguistic research in practically every session,
on topics ranging from syntactic theory to morphology to lexical semantics. This methodological change has gone hand in hand with the emergence of new theoretical approaches. Most major models of grammar
until recently cast linguistic structure as discrete, static, and categorical.
Recent years, however, have seen the emergence of more and more
models that conceive of structure as gradient, malleable, and probabilistic
(see for example the papers in Barlow and Kemmer (2000); Bod et al.
(2003); Bybee and Hopper (2001); and Gahl and Yu (2006)). In these
models, knowledge of language includes not just knowledge of syntactic,
morphological, and phonological categories, but also knowledge of the
frequency and probability of use of these categories in speakers’ experience. Families of frameworks such as ‘‘probabilistic linguistics’’, ‘‘usagebased’’ and ‘‘exemplar-based’’ models all recognize gradient activation of
linguistic units and probabilistic and gradient e¤ects of linguistic form
and meaning. The linguistic units in question include structures at all
levels of linguistic representation and varying degrees of abstraction (see
e.g. Borensztajn et al. 2009; Pierrehumbert 2001, 2002; Bybee 2002, 2006;
Johnson 1997). Taken together, these proposals constitute a major departure from a research tradition that imposed rigid boundaries between
competence and performance, sought to minimize redundancy in lexicon
and grammar, and assumed linguistic representations to be categorical
and discrete.
The development of these models has been possible in part thanks to
rich, large-scale corpora of naturalistic usage data and the availability of
statistical techniques for analyzing complex interactions of multiple factors. These tools have made it possible to build sophisticated models of
the many factors a¤ecting how speakers encode meaning in linguistic
form. For example, Bresnan et al. (2007) examined what drives speakers’
choice of syntactic realization patterns in the so-called dative alternation.
A given scenario can be expressed with either of two syntactic patterns,
either NP NP or NP PP, exempliﬁed in (1a) and (1b), respectively:
(1)

a. They sent us two of our coach tickets
b. They sent two of our coach tickets to us

(NP NP)
(NP PP)

Attempts to account for speakers’ choice between the dative alternants
have tended to invoke semantic di¤erences between the forms (Green
1974; Gropen et al. 1989), or constraints on the pronominality (Green
1971), information structure (Erteschik-Shir 1979) or length of the two

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 148)

Syntactic probabilities a¤ect pronunciation variation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

149

arguments involved (Hawkins 1994). Each one of these generalizations
covers many cases—but each is subject to exceptions. Indeed, corpus
analysis shows the choice between the two constructions to be far more
ﬂexible than ﬁrst appears to intuition (Fellbaum 2005; Bresnan and
Nikitina 2007; Bresnan 2008). Analyzing a large corpus of such ‘‘dative’’
sentences, Bresnan et al. (2007) showed that a multitude of such factors,
taken together, jointly predict speakers’ syntactic choice between NP NP
or NP PP alternants at very high accuracy. No analysis considering just
one factor at a time, be it semantic, phonological, or pragmatic, does
justice to the facts about the dative alternation. Grammatical models
seeking to describe syntactic realization patterns with any degree of accuracy must therefore take into account many factors at once.
Speakers’ syntactic choices can be accurately modeled using statistical
models incorporating interacting constraints that jointly estimate the
outcome probability. Moreover, (Bresnan 2008) found that acceptability
judgments reﬂect these factors, as well. However, the o¤-line judgment
task does not show whether the language production process is sensitive
to similar constraints as it unfolds: the models may achieve mere ‘‘descriptive adequacy’’. What constraints are speakers in fact sensitive to?
One means of investigating that question draws on observations about
pronunciation. Di¤erent tokens of one and the same word or phrase
typically sound slightly di¤erent. This variation may be random to some
degree; to some extent, however, it reﬂects planning processes during language production: A large body of evidence suggests that the duration of
words and pauses provides a sensitive diagnostic revealing speakers’ sensitivity to probabilities at various levels of linguistic structure, such as the
frequency and contextual predictability of words (Lieberman 1963; Bell
et al. 2009), morphemes (Pluymaekers et al. 2005; Kuperman et al. 2007),
and syntactic structures (Gahl and Garnsey 2004, 2006; Gahl et al. 2006).
Just as with research on syntactic alternations, research on pronunciation variation reveals speakers’ sensitivity to many probabilistic factors at
once. This point is ﬁrmly established in the study of word durations,
which simultaneously reﬂect static properties of single words such as orthographic regularity, and dynamically-changing properties related to the
speaker’s experience with that word: for example, its frequency, and its
likelihood of appearing in the context of the words before and after it
(Gahl 2008; Bell et al. 2009). Other things being equal, the production of
low-probability linguistic units—that is, low-frequency words and words
which are unlikely in a given context—tends to involve lengthening of
words and pauses. By contrast, the pronunciation of high-probability
linguistic units is characterized by phonetic reduction and durational
shortening.

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 149)

150
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

H. Tily et al.

Research on probabilistic pronunciation variation has often focused
on ‘‘string probability’’ measures such as n-grams, or transitional probabilities, i.e. the probability of a word conditioned on the word(s) that
precede or follow it (Jurafsky et al. 2001; Bell et al. 2009). However, if
grammars are indeed probabilistic, one should expect to see similar pronunciation e¤ects of more abstract syntactic probabilities, as pointed out
in Gahl and Garnsey (2004). In our previous research, we have shown
that syntactic probabilities can a¤ect pronunciation. That research was
based on the so-called subcategorization bias of a verb, or ‘‘verb bias’’.
Verb bias refers to the probability with which a given verb appears with
each of the subcategorization frames it is compatible with, such as the
sentential complement (SC) and double object (DO) frames shown in
(2). E¤ects of verb bias, i.e. a syntactic property, on sentence comprehension are well established (Trueswell et al. 1993; Garnsey et al. 1997).
(2)

a. We conﬁrmed the date was correct
b. We conﬁrmed the date

(SC)
(DO)

In Gahl and Garnsey (2004), we examined pronunciation variation
in these types of sentence, and showed that, among other things, the
acoustic-phonetic realization of the clause boundary following ‘‘conﬁrmed’’ in the SC-variant was in part a function of the probability of
encountering an SC following that verb. SCs after verbs that are highly
likely to take direct objects (‘‘DO-bias verbs’’) are realized di¤erently
from SCs following verbs that are likely to take SCs (‘‘SC-bias verbs’’),
independently of the speciﬁc words appearing in those structures. Importantly, this di¤erence was not due to the real-life probability of scenarios
described by sentences with high and low syntactic probability (cf. Gahl
and Garnsey 2006, for discussion, and Gahl et al. 2006) for a similar
e¤ect in a di¤erent pair of constructions).
While the observations in Gahl and Garnsey (2004) suggest that pronunciation variation reﬂects probabilities associated with syntactic structure, it is clear that the probability measure used there is overly simple.
To look only at a verb’s subcategorization bias, estimated from corpus
counts of various subcategorization frames in corpora, is to throw away
the mass of rich information available in sentences which speakers’
choices may be sensitive to. Subcategorization biases exist in tandem
with (and in some part, result from) a host of local and discourse-level
factors, as can be seen in the rich and detailed analyses in Bresnan et al.
(2007), (Szmrecsanyi and Hinrichs 2008), and (Wasow 2002), among
others.
The goal of the current study is to bring the tool of pronunciation variation to bear on understanding the richness of speakers’ probabilistic

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 150)

Syntactic probabilities a¤ect pronunciation variation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

151

knowledge of language. The current examines pronunciation variation in
the dative alternation. If pronunciation variation is a su‰ciently sensitive
reﬂection of the multiple probabilistic cues predicting the choice between
syntactic structures, then it can help show whether the human language
production system does indeed rely on the full range of available cues.
The current study also allows us to address serious questions left open
by previous research. Previous studies of syntactic probabilities (Gahl and
Garnsey 2004; Gahl et al. 2006) elicited speech from participants by asking them to read sentences. That fact constitutes a limitation: For one
thing, the prosody of read speech di¤ers from that of spontaneous speech
(Schafer et al. 2005). An even more serious problem is that the observed
e¤ect may have resulted from comprehension di‰culty, rather than directly reﬂecting the workings of the language production system. Sentences with local ambiguities often induce ‘‘garden-paths’’, i.e. incorrect
parses that temporarily throw the comprehension system o¤-track. Gahl
and Garnsey (2004) excluded tokens from the analysis that showed selfcorrection or marked overemphasis (‘‘we conﬁrmed, no wait, oh now I get
it, . . . we conFIRMED the date was correct’’). Still, the possibility cannot
be ruled out that the subjects in those studies initially misunderstood
some of the sentences they were asked to read and then decided to
emphasize low-probability prosodic phrasings. In fact, to keep subjects
from feeling self-conscious knowing their speech would be analyzed, they
were falsely given the impression that the researchers needed the recordings for a future comprehension experiment. Perhaps, then, speakers were
attempting to make the sentences easy to comprehend for an imaginary
listener. An analysis of spontaneous speech alleviates the problems caused
by possible garden-path e¤ects experienced by the speakers, if it is assumed that talkers are unlikely to induce garden-path e¤ects in themselves by their own speech. That assumption appears plausible, given that
talkers do not generally appear to be aware of local ambiguities in their
own speech here (Allbritton et al. 1996). In addition, though this is an
active area of research, it appears that speakers do not consistently
provide cues to listeners that would maximize ease of comprehension
(Ferreira and Dell 2000).
The dative alternation provides a particularly useful tool for an investigation of syntactic probabilities in that the two alternants (They sent us
two tickets P They sent two tickets to us) denote identical real-life scenarios (semantic di¤erences between the alternants notwithstanding, cf.
Green 1974; Gropen et al. 1989). If the phonetic realization of dative
sentences indeed reﬂects probability of construction choice, then it does
not simply reﬂect probability of real-world scenarios. Speakers’ choices
of dative alternants are subject to a range of probabilistic constraints at

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 151)

152
1
2
3
4
5
6
7
8

H. Tily et al.

least some of which are based on linguistic facts alone, not on real-world
denotata. Therefore, di¤erences in planning or processing di‰culty between the two alternants must be due to speakers’ store of linguistic experiences, not to di¤erences in the frequency of events in the world. Our
earlier studies controlled for real-life probability of denoted scenarios
(cf. the discussion in Gahl and Garnsey 2006), but they did so indirectly;
the dative alternation provides a direct means of teasing apart probability
of constructions and of real-world denotata.

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

2.

Background: The dative alternation

In Bresnan et al. (2007), we used multivariate statistical analysis to investigate the many factors that have been claimed to inﬂuence speakers’
choice between the dative alternants. As mentioned above, previous accounts explain the choice in terms of a single variable. Surprisingly perhaps, all of these accounts work fairly well despite the di¤erent constraints they invoke. This is because the properties that have shown to be
relevant tend to pattern together: For instance, pronominal themes tend
to favour the NP PP construction and pronominal recipients the NP
NP construction; but pronouns also tend to be short, deﬁnite, concrete,
and given. Using a logistic regression model, however, Bresnan et al.
(2007) were able to include many such correlated factors and test whether
speakers’ choices were inﬂuenced by each independently, controlling for
the others.
Bresnan et al.’s analysis used data from the Switchboard corpus of
spoken American English, which consists of recorded telephone conversations between strangers (Godfrey et al. 1992). Bresnan et al. handannotated each sentence containing one of the two dative alternants (NP
NP or NP PP; in a total of 2360 sentences), tracking a host of syntactic
and semantic variables that might have inﬂuenced the syntactic choice.
All of the variables were previously claimed to be relevant to the alternation in the theoretical or experimental literature. All in all, fourteen
variables were chosen and annotated in the data: the semantic class of
the verb (coding the type of relationship held between the recipient and
theme); the givenness, pronominality, deﬁniteness, animacy, person and
number of the recipient; the givenness, pronominality, deﬁniteness, number and concreteness of the theme; the (log) di¤erence in the number of
words of the recipient and theme; structural parallelism (whether there
had been instances of the same syntactic pattern in the preceding dialogue). A logistic regression model was then estimated which could predict the speaker’s choice between NP NP and NP PP as a function of
these variables. Except for number and person of recipient, and concrete-

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 152)

Syntactic probabilities a¤ect pronunciation variation
1
2

Table 1.

153

Factors found by Bresnan et al. (2007) to favor the NP NP or NP PP constructions

NP NP more likely

NP PP more likely

given recipient or nongiven theme
pronominal recipient or nonpronominal
theme
animate recipient or inanimate theme
deﬁnite recipient or indeﬁnite theme
short recipient or long theme
singular theme

given theme or nongiven recipient
pronominal theme or nonpronominal
recipient
animate theme or inanimate recipient
deﬁnite theme or indeﬁnite recipient
short theme or long recipient
plural theme

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

ness of theme, all of the factors were found to have an e¤ect on the choice
of NP NP or NP PP: the nature of some of these e¤ects is illustrated in
Table 1. On previously unseen data, the model correctly predicted in
94% of cases whether the NP NP or NP PP would be used.
The outcome variable in a logistic regression model is a continuous
number ranging between 0 and 1. This number can be interpreted as the
probability with which the model ‘‘expects’’ (or ‘‘predicts’’) the NP PP
construction—or equivalently, 1 minus the probability of the NP NP.
For example, when all the cues converge to make the outcome very certain, the output will be close to 1 or 0; in cases where the cues are more
equivocal, the output will be closer to .5. We can consider this output as a
measure of the probability of the construction choice, given the cues: for
each NP NP or NP PP, was the speakers’ choice of that construction
inevitable? Or was the choice more of coin ﬂip between the two, or
even—in a few cases—the less likely outcome?

29
30
31
32
33
34
35
36
37
38
39

3.

Methods

The Bresnan et al. data and model give us a set of tokens of NP NP and
NP PP sentences, along with an estimate of the probability of the alternant that was chosen: In some cases, the choice of the alternant that the
speaker in fact chose received strong support from the various factors in
the model. Other cases are assigned a lower probability by the model. For
example, the two sentences below had predicted probabilities of 0.01 and
0.99, respectively:
(3)

a.

40
41
42

b.

Yeah. I haven’t given much thought to it. I’m kind of busy
raising my kids (p ¼ 0.01)
if they can test the teachers, that gives them the full right to test
the kids (p ¼ 0.99)

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 153)

154
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

With these probabilities in hand, we examined the e¤ect of syntactic
probability on the phonetic realization of dative sentences. We examined
two aspects of phonetic realization: word duration and the presence of
disﬂuencies. Word durations and the presence of disﬂuencies are two
well established measures for ﬂuctuations in processing speed and processing di‰culty (Fox Tree and Clark 1997; Shriberg 2001; Clark and
Fox Tree 2002; Bell et al. 2003).
To study word durations, we focused on the preposition to in the NP
PP alternants, using durations extracted from the time-aligned transcript
of the Switchboard corpus (Deshmukh et al. 1998). Our choice of the
word to as our target was motivated largely by concerns about e¤ect
size: Previous studies of probabilistic pronunciation variation led us to expect that the size of any e¤ect of duration reduction would be quite small
(Bell et al. 2003; Pluymaekers et al. 2005; Kuperman et al. 2007; Gahl
2008; Bell et al. 2009), so it is important to minimize other e¤ects that
are not in the model, such as the length or frequency of other words in
the dative constructions. Examining many instances of the same word is
a way to control for word-speciﬁc information; hence we use the duration
of this word in all of the NP PP outcomes as our dependent variable.
Our models also included the following other variables as controls:
–

23
24
25
26
27

–

28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

H. Tily et al.

–

Rate of speech, measured in syllables per second, for the intonational
phrase surrounding the word to (excluding the duration of to itself ).
Following (Bell et al. 2003), we deﬁne the intonational phrase as the
longest region containing the word of interest that contains no sentence boundaries or pauses of 500ms or more.
Segmental context, speciﬁcally the presence of a preceding and following vowel, as this environment may favor ﬂapping and other
contextually-induced articulatory changes.
Other measures of contextual probability:
– Verb bias, i.e. the probability of NP NP or NP PP conditioned
only on the verb,
– Forward and backward bigrams, i.e. the probability of the word to
given the immediately preceding or following word (Bell et al. 2009)
obtained from the Web 1T ngram corpus (Brants and Franz 2006)

We removed cases with disﬂuencies immediately preceding or following
to. We consider the following to be disﬂuencies: a pause of 500ms or
more; repetition of a word; a ﬁlled pause (‘‘uh’’, ‘‘um’’); or a repair or
restart (‘‘give thi- that to them’’).
We then built a multiple linear regression model to test the e¤ects of
these variables. A linear regression model relates a set of predictor variables to an outcome variable, by considering the inﬂuence of all indepen-

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 154)

Syntactic probabilities a¤ect pronunciation variation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36

155

dent variables simultaneously. The model determines a coe‰cient for
each independent variable which shows how strongly it correlates with
the outcome variable when all other variables in the model are controlled.
The outcome variable in our case was the duration of the word to. The
critical predictor variable of interest was syntactic probability, i.e. the
probability assigned a sentence in the Bresnan et al. model. The coe‰cient for probability showed the average di¤erence, in milliseconds, of
the word to in high versus low probability instances of the construction,
after controlling for all other factors in the model. If this di¤erence is
signiﬁcantly di¤erent from zero, i.e. if it is large relative to the di¤erence that would be expected due to random variation in the data, the
inﬂuence of syntactic probability on duration is considered to be statistically signiﬁcant.
A second outcome variable of interest was the presence of disﬂuencies
in the dative sentences. A second regression model was constructed, this
time predicting the presence of disﬂuencies preceding or following the
verb or within either of its two arguments (the recipient or the theme) in
the NP NP and NP PP sentences. As this outcome variable is categorical,
we used logistic regression. Like linear regression, logistic regression relates a set of predictor variables to an outcome variable. Unlike in the
case of linear regression, the outcome variable in a logistic regression
model is a probability estimate, namely the probability of observing particular values of a categorical variable, here, the probability that the utterance contains a disﬂuency.
The only predictor variables in this model were verb bias, speech rate,
and the probability of the NP NP or NP PP variant, from the Bresnan
et al. database. Note that the other predictor variables in the model of
to-duration, such as the bigram probability measures, vary for each
word in a sentence. It would be possible to estimate the values of these
variables for every word in the sentences and to combine those measures
with the construction outcome probability to predict disﬂuency at each
point in the sentence. We are currently exploring this and other variants
of the disﬂuency model.
Data preparation and statistical analysis was carried out using the
statistical package R (R Development Core Team 2008) and in particular
the Design (Harrell 2007) and languageR (Baayen 2008) packages.

37
38
39
40
41
42

4.

Results

We ﬁrst turn to the model of the duration of the word to at the start of
the PP. Our dependent measure was the duration of this word in miliseconds. We removed datapoints with disﬂuencies adjacent to the word

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 155)

156
1

H. Tily et al.

Table 2. Final model for to duration in the PP outcome

2

b

Std. Error

T

P

0.17557
À0.34147
À6.92303
0.02486

0.01220
0.13782
2.12904
0.01038

14.397
À2.478
À3.252
2.396

0.000000
0.013603
0.001235
0.017001

3
4
5
6

Intercept
Outcome probability
Backward bigram
Previous vowel

7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

of interest, or with durations more than 2.5 standard deviations from the
mean (8.4% of the data). 446 cases remained. A speech rate control variable was calculated by taking the duration of the intonational phrase containing the word to (i.e. the maximum period containing no pause of
500ms or more and no sentence boundaries). We excluded the word to
itself from the region over which speech rate was calculated, to avoid
collinearity with the dependent variable. The number of syllables in the
region was divided by this duration, to determine the speaking rate,
measured as syllables per second. The independent variable of interest,
the probability of the actual outcome spoken, was calculated using the
Bresnan et al. model. Together with the other controls described above,
these variables were entered into a linear regression model. Regression
inputs were standardized by subtracting the mean and dividing by two
standard deviations, as recommended in Gelman (2008).
Although some of the predictor variables might be expected to co-vary,
in fact collinearity turned out to be unproblematic. All VIFs were less
than 1.2, meaning that the predictors were almost orthogonal. Because
the number of datapoints from each speaker varied greatly and because
speech rate accounted for much of the inter-speaker variability, we did
not use any random or ﬁxed e¤ect for speaker.
The following controls were not signiﬁcant, and were removed from the
model during model comparison by fast backwards elimination of factors
(Lawless and Singhal 1978): Forward bigram probability (p ¼ .43),
Speech rate of the surrounding region (p ¼ .27) and Verb bias (p ¼ .95).
The three factors shown in Table 2 were determined (by likelihood
ratio tests) to improve model quality (at p < .05). Importantly, the probability of the PP outcome is a statistically signiﬁcant predictor of the
duration of to, with higher probability outcomes resulting in shorter
pronunciations.
We now turn to our second variable of interest: disﬂuency. We coded
sentences for whether they contained a disﬂuency in the intonational
phrase surrounding the ‘‘dative’’ verb. Utterances were identiﬁed as disﬂuent if the longest stretch of pause-free speech surrounding the verb contained repetitions, ﬁlled pauses, repairs or restarts. Both NP PP and NP

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 156)

Syntactic probabilities a¤ect pronunciation variation
1

157

Table 3. Final model for disﬂuency in the dative VP

2

b

S.E.

Wald Z

p

0.6782
À0.8168
À0.2020

0.27773
0.09997
0.09403

2.44
À8.17
À2.15

0.0146
0.0000
0.0317

3
4
5
6

Intercept
speech rate
outcome probability

7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

NP outcomes were included. We removed sentences with speech rate 2.5
standard deviations from the mean (0.43% of the data). This left 2061
cases, of which 594 contained a disﬂuency in the verb region. Again, our
independent variable of interest was calculated using the Bresnan et al.
model. This time, because both NP PP and NP NP outcomes were included, and the variable was not the absolute probability of a NP PP,
but the probability of the actual outcome chosen (i.e. one minus the probability of the NP PP in the NP NP case). Collinearity between predictors
was found not to pose a problem: all VIFs were less than 1.3.
Verb bias proved non-signiﬁcant by likelihood ratio tests during model
comparison (p ¼ .26), and so was removed from the model.
The probability of the outcome (NP PP vs NP NP) is a signiﬁcant predictor of disﬂuency: more probable NP PPs and more probable NP NPs
are less likely to contain disﬂuencies. Additionally, sentences that are
spoken more quickly are less likely to contain disﬂuencies.
The size of the e¤ect of probability on duration is small. For the
to-model, the predicted di¤erence between the least and most probable
outcome in the actual data is just over 20ms, but since the data is so
heavily skewed towards likely outcomes, most datapoints are predicted
to have much more similar durations. The di¤erence between an utterance at the 25th percentile (the probability value which is greater than
the least probable 25% of the data) and the 75th percentile, for instance,
is predicted to be 15ms. Figure 1 shows the distribution of durations for
each utterances falling in each quartile. It is not entirely surprising that
the e¤ect on duration should be so small: the word to is very short
(mean duration of 129ms). Although standardizing the regression inputs
does make coe‰cients more comparable (see Gelman 2008), the probability measures used here have quite skewed distributions: In particular,
the bigram probabilities are much less evenly distributed than the outcome probabilities, with roughly two thirds of the probabilities smaller
than 0.05. This skewed distribution exaggerates the standardized e¤ect
size of the bigram relative to the outcome probability. As a result, we
cannot directly compare the bigram and outcome probability e¤ect sizes.
Even so, it is safe to say that the bigram probability has a greater e¤ect
on duration than the outcome probability.

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 157)

158

H. Tily et al.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

Figure 1. Duration of tokens of to as a function of outcome probability. Bars indicate one
standard deviation

26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

The size of the e¤ect of probabilities on disﬂuency was slightly
stronger: The probability of a disﬂuency in an average speed utterance
jumps from .27 among the highest probability outcomes to .40 among
the lowest probability outcomes.
To explore the e¤ect of syntactic probability further, we additionally
examined its e¤ect on the duration of other words besides to. Recall that
we chose the word to in the PP for methodological, rather than theoretical reasons: the within-item analyses allowed us to minimize noise, as well
as to avoid prosodic and structural confounds. Even more importantly,
we needed a word that was su‰ciently frequent in our database to allow
this kind of statistical analysis. To supplement our analyses, we in addition investigated the words which appear as the ﬁrst word of the second
argument in the NP NP outcome. We extracted all words that appeared
in this position at least 30 times in the database, and used the entire
Switchboard corpus to determine the average duration for each of these
words overall, to control to some extent for di¤erences between words.

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 158)

Syntactic probabilities a¤ect pronunciation variation

159

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

Figure 2. Average durations of words in initial position in the second argument of the NP NP
construction. Bars indicate 95% conﬁdence intervals

26
27
28
29
30
31
32
33
34

We do not report the resulting regression models here, except to note that
a duration e¤ect on the aggregated data is signiﬁcant and similar to the
model of to-duration. Figure 2 shows the durations for each of these
words in low and high probability NP NP outcomes. It is clear that almost all the words show a similar e¤ect: shorter duration when the actual
outcome NP NP is more likely than the alternative. This suggests that the
e¤ect is not limited to the word to and that it shows up in both the NP
NP and NP PP constructions.

35
36
37
38
39
40
41
42

5.

Discussion

The goal of this study was to explore ways in which the probabilistic constraints on syntactic choice might be reﬂected in speakers’ pronunciation
of dative sentences. An additional goal was to ascertain whether this
e¤ect existed in spontaneous speech, or whether it was limited to the
tightly-constrained artiﬁcial stimulus material used in previous studies.

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 159)

160
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

H. Tily et al.

Our crucial ﬁnding is that the probability of speakers’ choice between
alternants is indeed reﬂected in pronunciation, in spontaneous speech.
While our previous ﬁndings on syntactic probabilities and pronunciation
variation in read speech might have arisen from garden-path e¤ects, i.e.
a comprehension-based e¤ect, the current results suggest that syntactic
probabilities a¤ect language production. Several caveats are in order:
First, the observed e¤ect on the duration of to was very small, and the
unexplained variability substantial. A related caveat concerns the fact
that the corpus data are heavily skewed towards likely syntactic choices:
low-probability outcomes are rare by their nature—a persistent problem
facing corpus-based research.
The small e¤ect sizes and the sparseness of low-probability data raise
the question whether the observed e¤ect was spurious. However, we
found the same probability estimate to be a signiﬁcant predictor of disﬂuency in both constructions. Moreover, the e¤ect consistently seemed
to appear on other words in the NP NP construction. The pervasiveness
of these related patterns increase our conﬁdence in their stability and
generalizability.
It may seem surprising that verb bias, a measure that had revealed itself
as a signiﬁcant predictor of probabilistic pronunciation variation in previous research, did not emerge as a signiﬁcant predictor in the current
data. On closer consideration, this fact is to be expected: verb bias is a
crude measure of the probability with which a speaker will choose each
construction. The detailed analysis in Bresnan et al. of the factors a¤ecting the dative alternation reveals that verb bias is overridden in many
cases by the host of other factors shown to play a role. Naturally, a crude
measure only reveals large e¤ects—or small e¤ects as long as other factors are tightly controlled, as was the case in the scripted stimuli in our
earlier work.
Our data do not enable us to say which of the many factors inﬂuencing
the choice of syntactic alternant carried the e¤ect, or indeed whether any
single factor carried it. Our insistence that the dative choice is conditioned
on a multitude of factors might invite the objection that we only included
one summary measure in our models of phonetic variation, viz. the probability of the outcome conditioned on all of those factors. However, a
model including all factors as predictors of pronunciation variation
would be problematic, as it would unduly reﬂect phonetic properties of
particular words that tend to occur in one level of certain factors, rather
than the properties of those words that inﬂuence the syntactic outcome.
Hence, such a model would not have shed light on the role of syntactic
probabilities. Furthermore, the relatively small amount of data and the
large number of factors would have left us in danger of overﬁtting the

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 160)

Syntactic probabilities a¤ect pronunciation variation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36

161

model to the speciﬁc data in our corpus; and the collinearity between
factors wouldn’t have permitted us to see the importance of individual
factors with certainty.
The most promising way to tease apart the role of individual factors
probably lies in experimental research, for factorial manipulation of individual factors. In this way, corpus studies and experimental research can
be mutually supportive. But again, it is possible that no single factor or
small set of factors would emerge as signiﬁcant even then: the overall pattern result from the entire collection of factors working in concert.
Our results add to the growing body of evidence that the acoustic realization of words reﬂects higher-level linguistic information (Clark and
Wasow 1998; Gahl and Garnsey 2004). Taken together, these ﬁndings
argue for a model of language production in which high-level linguistic
representations of syntax and meaning are not strictly isolated from lowlevel processes such as articulation and speech rate control, but where
information can ﬂow between levels of representation. Computational
accounts that are consistent with the descriptive generalizations do exist.
For instance, Uniform Information Density (Aylett and Turk 2004; Levy
and Jaeger 2007) posits that speakers tend to make the rate at which
information is conveyed over the speech stream roughly constant, and
therefore more predictable words (which carry little information) should
be produced to take up a shorter duration than less predictable words.
This would be an e‰cient strategy for communication over the speech
channel, in that it makes utterances shorter without reducing the words
that the hearer would have the most di‰culty reconstructing. While psycholinguistic models of the language production system underlying these
e¤ects that could accommodate these ﬁndings are not yet available, we
believe that current work on exemplar-based and usage-based models
may yield a useful formalization of the relevant processing units, thanks
to its ability to represent linguistic units at arbitrary levels of abstraction
and probabilistic tendencies between them.
Finally, our results add further evidence to the view that probabilistic
e¤ects in language production are not due to probability of real-world
scenarios: There are multiple ways to express a given meaning. What we
have shown here is that meaning-equivalent alternants di¤er in pronunciation, as a function of the syntactic probability.

37
38
39
40
41
42

6.

General conclusion

Language production requires integrating many types of information.
The view of the mind that underlies this research is that language production system is an adaptive system that comes to process those structures

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 161)

162
1
2
3
4
5
6
7
8

H. Tily et al.

most e‰ciently that it has processed most often in prior experience. But
what aspects of prior language experience does the language production
system keep track of ? The present work supports the view that many factors jointly shape speakers’ probabilistic knowledge of language. We have
arrived at this view based on corpus evidence, experimentation, and statistical modeling. It is thanks to this methodological grounding that our
theoretical models can explore the consequences of abandoning the simplifying assumptions of grammar as categorical and deterministic.

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

References
Allbritton, D. W., G. McKoon & R. Ratcli¤. 1996. Reliability of prosodic cues for resolving
syntactic ambiguity. Journal of Experimental Psychology: Learning, Memory, & Cognition
22(3). 714–735.
Aylett, M. & A. Turk. 2004. The Smooth Signal Redundancy Hypothesis: A functional
explanation for relationships between redundancy, prosodic prominence and duration in
spontaneous speech. Language and Speech 47(1). 31–56.
Baayen, H. 2008. Analyzing linguistic data: A practical introduction to Statistics using R.
Cambridge: Cambridge University Press.
Barlow, M. & S. Kemmer (eds.). 2000. Usage-based models of language. Chicago: CSLI.
Bell, A., J. Brenier, M. Gregory, C. Girand & D. Jurafsky. 2009. Predictability e¤ects on
durations of content and function words in conversational English. Journal of Memory
and Language 60(1). 92–111.
Bell, A., D. Jurafsky, E. Fosler-Lussier, C. Girand, M. Gregory & D. Gildea. 2003. E¤ects
of disﬂuencies, predictability, and utterance position on word form variation in English
conversation. Journal of the Acoustical Society of America 113(2). 1001–1024.
Bod, R., J. Hay & S. Jannedy (eds.). 2003. Probabilistic linguistics. Cambridge, MA: MIT
Press.
Borensztajn, G., W. Zuidema & R. Bod. 2009. Children’s grammars grow more abstract
with age—Evidence from an automatic procedure for identifying the productive units of
language. Topics in Cognitive Science 1. 175–188.
Brants, T. & A. Franz. 2006. Web 1T 5-gram. Philadelphia, PA: LDC Data Consortium.
Bresnan, J. 2008. Is syntactic knowledge probabilistic? Experiments with the English dative
alternation. In S. Featherston & W. Sternefeld (eds.), Roots: Linguistics in search of its
evidential base, 75–96. Berlin & New York: Mouton de Gruyter.
Bresnan, J., A. Cueni, T. Nikitina & R. H. Baayen. 2007. Predicting the dative alternation.
In G. Bourne, I. Kraemer & J. Zwarts (eds.), Cognitive foundations of interpretation, 69–
94. Amsterdam: Royal Netherlands Academy of Science.
Bresnan, J. & T. Nikitina. 2007. The gradience of the dative alternation. In L. H. Wee &
L. Uyechi (eds.), Reality exploration and discovery: Pattern interaction in language and
life. Stanford: CSLI.
Bybee, J. & P. Hopper (eds.). 2001. Frequency and the emergence of linguistic structure
(Typological studies in language 45). Amsterdam: John Benjamins.
Bybee, J. 2002. Phonological evidence for exemplar storage of multiword sequences. Studies
in Second Language Acquisition 24(2). 215–222.
Bybee, J. 2006. From usage to grammar: The mind’s response to repetition. Language 82(4).
529–551.

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 162)

Syntactic probabilities a¤ect pronunciation variation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

163

Clark, H. H. & T. Wasow. 1998. Repeating words in spontaneous speech. Cognitive Psychology 37(3). 201–242.
Clark, H. H. & J. E. Fox Tree. 2002. Using uh and um in spontaneous speaking. Cognition
84(1). 73–111.
Deshmukh, N., A. Ganapathiraju, A. Gleeson, J. Hamaker & J. Picone. 1998. Resegmentation of Switchboard. International Conference on Spoken Language Processing, Sydney,
Australia, Australian Speech Science and Technology Association.
Erteschik-Shir, N. 1979. Discourse constraints on dative movement. In T. Givon (ed.), Discourse and syntax, 441–467. New York: Academic Press.
Fellbaum, C. 2005. Examining the constraints on the benefactive alternation by using the
World Wide Web as a corpus. In M. Reis & S. Kepser (eds.), Linguistic evidence: Empirical, theoretical and computational perspectives, 209–240. Berlin & New York: Mouton de
Gruyter.
Ferreira, V. S. & G. S. Dell. 2000. E¤ect of ambiguity and lexical availability on syntactic
and lexical production. Cognitive Psychology 40(4). 296–340.
Fox Tree, J. E. & H. H. Clark. 1997. Pronouncing ‘‘the’’ as ‘‘thee’’ to signal problems in
speaking. Cognition 62(2). 151–167.
Gahl, S. 2008. ‘‘Time’’ and ‘‘thyme’’ are not homophones: Word durations in spontaneous
speech. Language 84(3). 474–496.
Gahl, S. & S. M. Garnsey. 2004. Knowledge of grammar, knowledge of usage: Syntactic
probabilities a¤ect pronunciation variation. Language 80(4). 748–775.
Gahl, S. & S. M. Garnsey. 2006. Syntactic probabilities a¤ect pronunciation variation.
Language 82(2). 405–410.
Gahl, S., S. M. Garnse, C. Fisher & L. Matzen. 2006. ‘‘That sounds unlikely’’: Syntactic
probabilities a¤ect pronunciation. 28th Annual Conference of the Cognitive Science Society, CD-ROM.
Gahl, S. & A. C. L. Yu. (eds.). 2006. Special issue on Exemplar-based Models in Linguistics.
The Linguistic Review 23(3).
Garnsey, S. M., N. J. Pearlmutter, E. Myers & M. A. Lotocky. 1997. The contributions
of verb bias and plausibility to the comprehension of temporarily ambiguous sentences.
Journal of Memory & Language 37(1). 58–93.
Gelman, A. 2008. Scaling regression inputs by dividing by two standard deviations. Statistics in Medicine 27. 2865–2873.
Godfrey, J., E. Holliman & J. McDaniel. 1992. Switchboard: Telephone speech corpus for
research and development. International Conference on Acoustics, Speech and Signal
Processing.
Green, G. 1971. Some implications of an interaction among constraints. In Papers from the
seventh regional meeting, 85–100. Chicago: Chicago Linguistic Society.
Green, G. 1974. Semantics and syntactic regularity. Bloomington: Indiana University
Press.
Gropen, J., S. Pinker M. Hollander, R. Goldberg & R. Wilson. 1989. The learnability and
acquisition of the dative alternation in English. Language 65(2). 203–257.
Harrell, F. E. 2007. Design.
Hawkins, J. 1994. A performance theory of order and constituency. Cambridge: Cambridge
University Press.
Johnson, K. 1997. Speech perception without speaker normalization: An exemplar model.
In K. Johnson & Mullennix (eds.), Talker variability in speech processing, 145–165. San
Diego: Academic Press.
Jurafsky, D., A. Bell, M. Gregory & W. D. Raymond. 2001. Probabilistic relations between
words: Evidence from reduction in lexical production [References]. In Joan Bybee and

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 163)

164
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

H. Tily et al.

Paul Hopper (eds.), Frequency and the emergence of linguistic structure (Typological
Studies in Language 45), 229–254. Amsterdam: John Benjamins.
Kuperman, V. & M. Pluymaekers, M. Ernestus & R. H. Baayen. 2007. Morphological predictability and acoustic duration of interﬁxes in Dutch compounds. Journal of the Acoustical Society of America 121(4). 2261–2271.
Lawless, J. & K. Singhal. 1978. E‰cient screening on nonnormal regression models. Biometrics 34. 318–327.
Levy, R. & F. Jaeger. 2007. Speakers optimize information density through syntactic reduction. Twentieth annual conference on Neural Information Processing Systems.
Lieberman, P. 1963. Some e¤ects of semantic and grammatical context on the production
and perception of speech. Language and Speech 6. 172–187.
Pierrehumbert, J. B. 2001. Exemplar dynamics: Word frequency, lenition and contrast.
[References]. In Joan Bybee and Paul Hopper (eds.), Frequency and the emergence of
linguistic structure (Typological Studies in Language 45), 137–157. Amsterdam: John
Benjamins.
Pierrehumbert, J. B. 2002. Word-speciﬁc phonetics. In C. Gussenhoven & N. Warner (eds.),
Laboratory phonology VII, 101–140. Berlin: Mouton de Gruyter.
Pluymaekers, M., M. Ernestus & R. H. Baayen. 2005. Lexical frequency and acoustic reduction in spoken Dutch. Journal of the Acoustical Society of America 118(4). 2561–2569.
R Development Core Team. 2008. R: A language and environment for statistical computing. Vienna.
Schafer, A. J., S. R. Speer & P. Warren. 2005. Prosodic inﬂuences on the production
and comprehension of syntactic ambiguity in a game-based conversation task. In J. C.
Trueswell & M. K. Tanenhaus (eds.), Approaches to studying world-situated language use,
209–225. Cambridge, MA: MIT Press.
Shriberg, E. 2001. To ‘errrr’ is human: Ecology and acoustics of speech disﬂuencies. Journal
of the International Phonetic Association 31(1). 153–169.
Szmrecsanyi, B. & L. Hinrichs. 2008. Probabilistic determinants of genitive variation in
spoken and written English: A multivariate comparison across time, space, and genres. In
T. Nevalainen, I. Taavitsainen, P. Pahta & M. Korhonen (eds.), The dynamics of linguistic
variation: Corpus evidence on English past and present. Amsterdam: John Benjamins.
Trueswell, J. C., M. K. Tanenhaus & C. Kello. 1993. Verb-speciﬁc constraints in sentence
processing: Separating e¤ects of lexical preference from garden-paths. Journal of Experimental Psychology: Learning, Memory & Cognition 19(3). 528–553.
Wasow, T. 2002. Postverbal behavior. Stanford, CA: CSLI Publications.

30
31
32
33
34
35
36
37
38
39
40
41
42

(AutoPDF V7 7/8/09 08:22) WDG (148Â225mm) TimesM J-2167 LANGCOG, 1:2 (KN) PMU:(KN/W)31/7/2009 pp. 147–164 2167_1-2_01 (p. 164)

Focus, Topic, and Word Order: A
Compositional View
Michael Wagner.
Cornell University & McGill University

Abstract
This paper looks at the cartographic approach to word order restrictions
relating to focus and topic. The observed word order patterns are more
ﬂexible than is expected under the assumption of a totally ordered hierarchy of functional projections, suggesting that this standard assumption of the cartographic approach results in an all-too-Procrustean bed
for syntactic typology. The observed patterns are as expected under a
compositional view that tries to derive the observed word order patterns
from the semantics of the focus operators involved. The evidence suggests that a cartography of natural languages has to take into account a
broader range of syntactic conﬁgurations. A closer look at the compositional meaning of sentences can help understand existing constraints
on word order variation.
Keywords: focus, topic, prosody, scope, compositionality

1 Constituent Order and Cartography
Rizzi (1997) explores the syntax of interrogative and relative pronouns, topics, and focused constituents. Three of the many incisive observations of this
agenda-setting paper can be summarized as follows:
(1)

a.
b.
c.

Interrogative and relative pronouns, topics, and focused constituents
all occur in the left periphery of clauses.
Their order relative to each other is ﬁxed.
They differ with respect to iterability: There can be only one
focus or relative pronoun, but there can be multiple topics.

To appear in: Jeroen van Cranenbroeck (Ed.): Alternatives to Cartography.
Papers from the 2nd Brussels Colloquium on Generative Grammar.

2 Michael Wagner
The proposed solution, henceforth called the cartographic approach, involves
breaking up the earlier functional ‘CP’ projection into a sequence of functional projections:
(2)

ForceP

Force

TopP∗

Top◦

FocP

Foc◦

TopP*

Top◦

FinP

Fin◦

IP

This hierarchy plays a role in accounting for the three observations listed
above in the following way:
(3)

a.

b.
c.

Certain constituents occur in the left periphery because they move
to the speciﬁer of the relevant functional projection. These movements are induced by criteria: wh-criterion, focus-criterion, topiccriterion, ...
Their relative order is ﬁxed because the functional sequence is
ordered into a universal hierarchy.
Due to their interpretative properties topics but not foci can be
nested recursively.

The core assumption of the cartographic perspective as it is outlined in Rizzi
(1997) and Cinque (1999) is that the universal functional sequence is a totally
ordered sequence of functional heads. What’s universal is their hierarchical
order, but not every sentence may have every head in its functional spine
in every language. Rizzi (1997) hypothesizes for example that focus and
topic projections in the left periphery are activated only when needed, and
whenever they are both present their relative order is ﬁxed.
The cartographic approach to constituent ordering can be contrasted with
what I want to call the compositional approach, which holds that the rela-

Focus, Topic, and Word Order: A Compositional View 3

tive order of constituents follows from their semantics and the compositional
interaction of the pieces involved in building up sentence meaning.
Up to a point, the two approaches are compatible. Once the meaning of
the pieces has been identiﬁed, considerations of compositionality can provide
substantive reasons for why the observed hierarchy is what it is. An example
of such a rationalization of an observed generalization based on interpretative
properties is exempliﬁed by Rizzi (1997)’s account of why a sentence can
have more than one topic but not more than one focus (3c).
We may, however, ﬁnd ordering patterns that are incompatible with the
assumption of a universally ﬁxed total ordering. An example of ordering
patterns that are paradoxical from the point of view of cartography but can
be rationalized on semantic terms are those of certain adverbs in Norwegian
and other languages discussed in Nilsen (2002), who proposes a polaritybased account as an alternative to a ﬁxed hierachy. Similarly, Van Craenenbroeck (2006) observes transitivity failures in the ordering of complementizers, wh-phrases, and topics in Venetian Italian which are incompatible with
standard assumptions about what are possible triggers for movement in the
cartographic approach.
This paper explores the grammar of overt and covert focus and topic operators. As already observed in Cinque (1999), the syntactic distribution of
overt focus particles such as only and even poses a problem for a cartographic
analysis. I will ﬁrst discuss the issues that arise with these overt focus operators, and then argue that similar issues arise with unpronounced focus operators such as focus and topic. If the analogy is correct, this speaks against the
view of postulating a ﬁxed hierarchy of focus and topic projections in the left
periphery as it was proposed in Rizzi (1997).
The more general point that can be made based on the data discussed is
that some word order patterns found across languages might not ﬁt into the
rigid theory of a universally ﬁxed total ordering. Furthermore, a better understanding of the interpretative properties of the operators involved and how
they compose can provide the restrictive forces that constrain syntactic patterns without having to stipulate their hierarchical order. This is not incompatible with the essence of the cartographic project, which is to investigate
cross-linguistic similarities in word-order patterns, and which has proven a
very insightful project; but it may require changing some of the assumptions
about what kinds of generalizations we should be looking for.

4 Michael Wagner

2 Focus and Givenness
2.1

Overt Focus Operators

The focus operator only takes two arguments, a focus constituent (marked
by underlining), and a proposition in which focused constituents are replaced
with variables, sometimes called the presuppositional skeleton (marked in (4)
by hooks ):
(4)

Maria read only Moby Dick .

We can give only the following semantic entry (Wagner 2006a, adapting the
analysis in Horn 1969; Fintel 1999, i.a.):
(5)

Meaning of only
only = λp<e,t> .λ f .∀a ∈ C : p(a) → (∧ p( f ) →∧ p(a))
Presupposition: p( f ) Prejacent

The compositional view sets out to explain the syntactic distribution of only
by virtue of its semantic ‘needs.’ In particular, only as deﬁned in (5) needs
two arguments, and must be in the following conﬁguration in order to be
interpreted:
(6)

only at LF

Focus Constituent
only

λx.p(x)

In other words, only takes the open property as its complement, and its focus
must move to its speciﬁer. While the compositional approach would hold that
the syntactic conﬁguration is a result of the semantic denotation of only, a
cartographic approach could postulate a functional head for only somewhere
along the functional spine, with a speciﬁer (XP) and complement (YP), with
similar consequences for the conﬁguration at LF:
OnlyP
(7)

XP
only◦

YP

Focus, Topic, and Word Order: A Compositional View 5

This is actually not the analysis proposed for only in cartographic work. E.g.,
Cinque (1999) notes that the syntax of focus particles is different and more
complicated than such an analysis would predict.
As we will see, the reason for this is that focus operators occur in a second
syntactic conﬁguration in which they attach to the focus constituent (which
could be of various kinds and sizes) and then take a propositional argument as
their second argument—a conﬁguration for which the cartographic approach
does not provide any template. Focus operators like only and even appear
to able to occur in both conﬁgurations, suggesting a rather ﬂexible syntactic
patterning.
Even if (overtly pronounced) focus adverbs were excluded from a cartographic treatment by Cinque (1999) already, it is still worthwhile exploring
them further in order to ﬁnd out why they are not amenable to a cartographic
treatment. These insights can then be used to evaluate whether the approach
to unpronounced focus and topic operators that Rizzi (1997) proposes avoids
these problems.
A ﬁrst prediction of the compositional approach to only in (5) is that only
should attach to predicates, which could be sentence-sized in the case in
which an argument of a sentence is focused and abstracted over. In other
words, it should be able to occur in the left periphery. There is indeed some
evidence that in English, VP-only can attach to a sentence-sized node, and
that even though, in the surface word-order, VP-only follows the subject and
precedes the VP, the subject actually starts out below, and its surface position
is just a surface quirk of syntax. There is a generalization about the associate of only in English which is that it must be c-commanded by only. But
the subject of a sentence including VP-only can be contained in the focus of
only, as was observed in McCawley (1970, 296). An illustration from Wagner
(2006a):
(8)

They promised to stage Macbeth in its entirety. But then Macbeth
only gave his soliloquy. So the witches didn’t give their dialog.

The position of only in the cartographic approach would not be predicted
from its content but established empirically.
A prediction of both approaches is that, in cases where the associate of
only is smaller than the entire sentence, this focus constituent should move
to a position above the attachment site of only, in order to derive the correct
conﬁguration for interpreting only. Evidence for association by movement in
English is discussed in Wagner (2006a). In languages such as Hungarian, this
movement can happen overtly.

6 Michael Wagner
It seems, however, that there is variation as to which of the two arguments
an operator takes ﬁrst. This would not pose a problem to the compositional
approach, but would be a conundrum for the ﬁxed hierarchy approach. The
syntax of VP-only seems compatible with a theory that posits a functional
only projection as proposed above. The argument that semantically consists
of an open property is the complement of the functional head, and the focus
constituent moves to its speciﬁer. However, not every sentence involving only
ﬁts this pattern, and there seems to be another construal for only in English:
(9)

a.
b.

John only played baseball .
John played only baseball .

As discussed in Cinque (1999, 31), one obvious difference that sets DP-only
apart from sentential adverbs such as often is that it can directly follow the
verb:
(10)

*He forgot often his name.

This can be taken as evidence that only forms a constituent with the DP that
follows it.1 In fact, there is evidence that [only + focus constituent] can move
as a constituent, deriving the following conﬁguration:
(11)

[ Only baseball [ λx. john played x. ] ]
λx.p(x)
only

Focus Constituent

Evidence for this kind of movement comes from the following ambiguity,
observed by Taglicht (1984, 150):
(12)

They were advised to learn only Spanish.
a. They were advised not to learn any other language than Spanish.
b. They were not advised to learn any other language than Spanish.

One interpretation is that only Spanish undergoes covert movement. An alternative is that the surface structure actually is such that only Spanish attaches
high and takes scope overtly, as in a rightward movement analysis, or in a
Kaynian analysis involving movement to the left and subsequent remnant

Focus, Topic, and Word Order: A Compositional View 7

movement (Kayne 1998). There is also an analysis using an unorthodox constituent structure, where as long as linear order is observed the syntactic parse
can be re-bracketed on the surface, as in the categorial grammar analysis in
Blaszczak and G¨ rtner (2005). Both overt and covert movement approaches
a
correctly capture that the ambiguity disappears with VP-only:
(13)

They were advised to only learn Spanish .
a. They were advised not to learn any other language than Spanish.
b. *They were not advised to learn any other language than Spanish.

So there are two ways to construct the same meaning using only. It is not
clear how an analysis that treats only as a functional head can account for
this ﬂexibility. The syntactic pattern observed for DP-only is the reason that
Cinque (1999) exempts focus-sensitive adverbs from the restrictions imposed
by the functional hierarchy and proposes that they are ‘heads taking their
modifees as complements’ (p. 31).
This account remains moot, however, with respect to any structural restrictions on the occurrence of focus adverbs of this type, of which there are
quite a number. If only can freely attach to its focus, then why is it that it
is subject to more general restrictions on movement? Consider the following
example:
(14)

She claimed that only her dad knows.
a. ‘She claimed that her dad and no one else knows.’
b. *‘The only claim she made is that her dad knows, but she did not
claim that anyone else knows.’

Also, at least for the case of English, this account is not quite sufﬁcient, since
it leaves open the question of why there is also the possibility of VP-only,
which shows the kind of distributional pattern that the functional sequence
does try to account for. Furthermore, from the semantic point of view, it is
unclear which of the two arguments to call the ‘modifee’ in the ﬁrst place,
so the characterization of the distribution of these adverbs leaves much to be
desired. Ultimately, it seems a fundamental shortcoming of the treatment in
Cinque (1999) to invoke different kinds of explanations for focus adverbs and
other kinds of adverbs.
One difference between focus adverbs and other sentence adverbs such as
often is that the former but not the latter take two arguments rather than one,

8 Michael Wagner
and this difference might account for the difference in syntactic construals.
It seems that only can be pronounced attaching to either its ﬁrst or second
argument. The compositional approach might offer a solution for explaining
the distribution of the two types of construals of focus adverbs such as only,
since both conﬁgurations are compatible with the semantic needs of the operator. We could either stipulate an inherent ambiguity in the lexical entry of
only with respect to which argument it has to combine ﬁrst, or try to reduce
one occurrence of only to the other by invoking syntactic movement.
Maybe even VP-only is in fact interpreted in the conﬁguration in (11).
Suppose association with only involves movement to the complement position of only, an idea discussed in Lee (2005) based on evidence from Korean
and Wagner (2006a) for English. This movement analysis of focus association can be made compatible with the apparent island-insensitivity of focus
association (Anderson 1972) by the claim that a constituent containing the
focus can serve as the syntactic focus constituent (Drubig 1994; Krifka 1996;
Wagner 2006a):
(15)

I don’t know anyone who grows bananas,
I only know a guy who smokes them.

only

a guy who smokes them

λx
I
know

x

Rather than invoking movement to complement position one could also postulate two different entries for only, differing in the order in which they combine with the two arguments. For more discussion of syntactic restrictions on
the distribution of only see also Jacobs (1983), Bayer (1996) and Jaeger and
Wagner (2003).
One conclusion that we can draw based on the observed distribution of
only is that it does not lend itself easily to a cartographic analysis involving
a ﬁxed hierarchy. The compositional view, on the other hand, provides a
rationalization of the distribution.2
It is important to note that even in the compositional approach the particular syntactic conﬁguration does not follow from the overall meaning, but

Focus, Topic, and Word Order: A Compositional View 9

rather it follows from assumptions about how that meaning is carved up into
pieces and the order in which the pieces take their arguments. Natural language often picks out one out of many conceivable ways of divvying up complex meaning, and in the case of only any approach might have to stipulate
the order in which the two arguments are taken. If all languages use the same
decomposition, then we should indeed expect to be able to ﬁt all languages
into some sort of universal template. However, languages might simply differ
in how they carve up complex meanings, in which case we might not ﬁnd a
consistent syntactic pattern across languages.

2.2

Covert Focus Operators

Covert focus operators negotiate the encoding of focus and givenness in English and other languages. Rizzi (1997) postulates functional Focus and Topic
projections with a concrete semantic import in ways that mirror the conﬁguration observed in the case of only in (6). The Focus Projection marks its
speciﬁer XP as the focus and its complement YP as the presupposition:
(16)

Focus Projection
FocP
XP
Foc◦

YP

We can mimic the cartographic postulation of a functional focus projection
by postulating the following unpronounced focus operator, in analogy to only.
Note that the antecedent for focus marking does not to be a true proposition
and be part of the context set C. In fact, it is neither sufﬁcient nor necessary
for the antecedent to be true. Rather, it must be salient in the discourse. I
assume that there is a set ∆ which contains all constituents and entities salient
in the discourse, and F OCUS introduces a presupposition about there being
an antecedent in ∆ (for more discussion Wagner 2009).3
(17)

∀σ:
F OCUS = λp<σ,t> . λxσ . ∃a ∈ ALT(x) ∩ ∆ :∧ p(x) →∧ p(a)).p(x)

The LF-conﬁguration for the interpretation of sentences involving an unpronounced focus operator would then appear as follows:

10 Michael Wagner
(18)

F OCUS at LF

Focus Constituent
F OCUS

λx.p(x)

This two-place operator has the result that focus is interpreted in a conﬁgurational way, such that the second argument of the operator is the focus
constituent and the ﬁrst argument is the domain of the focus, not unlike the
predictions of the functional projection in (16).
It might be impossible to test whether this operator has the same ﬂexibility,
attaching to either the ﬁrst or the second argument leaving the meaning of
the result constant, as we observed for only. Since the meaning would be
identical, there is no way to tell unless we identify some phonological reﬂex
of the focus operator on the constituent it attaches to.
However, the operator as formulated here may be too restrictive in another
way. As we will see, this operator can applied very ﬂexibly. We can even
swap which of two sisters plays the role of ﬁrst or second argument of the
operator. This is also possible with only, but in a much more restricted way
since only operates on propositions. Still, one can attach only to the subject or
the VP, leading to different truth conditions: Bill only plays baseball, vs. Bill
plays only baseball. The F OCUS operator can apply to any node, taking either
sister as its focused or given argument. This ﬂexibility would be entirely
unexpected if it was a associated with a functional head in a ﬁxed hierarchy.
An indication of the ﬂexibility of this operator can be observed by looking at focus effects within a DP (cf. Rooth 1996), suggesting that maybe it
need not always take a proposition-sized argument. I will henceforth mark
accented words in small-caps, and prosodically subordinated material with
underlining.
(19)

A C ANADIAN farmer met an A MERICAN farmer on the STREET.

This utterance could have wide focus or focus on street, and yet embedded
within the DP there is an additional focus marking. So focus marking is
even possible between sister constituents within DPs, or with respect to any
other type of constituent. Moreover, one can swap what’s given and what’s
focused, so at any given node either sister could be marked as given relative
to the other. This reveals a ﬂexibility that is unexpected if focus resides in a
particular functional projection.

Focus, Topic, and Word Order: A Compositional View 11

The additional ﬂexibility of the focus operator can be captured by the
following semantic entry for a covert focus-sensitive operator, which operates
on constituents of any type:4
(20)

∀σ, ∀δ :
F OCUS = λxσ .λyδ .∃[ay] ∈ (∆∩ALT([xy])) : [ay] = [xy].[xy]

This is a 2-place operator, and it introduces a presupposition that requires
there to be a salient antecedent of a certain shape, just like the focus operator
introduced in Rooth (1992). The only difference is that in this case it takes
two arguments. Whenever this operator is used it takes two arguments, one
constituent that is marked as given (which one could call the ‘background’ in
Rizzi’s terminology), and one constituent that evokes alternatives, which we
could call the ‘focus.’
The F OCUS operator has an effect on prosody: if the given constituent
follows the focus, it is prosodically subordinated, which is at least reﬂected
by a pitch-range reduction and can result in complete deaccenting (Wagner
2005). I will not discuss why F OCUS has this effect.
This operator predicts that it is not enough for a constituent to have been
used before in order for it to be marked as given by F OCUS. In addition,
there must be an alternative to the ﬁrst argument, the focus, such that there
is an antecedent involving that alternative. Conversely, it is not enough for
a constituent to be focused; it must be focused relative to something else
(although that something else can sometimes be elided). The intuition behind
this can be brought out easily in the following type of example, a context in
which the predicate convertible is given, and yet deaccenting it is dispreferred
(Wagner 2006b):
(21)

A: Mary’s uncle, who produces high-end convertibles, is coming to
her wedding. I wonder what he brought as a present.
a. B: He brought a [CHEAP convertible].
b. #B: He brought [a RED convertible].
c. B: He brought [a red CONVERTIBLE ]

Convertible is destressed, but the stress that falls on red is ‘loaded,’ in that it
invokes alternatives. Red, however, is not an alternative to cheap, as can be
illustrated by other instances where red invokes alternatives:
(22)

high-end is not an alternative to red:
Mary only likes RED convertibles.

12 Michael Wagner
Deaccenting convertible is only possible if there is a salient alternative Y ′
to the sister of convertible such that [y′ convertible] is given in the context.
Constituents are marked as given relative to another constituent: its sister.
This analysis is similar to the one proposed in Williams (1997) in that it
holds that every instance of deaccentuation involves marking a constituent
anaphoric and another constituent dis-anaphoric. Williams (1997, 599) concludes: “The lesson is that it is impossible to destress one thing without
stressing another, and the stress that falls on the other is loaded, not empty.” It
differs, however, from Williams’s proposal in requiring that the corresponding constituent in the antecedent must be a true alternative.
Sometimes, however, there does not seem to be any sister effect, and a
constituent can be marked as given without a sense that alternatives for the
sister constituent are evoked:
(23)

A: Smith walked into a store. What happened next?
a. #B: A detective arrested S MITH.
b. B: A detective ARRESTED Smith.

Here it seems that the presupposition of F OCUS is too strong, since no alternative to arrested is necessarily invoked. That direct objects can be marked as
given even with sentence-wide focus was already observed in Halliday (1967)
and Ladd (1980).
Some earlier accounts have interpreted these observations as evidence
that apart from focus effects, there is a second and separate phenomenon of
anaphoric destressing (Neeleman and Reinhart 1998; Reinhart 2006), which
applies only to certain types of arguments, such as DPs. Similarly, Selkirk
(1995) proposed a focus projection mechanism that provided different conditions on givenness and focus. But not all given DPs can actually be marked
as given that freely. Consider the following observation:
(24)

A: Smith was suspected to have been involved in the burglary. Do
you know what happened in the end?
a. B: I’m not sure. The thing i heard is they were going to arrest
J ONES or S MITH.
b. ??B: ..to arrest Jones OR Smith.
c. #B: ...to arrest J ONES or Smith.

Marking Smith as given in a coordinate structure introduces a presupposition
stronger than just that there is an antecedent for Smith in the context, in fact,
it introduces precisely the presupposition of F OCUS. Consider a case where

Focus, Topic, and Word Order: A Compositional View 13

it can destress:
(25)

A; Did they arrest only Smith?
B: No, they arrested Jones AND Smith. (given constituent: only
Smith)

Or consider the following example. Sentence (26) requires a contrasting individual for John, such that the entire constituent x or Smith is given:
(26)

A: They want to arrest Lee or Smith?
B: No, they want to arrest J ONES or Smith. (Constituent given in
context: Lee or Smith)

Why is it that in these cases a DP cannot be destressed just by virtue of referring to a discourse-given entity when they occur in a coordinate structure?
The solution proposed byWagner (2005) is that Smith is not actually the sister of the predicate. The structure based on which givenness is evaluated is
rather the following:
(27)

[ F OCUS([ λ x. [ The police arrested x. ] ]) Smith ]

This movement changes the sister relations and consequently weakens the
presupposition introduced by F OCUS. Marking Smith as given relative to the
lambda-abstracted proposition invokes the presupposition that there is an alternative open property that applies to Smith that is salient. This is precisely
the weak presupposition that destressing a direct object intuitively encodes.
Movement in the present example facilitates givenness marking: givenness
marking in situ would not have been possible, since the presupposition introduced would have been too strong. And this movement is impossible out of
the coordinate structure in (24) due to the coordinate structure constraint.
The important lesson is that the presupposition introduced depends on
where the operator attaches. It may be possible to postulate a ﬂurry of focus
positions at various points within DPs and CPs and account for the facts, but I
think such an approach would miss the intrinsically relational nature of focus
and givenness marking that the data suggests. Moving a constituent changes
the sister relations, and as a consequence it changes the presupposition introduced by marking a constituent as given or focused.
This relational approach makes interesting predictions for focus- and givenness movement that differ from the ones expected in a cartographic approach.
Movement changes the syntactic conﬁguration, and thus changes what can
be marked as given relative to which other constituent. We observed that the

14 Michael Wagner
moved constituent can be marked as given relative to the remnant, and the
focus presupposition can thus be weakened.
(28)

F OCUS([ λ x. [ The police ARRESTED x. ] ]) Smith

Suppose that the moved constituent can either be the ﬁrst or second argument
of F OCUS, in other words, that the moved constituent can either be the one
that is marked as given or the one that its sister is marked as given relative to,
as proposed in Wagner (2005). Consider for the example what would happen
if we attached the F OCUS-operator to the moved constituent rather than the
remnant:
(29)

[λ x. [ The police arrested x. ] ] F OCUS(SMITH).

The predicted presupposition of (29) is: There is an alternative y to Smith
such that The police arrested x is given in the context. A context in which
this presupposition is fulﬁlled is the following:
(30)

A: Who did the police arrest?
B: The police arrested S MITH.

In other words, we can use the same givenness operator to analyze cases of
givenness marking and cases of ‘narrow focus.’ There may be some evidence
that such restructuring and focus marking is indeed possible even in surface
structure. Rooth (2005, cf.) notes the following contrast:
(31)

a.
b.

A: What did Mary eat? B: I think Mary ate | the APPLE.
A: What did Mary do? #? B: I think Mary ate | the APPLE.

According to Rooth (2005), it is possible to separate the direct object from the
verb by a break, arguably a reﬂex of overt restructuring along the lines of (29),
but only if the strong presupposition encoded by marking the entire remnant
constituent as given, as in (b). It is not sufﬁcient that Mary constitutes given
information, as in (a).
The same pattern holds for overt focus movement to the left in German,
providing more evidence that focus/givenness movement is indeed restricted
in the way that would be expected based on the presupposition encoded by
F OCUS:5
(32)

a.

A: What did Mary eat?
B: Ich glaube den A PFEL hat Maria gegessen.
I think the apple has Maria eaten

Focus, Topic, and Word Order: A Compositional View 15

b.

I think Mary ate the apple.
A: What did Mary do?
?# B: Ich glaube den A PFEL hat Maria gegessen.
I think the apple has Maria eaten

Moving the constituent the apple and subsequently marking the sister constituent as given results in the presupposition that there is an alternative x to
Apfel such that Maria has eaten x. Alternatives of this sort are made available
by the question in (32a) but not by the one in (32b), which does not include
the predicate eat.
Of course, in German, just as in English, it is also possible to move the
constituent the apple and to mark it as given relative to the remnant. This
is a case of givenness-related scrambling, which is in fact very common in
German:6
(33)

A: What about the apple?
B: Ich glaube den A PFEL hat Maria GEGESSEN.
I think the apple has Maria eaten

The compositional theory can thus rationalize givenness and focus movement: it adjusts the presuppositions encoded by marking one constituent
given relative to its sister. This view of the interaction of movement and
focus marking is intrinsically ﬂexible: it is sister constituents that are marked
as given relative to each other, and movement adjusts the syntactic relations
of constituents.7
The cartographic view, on the other hand, ﬁxes the syntactic conﬁguration
of focus marking by stipulating several focus projections in the spine of functional projections. This approach does not do justice to the relational nature
of information structuring observed in the data discussed here.

3 Recursive Nesting of Focus Operators
A sentence can include more than one focus operator. In such situations, the
two operators can take either scope with respect to each other. This presents
a problem for the view that certain focus operators occupy a ﬁxed position
in a hierarchy. Again, ﬁrst I will ﬁrst illustrate the point with overt focus
operators, and then extend the discussion to unpronounced operators.

16 Michael Wagner
3.1

Nesting of Overt Operators

Two overt focus operators can occur in the same sentence (Krifka 1992). I
will use the focus operator even in addition to only to illustrate the point:
(34)

Even John read Moby Dick.
a. Asserted: John read Moby Dick.
b. Conventional Implicature of even (Karttunen and Peters 1979,
25/26): There are other x under consideration besides John such
that x read Moby Dick and for all x under consideration besides
John, the likelihood of x reading Moby Dick is greater than or
equal to the likelihood of John reading Moby Dick.

Here’s an example from (Wagner in press) with both focus operators in one
sentence (I’m using underlining here to mark the focus of the operators):
(35)

even > only: Except for Bill, the kids in this summer camp have
no respect for animals and the potential dangers, which makes them
take too many risks, including with poisonous snakes.
a. Even the most poisonous snake only frightens Bill.
b. Even the most poisonous snake frightens only Bill.

In this example, even outscopes only. The alternative propositions computed
for even must include only, since otherwise the probabilities would not line
up in the right way. Only Bill must be part of every alternative considered. It
is clearly more likely that a more poisonous snake frightens Bill, but it is less
likely that it would frighten only Bill:
(36)

a.
b.
c.

least likely: The most poisonous snake frightens only Bill.
more likely: Average poisonous snakes frighten only Bill.
even more likely: Mildly poisonous snakes frighten only Bill.

The representation of the sentence at LF looks as follows:
(37)

λP1 . even ([the most poisonous snake])(P1 ) ([λx.( only (Bill) (λy. x
frightens y)]).

In (35), word order matches the scope. The word order, however, can also be
reversed:
(38)

even > only: Except for Bill, the kids in this summer camp have
no respect for animals and the potential dangers, which makes them

Focus, Topic, and Word Order: A Compositional View 17

take too many risks, including with poisonous snakes.
a. Only Bill is afraid of even the most poisonous snake.
b. #Only Bill is even afraid of the most poisonous snake.
When the focus operator attaches to the focus constituent, wide scope is possible. The analysis involves moving the focus operator together with the DP,
i.e., even the most poisonous snake, to take the right scope. The fact that (38b)
is bad constitutes evidence that movement is indeed involved. It is impossible
to move even in (38b) such that it can outscope only.
What’s puzzling about these data is that there is compelling evidence that
there is an NPI version of even with a reversed presupposition requiring lowest probability (Rullmann 1997). So why is movement necessary here? I
think the reason may be that the NPI-version of even is not licensed by only,
similar to other strong NPIs such as in weeks or the punctual reading of until
(Zwarts 1998; Gajewski 2005). Compare the following example with (38b):
(39)

No one is even afraid of the most poisonous snake.

Negation happily licenses the NPI-version of even and the sentence becomes
acceptable. Note that if this was in fact the standard version of even, movement would be necessary here to yield the right truth condition, but that
should be ruled out for the same reasons as in (38b).
While the examples so far require that even outscope only, which in cartographic terms could be achieved by positing an even projection oustcoping
an only projection, the inverse scope is also possible. Consider the following
context:
(40)

Context II: only > even: The kids in the summer camp are afraid
of snakes to some degree, but it depends on how dangerous they are.
Everyone is afraid of rattlesnakes, since they’re really poisonous, but
almost everyone is ok with some less poisonous snake.
a. Only Bill is afraid of even the least poisonous snake.
b. Only Bill is even afraid of the least poisonous snake.

In this context, only must outscope even for the probabilities to come out
right. Once again, the reverse linear order is also possible, at least for some
of my informants:
(41)

Context II: only > even
?
Even the least poisonous snake would frighten only Bill.

18 Michael Wagner
#

Even the least poisonous snake would only frighten Bill.

The reason these data raise a problem if one were to analyze it in cartographic
terms is the following: suppose that there was an even and an only projection,
then one should be higher in the hierarchy than the other. The fact that two
different scopes are possible would have to be resolved by postulating multiple only and even projections, analogous to the multiple projections for topic
and focus in (2) based on the observation that they can have different hierarchical arrangements. This is a possible but not very insightful move, and
again does not do justice to the compositional and relational nature of the
facts.
The compositional view on the other hand can account for the data by
virtue of the fact that adding one operator leads to an expression that is compatible with adding the other, and it is simply the syntactic scope that the two
operators take with respect to each other that will decide on the ultimate truth
conditions. This is not unlike the explanation in Rizzi (1997) for the iterability of topics, but in fact it does away with the need to postulate a place for
either even or only in the functional hierarchy.
Once again, we saw evidence that the syntax of overt focus operators such
as only and even cannot be accounted for by the functional sequence—but
that does not come as a surprise. As discussed before, it was already acknowledged in Cinque (1999) that focus operators are different. The compositional
view might provide a better grip on the word order patterns observed for this
class of adverb. It turns out, however, that an analogous kind of argument
can be made for nested covert focus operators, in particular for the kind of
topic-focus constructions that Rizzi (1997) discusses.
3.2

Nested Covert Focus Operators

The notion of topic discussed in Rizzi (1997) and referred to in the functional
hierarchy in (2) seems to be that of what is usually called a ‘contrastive topic.’
This becomes evident in the examples used as an illustration of topics, (e.g.,
Rizzi 1997, 285):
(42)

Your book, you should give t to Paul (not to Bill).

The contrastive topic, your book, invokes alternatives, and constituents that
generally do not allow for the invocation of alternatives are prohibited:
(43) ?*It, you should give t to Paul.

Focus, Topic, and Word Order: A Compositional View 19

B¨ ring (1997, 2003) observes that both the contrastive topic (CT) and the
u
focus (FOC) of the sentence invoke alternatives. An example of a contrastive
topic is the following:8
(44)

A: What did you buy on 59th street?
B: On /F IFTYNINTH Street∨, I bought the SHOES\ .

The answer in this dialog addresses the question under discussion, and the
constituent shoes constitutes the focus. In addition to answering the question
under discussion, this utterance also invokes a set of alternative questions
about what the addressee bought at other locations. Invoking a set of a alternative questions is a typical use of contrastive topics. Interestingly, in English
the contrastive topic can also follow the focus:
(45)

A: What did you buy on 59th street?
B: I bought the SHOES on /FiftyNINTH Street∨.

The analysis in B¨ ring (1997, 2003) consists of marking one constituent as a
u
contrastive topic using the diacritic CT and the other as a focus using FOC. A
non-compositional two-step process then creates a topic-semantic value for
the expression. The prediction is that there should be no ordering restrictions
between contrastive topic and focus.
Wagner (in press), however, argues that there is a simpler, compositional
way of accounting for contrastive topics, which does not involve positing two
different features CT and FOC. The idea is that contrastive topics simply involve a recursive nesting of two focus operators, similar to the nesting of
overt focus operators discussed in the previous section. The constituent that
associates with the focus operator taking wider scope is what we call a contrastive topic. Compositional views of contrastive topics similar in spirit were
discussed in Williams (1997); Van Hoof (2003); Sauerland (2005). I will not
discuss the various parallels and differences between the analyses here.
The basic idea put forward in Wagner (in press) is that a sentence involving a contrastive topic involves the same syntactic conﬁguration as the one
involved in a sentence containing both only and even: two focus operators
each takes a focus constituent as its complement that then takes scope relative to the other. If this parallel is correct, then rather than deriving the
syntactic distribution of topic and focus based on the functional sequence, we
can simply derive it from the compositional structure. A semantic analysis
of contrastive topic in those compositional terms is presented in Wagner (in
press).

20 Michael Wagner
Suppose that contrastive topics are really just the associates of focus operators taking wide scope over another focus operator. Then we expect them
to have an identical syntactic distribution to parallel cases with overt focus
operators. We already saw that inverse scope between focus operators in English is possible, so it is no surprise that contrastive topic and focus can occur
in either word order. But I would now like to turn to Italian, where we will
see that (i) scope between focus particles is ﬁxed at surface structure, and
(ii) contrastive topics must precede foci. This is exactly as expected if covert
focus particles are syntactically construed just like overt focus particles.
Rizzi (1997, 289/290) observes that contrastive topics differ from foci in
various ways. They are left-dislocated, and precede foci in general. Another
difference, already noted in in Cinque (1990, 14, 56ff), is that contrastive topics can and sometimes must involve resumptive clitics while left-dislocated
foci cannot. A clitic is obligatory in the case of direct objects:
(46)

a.

Il tuo libro, lo ho
comprato.
the your book, it have.I bought
‘Your book, I bought.’
b. *Il tuo libro, ho
comprato.
the your book, have.I bought

Using a resumptive clitic in the case of foci, however, is reported to be impossible:
(47)

a. *I L TUO LIBRO lo ho
comprato (non il suo).
the your book it have.I bought (not the his)
b. I L TUO LIBRO ho
comprato (non is suo).
the your book have.I bought (not the his)
‘I bought your book, not his.’

This observation was used as evidence that contrastive topics and foci are substantively different. Within the cartographic approach, one involves movement to a topic projection and one involves movement to a focus projection,
two different and hierarchically ordered projections in the functional spine.
Under the compositional view, however, contrastive topics involve focus
operators that simply outscope a lower focus operator. We would expect then
that the clitic data reported here would generalize to examples with overt
focus operators. This is indeed correct. If the direct object associates with
a focus operator that outscopes a focus operator associating with a higher
argument, e.g., the subject, then clitic-left-dislocation is obligatory:9

Focus, Topic, and Word Order: A Compositional View 21

(48)

There is a petting zoo, and for some reason they put some poisonous
snakes in there, but people didn’t really want to pet them. In fact,
most of of the snakes were not petted at all, except for one: the least
poisonous snake. And even that got petted by only one visitor. So:
a. *Soltanto uno dei visitatori (lo) ha toccato anche il meno
only
one of.the visitors (it) has touched even the least
velenoso dei serpenti.
poisonous of.the snakes
‘Only one of the visitors touched even the least poisonous snakes.’
b. Anche il meno velenoso dei serpenti, *(lo) ha toccato
even the least poisonous of.the snakes it
has touched
soltanto uno dei visitatori.
only
one of.the visitors
‘Only one of the visitors touched even the least poisonous snakes.’

The reason for the linear order of constituents lies in their relative semantic
scope, rather than in an inherently speciﬁed position in a ﬁxed hierarchy.
In Italian, but not in English, the relative order between them is ﬁxed, and
inverse scope is not possible.
Arguments other than direct objects show similar restrictions with respect
to linear order, but behave differently with respect to clitics. The surface
word order must reﬂect the scope of the focus operators. The examples corresponding to the English ones discussed above in the context that motivates
the reading in which only outscopes even are as follows:
(49)

only > even
a. Solo Paolo ha paura anche dei serpenti meno velenosi.
only Paolo has fear even of.the snakes least poisonous
b. *Anche il serpente meno velenoso fa
paura solo a Paolo.
even the snakes least poisonous make fear only to Paolo

If the intended meaning with only outscoping even is to be expressed using
the predicate fa paura ‘make fear’ in (49b), the only way is to use a leftdislocated construction:
(50)

only > even
Solo a Paolo anche il serpente meno velenoso (gli) fa paura.
only to Paolo even the snakes least poisonous make fear

22 Michael Wagner
If we switch to the context that motivates the reading in which even outscopes
only, the judgments go exactly the other way, and only the sentence in which
is the basic word order reﬂects the scope is grammatical:
(51)

even > only
a. *Solo Paolo ha paura anche dei serpenti piu’ velenosi.
only Paolo has fear even of.the snakes most poisonous
paura solo a Paolo.
b. Anche il serpente piu’ velenoso fa
even the snakes most poisonous make fear only to Paolo

If the verb ha paura ‘has fear’ is to be used, again the only way is to get the
right scope is by left dislocation:
(52)

even > only
Anche dei serpenti piu’ velenosi, solo Paolo ha paura.
even of.the snakes most poisonous, only Paolo has fear

The interesting difference between Italian and English is that overt focus operator take surface scope in Italian but not in English. We also saw that contrastive topics in English can either precede or follow the focus of a sentence.
If overt and covert focus operators behave alike, we would then expect that
the order between topics and foci is also ﬁxed in Italian. This is indeed correct:
(53)

A: Cosa hai
comprato sulla cinquantanovesima strada?
what have.2nd bought on-the 59th
street?
‘What did you buy on 59th street?’
B: Sulla cinquantanovesima strada ho
comprato le scarpe.
on-the 59th
street have.1st bought the shoes.
E sulla cinquantaduesima strada ho
comprato la giacca.
And on-the 52th
street have.1st bought the jacket.
‘On 59th street, I bought the shoes. And on 52nd street I bought the
new jacket.’

Inverting the two focus constituents is dispreferred in this context:
(54)

A: Cosa hai
comprato sulla cinquantanovesima strada?
what have.2nd bought on-the 59th
street?
‘What did you buy on 59th street?’

Focus, Topic, and Word Order: A Compositional View 23

B: ?? Le scarpe, lo ho
comprato sulla
the shoes them have.1st bought on-the
cinquantanovesima strada. E la giacca, la ho
comprato
59th
street. And the jacket it have.1st bought
sulla cinquantaduesima strada.
on-the 52th
street
‘I bought the shoes on 59th street. And I bought the new jacket on
52nd Street.’
This is converging evidence that overt and covert focus operators behave
alike, and since the former are not amenable to a cartographic analysis, the
same reasoning applies to covert ones.
3.3

Why is there a Clitic in Topicalization but not in Focus?

Focus Operators in Italian take overt scope. In order to get a focus operator to
take scope over a preceding argument, left-dislocation is necessary to change
the word order. In the case of direct objects, the left-dislocated constituent
must be resumed by a clitic. But why would it be that clitic resumption is
necessary?
The reason might be related to a restriction noted in Calabrese (1984),
and discussed, more recently, in Stoyanova (2008). Pair-list wh-questions
are not grammatical in Italian. Pair-list-questions could be analyzed as questions involving two nested focus operators, each associating with a wh-word.
Perhaps in Italian, there is a general constraint against having more than one
focus operator within a clausal domain:
(55)

Focus constraint on Italian:
Each clause can contain only one focus operator.

If this is correct, and if contrastive topics involve nesting focus operators,
then contrastive topics as we know them from English should actually be
impossible in Italian. However, we might be able to add a focus/topic to a
clause that already contains one if we add it outside of that clause. Take an
analogous construction in English:
(56)

a. As for John, Mary really likes him.
b. *As for John, Mary really likes.

as for introduces a topic, but it’s clearly not part of the main clause itself, but

24 Michael Wagner
seems to attach higher, perhaps taking a speech act as its argument. Similar to CLLD in Italian, we have to resume the direct object with a pronoun.
The reason the pronoun is obligatory is because it’s an obligatory argument.
Consider:
(57)

As for Monday, we’ll go shopping (then).

If a left-dislocated constituent in Italian is indeed construed with an as forlike construction, this can explain various of the peculiar properties of CLLD.
First of all, we expect obligatory clitics for all obligatory arguments (except
the subject, since Italian is a pro-drop language).
But other peculiarities can be explained as well. Rizzi (1997, 290-291)
and Cinque (1990, 57-60) observe, for example, that certain constituents that
Rizzi (1997) calls ‘bare quantiﬁcational elements like nessuno ‘no one’ and
tutto ‘tutto’ cannot function as contrastive topics but can function as foci. But
more generally, these elements cannot be pronominalized:
(58)

a.

b.

A: You saw no one?
*B: Si, lo ho
visto.
Yes, it have.I seen.
A: You did everything?
fatto.
?? B: Si, lo ho
yes, it have.I done

The example in (58b) can only be used when ‘lo’ refers to an entire set of
actions that are salient in the context. This is the same condition that allows
for topicalization of everything (Cinque 1990).
Negative quantiﬁers are also unacceptable in as for clauses, which also
require a referential expression in order be able to link up to the main clause:
(59)

*As for no one, I don’t like him.

Finally, the presented analysis can account for the difference in weak crossover between focus movement and CLLD (Rizzi 1997).
(60)

Giannii , suai madre loi ha sempre apprezzato.
Gianni, his mother him has always appreciated
b. ??Giannii suai madre ha sempre apprezzato.
Gianni his mother has always appreciated

a.

The difference is simply that Gianni and sua madre are within the same do-

Focus, Topic, and Word Order: A Compositional View 25

main in (b) but not in (a). Compare parallel cases in English:
(61)

a. As for Jimi , hisi mother always appreciated himi .
b. *It’s Jimi that hisi mother always appreciated.

The compositional analysis also sheds light on the question why it appears as
if there can be more than one topic but only one focus. It’s simply because
there is only one focus operator in a clause, and that’s what we call the focus.
All higher ones must resort to a strategy that adds them outside of the main
clause, but that can be done iteratively stacking more than one.

4 Another Alternative: The Templatic Approach
Neeleman and van de Koot (2007) discuss the relative order between focus
and topic, and, similar argument in this paper, they present evidence that
word order is more ﬂexible than expected under the cartographic account of
Rizzi (1997). The alternative explanation for the observed ﬂexibility and also
several restrictions proposed in Neeleman and van de Koot (2007) involves
the postulation of so-called ‘discourse templates’ that are used to evaluate
the well-formedness of certain syntactic conﬁgurations. I will henceforth
call this the ‘templatic approach.’ The main difference from the functional
hierarchy proposed in Rizzi (1997)—also a kind of discourse template—is
that focus and topic can occur in multiple conﬁgurations, and only certain
ones are ruled out. It can thus be characterized as a version of the cartographic
approach with a more ﬂexible hierarchy compared to the strict total ordering
underlying the proposal in Rizzi (1997).
In the following I will look at the particular discourse templates proposed
in Neeleman and van de Koot (2007) and argue that they are based on incorrect assumptions about the actual distribution of topic and focus. A caveat
should be made. The discussion here will only address aspects of Neeleman
and van de Koot (2007) concerning the relative order of topic and focus; it
does not address the substantial part of the paper on the linear order of discourse given constituents. It will hence not do justice to the full range of
arguments put forward for the templatic approach in this work.
Discourse templates have the effect that certain parts of the syntactic structure are mapped to either background or comment:
(62)

Discourse Templates

26 Michael Wagner

The treatment of focus is similar to Rizzi (1997)’s proposal that the complement of focus projections is part of the background of an utterance; it shares
with the compositional approach in Wagner (2005) in that it is generally the
sister of the constituent that evokes alternatives that is taken to be part of the
background, i.e., part of an antecedent in the discourse. While in the templatic
and cartographic approach this is a stipulated for the template or functional
sequence respectively, in the compositional approach this is a result of the
semantic denotation of the 2-place focus operator.
The templatic approach is also similar to the compositional approach in
Wagner (2005) in that the size of the background can be adjusted ﬂexibly by
moving the alternatives-invoking constituent more or less high in the phrase
marker, which is not the case in the cartographic approach, where that position is ﬁxed by the functional sequence.
Once again, it is important to point out that, similar to the cartographic
approach based on the functional sequence, the templatic approach is not in
principle incompatible with a compositional semantic analysis. However, the
actual proposal in Neeleman and van de Koot (2007) does not provide such
a compositional theory, and in fact crucially assumes a non-compositional
treatment of contrastive topics since focus and topic can stand in very different syntactic relations with no interpretive effect. One main prediction of the
templatic approach is summarized in Neeleman and van de Koot (2007) as
follows:
(63)

Predictions of the Templatic Approach
“The full range of predictions, then, is as follows. As long as we are
dealing with in-situ topics and foci, their relative order is free. However, things are different when movement comes into play. While a
topic can move across a focused constituent (whether in situ or not),
a focused constituent cannot move across a topic (whether in situ or
not).”

The starting point for this characterization of focus/topic distribution is the

Focus, Topic, and Word Order: A Compositional View 27

observation by Jackendoff (1972, 261) that contrastive topics can either precede or follow a focus in English, as discussed above:
(64)

a.

b.

Contrastive Topic ≺ Focus:
A: Well, what about Fred, what did he eat?
B: /F RED∨ate the BEANS.
Focus ≺ Topic:
A: What about the beans? Who ate them?
B: F RED ate the /BEANS∨.

In the discourse-templatic approach, the contrastive topic can be analyzed
here as being generated in situ, so this is just as expected. However, we saw
above that languages that do not allow focus operators to take inverse scope
cannot actually have contrastive topics following a focus. In fact, Dutch,
the language Neeleman and van de Koot (2007) mostly focus on, patterns
with Italian in this regard: in contrast to English, in Dutch it is impossible
for contrastive topics to be ‘base-generated’ below foci and pronounced after
them.
As before, the context that we can use to control what’s the topic and
what’s the focus is the one that B¨ ring (1997) calls contrastive aboutnessu
topics. These are utterances that answer the question under discussion but in
addition evoke a set of alternative questions. It turns out that in this kind of
context, it is impossible in Dutch to let an accented constituent that evokes
the alternatives for the topic set follow the focus of the question:
(65)

A: Wat heb je
gekocht op de
Nieuwstraat?
what have you bought on
the Niewstraat?
a.

B: Op de N IEUWSTRAAT heb ik SCHOENEN gekocht.
on the Nieuwstraat
have I shoes
bought.
b. #B: Ik heb scshape schoenen op de N IEUWSTRAAT gekocht.
i have shoes
on the Nieuwstraat
bought
The only way to pronounce (65b) such that it is felicitous is to deaccent
Nieuwstraat, in which case it is marked as given and is no longer acting as a
contrastive topic.
The templatic approach thus fails to explain why topics must precede foci
in Dutch but not in English.10
The compositional approach explains the observed restriction straightforwardly: contrastive topics are constituents that associate with a focus operator
that takes wide scope over another focus operator. Dutch belongs to the class

28 Michael Wagner
of languages in which LF-movement of a focus operator plus its focus constituent is impossible, hence the word order restriction. The reason why a
focus cannot move across a contrastive topic on the compositional view is
simply that this would lead to the wrong interpretation, since it would mean
that the ‘focus’ takes wide scope and ends up being the contrastive topic.
There are a number of restrictions that Neeleman and van de Koot (2007)
attribute to templatic restrictions that might receive a more insightful explanation in the compositional approach. For example, it seems to be impossible
to move a constituent [only x] across another focus:
(66)

A: Wie lezen er heden ten dage eigenlijk nog dichters?
a. B: Piet leest veel dichters, maar ik geloof dat F RED alleen
Piet reads many poets, but I believe that Fred only
Bloem leest.
Bloem reads
‘Piet reads many poets, but I believe that Fred reads only Bloem.’
b. *B: Piet leest veel dichters, maar ik
geloof dat
Piet reads many poets, but
I
believe that only
alleen Bloem F RED leest.
Bloem Fred reads

Under the compositional view, the focus operator attaching to alleen Bloem
would take scope over the one attached to Fred, which would result in a reading in which alleen Bloem as a whole is a contrastive topic. However, as observed by Rizzi (1997), contrastive topics do not like to be ‘non-referential,’
and elements such as negative quantiﬁers are ruled out as contrastive topics.
Clear topic contexts also reject constituents such as only x, including constructions that do not involve any movement at all:
(67)

*As for only Bloem, Fred reads him.

Thus the compositional view also provides an explanation for this kind of
restriction on word order, and in fact one that is linked to independently motivated restrictions on contrastive topics.

5 Conclusion
This paper looked at the cartographic approach to word order restrictions relating to focus and topic, and compared it with an alternative compositional
approach that tries to derive certain word order restrictions from the way the

Focus, Topic, and Word Order: A Compositional View 29

individual pieces are assembled to derive complex meaning. The observed
word order patterns were argued to be incompatible with the strong assumption of a totally ordered universal functional spine, but were as expected under
the compositional view.
What can we learn from the case of focus particles about the cartographic
project? One important point that was also raised in the insightful discussion after Peter Svenonius’s talk at the cartography conference in Brussels
is that it is not possible to derive word order from the overall meaning of
a sentence, since the tools of semantics offer all sorts of ways to decompose
complex meaning into separate pieces. So even if we embrace a more compositional approach, we have to ﬁrst ﬁnd out in what particular way languages
break down complex meaning. The construction of expressions with identical
truth conditions might be fundamentally different across languages, depending on their particular decomposition of meaning into pieces. This means
that the very rigid expectation of an universal template for all languages such
as the functional sequence may form an all-too-Procrustean bed for syntactic structure. Ultimately, however, this insight should simply lead to a more
reﬁned and semantically more informed set of questions compared with the
ones already under investigation in the cartographic approach. The project of
mapping out the composition of sentence structure across languages remains
essential.

Acknowledgements:
Thanks to the audience at BCGL and to Jeroen van Craenenbroeck, Gisbert
Fanselow, Jon Gajewski, Hotze Rullman and two very insightful reviewers
for helpful comments.

Notes
1 B¨ ring and Hartmann (2001) argue that at least in German, apparent DPu
only in fact attaches to C. This analysis has not, however, been extended
to English, and I think there is reason to be skeptical also for the case of
German.
2 A puzzle for the compositional view should be noted, however. The syntactic distribution of only and even differs in subtle ways, and it is not
clear whether these differences can be reduced to their semantics. Jackendoff (1972, 251) observes that even but not only can associate with the
subject across an intervening auxiliary (example adapted):
(i.)

a.

John will even have given his daughter a new

30 Michael Wagner
bycicle .
b. *John will only have given his daughter a new
bycicle .
3 Note that the notion of alternative set ALT still needs to be deﬁned. See
Wagner (2005) for discussion and Wagner (2009) for a formal deﬁnition.
4 Cf. the ‘relative givenness’ operator in (Wagner 2006b). To make it more
parallel to the entry of other focus operators such as only, the operator
here takes the alternative-evoking argument ﬁrst.
5 The example here contrasts with the case of a matrix clause, where fronting
the direct object to the ﬁrst position and deaccenting the rest is perfectly
acceptable (Fanselow and Lenertov´ 2006, and references therein):
a
(i.)

A: What did Maria do?
B: D EN A PFEL hat Maria gegessen.
has Maria eaten
the
apple
‘She ate the apple.’

I think the movement here adds a speech-act oriented exclamative meaning
to this sentence. Alternatively, one can also insert an expletive here and deaccent the entire sentence, again with an exclamative impact, albeit a slightly
different one:
(ii.)

A: What did Maria do?
B: M ANN hat die einen Apfel gegessen.
man has she an apple eaten
‘Boy, what an apple she ate.’

Adding such an exclamative force and hence triggering this movement seems
possible only in matrix clauses, however. For example, adding the expletive
to the ﬁrst position is out of the question in an embedded clause unless it’s a
quote—which would be odd under a propositional attitude verb like believe:
(iii.)* Peter glaubt MANN hat die einen Apfel gegessen.
Peter believes man has she an apple eaten
6 Scrambled given constituents can but need not be deaccented, in fact
they’re often not deaccented when placed in ﬁrst position. It is only when

Focus, Topic, and Word Order: A Compositional View 31

the constituent marked as given occurs to the right of its sister (or some
other constituent that it can become prosodically dependent on) that it
must be deaccented (Wagner 2005).
7 This also raises many questions, as a reviewer points out: if (32a) and (33)
are really ﬂip sides of the same type of givenness marking, then why do
the two movements appear to differ in in their syntactic properties? This
would require a discussion that goes beyond the scope of this paper.
8 Notation: Accented elements are in capitals, some diacritics: fall: \ ; rise:
/; fall–rise: ∨.
9 A reviewer suggested that Spanish shows the same pattern, but if one of
the two focus operators is not overt, the facts are different:
(ii.)

There is a girl that only John visited. After discussing who that
girl is and going back and forth about it possibly being Mary, the
speaker, a bit impatiently, says:
A M AR´A ha visitado s´ lo J UAN.
I
o
to Maria has visited only Juan
’Only Juan visited M ARY’, or ’It is M ARY that only Juan visited.’

A difference between overt and covert focus particles would be unexpected
here. Since I didn’t test the nested focus data in Spanish and know little about
the parallels and differences between Italian and Spanish CLLD I’m not sure
how to address this issue.
10 At least in base-generated order. See Wagner (in press) for some discussion of cases where in German in non-base generated orders both word
orders are possible, although the order is ﬁxed in the base-generated order.
The reason might be that the derived word order allows for reconstruction.

References
Anderson, Stephen R.
1972
How to get ‘even’. Language, 48(4): 893–906.
Bayer, Josef
1996
Directionality and Logical Form: On the scope of focusing particles
and wh-in-situ. Kluwer, Dordrecht.
Blaszczak, Joanna and Hand-Martin G¨ rtner
a
2005
Intonational phrasing and the scope of negation. Syntax, 7: 1–22.

32 Michael Wagner
B¨ ring, Daniel
u
1997
59th Street Bridge Accent On the Meaning of Topic & Focus. Routledge, London.
B¨ ring, D.
u
2003

On D-trees, beans, and B-accents. Linguistics and Philosophy, 26(5):
511–545.

B¨ ring, Daniel and Katharina Hartmann
u
2001
The syntax and semantics of focus-sensitive particles in German. Natural Language and Linguistic Theory, 19: 229–281.
Calabrese, Andrea
1984
Multiple questions and focus in Italian. In W. de Geest and Y. Putseys,
(eds.), Sentential complementation: Proceedings of the International
Conference Held at UFSAL, pp. 67–74.
Cinque, Guigliemo
1990
Types of A’-Dependencies. MIT Press, Cambridge, Ma.
Cinque, Guglielmo
1999
Adverbs and functional heads: A cross-linguistic perspective. Oxford
University Press, Oxford.
Drubig, Hans Bernhard
1994
Island constraints and the syntactic nature of focus and association
with focus. In Arbeitspapiere des Sonderforschungsbereichs 340:
Sprachtheoretische Grundlagen der Computerlinguistik, volume 51.
Sonderforschungsbereich 430, T¨ bingen/Stuttgart.
u
Fanselow, Gisbert and Denisa Lenertov´
a
2006
Left peripheral focus: Mismatches between syntax and information
structure. Ms., Universities of Potsdam and Leipzig.
Gajewski, Jon R.
2005
Neg-Raising: Polarity and Presupposition. Ph.D. thesis, MIT.
Halliday, M.A.K.
1967
Notes on transitivity and theme in English, part 2. Journal of Linguistics, 3: 199–244.
Horn, Laurence R.
1969
A presuppositional analysis of only and even. In Robert I. Binnick,
Alice Davison, Georgia Green, and Jerry Morgan, (eds.), Papers from
the 5th annual meeting of the Chicago Linguistic Society, volume 4.
Chicago Linguistic Society, Chicago.

Focus, Topic, and Word Order: A Compositional View 33
Jackendoff, Ray S.
1972
Semantic Interpretation in Generative Grammar. MIT Press, Cambridge, Ma.
Jacobs, Joachim
1983
Fokus und Skalen. Zur Syntax und Semantik der Gradpartikel im
Deutschen. Niemeyer, T¨ bingen.
u
Jaeger, Florian and Michael Wagner
2003
Association with focus and linear order in German. Ms., Stanford
University. semanticsarchive.net/Archive/DVkNDY4M.
Karttunen, Lauri and Stanley Peters
1979
Conventional implicature. In D.A. Dinneen and C.-K. Oh, (eds.),
Syntax and Semantics 11: Presupposition, pp. 1–56. Academic Press,
New York.
Kayne, Richard S.
1998
Overt vs. covert movement. Syntax, 2: 128–191.
Krifka, Manfred
1992
A compositional semantics for multiple focus constructions. In
Joachim Jacobs, (ed.), Informationsstruktur und Grammatik, volume
Sonderheft 4 of Linguistische Berichte, pp. 17–53. Westdeutscher
Verlag, Opladen.
1996
Frameworks for the representation of focus. In Geert-Jan M. Kruijff,
Richard T. Oehrle, and Glyn Morrill, (eds.), Formal Grammar Conference. Proceedings of the Conference on Formal Grammar at the
9th European Summer School in Logic, Language, and information,
pp. 99–112. Follli, Prague.
Ladd, D. Robert
1980
The Structure of Intonational Meaning. Indiana University Press,
Bloomington.
Lappin, Shalom, (ed.)
1996
The Handbook of Contemporary Semantic Theory. Blackwell, London.
Lee, Youngjoo
2005
Exhaustivity as agreement: The case of Korean man ‘only’. Natural
Language Semantics, 13: 169–200.
McCawley, James D.
1970
English as a VSO language. Language, 46(2): 286–299.

34 Michael Wagner
Neeleman, Ad and Tania Reinhart
1998
Scrambling and the PF-interface. In Miriam Butt and Wilhelm
Geuder, (eds.), The projection of arguments, pp. 309–353. CSLI.
Neeleman, Ad and Hans van de Koot
2007
The nature of discourse templates. Ms. UCL.
Nilsen, Øystein
2002
Eliminating Positions. Ph.D. thesis, University of Utrecht.
Reinhart, Tanya
2006
Interface Strategies. MIT Press, Cambridge, Ma.
Rizzi, Luigi
1997
The ﬁne structure of the left periphery. In Liliane Haegeman, (ed.),
Elements of Grammar. Kluwer.
Rooth, Mats
1992
A theory of focus interpretation. Natural Language Semantics, 1:
75–116.
1996
Focus. In Lappin (1996), pp. 271–297.
2005
Topic accents on quantiﬁers. Ms., Cornell University.
Rullmann, Hotze
1997
Even, polarity, and scope. Papers in Experimental and Theoretical
Linguistics, 4: 40–64.
Sauerland, Uli
2005
Contrastive topic: A reductionist approach. Ms. ZAS Berlin.
Selkirk, Elizabeth O.
1995
Sentence prosody: Intonation, stress, and phrasing. In John A. Goldsmith, (ed.), Handbook of Phonological Theory, pp. 550–569. Blackwell, London.
Stoyanova, Marina
2008
Unique Focus. Languages without multiple wh-questions. John Benjamins.
Taglicht, Josef
1984
Message and Emphasis. On focus and scope in English, volume 15 of
English Language Series. Longman, London and New York.
Van Craenenbroeck, J.
2006
Transitivity failures in the left periphery and foot-driven movement
operations. Linguistics in the Netherlands, 23(1): 52–64.

Focus, Topic, and Word Order: A Compositional View 35
Van Hoof, Hanneke
2003
The rise in the rise-fall contour: does it evoke a contrastive topic or
a contrastive focus? Analogy, Levelling, Markedness: Principles of
Change in Phonology and Morphology, 41(3): 515–563.
von Fintel, Kai
1999
NPI licensing, Strawson entailment, and context dependency. Journal
of Semantics, 16: 97–148.
Wagner, Michael
2005
Prosody and Recursion. Ph.D. thesis, MIT.
2006a
Association by movement. Evidence from NPI-licensing. Natural
Language Semantics, 14(4): 297–324.
2006b
Givenness and locality. In Masayuki Gibson and Jonathan Howell,
(eds.), Proceedings of SALT XVI, pp. 295–312. CLC Publications,
Ithaca, NY.
2009
Relatively given, relatively focused. Ms. Cornell University/McGill
University.
in press
A compositional analysis of contrastive topics. In Muhammad Abdurrahman, Anisa Schardl, and Martin Walkow, (eds.), Proceedings
of NELS 38, volume 2, pp. 415–428.
Williams, Edwin
1997
Blocking and anaphora. Linguistic Inquiry, 28: 577–628.
Zwarts, Frans
1998
Three types of polarity. In Fritz Hamm and Erhard Hinrichs, (eds.),
Plural Quantiﬁcation, pp. 177–238. Kluwer, Dordrecht.

Contrastive Topics Decomposed$
Michael Wagner
McGill University

Abstract
In the analysis proposed in B¨ ring (1997, 2003), contrastive topics are characterized as constituents that
u
evoke alternatives and, together with a distinct focus operator, form what is called a topic-semantic value:
a set of questions. This set of questions is then used to introduce an implicature which constrains the
discourse. The goal of this paper is to argue that instances of contrastive topics can be analyzed as involving
two recursively nested instances of the same focus operator; he associate of the operator taking wider scope
is what we call a contrastive topic. This compositional analysis appears not to account for the pragmatic
import of contrastive topics. However, experimental evidence suggests that previous assumptions about their
pragmatic import were mistaken, and the existing implicatures can be attributed to independently motivated
intonational tunes which should be dissociated from contrastive topics.

Keywords: contrastive topics, focus, alternatives, scope, intonation, prosody

1

1. Syntactic Restrictions on Contrastive Topics

2

Contrastive topics are usually identiﬁed by the contexts they occur in, by the implicatures

3

that accompany them, and also by certain intonational correlates. A typical context in which they

4

occur is what B¨ ring (1997, 55–56) calls their use as a contrastive ‘aboutness topic.’ Here, the
u

5

question under discussion is addressed by the answer, and its focus is marked by a focus feature

6

FOC. In addition, a contrastive topic, marked by a CT-feature, is employed to invoke a set of

7

additional questions, which the speaker would turn her attention to next.

$

I am indebted for helpful comments to Pranav Anand, Noah Constant, Edward Flemming, Jon Gajewski, Irene
Heim, Mats Rooth, Uli Sauerland, Bernhard Schwarz, and Junko Shimoyama, and especially to Daniel B¨ ring for
u
a commentary at the workshop on Information Structure at UCL London in September 2008. Thanks also to the
audiences of talks at ZAS Berlin, MIT, UC Santa Cruz, UCLA, UCL London, the Brussels conference on ‘Alternatives
to cartography,’ and a workshop on Syntactic Interfaces at Syracuse. Thanks to Steffani Scheer for help in designing
the experiments, and to Elizabeth-Jane Smith for editorial comments on an earlier version of this paper.
∗
Corresponding Address:
Michael Wagner, McGill Linguistics, 1085 Dr. Penﬁeld Avenue, Montr´ al, QC H3A 1A7, chael@mcgill.ca
e
Paper submitted to Semantics & Pragmatics

October 16, 2010

8

(1)

a. English: ‘BA’-Contour:
A: What did you buy on 59th street?

9

10

B: On ﬁftyNINTH street, I bought the SHOES.

11

.

L* H

L- H%

H* L- L%

12

b. German: ‘Hat’-Contour:

13

A: Was hast du auf der neunundf¨ nfzigsten Straße gekauft?
u
what have you on the 59th
street bought?

14

B: Auf der NEUNundf¨ nfzigsten Straße habe ich die S CHUHE gekauft.
u
on the 59th
street have I the shoes
bought

15

.

16

L* H

H* L L-

L%

17

18

These utterances might be used by speaker B to answer the question under discussion presented by

19

speaker A, while also anticipating that he intends to address an alternative question that substitutes

20

ﬁftyninth: maybe ‘What did you buy on 58th street?’ Contrastive topics thus play an important role

21

in navigating through discourse.

22

In the analysis of B¨ ring (1997), sentences involving contrastive topics necessarily also
u

23

involve a FOC-marked constituent which constitutes the sentence focus. Together these two oper-

24

ators form a contrastive-topic-focus conﬁguration, henceforth CTFC. A sentence with a contrastive

25

topic thus typically contains two focused and accented constituents.

26

Which constituent acts as a contrastive topic and which as a sentence focus is reﬂected in

27

their intonation. B¨ ring (1997, 2003), following Jackendoff (1972), characterizes CTs in English
u

28

as constituents marked by background-accents, or ‘B-Accents.’ In ToBI notation they are usually

29

transcribed as L*+H pitch accents which are followed by a L-H% boundary. The sentence focus

30

is marked by an answer-accent, or ‘A-accents.’ These are typically H* pitch accents, followed by

31

a L- L% boundary if the FOC constituent is the last accented constituent in a declarative sentence.

32

Contrastive topics are usually assumed to occur in an intonational phrase of their own (cf. Bader

33

2001).
2

34

In German, CTs often involve a rising pitch accent, possibly an L* H as in English. The

35

FOC-marked constituent involves a sharply falling accent, transcribed as H* L according to F´ ry
e

36

(1993). The fall in German seems much sharper compared to the one observed in English. The

37

two pitch accents in German are linked with a high pitch plateau, a conﬁguration often referred to

38

as ‘hat-’, ‘bridge-’, or ‘root-contour’ (F´ ry 1993, Jacobs 1997).
e

39

In the following, I use a simpliﬁed notation: the relevant accented words are in small caps,

40

and I mark the rise that accompanies CT-marked constituents as ‘/’, and the fall-rises marking the

41

boundaries of CTs as ‘∨’. I mark the fall following the pitch accent observed on FOC-marked

42

constituents as ‘\ ’ (and will not distinguish notationally the German and English variants of this

43

falling accent, although they seem quite different). Some other ingredients of the intonational

44

contour are not marked, e.g. the ﬁnal fall that is part of a declarative contour. The notation is

45

illustrated in the following examples of answers to pair-list questions, which constitute another

46

typical use of contrastive topics (cf. B¨ ring 2003, Van Hoof 2003), :
u

47

(2)

a. English:

48

A: Who ate what?

49

B: /F RED∨, ate the BEANS\ and /M ARY∨, ate the SPINACH\ .

50

51

52

b. German:
A: Wer hat was gegessen?
who has what eaten?
B: /F RED hat die B OHNEN\ gegessen, und /M ARIA den S PINAT\ .
FRED has the beans
eaten,
and Maria the spinach

53

54

The analysis of CTFCs in B¨ ring (1997, 2003) has two main components: the ﬁrst is that of a
u

55

‘topic-semantic value’ of expressions containing contrastive topics. Just as an expression can be

56

associated with a focus-value (Rooth 1985, 1992, 1996), i.e., a set of alternatives, in addition to

57

its regular denotation, an expression can also be associated with a topic-semantic value, which

58

consists of a set of sets of alternatives, i.e., a set of questions. B¨ ring (2003, 519) provides the
u

59

following procedure to obtain the topic semantic value, based on the assumption that there is one

60

constituent marked as a FOC and one as a CT:
3

61

(3)

CT-value Formation
a. Step 1: replace focus with wh-word and front the latter; if focus marks the ﬁnite verb

62

or negation, front the ﬁnite verb instead.

63

b. Step 2: form a set of questions from the result of step 1 by replacing the contrastive

64

topic with some alternative to it.

65

66

The topic-semantic value of the answer in (1a) would then be the set {What did you buy on 59th

67

street?; What did you buy on 58th street?; What did you buy on 57th street?, ...}. The ﬁnal notation

68

for the topic-semantic value given in B¨ ring (2003) is the following:
u

69

(4)

70

This analysis in terms of CT and FOC features predicts that the linear order between the two should

71

be free: nothing about (3) would suggest otherwise. Indeed, as observed in Jackendoff (1972), CTs

72

in English can either precede or follow the FOC-marked constituent. Compare (1a) with (5):

73

(5)

On 59 THCT street, I bought the SHOESF OC .

CT

= {{at x, I bought y|y ∈ D e }|x ∈ D e }

FOC ≺ CT in English: A: What did you buy on 59th street?
B: I bought the SHOES\ on /ﬁftyNINETH Street∨.

74

75

However, their order is not as free in some other languages. In German, e.g., if the linear order of

76

(1b) is inverted, there is no felicitous pronunciation such that both constituents are accented:1

77

(6)

FOC ≺ CT in German:
A: Was hast Du auf der neunundfnfzigsten Straße gekauft?
what have you on the 59th
street bought?

78

B: # Ich habe die S CHUHE auf der NEUNundf¨ nfzigsten Straße gekauft.
u
I have the shoes
on the 59th
street bought

79

1

The locative can be deaccented, but then it is simply marked as given and the sentence does not involve a con-

trastive topic.

4

80

That word order in German CTFCs is more restricted has already been observed in B¨ ring (1997),
u

81

but the proposed analysis for contrastive topic remains mute with respect to the source of such

82

syntactic restrictions. One possible explanation considered in B¨ ring (1997) and F´ ry (2007) is
u
e

83

that there might be a phonological reason why German is more restricted: in order to achieve the

84

hat-shape for the contour, the rise must precede the fall.

85

This phonological explanation, however, fails to account for a correlation between the syn-

86

tax of contrastive topics and other scope facts across languages discussed in this paper. In a nut-

87

shell, the empirical observation goes as follows: in English, when two focus operators are nested,

88

they can take either scope with respect to each other—at least under certain syntactic conditions.

89

This is not the case in German, where the focus operator taking wider scope must precede the one

90

it outscopes. This restriction, it is argued, also explains the restrictive relative word order in CTFCs

91

once we assume a compositional analysis of contrastive topics: the contrastive topic conﬁguration

92

involves two recursively nested focus operators, and the associate of the one taking wider scope is

93

what we call a CT. If successful, the analysis does away with the need for two separate features,

94

CT and FOC, and provides a truly decompositional analysis of contrastive topics.

95

The main goal of this paper is twofold: the ﬁrst goal is to present an argument in favor of

96

a compositional approach showing evidence for the predicated correlation. Across languages, the

97

syntax of nested overt focus operators mirrors syntactic restrictions on contrastive topic construc-

98

tions. This correlation is predicted under the compositional view and entirely unexpected under

99

the standard view.

100

A compositional analysis of contrastive topics was already considered and rejected in

101

B¨ ring (1997). While in that analysis just as in the compositional analysis presented here both
u

102

contrastive topic and focus are alternative-evoking constituents (and hence foci, broadly speaking),

103

it stopped short of analyzing a contrastive topic as the associate of a regular focus operator because

104

of apparent differences to actual multiple focus constructions. We will see that it is crucial, how-

105

ever, to compare contrastive topics to multiple focus constructions of the right kind–constructions

106

that involve multiple focus-sensitive operators, rather than a single operator that associates with

107

two or more foci. The analysis presented here improves on the one given in Wagner (2008), which

5

108

failed to capture the asymmetry between constrastive topic and focus. Alternative compositional

109

analyses were presented in Williams (1997), Van Hoof (2003), Sauerland (2005), and more re-

110

cently in Tomioka (to appear). We will see similarities and differences when discussing the details

111

of the analysis presented here.

112

The second goal of this paper is to present a new interpretation of the pragmatic implica-

113

tures that seem to come along with contrastive topics. At ﬁrst sight, these constitute a challenge for

114

the compositional view of contrastive topics proposed here: CTFCs seem to have a special prag-

115

matic import which nested focus operators usually lack. These pragmatic effects constitute one

116

of the main arguments used in B¨ ring (1997) to argue against analyzing CTFCs as multiple focus
u

117

constructions. The pragmatic implicature of CTFCs proposed in B¨ ring (1997) can be paraphrased
u

118

as follows:

119

(7)

120

B¨ ring (2003) states the implicature in terms of ‘strategies’ (Roberts 1996), with similar effects:
u

121

(8)

There is a disputable question in the topic semantic value that remains open.

An utterance U containing a contrastive topic must be part of a strategy, i.e., a non-singleton

122

set of questions that jointly answer a super-question, such that this set contains U and each

123

element in it is an element of the topic semantic value of U.

124

The proposed response to this challenge is to further decompose the meaning of contrastive topics.

125

We will see that the pragmatic implications in fact must be dissociated from the CTFC.

126

One source of the alleged pragmatic import of contrastive topics are speech-act related

127

focus sensitive operators that are realized as intonational tunes, such as the English Rise-Fall-Rise

128

contour (RFR) (cf. Ward and Hirschberg 1985, Constant 2006). Their meaning interacts with the

129

focus operators that a sentence may contain, but only indirectly. This is further conﬁrmed by the

130

fact that the alleged intonational correlate of contrastive topics in German, the ‘hat’-contour, can

131

be shown to have a related but different meaning to its English counterpart, and again this meaning

132

can combine with expressions that do not involve a CTFC.

133

A second source of additional pragmatic implications are syntactic constructions such as

134

preposing, left-dislocation, constructions involving additional lexical material such as the ‘as for’6

135

construction, and other operators related to topicality such as Japanese ‘-wa’ or Korean ‘-nun’

136

marking. While these constructions share that they can involve a CTFC in the sense discussed

137

here, they introduce a variety of different additional pragmatic restrictions on the discourse (Prince

138

1981, Ward 1988, Birner and Ward 1998, Heycock 2008), and thus these pragmatic effects must

139

be seen as separate from the CTFC conﬁguration itself.

140

In general, the range of examples discussed under the label of ‘contrastive topics’ turn out

141

to share the presence of nested focus operators, but differ in subtle and not-so-subtle ways in their

142

precise contribution to the discourse. The use of CTFCs, i.e. the nested focus conﬁguration that

143

was argued to introduce topic-semantic values, is thus much wider than the use of ‘contrastive

144

topics’ in the narrow sense, and many occurrences may not involve ‘topics’ at all. If the decom-

145

position of the components of contrastive topics proposed in this paper is correct, then a different

146

terminology that is less loaded with the discourse function should be used to discuss the syntactic

147

nesting of focus operators involved.

148

2. A Correlation

149

A sentence in languages such as English or German can contain multiple focus operators.

150

These can stand in various scope relations to each other, and to their respective associates. Krifka

151

(1992, 24) identiﬁes ﬁve conﬁgurations for multiple focus constructions:

152

(9)

a. John only1 introduced [Bill]F 1 to [Sue]F 1

153

b. Even1 [John]F 1 drank only2 [water]F 2

154

c. John even1 [only2 drank [water]F 2 ]F 1 .

155

d. John even1 only1 drank [[water]F 2 ]F 1 .

156

e. John even1 drank [only2 ]F 1 [water]F 2 .

157

Suppose that a sentence can include two separate unpronounced focus operators, then these as well

158

might occur in any or all of these conﬁgurations. Some earlier arguments against a multiple focus

7

159

analysis of CTFCs compared them to multiple foci bound by a single operator, as in (9a).2 Here, I

160

will explore the hypothesis that the conﬁguration relevant for CTFCs as they are discussed in the

161

literature is (9b), and that what we called earlier ‘CT’ is the associate of the focus operator that

162

takes wide scope, and what we called ‘FOC’ is the focus that associates with the operator that takes

163

narrow scope.

164

If the analysis of contrastive topics as involving a nesting of focus operators analogous to

165

(9b) is correct, then we predict a correlation between the syntax of CTFCs and the syntax of nested

166

overt focus operators in the same conﬁguration. In the following sections I present evidence that

167

this prediction is borne out.

168

2.1. The Case of English
In English, a focus operator can outscope material to its left (from Taglicht 1984, 150):

169

170

(10)

They were advised to learn only Spanish.

171

a.

They were advised not to learn any other language than Spanish.

172

b.

They were not advised to learn any other language than Spanish.

173

One analysis of (10b) is that ‘only Spanish’ undergoes covert movement (or overt movement as

174

in Kayne (1998)). An alternative is that there is an unorthodox surface structure bracketing as

175

proposed in the categorial grammar analysis in Blaszczak and G¨ rtner (2005). Both approaches
a

176

correctly capture that the ambiguity disappears with VP-only:

177

(11)

They were advised to only learn Spanish.

178

a.

They were advised not to learn any other language than Spanish.

179

b.

* They were not advised to learn any other language than Spanish.

2

This is arguably the case in B¨ ring (1997) and Neeleman and van de Koot (2007). See Van Hoof (2003) for some
u

discussion of B¨ ring (1997)’s original arguments against an analysis in terms of multiople foci.
u

8

180

Late placement of wide-scope only becomes more acceptable to some if the complement of only

181

is heavy. Also, it is much easier to see the wide scope reading if the narrow scope reading is

182

implausible:3

183

(12)

a.

...[I] found that I needed to understand only three to ﬁve basic concepts about

184

Eclipse to instantly become productive with this tool. ...

185

found on Google on 17/03/10, visualpatterns.com/resources/11 0672328968 ch08.qxd.pdf

b. ?# ...[I] found that I needed to only understand three to ﬁve basic concepts about

186

Eclipse to instantly become productive with this tool. ...

187

188

What happens when two focus operators occur in one sentence? Let’s consider only and even. A

189

sentence including only presupposes the prejacent and excludes all alternatives that are not already

190

entailed by the presupposition (cf. Horn 1969, von Fintel 1999):

191

(13)

Only John read Moby Dick.

192

a. Presupposed: John read Moby Dick.

193

b. Asserted: For all x, such that x read Moby Dick, John read Moby Dick → x read Moby
Dick.

194

195

A sentence including even asserts the prejacent and implicates that there are alternatives that are

196

true but less likely (cf. Karttunen and Peters 1979, 25/26):

197

(14)

Even John read Moby Dick.

198

a. Asserted: John read Moby Dick.

199

b. Conventional Implicature of even:
3

While placing only between to and the predicate understand makes the example worse, it is not outrightly rejected

by some informants, which is surprising. Maybe this is a case in which a reading seems available because all the
material to make the sentence acceptable is in the sentence, even if syntactically it’s not quite in the right place.
Vasishth et al. (2008) observe that sentences with unlicensed NPIs are rated as much better when they include a
negation in the wrong syntactic position compared to sentences that do not contain negation at all.

9

200

i.

There are other x under consideration besides John such that x read Moby Dick.

201

ii. For all x besides John, the likelihood of x reading Moby Dick is greater than or
equal to the likelihood of John reading Moby Dick.

202

203

Here’s an example in which both focus operators appear in one sentence:

204

(15)

Context I: even > only: Except for Bill, the kids in this summer camp have no respect for

205

animals and the potential dangers, which makes them take too many risks, including with

206

poisonous snakes.

207

a.

Even the most poisonous snake only frightens Bill.

208

b.

Even the most poisonous snake frightens only Bill.

209

In this context, even must outscope only in order for the probabilities to come out right. Consider

210

the alternatives relevant for only and even:4

211

(16)

a. alternatives for only (exclude even): {The most poisonous snake frightens Bill; the

212

most poisonous snake frightens Bill and individual x; ... ; the most poisonous snake

213

frightens everyone.}

214

b. alternatives for even (include only): {The most poisonous snake frightens only Bill

215

(least likely); average poisonous snakes frighten only Bill (more likely); mildly poi-

216

sonous snakes frighten only Bill (yet more likely); ... }

217

Only must take low scope and be part of every alternative considered for even since otherwise the

218

probabilities in (16b) reverse and the use of even should be infelicitous. It is clearly more likely

219

that a more poisonous snake frightens Bill, but it is less likely that it would frighten only Bill. So

220

the meaning of this sentence can be characterized as follows:

221

(17)
4

[λP 1 .even([the most poisonous snake])(P1 )] ([λx.( only (Bill) (λy. x frightens y)]).

The set of alternatives evoked by Bill in order to achieve the intuitively correct reading are the principal ﬁlter

deﬁned by the individual Bill, i.e., all sets of individuals that contain ‘Bill’ as an element.

10

222

In (15), word order matches the scope. What about switching the word order?

223

(18)

Context I: even > only

224

a.

Only Bill is afraid of even the most poisonous snake.

225

b.

# Only Bill is even afraid of the most poisonous snake.

226

When even directly attaches to the focus, inverse scope is possible. The fact that (18b) is bad con-

227

stitutes evidence that movement is involved. The wide-scope reading can be obtained by moving

228

the constituent even the most poisonous snake to a position dominating only Bill. We can con-

229

clude that in English, a focus operator can take scope over focus operators to its left—as long as

230

constraints on movement are obeyed.

231

There is a controversy, however, about whether apparent wide-scope uses of even are not

232

in fact due to a lexical ambiguity, a ‘least likely’ vs. a ‘most likely’ version, which is a negative

233

polarity item, as proposed in (Rooth 1985).5 If indeed there is a lexical ambiguity between two

234

kinds of even, then this could invalidate the scope argument above.
Rullmann (1997) presents especially convincing arguments in favor of a lexical ambiguity.

235

236

Consider the following example (Rullmann 1997):

237

(19)

238

The most plausible reading of this sentence is one that can be roughly paraphrased as ‘Even Syn-

239

tactic Structures none of the linguists they hired had read.’, in other words, if even is given its

240

standard interpretation, it would have to take wide scope over negation. But if even were indeed

241

to move to take wide scope, then this would be movement from a relative clause island, which is a

242

highly implausible analysis.

They hired no linguist who had even read ‘Syntactic Structures.’

243

Under the lexical ambiguity account, the ‘most likely’-reading of even has to be restricted

244

to downward entailing environments, since these ‘most likely’-readings are only available in

245

downward-entailing environments. We have to determine then in which kinds of environments
5

Others have argued against this view and in favor of a scope-analysis, but I will not review this debate in detail

here (cf. Wilkinson 1996, Guerzoni 2003).

11

246

exactly the negative polarity version of even is licensed. Zwarts (1998) and Gajewski (2005) dis-

247

cuss evidence that certain NPIs like until or in weeks are very restrictive and are only licensed

248

in anti-additive contexts, while others such as ever or any are licensed in all downward-entailing

249

environments—including Strawson-downward entailing environments such as the scope of only

250

(von Fintel 1999).

251

If there was a negative polarity ‘most-likely’-version of even licensed by only, then this

252

would undermine our scope argument. It seems, however, that the downward entailing version

253

of even is only licensed in anti-additive environments. In particular, only does not seem to be a

254

sufﬁcient licenser:

255

(20)

256

This sentence, in contrast to (19), does not allow for a ‘wide-scope’ reading of even that could be

257

paraphrased as ‘Even for Syntactic Structures it is true that they hired only linguists who had read

258

it.’

They hired only linguists who had even read ‘Syntactic Structures.’

259

Since the negative polarity version of even is only licensed in anti-additive contexts our

260

scope argument based on the relative scope between even and only is still valid after all. In (18a),

261

the wide-scope reading of even really must involve movement, since the negative polarity of even

262

is not licensed by only.

263

The observation that (18b)—where even attaches to the VP rather than the movable NP—

264

does not allow for a wide-scope reading provides strong additional evidence that indeed movement

265

is involved. Note, however, that this analysis makes an interesting prediction: if in our crucial

266

example in (18) we replace only with a negative quantiﬁer as in Rullman’s example, then both

267

word orders should be grammatical since the negative polarity version of even should be licensed.

268

This is indeed correct:

269

(21)

The kids in this summer camp have no respect for animals and the potential dangers, which

270

makes them take too many risks, including with poisonous snakes.

271

a. No one is afraid of even the most poisonous snake.

272

b. No one is even afraid of the most poisonous snake.
12

273

We can also consider the use of a context which motivates a reading with the inverse scope of

274

only outscoping even. This reading seems easier to access with emphasis on ‘least,’ which is

275

marked by small-caps, and deaccentuation of the following ‘poisonous snake,’ which is marked by

276

underlining:

277

(22)

Context II: only > even: The kids in the summer camp are afraid of snakes to some degree,

278

but it depends on how dangerous they are. Everyone is afraid of rattlesnakes, since they’re

279

really poisonous, but almost everyone is ok with some less poisonous snakes.

280

a. Only Bill is afraid of even the LEAST poisonous snake.

281

b. Only Bill is even afraid of the LEAST poisonous snake.

282

In this context, only must outscope even for the probabilities to come out correctly, since only must

283

not be part of the alternatives considered for ‘even:’

284

(23)

a. alternatives for even (exclude only):{Bill is afraid of the least poisonous snake (least

285

likely); Bill is afraid of more poisonous snakes (more likely); ... ; Bill is afraid of the

286

most poisonous snake (yet more likely).}

287

b. alternatives for only (include even): {Bill is afraid of even the least poisonous snake;

288

Bill and individual x are afraid of even the least poisonous snake; ... ; everyone is

289

afraid of even the least poisonous snake.}

290

In this case, for some speakers, the inverse scope is much harder than in the case of even. Those

291

speakers who can get inverse scope prefer (24a):

292

(24)

Context II: only > even

293

a.

? Even the LEAST poisonous snake would frighten only B ILL.

294

b.

# Even the LEAST poisonous snake would only frighten B ILL.

295

Sentence (24a) seems to improve when the focus of only is made heavier, just like in the case of

296

wide-scope only in Taglicht sentences (see 10):6
6

Furthermore, (24) seems to worsen without the modal, for reasons that are unclear to me.

13

297

(25)

Even the

LEAST

poisonous snake would frighten only my truly pathetic roommate Bill

Johnson.

298

299

The following example compares the inverse scope of only over even with inverse scope over all,

300

which seems similarly hard but not impossible:

301

(26)

Most problems were solved by very few students.

302

a. Even the worst student was able to solve only the easiest of the problems.

303

b. All of the students solved only one of the easiest problems.

304

Especially based on the ﬁrst case, where even outscopes only and the judgments are clearer, we

305

can conclude that in English, a focus operator can at least sometimes outscope a preceding focus

306

operator. Once conditions seems to be that movement constraints are obeyed. The observed pattern

307

provides an argument in favor of a scope analysis of ambiguities involving even and only. The

308

distribution of even furthermore shows evidence in favor of a negative-polarity version of even

309

which appears to be licensed in anti-additive contexts, and crucially not by only.
As expected based on the compositional view, the distribution of wide-scope focus opera-

310

311

tors in English mirrors that of CTs, which can also precede or follow the focus they outscope.

312

We have only looked at the interaction of two particular focus operators, only and even.

313

The syntactic parallel between sentences involving a contrastive topic and a focus and a sentence

314

involving two overt focus operators at least holds for the case where even outscopes only.7 It

315

remains to be seen how general the pattern observed for even and only will turn out to be. However,

316

the pattern observed in English sufﬁces to take a controlled look at a few other languages and

317

establish whether they exhibit a similar ﬂexibility in their word order, and whether this correlates

318

with the syntax of contrastive topics.
7

These operators are special in that they can attach either to the focus constituent they associate with or to the VP

of the proposition they scope over (Wagner 2009). We have not explored the interaction with other kinds of focus
operators, for example those that were characterized in Beaver and Clark (2003, 2008) as not associating obligatorily
with focus.

14

319

2.2. The Case of German
German does not allow for Taglicht-like ambiguities in which a focus operator or negation

320

321

outscopes material to its left8:

322

(27)

Ihnen wurde geraten nur Spanisch zu lernen.
They were advised only Spanish to learn.

323

a.

They were advised not to learn any other language than Spanish.

324

b.

# They were not advised to learn any other language than Spanish.

325

There are ambiguities in the scope of focus operators in the German Mittelfeld, but they affect how

326

much material to the right is in the scope of an operator (examples in Blaszczak and G¨ rtner 2005).
a

327

The same scope restriction seems to hold for the relative scope between two focus operators:

328

(28)

Context I: even > only: Außer Bill sind die Kinder im Sommerzeltlager zu unvorsichtig

329

mit wilden Tieren, und sie gehen zu viele Risiken ein, zum Beispiel mit Schlangen.

330

a.

331

b.

332

even ≺ only
Sogar die GIFTIGSTE Schlange angstigt nur
¨
BILL.
Even the most
poisonous snake frightens only Bill
# only ≺ even
Nur den Bill angstigt sogar die GIFTIGSTE Schlange.
¨
only the Bill frightens Even the most
poisonous snake

333

334

In the context motivating the inverse scope, linear order must be reversed:

335

(29)

Context II: only > even: Alle Kinder im Zeltlager haben Angst vor Schlangen, aber nor-

336

malerweise nur vor den giftigsten. Kaum eines der Kinder at Angst for den weniger gifti-

337

gen.

338

a.
8

only ≺ even

There are some apparent exceptions to this generalization when considering the ‘Vorfeld’ in a V2 sentence and

their scope relative to the predicate in second position, cf. Jacobs (1983) and Jaeger and Wagner (2003), but these
probably involve reconstruction.

15

Nur den BILL angstigt sogar die am WENIGSTEN giftige Schlange.
¨
only the Bill frightens even the at least
snake
poisonous
b. #? even ≺ only

339

340

¨
den Bill.
Sogar die am WENIGSTEN giftige Schlange angstigt nur
snake frightens only the Bill
Even the at least
poisonous

341

342

German sogar is often characterized as a positive polarity item, so it may seem surprising that it

343

could take scope under only. However, it seems that sogar only does not tolerate anti-additive en-

344

vironments such as the scope of sentential negation or negative quantiﬁers. In those environments

345

there are morphologically different ways of encoding even like ‘auch nur’ and ‘nicht mal:’

346

(30)

347

a. Nur Peter hat sogar/*auch nur Maria gesehen.
only Peter has even
Maria seen
‘Only Peter saw even Maria.’
b. Peter hat *nicht sogar/nicht mal Maria gesehen.
Peter has not even/not even Maria seen.
‘Peter didn’t see even Maria.’

348

349

c. Es hat kein Student *sogar/auch nur Maria gesehen.
it has no student even/even
Maria seen
‘No student has seen even Maria.’

350

351

352

As is observed in Rullmann (1997), items like auch nur and nicht mal are very much like the

353

negative-polarity version of even in English. We can further support this parallel by the observation

354

that ‘auch nur’ is not licensed by nur in examples like Rullmann’s which we saw do not license the

355

negative-polarity version of even:

356

(31)

a.

357

They hired no linguists who had even read ‘Syntactic Structures.’

358

359

360

361

Sie stellten nicht mal Linguisten ein die auch nur ‘Syntactic Structures’ gelesen
they hired not even linguists in that even
syntactic structures read
hatten.
have

b.

* Sie stellten nur Linguisten ein die auch nur ‘Syntactic Structures’ gelesen
they hired only linguists in that even
syntactic structures read
hatten.
have
They hired only linguists who had even read ‘Syntactic Structures.’
16

362

The contrast between English and German with respect to the scopal constraints on overt focus

363

operators exactly mirrors the restrictions on contrastive topics observed above: contrastive topics

364

have to precede the focus just as wide-scope focus operators have to precede low-scope focus

365

operators. This is as expected under the decompositional view.

366

2.3. The Case of Hungarian

367

In Hungarian, contrastive topics are characteristically realized in a position preceding the

368

focus and predicate of a sentence, a position called ‘T-Position’ in Szabolcsi (1981, 531). A typical

369

word order for a sentence of Hungarian would be Topic ≺ Focus ≺ verb. Contrastive Topics occur

370

in the topic-position and come with a special contrastive intonation, described in more detail in Kiss

371

(1987, 81). Kiss (2002, 22) characterizes this intonation as a ‘hat’-contour, similar to that observed

372

in German. When the subject acts as a contrastive topic, for example in pair-list questions and their

373

answers, the resulting word order is SOV:

374

(32)

A: Ki mit
ivott?
Who what.acc drank?
‘Who drank what?’

375

B: J´ nos vizet ivott...
a
Janos drank water....’

376

377

SOV-order is also used in the ‘contrastive aboutness topic’-context familiar from above in the case

378

where the subject is the contrastive topic:

379

(33)

380

381

382

383

A: Mit vett´ l
e
a V´ ci utc´ n?
a
a
what bought.2sg the V´ ci street.on
a
‘What did you buy on V´ ci street?’
a
B: A V´ ci utc´ n a cip¨ t
a
a
o
vettem.
De a kab´ tot a Kossuth Lajos utc´ n
a
a
the V´ ci street.on the shoe.acc bought.1sg. but the jacket the Kossuth Lajos street.on
a
vettem.
bought.1sg
‘On V´ ci street, I bought the shoes. But I bought the jacket on Kossuth Lajos street.’
a

17

384

Just like in German, in this context it is not possible to invert the word order, in other words, the

385

relative order between contrastive topic and focus is ﬁxed:

386

(34)

A: Mit vett´ l
e
a V´ ci utc´ n?
a
a
what bought.2sg the V´ ci street.on
a
‘What did you buy on V´ ci utca?’
a

387

a
A kab´ tot pedig a Kossuth Lajos
a
a
a V´ ci utc´ n vettem.
B: # A cip¨ t
o
a
the shoe.acc the V´ ci street.on bought.1sg. the jacket but the Kossuth Lajos
utc´ n
a
vettem.
street.on bought.1sg

388

389

‘On V´ ci street, I bought the shoes. But I bought the jacket on Kossuth Lajos street.’
a

390

391

The answer does not ﬁt the context in question, but would be considered acceptable if the con-

392

text were such that the discourse roles of the two constituents were swapped, as in the following

393

example:

394

(35)

A: Hol vetted a cipot?
where bought the shoes?
‘Where did you buy the shoes?’

395

B: A cip¨ t
o
a V´ ci utc´ n vettem.
a
a
A kab´ tot pedig a Kossuth Lajos
a
the shoe.acc the V´ ci street.on bought.1sg. the jacket but the Kossuth Lajos
a
utc´ n
a
vettem.
street.on bought.1sg

396

397

‘On V´ ci street, I bought the shoes. But I bought the jacket on Kossuth Lajos street.’
a

398

399

As expected based on the decompositional view, SOV order is also used when two overt focus

400

operators are nested in conﬁguration (9b)—at least in the case when even outscopes only:

401

(36)

402

H´ t ez unalmas party volt! M´ g J´ nos is csak vizet
a
e a
ivott!
well this boring party was even John too only water.acc drank
‘This was a boring party! Even John drank only water.’

403

A look at the ‘snake’-examples further conﬁrms the correlation. Just as in German, the surface

404

order must reﬂect the scope between the focus operators:
18

405

(37)

Context I: even > only: J´ noson k´v¨ l ebben a ny´ ri t´ borban a gyerekek nem respekt´ lj´ k
a
ı u
a a
a a

406

az allatokat es a lehets´ ges vesz´ lyeket, ´gy t´ l sok kock´ zatot v´ llalnak, t¨ bbek k¨ z¨ tt a
´
´
e
e
ı
u
a
a
o
o o

407

m´ rges k´gy´ kkal is. M´ g a legvesz´ lyesebb k´gy´ t´ l is csak J´ nos f´ l.
e
ı o
e
e
ı oo
a
e

408

a.

# Csak J´ nos f´ l
a
e
m´ g a legvesz´ lyesebb kigy´ t´ l is.
e
e
oo
only janos afraid.3sg also the most.dangerous snake-from too.
‘Only Janos is afraid of even the most poisonous snake.’

409

b.

410

M´ g a legvesz´ lyesebb kigy´ is csak J´ nost
e
e
o
a
riasztja el.
also the most-dangerous snake too only Janos-acc scares away
‘Even the most poisonous snake frightens only Janos.’

411

412

However, a complication arises if only outscopes even, or if both operators are exclusive operators,

413

in which case SVO is used and SOV is ungrammatical:9

414

(38)

A legt¨ bb ember ber´ gott a partin.
o
u
the most person in.kicked the party.on.

415

a.

* Csak J´ nos csak vizet ivott.
a
only John drank only water.acc

416

b.

Csak J´ nos ivott csak vizet.
a
only John drank only water.acc
‘Most people got drunk at the party. Only John drank only water.’

417

418

This is related to a general pattern in the syntax of exhaustive focus in Hungarian (Horvath 1986,

419

Kiss 2002, cf.). Exhaustive/exclusive foci and certain negative elements obligatorily occupy the

420

verb-adjacent focus position, unless it is already occupied by another focused constituent, in which

421

case they occur after the verb. Kiss (2002, 91) notes that in this case the focus in the focus position

422

outscopes the focus operator that remains in the VP:
9

That two exclusive operators in the same sentence result in different patterns compared to only and even occurring

in the same sentence may also be the case in English. Consider the following example:
(1)

Only Mary solved only one problem.

It seems hard if not impossible for ‘only one problem’ to outscope ‘only Mary’. This is not expected based on the
previous discussion.

19

423

(39)

´
´
´
a. Csak M ARI kapott csak K E T T ARGYB OL jelest
only Mary received only two subjects-from A+
‘It was only Mary who got an A+ only in two subjects.’

424

´
´
´
b. Csak K E T T ARGYB OL kapott csak M ARI jelest
only two subjects-from received only Mary A+

425

‘It was only two subjects in which only Mary got an A+.’

426

427

By the same token, when only outscopes even, again it is SVO order and not SOV order that

428

must be used, since the exclusive operator must be immediately in preverbal position unless that

429

position is already ﬁlled by another instance of an exhaustive or exclusive focus. In order for only

430

to outscope even it must precede it, so even plus its focus must be placed inside the VP:

431

(40)

Context II: only > even: A gyerekek a ny´ ri t´ borban valamelyest f´ lnek a a k´gy´ kt´ l,
a a
e
ı o o

432

att´ l f¨ gg¨ en, mennyire vesz´ lyesek. Mindenki f´ l a cs¨ rg¨ k´gy´ kt´ l, mert azok val´ ban
o u o
e
e
o o ı o o
o

433

m´ rgesek, de szinte senki nem f´ l n´ h´ ny kev´ sb´ m´ rges k´gy´ t´ l.
e
e e a
e e e
ı oo

434

a.

Csak J´ nos f´ l
a
e
m´ g a legkev´ sb´ vesz´ lyes kigy´ t´ l is.
e
e e
e
oo
only janos afraid.3sg also the least
dangerous snake-from too.
‘Only Janos is afraid of even the least poisonous snake.’

435

b.

436

# M´ g a legkev´ sb´ vesz´ lyes kigy´ is csak J´ nost
e
e e
e
o
a
riasztja el.
also the least
dangerous snake too only Janos-acc scares away
‘Even the least poisonous snake frightens only Janos.’

437

438

This pattern forms part of a more general set of restrictions on the structural position of focus

439

operators and quantiﬁcational phrases in Hungarian, which goes beyond the scope of this paper

440

(see Balogh 2009, for a recent overview). It is interesting to observe that the syntax of multiple

441

exclusive/exhaustive foci is similar to the syntax of single-pair-questions (cf. Balogh 2009, 152),:

442

(41)

443

444

Context:
Ki h´vott fel kit?
ı
who called VM who
‘Who called who?’

20

The syntax of a sentence with even outscoping only on the other hand can have the SOV

445

446

order that is also observed in pair-list questions as in (32).
One generalization in Hungarian seems clear: the focus operator taking wider scope has to

447

448

precede the focus operator taking narrow scope.10

449

Furthermore, the syntax of sentences in which even outscopes only is exactly parallel to

450

that of sentences including a contrastive topic and a focus. This is exactly the pattern observed

451

in English and German. Once again the syntax of nested overt operators and contrastive topics

452

correlates, at least when even outscopes only.

453

Finally, contrastive topics in Hungarian come with a similar pragmatic import (character-

454

ized as a conventional implicature in Szabolcsi (1981)–but this pragmatic is tied to a characteristic

455

intonation, and is not a general property of constituents that occur in the syntactic position in

456

which contrastive topics occur (Szabolcsi 1981, Kiss 1987). The pragmatic import can be dissoci-

457

ated from the syntactic conﬁguration of contrastive topics, and is arguably due to a sentence-level

458

intonational tune, just as in English and German–as will be argued in more detail later in this paper.

459

2.4. The Case of Italian
A ﬁrst step in approaching the realization of contrastive topics in Italian is to check whether

460

461

their order can be inverted as in English or whether it’s ﬁxed as in German.

462

(42)

A: Cosa hai
comprato sulla cinquantanovesima strada?
what have.2nd bought on-the 59th
street?
‘What did you buy on 59th street?’

463

B: Sulla cinquantanovesima strada ho
comprato le scarpe. E sulla
on-the 59th
street have.1st bought the shoes. And on-the
cinquantaduesima strada ho
comprato la giacca.
52th
street have.1st bought the jacket.

464

465

‘On 59th street, I bought the shoes. And on 52nd street I bought the new jacket.’

466

10

This is at least true when there are only two focus operators involved. Kiss (2002, 92) discusses evidence that

when there are three focused constituents, the scope between the two focused constituents that remain in the VP is
free.

21

467

Inverting the two focus constituents is dispreferred in this context:

468

(43)

A: Cosa hai
comprato sulla cinquantanovesima strada?
what have.2nd bought on-the 59th
street?
‘What did you buy on 59th street?’

469

B: ?? Le scarpe, lo ho
comprato comprato sulla cinquantanovesima strada. E
the shoes them have.1st bought on-the 59th street
.
And
comprato sulla cinquantaduesima strada.
la giacca, la ho
street
the jacket it have.1st bought on-the 52th

470

471

‘I bought the shoes on 59th street. And I bought the new jacket on 52nd Street.’

472

473

Italian thus groups with German, and contrastive topics precede foci. Let’s consider now the

474

‘snake’ examples. The ﬁrst context to consider is the one which motivates even outscoping only.

475

The facts line up with those in German in that only the sentence in which the basic word order

476

reﬂects the scope is grammatical:

477

(44)

even > only: I ragazzi del campeggio estivo hanno tutti un po’ paura dei serpenti, ma

478

dipende da quanto sono pericolosi. Tutti hanno paura dei serpenti a sonagli, perche’ sono

479

veramente velenosi, ma quasi nessuno ha paura di quelli meno velenosi.

480

a.

481

b.

* Solo Paolo ha paura anche dei serpenti piu’ velenosi.
only Paolo has fear even of.the snakes most poisonous
Anche il serpente piu’ velenoso fa
paura solo a Paolo.
even the snakes most poisonous make fear only to Paolo

482

If the verb ha paura ‘has fear’ is to be used, then the only way is to get the right scope is by overtly

483

preposing. And, also as expected, preposing is out for fa paura ‘frighten:’

484

(45)

even > only

485

a.

486

b.

Anche dei serpenti piu’ velenosi, solo Paolo ha paura.
even of.the snakes most poisonous, only Paolo has fear
* Solo a Paolo anche il serpente meno velenoso fa
paura.
only to Paolo, even the snake least poisonous makes fear.

22

487

Focus operators in Italian take overt scope. In order to get a focus operator to take scope over

488

a preceding argument, preposing is necessary to change the word order. The second context to

489

consider is the one that motivates the reading in which only outscopes even:

490

(46)

only > even: I ragazzi del campeggio estivo hanno tutti un po’ paura dei serpenti, ma

491

dipende da quanto sono pericolosi. Tutti hanno paura dei serpenti a sonagli, perche’ sono

492

veramente velenosi, ma quasi nessuno ha paura di quelli meno velenosi.

493

a.

494

b.

Solo Paolo ha paura anche dei serpenti meno velenosi.
only Paolo has fear even of.the snakes least poisonous
* Anche il serpente meno velenoso fa
paura solo a Paolo.
even the snakes least poisonous make fear only to Paolo

495

Again, the surface order of the focus operators must match their scope. If the intended meaning

496

with only outscoping even is to be expressed using the predicate fa paura ‘make fear’ in 46b, then

497

the only way is once again to use a preposing construction, and conversely, preposing is impossible

498

precisely when it would yield a word order contradicting the scope between the focus operators:

499

(47)

only > even

500

a.

* Anche dei serpenti meno velenosi, solo Paolo ha paura.
even of.the snakes least poisonous, only Paolo has fear

501

b.

? Solo a Paolo anche il serpente meno velenoso (gli) fa paura.
only to Paolo even the snakes least poisonous make fear

502

Rizzi (1997, 289/290) observes that contrastive topics differ from foci both in their morpho-

503

syntactic realization and in certain semantics restrictions. This seems unexpected if contrastive

504

topics are just foci taking wider scope. How is this compatible with the present analysis?

505

One difference, already noted in Cinque (1990, 14, 56ff), is that contrastive topics can, and

506

sometimes must, involve resumptive clitics while left-dislocated foci cannot. A clitic is obligatory

507

in the case of direct objects:

508

(48)

a.

Il tuo libro, lo ho
comprato.
the your book, it have.I bought

23

b.

509

* Il tuo libro, ho
comprato.
the your book, have.I bought
‘Your book, I bought’

510

511

Using a resumptive clitic in the case of foci, however, is reported to be impossible:

512

(49)

* I L TUO
the

LIBRO

lo ho comprato (non il
suo).
your book it
have.I bought (not the his)

b.

513

a.

I L TUO
the

LIBRO

suo).
ho comprato (non is
have.I bought (not the his)
your book

‘I bought YOUR book, not HIS.’

514

515

This observation was used as evidence that contrastive topics and foci are substantively different.

516

Within the cartographic approach, one involves movement to a topic projection and one involves

517

movement to a focus projection, which are analyzed as two different and hierarchically ordered

518

projections in the functional spine.

519

Under the analysis proposed here the sentences contrastive topics as in 48 simply involve

520

a covert focus operator that outscopes a lower covert focus operator. In fact, we can observe the

521

same clitic facts in cases with two overt focus operators. If the direct object associates with a focus

522

operator that outscopes a lower focus operator associating with a higher argument, e.g., the subject,

523

then clitic-left-dislocation is obligatory:

524

(50)

There is a petting zoo, and for some reason they put some poisonous snakes in there.

525

However, people didn’t really want to pet them. In fact, most of the snakes were not petted

526

at all, except for one: the least poisonous snake. And even that was petted by only one

527

visitor. So:

528

a.

529

530

* Soltanto uno dei visitatori (lo) ha toccato anche il meno velenoso dei
one of.the visitors (it) has touched even the least poisonous of.the
only
serpenti.
snakes
‘Only one of the visitors touched even the least poisonous snake.’

24

b.

531

Anche il meno velenoso dei serpenti, *(lo) ha toccato soltanto uno dei
even the least poisonous of.the snakes it
has touched only
one of.the
visitatori.
visitors

532

‘Only one of the visitors touched even the least poisonous snake.’

533

534

The reason why arguments that are preposed must be resumed by a pronoun might be part of a more

535

general restriction: Calabrese (1984), and, more recently, Stoyanova (2008), present evidence that

536

multiple pair-list wh-questions are not grammatical in Italian. More generally speaking there are

537

various restrictions on multiple focus constructions.

538

Quite possibly, every nested focus conﬁguration in Italian necessitates a construal in which

539

the outer focus is ‘outside’ the main clause. Suppose that Italian is such that every clause can

540

only contain one focus-operator. When there are two nested focus operators, these must in fact be

541

multi-clausal constructions, and the focus operator taking wider scope must be in a separate clause.

542

The constraints on argument-expression in clauses have to be respected: When the subject

543

associates with the wide-scope operator, we might not see an overt pronoun in the lower clause,

544

since Italian is pro-drop. However, if a direct object associates with the wide-scope focus operator

545

and is not part of the lower clause, we expect that a pronoun is necessary to realize the pronoun.

546

In sum, there is no reason to assume that a contrastive topics in B¨ ring’s sense in Italian
u

547

are intrinsically different from other types foci. This is not to say that grammatical sentence topics

548

might not show special grammatical properties. Lambrecht (1994, 292) observes that topics in

549

French, usually occur in front of the verb (and are resumed by a clitic or pronoun), similar to

550

what we observe in Italian. But this might simply be evidence that they are related to a focus

551

sensitive operator that is located outside of the core clause, maybe in a speech-act related domain

552

dominating it.
Contrastive topics in Italian are similar to as-for-topics in English. Consider the case where

553

554

an NP co-referent with a direct object acts as an as-for-topic:

555

(51)

556

a.
b.

As for John, Mary really likes him.
* As for John, Mary really likes.

25

557

As for introduces a topic, but it’s clearly not part of the main clause itself, but seems to attach

558

higher, perhaps taking a speech act as its argument. Similar to CLLD in Italian, we have to resume

559

the direct object with a pronoun. The reason the pronoun is obligatory is because it’s an obligatory

560

argument. The pronoun would be obligatorily even if the topic is co-referent with the subject since

561

English is not pro-drop. But consider:

562

(52)

563

The fact that a left-dislocated lower argument in Italian has to pronominalize if it associates with

564

a focus operator taking wide scope explains a number of apparent interpretive differences between

565

Topic and Focus. Rizzi (1997, 290-291) and Cinque (1990, 57-60) observe, for example, that

566

certain constituents that Rizzi (1997) calls ‘bare quantiﬁcational elements’ like nessuno ‘noone’

567

and tutto ‘everyone’ cannot function as contrastive topics but can function as foci. More generally,

568

these elements cannot be pronominalized:

569

(53)

As for Monday, we’ll go shopping (then).

a. A: You saw no one?
*B: Si, lo ho
visto.
Yes, it have.I seen.
b. A: You did everything?

570

571

?? B: Si, lo ho
fatto.
yes, it have.I done

572

573

The example in 53 can only be used when ‘lo’ can refer to a set of individuals that are salient in

574

the context, i.e. under the same circumstances that allow for topicalization (cf. Cinque 1990). The

575

interpretative differences between focus and contrastive topic in Italian are thus unsurprising once

576

we understand why the pronominal realization is necessary.
Note that negative quantiﬁers are also unacceptable in as for clauses, which also require a

577

578

referential expression in order be able to link up to the main clause:

579

(54)

580

The compositional analysis also sheds light on the question of why it appears as if there can be

581

more than one topic but only one focus. The reason is that only the lowest focus will not have to

*As for no one, I don’t like him.

26

582

involve pronominalization, and thus all higher foci will display the properties that Rizzi associates

583

with topic-hood.

584

In addition to correctly predicting the correlation in the syntax between contrastive topics

585

and nested overt focus operators, our analysis thus sheds new light on some well-known semantic

586

and morpho-syntactic differences between contrastive topics and foci in Italian.

587

2.5. Summary

588

Across a variety of languages, a correlation was observed between the syntax of contrastive

589

topics and the syntax of nested focus constructions. This correlation would be expected if indeed

590

contrastive topics in the sense of B¨ ring (1997) involved two nested focus operators. Of course,
u

591

a broader typological investigation would be desirable to further substantiate the correlation, but

592

rather than pursue this typological angle, we will shift gear now and try to develop a particular

593

analysis of contrastive topics that actually involves nested focus operators and can thus explain

594

the correlation. The goal is then to show how such an analysis can still explain the observed

595

distributional generalizations of CTFCs.

596

3. Decomposing Contrastive Topics

597

The decompositional view holds that CTFCs actually involve two nested unpronounced

598

focus operators. The associate of the focus operator taking wider scope, the contrastive topic, is

599

the one called CT in the approach of B¨ ring (1997, 2003), and the associate of the one taking lower
u

600

scope is called FOC.

601

In order to see whether this approach can in fact explain the distribution of contrastive

602

topics, we need to be explicit about the focus operator that we assume. I propose the following

603

focus operator, a version of Rooth (1992)’s ∼-operator which takes two overt arguments just like

604

only and even (and similar to GR in Wagner 2005, 2006). One version of the entry in previous

605

instances of this paper, which did not involve alternatives, actually made a wrong prediction with

606

respect to the presuppositions of nested focus operators. Thanks to Pranav Anand, Jon Gajewski,

607

and Uli Sauerland for helpful discussion. In a commentary on an earlier version of this paper,

27

608

B¨ ring (2008) proposed a way to resolve the issue, and Heim (2010) suggested a similar revision.
u

609

The following analysis is adapted from their suggestions, all errors are my own.
The basic idea is that the FOCUS operator, rather than ‘consuming’ the alternatives intro-

610

611

duced by a focus in its scope, is able to to pass them up to a higher focus operator.

612

(55)

∀σ: FOCUS

g
o

= λxσ .λP <σ,st> .
g
a

613

Presupposition: ∃a ∈ x

: P (a) is salient and P (x) ⊆ P (a)).

614

Ordinary Semantic Value: P(x)

615

Focus Semantic Value: FOCUS

g
a

={a∈ x

g
a

| P (A) }

616

Tomioka (to appear) proposes a similar analysis of contrastive topics that involves a topic operator

617

that also rebinds a lower focus, in this analysis this accomplished using a representation with in-

618

dexed focus-variables following Kratzer (1991) and Wold (1996). I will return to some differences

619

to the present approach in the second part of the paper. Here’s an example of a sentence involving

620

a FOCUS:

621

(56)

FOCUS (Moby Dick) (λx. John read x):

622

a. Assertion: John read Moby Dick.

623

b. Presupposition: There is an a in the alternative set to Moby Dick, such that John read
a is salient and John read Moby Dick does not entail John read a.

624

c. Focus Semantic Value: { John read Moby Dick, John read Crime and Punishment,

625

John read War and Peace, ...}

626

627

What happens when we nest two FOCUS operators?

628

(57)

629

The proposed LF parallel to that of nested overt focus operators looks as follows:

630

(58)

631

The presuppositions introduced by the two focus operators can be paraphrased as follows:

A: What did you buy on 59th street? B: /On 59th street∨ I bought the SHOES\ .

[λP 1 . FOCUS (on 59th street)(P1 )] ([λx.( FOCUS (shoes) (λy. I bought y at x)]).

28

632

(59)

a. Inner FOCUS: ∃ y’ such that I bought y’ on 59th street is salient, and is not entailed
by I bought shoes on 59th street.

633

b. Outer FOCUS: ∃ x’ such that ∃ y’ such that I bought y’ at x’ is salient and not entailed

634

by (59a) and I bought the shoes on 59th street.

635

636

There is an asymmetry in the strength of the presupposition associated with the two focused con-

637

stituents, and switching the roles in this context changes the meaning:

638

(60)

[λP 1 .FOCUS (the shoes)(P1 )] ([λx.( FOCUS (on 59 street) (λy. I bought x at y)]).

639

(61)

a. Inner FOCUS: ∃ y’ such that I bought shoes at y’ is salient, and is not entailed by I
bought shoes on 59th street.

640

b. Outer FOCUS: ∃ x’ such that ∃ y’ such that I bought y’ at x’ is salient and not entailed

641

by (61a) and I bought the shoes on 59th street.

642

643

The proposed analysis can therefore account for why switching the roles of CT and FOC can lead

644

to an infelicitous result.

645

In our analysis, contrastive topics in B¨ ring’s sense are the constituents associated with
u

646

focus operators that take wide scope, but the constituent itself can actually scope low. In fact, any

647

analysis claiming contrastive topics themselves must take wide scope would be doomed, since,

648

as is well-known, the intonation contour that often accompanies CTFCs often has the effect of

649

scope reversal, such that a CT is outscoped by a FOC. How is this compatible with the presented

650

analysis? That operator and associate can take different scope is in fact a general property of focus

651

operators—overt focus operators in German cannot reconstruct, yet their associate is free to do so

652

(B¨ ring and Hartmann 2001, 262):
u

653

(62)

654

Nur ein Bild von seiner Frau besitzt jeder Mann t.
only a picture of his wife possesses every man
a.

657

possesses every mani [a picture of hisi wife]

The only person every man possesses a picture of is his wife.

655

656

LF: only

b.

* LF:

possesses every mani [ only a picture of hisi wife]

Every man only possesses a picture of his wife.
29

658

A look at the LF in (58) reveals what’s going on: while the higher focus operator is interpreted

659

taking scope over the embedded one, the constituent it associates with is actually interpreted in its

660

base position via λ-abstraction, and may take wide or narrow scope relative to the associate of the

661

lower focus operator, depending on where it starts out. The scope of the associate of the higher

662

focus operator is thus expected to be whatever its scope is before associating with it (as in Jacobs

663

1997). This is familiar from wh-movement, which allows for similarly split scopes. I will return to

664

why it is that sometimes contrastive topics seem to favour scope inversion at the end of the paper.

665

Sometimes, however, the roles can be freely switched (cf. Neeleman and van de Koot

666

2007). This can now be explained since in a pair-list context, the presuppositions of either scope

667

are fulﬁlled, so what changes is arguably not the order of CT relative to FOC but instead which

668

constituent is the CT:

669

(63)

A: Wer hat wen
eingeladen?
Who has who invited?
B: /H ANS hat P ETRA\ eingeladen und /RUDI wurde von S UZA\ eingeladen.
Hans has Petra
invited and Rudi was by
Suza
invited

670

671

A different analysis with a less fanciful way of getting the topic operator to associate with two foci

672

is conceivable. Suppose that for ‘asymmetric’ cases where focus and contrastive topic cannot be

673

switch role, the wide-scope operator is actually not a simple focus operator, but one that operates

674

on alternatives to both the focus constituent (e.g., the subject if it attaches to the subject) and to the

675

open proposition (e.g., the remaining VP):

676

(64)

∀σ: TOPIC

g
o

= λxσ .λP <σ,st> .
g
a

:, ∃P ′ ∈ P g . P ′ (a) is salient and P (x) ⊆ P ′(x)).
a

677

Presupposition: ∃a ∈ x

678

Ordinary Semantic Value: P(x)

679

Focus Semantic Value: { P(x)}

680

This would remove the need to deﬁne a non-typical focus operator that can ‘rebind’ alternatives

681

introduced for a lower focus variable. Of course, the alternatives considered for the proposition

682

might be inﬂuenced by those introduced by the lower focus.
30

683

It seems plausible that operators such as TOPIC exist: The German discourse particles

684

‘zumindest,’ ‘aber,’ ‘jedenfalls,’ and ‘jedoch’ seem to require an analysis similar to that of TOPIC

685

(maybe one with a slightly more contrastive presupposition), at least in their use as discourse

686

modiﬁers that attach to a topicalized constituent:

687

(65)

Peter hingegen
hat Maria eingeladen.
Peter on the other hand has Mary invited
‘Peter, on the other hand, invited Mary.’

688

689

The discourse particle hingegen seems to evoke alternatives to the topicalized constituent and re-

690

quire that there be a property that applies to one of them that contrasts with the one that applies to

691

the topicalized constituent itself. The particles aber’ and ‘jedoch’ are quite similar, while jedenfalls

692

means something closer to ‘at any rate.’

693

An unresolved issue for the analysis proposed here 11 is that focus operators like even and

694

only seem to behave in a different way when they are attached to an NP: They cannot associate with

695

the lower focus at the same time. For example, (66a) is infelicitous because Germans, unlikely to

696

wear Lederhosen though they truly are, are still considered more likely to do so than any alternative

697

nationality. The sentence is infelicitous except in the reading where there is some other alternative

698

to Germans that is more likely to wear Lederhosen (which is not an implausible reading, say one

699

assumed they are very reluctant to embrace the Bavarian sterotype which has been unjustly gener-

700

alized to all Germans).The unavailability of the reading paraphrased below the example shows that

701

the alternatives considered are of the form x wore Lederosen and not x wore y, which means that

702

even cannot associate with both foci. In (66b), however, this reading is available, and association

703

with the entire sentence (or maybe with two separate foci, one on the subject and one on the VP)

704

is possible (cf. Jackendoff 1972, 247/248):

705

(66)

At this costume party, everyone came dressed up as a clich´ representative of their country.
e

706

a.

# Even the Germans wore Lederhosen.

707

b.

The Germans even wore Lederhosen.

11

And a similar issue arises in that of Tomioka (to appear).

31

‘It was even the case that the Germans wore Lederhosen.’

708

709

Maybe this means that a true topic operator which associates with both its complement and an

710

open proposition as the one in (64) should in fact attach to a VP rather than an NP.

711

There have been several earlier attempts to derive the semantics of CTFCs by recursively

712

nesting two focus operators. Williams (1997) proposes such an analysis, but posits that contrastive

713

topics are embedded foci and take narrow scope. This, however, conﬂicts with the scope facts

714

discussed here. Sauerland (2005) proposes that contrastive topics involve two nested givenness

715

operators, leading to entirely symmetric presuppositions for the operators associating with CT and

716

FOC:

717

(67)

John saw Mary.

718

John G-[x G-[y . x saw y] Mary]

719

a. Presupposition 1: John saw someone.

720

b. Presupposition 2: Someone saw Mary.

721

c. Presupposition 3: Someone saw someone.

722

The approach is similar to the approach in B¨ ring (1997) in that it cannot capture constraints on
u

723

relative order. The CT and FOC should henceforth always be interchangeable, and there should

724

not be any syntactic constraints on relative word order.12

725

4. The Pragmatic Import of Contrastive Topics

726

What about the pragmatic implications and the special intonation attributed to CTFCs in

727

B¨ ring (1997, 2003) and other earlier approaches? Aren’t they good reasons to distinguish senu

728

tences involving contrastive topics from sentences involving two nested focus operators?

729

The response proposed in the following is that the pragmatic effects of contrastive topics

730

can be dissociated from CTFCs. The source of this additional pragmatic import observed in some
12

A non-compositional approach with very different predictions about the syntactic distribution of CTFCs is pre-

sented in Neeleman and van de Koot (2007). A more detailed discussion can be found in Wagner (2009).

32

731

uses of CTFC are actually due to a focus-sensitive operator that introduces additional presupposi-

732

tions or implicatures. This operator can remain unexpressed, or be realized as lexical entries like

733

only or ‘even,’ and maybe as particles such as Japanese ‘-wa,’ or as intonational tunes.

734

In particular, the intonational tunes associated with contrastive topics since Jackendoff

735

(1972, 261) are actually separate operators that can be dissociated from the CTFCs themselves,

736

and, consequently, the implicatures attributed to contrastive topics in B¨ ring (1997, 2003) are due
u

737

to the rise-fall-rise contour (RFR) in the case of English, and the so-called ‘hat’-contour (HAT) in

738

the case of German.

739

If this response is valid, then there are two expectations: ﬁrst, the tune should be able to

740

occur in the absence of a CTFC; and second, CTFCs should be possible without these pragmatic

741

effects and without the correlating intonation or syntactic operation. The new account makes very

742

different predictions from those of B¨ ring (1997, 2003) and other approaches, even for some of
u

743

the most basic data on contrastive topics.

744

4.1. Dissociating the Pragmatic Import: The Case of English AB-BA

745

One diagnostic for contrastive topics often used in the literature is intonation. Jackendoff

746

(1972, 261) claims that foci come with A-accents or ‘answer-accents,’ and contrastive topics with

747

B- accents, or ‘background-accents,’ and these can occur in either order:

748

(68)

749

750

751

a. A: Well, what about Fred, what did he eat?
B: /F RED∨ate the BEANS.
b. A: What about the beans? Who ate them?
B: F RED ate the BEANS∨.

752

B¨ ring (1997, 2003) follows this basic assumption and takes B-accents as an exponent of CTu

753

marking, and A-accents as exponents of FOC-marking.

754

The close relationship between topic-marking and intonation assumed in B¨ ring (1997)
u

755

predicts that all utterances involving either AB- or BA-contour should trigger the disputability

756

implicature attributed to contrastive topics in (7): a proposition that is a member of the topic-

33

757

semantic value remains open (‘disputable’). In the two examples in (68) this would lead to the

758

following implicatures:

759

(69)

a. (68a) → There is a disputable question of the form ‘What did y eat?’, where y is a
salient alternative to Fred.

760

b. (68b) → There is a disputable question of the form ‘Who ate y?’, where y is a salient

761

alternative to beans.

762

763

B¨ ring (2003) states the pragmatic import differently but concurs in predicting that both contours
u

764

should be pragmatically equivalent.

765

The claim that the AB- and the BA-contour are mirror-images of each other with the same

766

pragmatic import is central to B¨ ring (1997, 2003)’s and other approaches to contrastive topics but
u

767

to my knowledge it has not been tested in a controlled way.

768

There are reasons to be skeptical, however. If the BA-contour really had the implicature

769

in (68a), then it should be infelicitous to use this contour in the last answer of a pair-list question,

770

since at this point all alternatives have been resolved. However, as was observed in Krifka (1999),

771

the BA-contour is perfectly acceptable at the end of such a list of answers. Constant (2006), on the

772

other hand, observes that the ‘AB’ cannot occur on the last constituent in a pair-list answer.

773

(70)

774

a. A: Who kissed whom? B: A NNA ∨kissed J OHN, and J IM ∨kissed B ERTA.
b. ?# A: Who kissed whom? B: A NNA kissed J OHN, and J IM kissed B ERTA∨.

775

It seems that 70b is not impossible, but the sense one gets from the answer is that the speaker wants

776

to insinuate something above and beyond what is conveyed by the pair-list answer itself. Since the

777

context doesn’t really give any guidance as to what this insinuation might be the dialogue sounds

778

somewhat infelicitous. Suppose A says, ‘This was a boring party.’ Then B could use the pair-list

779

answer in (70b) to answer the accommodated question ‘Who kissed whom?,’ and to insinuate with

780

the entire pair-list: ‘No, it wasn’t!’ Crucially, no such context is necessary in order to use ‘BA’,

781

and there is no such sense of an insinuation being made in (70a).

782

How can we explain this asymmetry between AB and BA? The proposal put forward in

783

Wagner (2008) is the following: AB actually involves a sentence-level tune, the RFR, and this tune
34

784

is responsible for the implicature. Consider the following use of an RFR-contour:

785

(71)

A: Do you think Mary was involved in the candy store robbery?
B: She likes CHOCOLATE∨.

786

787

This utterance can be used to state that Mary likes chocolate and to insinuate that yes, Mary was

788

involved in the robbery. Ward and Hirschberg (1985) analyzed the RFR-contour to be a sentence

789

tune indicating speaker uncertainty. Oshima (2005) and Constant (2006) subsume the meaning of

790

the intonational morpheme involved to that of focus operators operating on alternatives. Constant

791

(2006) adapts the speaker-uncertainty approach and holds that RFR introduces the conventional

792

implicature that ‘none of these alternatives can safely be claimed.’ I propose a slightly different

793

existential operator that requires that a salient alternative is possibly true, closely following an

794

analysis Japanese wa (to which I will return below) in Oshima (2005) (see also a related proposal

795

on wa in Hara 2006):

796

(72)

797

Gricean reasoning leads to the implicature that the speaker did not assert the alternative for a rea-

798

son, be it uncertainty, politeness, or something else. Why else would a speaker mark a proposition

799

as possibly true, rather than just asserting it?

RFR = λp : ∃a salient p′ ∈ C, ∧p → ∧ p′ and ⋄ p′ . p

800

Speaker uncertainty as it is evoked in Constant (2006), following Ward and Hirschberg

801

(1985), is one possible motivation for using the RFR, but there are others. Maybe the speaker in

802

(71) does not want to ﬂat out incriminate Mary for being involved in the robbery although (s)he

803

believes that it’s obvious that she was. So ‘speaker uncertainty’ seems too narrow to characterize

804

the usage of this contour.13

805

The RFR actually seems to combine with speech acts and operate on alternative assertions,

806

as it cannot be embedded even under speech-act embedding predicates. An insinuation with the
13

Several people made the plausible suggestion to me that RFR should mean something stronger, namely that the

alternative is possibly true and possibly false. I do not have room to discuss this here, but it would make the analysis
of RFR more similar to the one proposed for CTs in Hara and van Rooij (2007).

35

807

RFR is always made at the matrix level, e.g., in the following example it is the speaker and not

808

John who is making the insinuation:14

809

(73)

810

If indeed RFR quantiﬁes over speech acts, this would suggest that in fact, the meaning of the RFR

811

might have to be characterized not based on the truth-value of an alternative proposition but based

812

on the possibility of an alternative assertive speech act:

813

(74)

814

If the expression that RFR attaches to contains a focus, then the evoked alternative can, but doesn’t

815

have to, be one of the focus alternatives—it could also be an alternative assertion altogether. For

816

example, the assertion could convey the implied statement ‘Don’t you know this?.’ This type of

817

implicature is a very common use of RFR, and in fact the word ’Duh’ in English seems to be used

818

as a dummy carrier for RFR if a speaker wants to make this implicature without asserting anything

819

(Constant 2006).

John thinks that she likes CHOCOLATE∨.

RFR = λp : ∃a salient p′ ∈ C, ∧p → ∧ p′ and asserting p′ might be justiﬁed. p

820

This analysis of the RFR accounts for why (75) is infelicitous; an observation from Ward

821

and Hirschberg (1985, 755). If ‘they’ had a boy, the salient alternative—‘it’s a girl’—must be false,

822

and hence all salient alternatives have been resolved. But now observe (76), which, in contrast to

823

Ward and Hirschberg (1985)’s example, is felicitous: ‘they’ may still also have a girl, even if they

824

have a boy, but maybe the speaker is not sure about this:
(75)

A: Did they have a boy or a girl?

(76)

A: Do they have kids of both genders?

B: ?# They had a BOY∨.

B: They have a BOY∨.

(Ward and Hirschberg 1985, 755)

825

(Wagner 2008)

826

While the new analysis departs from the view in B¨ ring (1997, 2003) which ties contrastive topics
u

827

closely to disputability implicatures, an important insight of B¨ ring’s analysis is still captured by
u

828

incorporating something very similar to the ‘disputability’-implicature as part of the meaning of a

829

sentence-level tune. But this disputability implicature is now predicted to only come into play in
14

For a contrary view on English see Constant (2006, 40, fn. 37).

36

830

the case of AB. Furthermore, the precise implicature differs in that the relationship between the

831

insinuated proposition and the alternatives to the A- and B-marked constituents is less direct.

832

The BA-contour does not involve such a sentence-level intonational contour and does not

833

trigger an implicature of the kind attributed to it in the literature. The claim is that BA only

834

conveys the much weaker condition on the context that there is a contrasting alternative. This does

835

not mean that using the BA is incompatible with use in a contrastive-topic scenario, it just means

836

that it should be compatible with a wider set of contexts.

837

This is compatible with the observation that a BA-contour seems to be possible whenever

838

there are two accented constituents separated by a boundary, e.g., when an accented word precedes

839

a prosodic boundary induced by syntactic constituent structure:

840

(77)

A: Who came to the party?

841

a. B: /M ARY∨, || or S UE and JANE.

842

b. B: /M ARY∨, || and S UE or JANE.

843

In other words, what has been called a B-accent in the ‘BA’ contour might actually just a possible

844

rendition of a sentence-medial pitch accent when it’s followed by an intonational boundary.
The claims about the pragmatic import of BA and AB are summarized in (78):

845

846

(78)

a. BA: There is a salient contrastive alternative to B.

847

b. AB: There is a salient alternative to the entire proposition that is possibly true.

848

In the following, the results of a perception experiment are reported that tested contrasting predic-

849

tions of the received view and the claims outlined in (78).15

850

4.2. No Disputability Implicature for BA

851

If a sentence with a BA-contour implicates that a member of the topic-semantic value is

852

still open and disputable, then it should be infelicitous in a context where all relevant alternatives
15

The stimuli for the experiment are posted at http://www.prosodylab.org/∼chael/www/topic.

37

853

have been excluded already by the time the sentence with the BA-contour has been asserted. This

854

prediction is tested in B¨ ring (1997, 2003) based on dialogues similar to (79):
u

855

(79)

Corrective Dialogue
a. AB-Contour

856

857

A: Did John insult Mary?

858

B: No! M ARY\ insulted J OHN\ .
b. BA-Contour

859

860

A: Did John insult Mary?

861

B: No! M ARY∨ insulted J OHN\ .
c. AB-Contour

862

863

A: Did John insult Mary?

864

B: No! M ARY insulted J OHN∨.

865

In these dialogues, each answer disputes the proposition made salient by the yes/no question,

866

and since they are directly contradictory the only salient alternative seems to have been ruled out

867

once the answer has been asserted. But if the AB- and the BA-contour both carry the implicature

868

attributed to them, then both answers should be infelicitous. The question is: are these predictions

869

really born out? A dialogue-rating experiment was carried out to test them.

870

Stimuli. We recorded a total of sixteen dialogues. Each dialogue was recorded with three

871

different intonations in the answer, AA, BA, or AB. Eight of the dialogues involved a single correc-

872

tive contrast between two sentences that are identical except that agent and patient were switched,

873

an example is the dialogue in (79). These cases were compared with eight control dialogues with

874

a context similar to Jackendoff’s in (68). This dialogue motivates making the disputability impli-

875

cature and should hence be compatible with the use of a contrastive topic. Each dialogue contains

876

a ﬁrst question, which sets up the contrastive topic, and is followed by a subquestion about the

877

individual picked out in the ﬁrst:

878

(80)

879

a. AB-Contour
A: What about Wallace? Where will he go over the break?
38

B: WALLACE will go to E UROPE\ .

880

881

b. BA-Contour

882

A: What about Wallace? Where will he go over the break?

883

B: WALLACE∨ will go to E UROPE\ .

884

c. AB-Contour

885

A: What about Wallace? Where will he go over the break?

886

B: WALLACE will go to E UROPE∨.

887

The predictions of the received view about BA and AB are clear-cut: BA should be infelicitous in

888

the corrective examples and felicitous in the sub-question context. The predictions of AA are less

889

clear since it was noted already in Liberman and Pierrehumbert (1984) that B-accent renditions,

890

like in 68, with the full glory of the wiggly contour of contrastive topics are rare. This could

891

either mean that the realization of the B-accent is optional, or that the difference to an AA-contour

892

is sometimes imperceptible. Either way, we might expect AA to be felicitous in both types of

893

contexts.

894

The alternative hypothesis makes very different predictions: the BA-contour should be

895

felicitous in both dialogues, since it does not trigger a disputability implicature and therefore is

896

compatible with a corrective statement. The AB-contour should be infelicitous in the corrective

897

dialogue, since all alternative propositions have been ruled out. It might be acceptable in the sub-

898

question dialogue—but only if this kind of dialogue is really sufﬁcient to make salient that there are

899

disputable alternatives, and it is plausible that the speaker wants to insinuate them. The predictions

900

of the two theories are summarized in Table 1:

901

Methods: Questions and answers were recorded separately by a native speaker of Cana-

902

dian English and were played to the 29 participants as dialogues in a pseudo-randomized order.

903

Most participants were undergraduates at McGill and all were native speakers of North American

904

English. They heard only one version of each dialogue with one of the three intonations in pseudo-

905

randomized order, and a total of 33 dialogues. Each participant had an equal number of dialogues

906

from each condition. After listening to the dialogue they had to rate how acceptable the answer

907

sounded given the question. Their answer and their reaction times were recorded on each trial.
39

Received View

Decompositional View

Intonation Corrective

Sub-Question Corrective

Sub-Question

AA

?Felicitous

?Felicitous

?Felicitous

?Felicitous

BA

Infelicitous Felicitous

Felicitous

Felicitous

AB

Infelicitous

Infelicitous ?Infelicitous

Infelicitous

Table 1: Predictions: Corrective vs. Sub-Question Context

908

Since the reaction time data were not particularly revealing only the responses are reported here.

909

910

911

Results: The responses show a three-way signiﬁcant distinction between different into-

912

nations, in both the corrective dialogues and the sub-question dialogues. Interestingly, the AA-

913

contour was rated most acceptable in both types of dialogues.

914

The overall ratings of the AB contour were rather high, considering that both theories

915

would predict this contour to be infelicitous, at least in the corrective condition. However, it is

916

important to keep in mind that the participants were not instructed to rate the intonation, but to rate

917

the overall acceptability of the utterance. It is likely that a sentence that is otherwise acceptable

918

but has a funny intonation will still be considered quite acceptable. In other words, the numerical

919

value of the ratings might not be very informative, and the difference between intonations may still

920

reveal contrasts in felicity.

921

We tested the signiﬁcance of the differences between the conditions, and also the interaction

922

of these differences with the type of dialogue, using a mixed model regression controlling for

923

random effects due to participant and item.16
16

In the statistical analysis the ratings were centered to a scale ranging from -2 to 2. We used the ‘lmer’ function of

the lme4 package in R. The model we used looked as follows: model.lm < − lmer(response ∼ intonation*dialogue
+ (1|item) + (1|subject), data = data.abba). Baayen et al. (2008) note that in a mixed-model regression a comparison
can be considered signiﬁcant if the t-value for a comparison exceeds the absolute value 2. In addition, we also report
a conservative estimate of a p-value based on mcmc-sampling.

40

Figure 1: Ratings for each Intonation for Corrective and Sub-Question Dialogues.

924

There was a main effect of intonation such that the AA-contour was more acceptable than

925

the BA-contour (t=-2.2, p<0.03) and the AB-contour (t=-6.5, p<0.0001). The ﬁrst surprise for

926

the received view is that the BA-contour is also less acceptable than the AA in the sub-question

927

dialogue, and the interaction between intonation and dialogue is not signiﬁcant (t=-1.3, p< 0.21).

928

It would be expected to be infelicitous in the corrective dialogue and felicitous in the sub-question

929

dialogue, but this is not the case. This suggests that the received theory is incorrect in attributing an

930

implicature of disputability to this contour: If there was an implicature, there should be a difference

931

between the types of dialogues in how BA is rated.

932

The alternative hypothesis that BA only carries the implicature that there must be a salient

933

alternative for ‘B’ fares better: it is plausible that the contrast for the subject is more salient in the

934

corrective condition since it is explicit there, but it is only evoked in the sub-question dialogue.

935

This does not yet explain why AA should be better than BA though. However, Liberman and

936

Pierrehumbert (1984) note in the context of their production experiment that the BA and AB-

937

contour are rare in production unless one trains speakers to use them. Maybe the more elaborate

938

contours sound stilted and artiﬁcial, and the ratings are a reaction to an overly exuberant way of

939

expressing oneself.

940

Discussion: The results speak against the view that BA carries a disputability implicature,

941

but are compatible with the view that AB does. The fact that BA was, if anything, rated as better in
41

942

the corrective dialogue compared to the sub-question dialogue goes directly against the predictions

943

of the received view, while it makes sense under the alternative hypothesis proposed here.

944

The results discussion so far could lead one to suspect a simpler explanation: maybe the

945

experiment failed, and people for some reason did not relate these utterances to their context, and

946

they rated how the responses sounded by themselves, disregarding the context. This cannot be

947

ruled out based on the data so far, but a look at the second set of conditions from the experiment

948

will show that this was not the case.

949

4.3. The AB Implicature

950

A second experiment was conducted to test exactly which pragmatic contribution AB really

951

makes. The received view holds that the constituent carrying the B-accent has to be the contrastive

952

topic, but the alternative view actually makes no such prediction. Since the RFR is a sentence-level

953

tune, it should be possible to ﬁnd a sentence with an ‘AB-contour’ in which A is the contrastive

954

topic and B the focus.

955

Stimuli: In order to get at this question, our experiment contained eight dialogues in addi-

956

tion to those described above that contained a simple question and a two-part answer. The ﬁrst part

957

of the answer addresses the question under discussion and at the same time anticipates intonation-

958

ally which question that is part of a shared super-question will be addressed next. This is the type

959

of context classiﬁed as ‘contrastive aboutness topic’ in B¨ ring (1997).
u

960

In four out of eight dialogues the question probes for the ﬁrst accented constituent in the

961

answer, and the second accented constituent acts as the contrastive topic, which anticipates the

962

focus of the sub-question that will be answered next:

963

(81)

a. AA-Contour

964

A: What did you buy on 59th street?

965

B: I bought SHOES on 59th Street\ . And I bought a JACKET on 58th street.\ .

966

b. BA-Contour

967

A: What did you buy on 59th street?

968

B: I bought SHOES∨on 59th Street\ . And I bought a JACKET on 58th street.\ .
42

c. AB-Contour

969

970

A: What did you buy on 59th street?

971

B: I bought SHOES on 59th Street∨. And I bought a JACKET on 58th street.\ .

972

In the other four dialogues the question probes for the second accented constituent in the answer,

973

and the ﬁrst accented constituent acts as a contrastive topic, again anticipating the focus of the

974

sub-question to be addressed next:

975

(82)

a. AA-Contour

976

A: Where did John buy his presents?

977

B: J OHN bought his presents at I NDIGO’s\ . And M OLLY bought her presents at

978

C HAPTERS.\ .

979

b. BA-Contour

980

A: Where did John buy his presents?

981

B: J OHN∨bought his presents at I NDIGO’s.

982

atC HAPTERS\ .

983

And M OLLY bought her presents

c. AB-Contour

984

A: Where did John buy his presents?

985

B: J OHN bought his presents at I NDIGO’s∨. And M OLLY bought her presents at

986

C HAPTERS.\ .

987

The received view predicts that in both contexts the contrastive topic, that is, the constituent that is

988

accented in order to anticipate which other sub-question will be addressed next, should be allowed

989

(or maybe even required) to carry a B-accent, while the focused constituent, i.e., the constituent

990

corresponding to the wh-word of the question, should not be allowed to carry a B-accent. If

991

correct, then the BA-contour should be infelicitous in dialogue (81) and felicitous in (82), and the

992

AB-contour should be felicitous in (81) and infelicitous in (82).

993

The decompositional view makes different predictions: since the BA-contour does not carry

994

a disputability-implicature it should be felicitous in both (81) and (82). In both dialogues there is

995

a contrasting constituent for the ﬁrst constituent, so a B-accent should be legitimate in both cases.
43

996

The AB-contour should also be felicitous in both contexts, since after the ﬁrst answer there is a

997

salient and still disputable sub-question. The fact that the alternative question is salient at least to

998

the speaker is obvious: After answering the ﬁrst question he immediately goes on to address this

999

other sub-question. So we expect AB to be acceptable in both types of dialogues.
Received View

Decompositional View

Intonation FOC-CT

CT-FOC

FOC-CT

AA

?Felicitous

?Felicitous

?Felicitous ?Felicitous

BA

Infelicitous

Felicitous

Felicitous

Felicitous

AB

Felicitous

Infelicitous

Felicitous

Felicitous

CT-FOC

Table 2: Predictions: FOC-CT vs. CT-FOC Context

1000

Results: The results show a clear main effect of order of CT and Foc: the order CT ≺ Foc

1001

is rated as better on average that the order Foc ≺ CT (t=-4.0,p<0.001). Within each word order,

1002

however, there are no signiﬁcant effects of intonation. All three intonations are rated as acceptable,

1003

with AA being slightly more acceptable on average. However, this effect did not reach signiﬁcance

1004

(t<-1.4,p<0.81).

Figure 2: Ratings for each Intonation for FOC≺CT and CT≺FOC Dialogues.

1005

BA was numerically rated higher than AB in the context FOC-CT, and lower in CT-FOC,
44

1006

though this interaction did not reach signiﬁcance ( t<1.4, p<0.23). Note that this trend goes in the

1007

opposite direction from the one expected by the received view.

1008

Discussion: The results differ straightforwardly from the results reported from the correc-

1009

tive and sub-question dialogues, but again they do not follow the predictions of the received view.

1010

The ratings for the two intonations of interest did not depend on whether, based on the contextual

1011

manipulation, the contrastive topic preceded or followed the focus. In other words, both with a BA

1012

and an AB-contour, it doesn’t actually matter whether it is the ﬁrst or the second constituent that is

1013

the contrastive topic.

1014

The differences in the rating of AB in this set of dialogues compared to the previous two

1015

sets of dialogues shows that AB indeed has a pragmatic import that was compatible only with

1016

the third set of dialogues. And crucially, the fact that AB is compatible independent of whether

1017

B is realized on the contrastive topic of the sentence focus provides evidence that AB involves a

1018

sentence-level tune such as the RFR.

1019

The main effect of word order may seem surprising, but the explanation for this effect

1020

could be that there is an alternative pronunciation of the answer that might be more expected in the

1021

present context: a rendition in which the second constituent is deaccented:

1022

(83)

1023

A: What did you buy on 59th street?
B: I bought the SHOES on 59th street. And bought a jacket on 58 TH street.

1024

The fact that the AB-contour was rated as compatible with a CT ≺ FOC sequence in the present

1025

dialogue-type supports the hypothesis that AB really involves a sentence-level RFR which can

1026

be added to an utterance independent of whether the ﬁrst or second focused constituent acts as

1027

the contrastive topic or the sentence focus. This aspect of the results is perhaps the clearest sign

1028

that there is something wrong with the received view. The pragmatic import of the AB-contour is

1029

actually compatible with either constituent being the contrastive topic.

1030

4.4. The Case of English AB-BA: Conclusions

1031

The evidence suggests that the BA-contour does not, in fact, necessarily come with the

1032

pragmatic import attributed to it in B¨ ring (1997, 2003). Furthermore, while the AB contour
u
45

1033

may trigger an implicature similar to (7), it can be either the ﬁrst or the second constituent that

1034

constitutes the contrastive topic.

1035

A number of caveats are in order: The experiment reported here is a ﬁrst empirical test of

1036

the two views, but there are various other tests that could and should be run. Also, the experiment

1037

could probably be improved and varied in various ways in order to further back up the conclusions

1038

drawn here.

1039

Most importantly, it is very possible that our renditions of the BA- and AB-contour were not

1040

successful in conveying the meaning attributed to these contours by earlier theories because they

1041

were subtly different from the intonation that Jackendoff and B¨ ring had in mind; only further
u

1042

experimentation with varied sets of data can rule out this possibility. Our methodology could also

1043

be improved on in various ways, for example by testing the felicity of various intonational tunes

1044

based on the production of different speakers. Production experiments could help to establish

1045

under what circumstances speakers really use the various intonational tunes.

1046

With these caveats in mind, we can conclude that the evidence presented here provides

1047

some initial support for the decompositional view, and an argument against the received view.

1048

Even if the ultimate generalization will be more complex than what is outlined here, one effect of

1049

these results will hopefully be to show that it is worthwhile to collect quantitative data on these

1050

extremely subtle effects of intonational meaning that may not be easily accessible to native speaker

1051

intuition.
Constituents in English are sometimes preposed or left-dislocated, where the latter term is

1052

1053

usually reserved for cases in which the preposed element is resumed by a pronoun:

1054

(84)

1055

1056

1057

a. Preposed Direct Object:
/B LACK

BEANS∨,

I LIKE

b. Left-Dislocated Direct Object:
B LACK B EANS, I LIKE them.

1058

These types of constructions are related to the cases of contrastive topics discussed so far. Prepos-

1059

ing a direct object often involves the ‘BA’ intonation, and seems to introduce the pragmatic import

46

1060

that we found absent in the case of ‘BA’ when the constituents occur in the base order. This is

1061

conﬁrmed when looking at contexts where the only salient alternative is ruled out:

1062

(85)

Did Mary really hit John last night?
# No! /M ARY∨, J OHN hit.

1063

1064

Clearly, whatever the pragmatic import at play in (85), it is due to left-dislocation rather than the

1065

‘BA’ contour. Prince (1981), Ward (1988), Prince (1997) and Birner and Ward (1998) discuss the

1066

meaning of preposing and left-dislocation in detail, and identify various kinds of uses. To discuss

1067

these constructions in detail would go beyond the scope of this paper, but they ﬁt straightforwardly

1068

into the current analysis if we assume that there is a focus-sensitive operator involved and its focus

1069

moves to its speciﬁer. This focus operator outscopes the lower sentence focus.

1070

There are more constructions that resemble contrastive topics in that they involve double

1071

foci, but they come with additional presuppositions or implicatures. Consider the previously men-

1072

tioned ‘as for’-construction in English: This may be the prototypical construction in the literature

1073

when it comes to sentence topics in English:

1074

(86)

1075

The ‘as for’-construction is similar to topicalization in Italian when accompanied with clitic dou-

1076

bling in that the argument is resumed by a pronoun in the main clause, and consequently the

1077

construction is incompatible with negative quantiﬁers and other kinds of constituents that cannot

1078

provide the antecedent for the referential pronoun, as discussed before. These example lend further

1079

credence to the idea that the pragmatic import observed in some topic constructions is not really

1080

due to the occurrence of the CTFC itself, but to additional operators that they are accompanied by.

1081

5. Dissociating the Pragmatic Import: The German Hat-contour

As for the /SHOES∨, I bought them on ﬁftyNINTH street.

1082

German features a very salient intonational contour, the ‘hat’-contour, which consists of a

1083

rise and a fall, which are separated by a high plateau, thus leading to a pitch-contour that resem-

1084

bles a hat. Instances of the HAT-contours often involve a rise on a topicalized constituent, which

1085

prompted Jacobs (1982) to coin the term I-Topicalization. According to Ludwig (2006, 76), the
47

1086

meaning of a sentence that shows a hat-contour involves an operator taking two foci as its argument

1087

and implicates that: “there is at least one true proposition (sentence) that is the result of replacing

1088

both foci with respective alternatives.” This analyses departs from B¨ ring (1997) in that it treats
u

1089

both constituents with a pitch accent as being associates of the same focus-sensitive operator.

1090

Suppose now that we try to dissociate the existence condition that forms part of the meaning

1091

proposed by Ludwig (2006) for the CTFC from the presence of the two associated foci, and make

1092

it part of the meaning of a sentential intonational tune, analogous to the case of the RFR in English:

1093

(87)

1094

Jacobs (1997, 2001) notes that there are actually subtle phonetic differences between hat-contours

1095

that superﬁcially seem similar. He identiﬁes hat-contours that begin with a characteristic ‘root-

1096

contour’ as the ‘true’ intonational contour involved in contrastive topics. Let’s look at two exam-

1097

ples:

1098

(88)

Hat-Contour: HAT = λp : ∃p′ ∈ C, ∧ p → ∧ p′ and p′ . p

A: Wer hat denn wie auf das Buch reagiert?
Who has
how to the book reacted

1099

Who reacted in what way to the book?

1100

¨
a. B: /L OFFLER hat es em/PFOHlen, /K Arasek hat es ver\ RISsen.

1101

1102

¨
b. B: /LOFFler hat es em\ PFOHLen, /K Arasek hat es ver\ RISsen.
L¨ fﬂer has it recommended, Karasek has it torn.apart
o
‘L¨ fﬂer recommended it, Karasek tore it apart.’
o

1103

The response in (88a) is pronounced as a list of two answers. The intonation differs from a se-

1104

quence of two hat-contours in (88b) in that there is a rise, and not a fall, on the second constituent

1105

in the ﬁrst clause. According to Jacobs, the two utterances differ in that (88b) places pragmatic

1106

restrictions on the discourse that are missing in (88a), namely that the two answer stand in a cer-

1107

tain contrastive relationship to each other. In the analysis here, this pragmatic import is due to the

1108

contribution of the hat contour speciﬁed in (87).

1109

This example clearly illustrates that the hat-contour, unlike the RFR-contour in English,

1110

is compatible with the end of a pair-list answer (cf. Krifka 1998), which is unexpected based on
48

1111

the pragmatics attributed to the contour in B¨ ring (1997). This can be explained based on the
u

1112

difference in meaning between the two contours postulated here: The RFR-contour implies that

1113

there is an alternative proposition that is possibly true, and triggers the Gricean implicature that

1114

it is either not clear whether it is actually true or at least the speaker wants to avoid to assert the

1115

truth. This impicature is odd at the end of a pair-list question, when all alternatives are actually

1116

resolved as either true or false—and indeed, as we observed before, the RFR is not felicitous on

1117

the last answer of a pair-list question unless the relevant alternative is some other statement that is

1118

not directly related to the current question under discussion but maybe to a super-question.

1119

The hat-contour has no such Gricean implicature since it implies the truth (rather than the

1120

possibility) of an alternative proposition, and hence it is felicitous at the end of a pair-list question

1121

(any previous answer of the list ﬁts the bill).

1122

As would be expected, the hat-contour, just like RFR, is impossible when all alternatives

1123

must be false, as in (89). The response is only felicitous if someone else insulted somone else but,

1124

in the present context such an antecedent is not readily available:

1125

(89)

Either Hans insulted Pia or Pia insulted Hans. Did Hans insult Pia?

1126

# Nein. /P IA hat H ANS\ beleidigt.

1127

‘No, Pia insulted Hans.’

1128

The hat-contour is also acceptable when there is a true alternative, which is already clearly known

1129

to be true at this point in the discourse (90):

1130

(90)

I know that on two occasions someone insulted someone else. Did Hans insult Pia?

1131

Ja genau. Und /M ARIA hat D IETER\ beleidigt.

1132

‘Yes, exactly. And Maria insulted Dieter.’

1133

This is quite different from the RFR, which is infelicitous in this context, unless some unrelated

1134

alternative is accommodated that is insinuated here:

1135

(91)

1136

I know that on two occasions someone insulted someone else. Did Hans insult Pia?
# Yes, you’re right. And M ARIA insulted D IETER∨.
49

1137

Another difference to the RFR is that the HAT-contour can be embedded. For example, the follow-

1138

ing example is compatible with a reading in which it is the subject of the matrix clause, Mary, who

1139

conveys that some politicians are corrupt, without any implicature being made by the speaker:

1140

(92)

Maria widersprach mir weil
sie glaubt dass /ALLE Politiker NICHT\ korrupt sind.
Maria contradicted me because she believes that all
politicians not
corrupt are.
‘Mary objected to me because she doesn’t think that all politicians are corrupt.’

1141

1142

Using the RFR on the same kind of example necessarily requires some insinuation by the speaker:

1143

(93)

1144

That the HAT-contour can be embedded in this way contradicts Jacobs (1997)’s claim that the HAT-

1145

a
contour necessarily operates over speech acts, as was also pointed out in Moln´ r and Rosengren

1146

(1997).

Mary objected to me because she thought ALL politicians aren’t corrupt∨...

1147

A problem for the analysis presented here appears to be that, according to Ludwig (2006,

1148

50), the hat-contour seems to require two constituents with true alternatives, in contrast to the RFR,

1149

where we saw that only one focus is needed. If true, this might mean that the entry for the HAT-

1150

contour has to look more similar to our entry for TOPIC in (64). One example given buy Ludwig

1151

to make this point is:

1152

(94)

1153

1154

# /F RITZ hat das R ADIO\ eingeschaltet, und /F RITZ hat den F ERNSEHER\
Fritz has the radio
on.switched and Fritz has the TV
eingeschaltet.
on.switched
‘Fritz turned on the Radio and Fritz turned on the TV.’

1155

However, the problem with (94) (and related examples in Jacobs (1988)) is simply that Fritz is not

1156

marked as given, as it could and therefore should be, and there is no contrast to Fritz that would

1157

provide a motivation for not marking it as given. Once the prominence relations are changed such

1158

that they reﬂect the information structure appropriately, the hat-contour is possible even without a

1159

double contrast. In the following modiﬁed example, Fritz is marked as given and deaccented, and

1160

the accent falls on the connector und ‘and.’ The alternative proposition evoked here could be that

1161

Fritz is responsible:
50

1162

(95)

A: Why was there a short circuit? Who do you think was responsible?
B: /F. hat das R ADIO\ eingeschaltet, /UND F. hat den F ER .\ eingeschaltet.
F. has the radio
on.switched and F. has the TV
on.switched

1163

1164

In fact, the hat-contour can be ‘clipped’ and imposed on an utterance that only contains a single

1165

accent:

1166

(96)

1167

In addition, the hat-contour can be draped over fragments that involve two accented words, and yet

1168

they don’t each evoke alternatives:

1169

(97)

A: Was anyone at the party? B: S CHON\ . ‘Yes.... (...but the wrong people)’

A: What do you think of the candidate?
B: Eine /TOLLE R EDE.\ ‘A great speech... (...but he’s still a scoundrel)’

1170

1171

The claim that the hat-contour does not operate directly on two foci also ﬁts in with an observation

1172

from Ludwig (2006, 58), which casts doubt on claims that the hat-contour directly operates on

1173

topic-semantic value and juxtaposes pairs of alternatives:17

1174

(98)

1175

¨
¨
Der /P R ASIDENT wird geW AHLT\ , aber ein /M ITTAGESSEN wird es heute NICHT\
the president
will.be elected,
but a lunch
will there today not
geben.
be
‘The president will be elected, but there will be no lunch today.’

1176

1177

F´ ry (1993) reports that there is are different types of HAT-contours that need to be distinguished.
e

1178

So it is possible that the previous examples all involve a HAT-contour that in fact is different

1179

from the one involved in contrastive topics. One contour mentioned in F´ ry (1993) consists of
e

1180

an H* pitch accent (as opposed to a L*H accent), and places both rise and fall within the same

1181

intermediate phrase. It is that latter contour that is claimed to be involved in hat-patterns over

1182

a single DP and certain hat-contours in the middle ﬁeld. It seems, however, that the distinction
17

A further problem for an analysis that assumes that exactly two constituents evoke alternatives is that more than

two foci can be involved (Van Hoof 2003, 519).

51

1183

between these HAT-patterns is confounded by the closer syntactic and phonological proximity of

1184

the constituents carrying the pitch accents. I am a little skeptical whether this distinction can

1185

really be reliably made, either phonologically or pragmatically, but an experimental study would

1186

be necessary to get at this subtle distinction.

1187

The analysis here also correctly captures that the German hat-contour (but not the English

1188

RFR-contour) can occur at the end of a pair-list question—even after a complete answer—since it

1189

does not imply that a proposition is merely possibly true, rather it implies that some other propo-

1190

sition is true. The antecedent for this anaphoric relation can be one of the previous answers in

1191

the pair-list. In English these can’t act as antecedents for the RFR-contour since by the end of the

1192

pair-list, all answers are either clear to be true or false, but none are merely possibly true.18

1193

5.1. Parallels to Japanese Contrastive wa

1194

The analysis of the English RFR proposed in this paper is similar to the analysis of Japanese

1195

wa-marking proposed in Oshima (2005) and Hara (2006). The contrastive use of -wa (see Heycock

1196

2008, for an overview) also seems to evoke something like speaker-uncertainty, at least according

1197

to Hara (2006) and Oshima (2005). Additionally, Tomioka (2009) argues that contrastive wa also

1198

operates on alternative speech acts, just like we observed to be the case for the RFR in English.

1199

However, there is reason to doubt that the parallels to the English RFR are as close as it

1200

has been suggested. Hara (2006, 60) provides an example in which -wa is be embedded under

1201

‘believe’, a predicate that clearly embeds propositions and not speech acts:
18

The analysis may also cover the use of the hat-contour in alternative questions. When draped over a disjunction,

the HAT-contour makes the same implication: one alternative of its complement must be true:
(1)

a.

/Bier oder Wein\ ? (one of the two is presupposed to be true)
Bear or Wine?

b.

# /Milch or Zucker\ ? (not taking either should be an option)
‘Milk or sugar’?

For an analysis of alternative questions compatible with this view see Zimmermann (2000).

52

1202

1203

1204

1205

1206

(99)

[ M ARY-wa kitato]
John-ga shinjite-iru.
Mari-CON came-COMP John-NOM believe-PROG
a. Asserted: The speaker knows that John believes Mary came.
Implicated: John doesnt know whether someone else came.
b. Asserted: The speaker knows that John believes Mary came.
Implicated: The speaker doesnt know whether John believes that someone else came.

1207

This suggests that wa in fact does not necessarily operate on alternative speech acts, in contrast

1208

to Tomioka (2009). Based on the observation Hara (2006, 66ff) that contrastive wa cannot be

1209

embedded in relative clauses and adjunct clauses, Hara concludes that contrastive wa can only

1210

be embedded under attitude predicates, otherwise the -wa marked constituent needs to move at

1211

LF to the matrix clause. Adjunct and relative clauses that do not themselves express an attitude

1212

cannot license contrastive wa, and movement from them constitute an island violation, and hence

1213

contrastive -wa is not permitted. Interestingly, German HAT, in contrast to English RFR, can also

1214

be embedded, and shows further parallels to wa-marking in that the HAT-contour sounds strange

1215

in relative clauses (cf. Jacobs 1997, Moln´ r and Rosengren 1997).
a

1216

Contrastive wa differs on another crucial dimension from the English RFR-contour. Con-

1217

trastive wa can be used at the end of a list-answer, at least if one of the answer involves a negative

1218

answer, as observed in Hara (2006, 36). The RFR cannot be used at the end of a list-answer, unless

1219

of course, as illustrated above, the entire list is used to insinuate the possibility of a related propo-

1220

sition. No such requirement of an additional implicature is present in the Japanese example (this

1221

point was also made in Hara and van Rooij (2007, 36)):

1222

1223

1224

1225

(100) JOHN-to MARY-to Bill-no nakade, dare-ga party-ni ki-ta-no?
John-and Mary-and Bill-Gen among, who-Nom party-Dat come-Past-Q?
‘Among John, Mary and Bill, who came to the party?
*John-to Mary-wa ki-te,
Bill-wa ko-nakat-ta.
John-and Mary-wa come-Past-and, Bill-Con come-Neg-Past
John and Mary came, and Bill didn’t come.

53

1226

This suggests that the implicature of contrastive wa-marking is in fact quite different from that

1227

contributed by the RFR and more similar to the HAT-contour . However, wa-marking seems to

1228

have an adversative implication that the HAT-contour lacks, and is incompatible with a list-answer

1229

in which all answers are afﬁrmative (Hara 2006, 38):

1230

(101) JOHN-to MARY-to Bill-no nakade, dare-ga party-ni ki-ta-no?
John-and Mary-and Bill-Gen among, who-Nom party-Dat come-Past-Q?
‘Among John, Mary and Bill, who came to the party?

1231

*John-to Mary-wa ki-te,
Bill-wa ki-ta.
John-and Mary-wa come-Past-and, Bill-Con come-Past

1232

John and Mary came, and Bill came.

1233

1234

Based on these examples, it seems to me that the contribution of wa-marking seems similar to

1235

the contribution of the German discourse particles zumindest ‘at least’ or jedenfalls ‘in any case,’

1236

which can accompany the use of the HAT-contour.19

1237

The -wa-contour is sometimes compatible with a context in which the evoked alternative

1238

is already known to be true, and there are no alternatives that are still unresolved, even when no

1239

negation is involved, which further adds to the puzzle. The following type of example, new to my

1240

knowledge, further sheds doubt on the idea that the pragmatic import of wa-marking is parallel

1241

to that of the RFR, and again makes it seem more similar to the HAT-contour (thanks to Junko

1242

Shimoyama and Mina Sugahara in constructing this example):

1243

1244

(102) A: John-ga
dareka-o
bujoku-si, Henry-mo dareka-o
John-NOM someone-ACC insult-do Henry-also someone-ACC
bujoku-si-ta-no?
John-wa Mary-o
bujoku-si-ta-no-o.
insult-do-PAST-NOM-ACC know-PRES John-TOP Mary-ACC insult-do-PAST-Q
‘A: I know that John insulted someone and Henry also did. Did John insult Mary?’

1245

B: Hai, sosite Henry-wa Sally-o
bujoku-si-ta.
yes, and Henry-TOP Sally-ACC insult-do-PAST

1246

19

A promising approach to the meaning of wa seems to me to be the one pursued in SAWADA (2010), who analyzses

wa as an odd cousin of even, which carries the conventional implicature all alternatives to the wa marked constituent
lead to less likely propositions once they are substituted.

54

‘B: Yes, and Henry insulted Sally’

1247

1248

The hat-contour is also compatible with such contexts (90), but this is not true for the RFR contour

1249

(91). Lambrecht (1994) reports a use of wa marking which looks like a simple pair-list:

1250

(103) Roommates Hanako and Mary discussing householf chores:
H: Mary-san, anata-wa osoji
shite kudasai, watashi-wa oryori shimasu kara
Mary-VOC you-TOP cleaning do please I-TOP
cooking do
CONJ

1251

‘Mary, you do the cleaning, I’ll do the cooking.’

1252

1253

A ﬁnal parallel between HAT and wa-marking is that they can both be used even when the alter-

1254

natives evoked in the contrastive topic are actually not really contrastive. Consider the following

1255

example, in which ‘rain’ and ‘umbrella’ are not really contrasted to each other:

1256

(104) a.

1257

b.

1258

Ame
wa
hutte imasu ga
kasa wa motte ikimasen
rain-WA falling is
but umbrella WA take go-NEG
(Heycock 2008)
R EGNEN tut es, aber einen S CHIRM nehme ich NICHT mit.
to rain does it, but an umbrella take I not
with
‘It is raining, but Im not taking my umbrella with me.’

1259

1260

The RFR-contour is similar to both in that it does not obligatorily associate with focus and it can

1261

evoke an alternative that is not actually structurally similar in any way to the sentence is used with,

1262

as discussed before.

1263

There is at least one dimension in which wa is different from both HAT- and RFR-contour,

1264

which both seem to only be compatible with assertive speech acts. While wa-marking is compatbile

1265

with usage in a question (Heycock 2008), the hat-contour was observed to be incompatible with

1266

questions in Jacobs (1997, 94):

1267

(105)

1268

* Welchen von /ALLEN Grass Romanen w¨ rdest Du mir denn NICHT\ empfehlen?
u
which of every Grass novels would you to.me
not
recommend?
Which of all Grass novels wouldn’t you recommend to me?’

55

1269

Intonational morphemes like the HAT or RFR might be incompatible with usage in questions for

1270

the simple reason that questions come with their own distinct intonational tune, while segmental

1271

morphological markers like wa create no such conﬂict. Of course, there could also be semantic

1272

reasons for the (in-)compatibility. To summarize, it seems that contrastive wa is more similar to

1273

the German hat-contour than to the English RFR-contour. A closer look at contrastive wa will have

1274

to wait for a future occasion.

1275

5.2. RFR- and HAT-Contour, and Disambiguation

1276

Sometimes, the intonational contours associated with contrastive topics seem to disam-

1277

biguate an utterance. B¨ ring (1997) proposed this explanation for disambiguating effects of CTFCs
u

1278

based on the implicature in (7). This explanation for the disambiguating effect is entirely compati-

1279

ble with the analysis proposed here, except that disambiguation is tied to the RFR or HAT-contour

1280

and not to the occurrence of CTFCs.

1282

(106) /A LLE Politiker sind NICHT\ corrupt.
all
politicians are not
corrupt
∗
∀ > ¬; ¬ > ∀

1283

The distribution of accents makes it clear that alternatives to the universal operator alle and the

1284

negation nicht are evoked. If the universal quantiﬁer took scope over negation, then all alternatives,

1285

e.g., ‘some politicians are corrupt’ would be entailed as false. Only the narrow scope reading is

1286

compatible with the contribution of the hat-contour.

1281

1287

Bring’s analysis (see also Ludwig (2006)) of the disambiguating effect of the hat-contour

1288

also makes the correct prediction that using an indeﬁnite article reverses the situation and only

1289

surface scope is available:

1291

(107) /E INIGE Politiker sind NICHT\ corrupt.
some politicians are not
corrupt
∗
∃ > ¬; ¬ > ∃

1292

According to Krifka (1998), the hat-contour in German does not just resolve ambiguities, it also

1293

makes certain readings accessible that are inaccessible without the hat-contour. This is unexpected

1290

56

1294

under B¨ ring (1997)’s analysis, and also under the analysis presented here. A common assumption
u

1295

is that a sentence in surface word order only has surface scope. One of Krifka’s examples is the

1296

following:

1297

1298

(108) Jeder Student hat mindestens einen Roman gelesen.
every Student has at least
one novel read.
‘Every student read at least one novel.’

1299

The example in (108) is hard to judge, since the inverse scope reading entails the truth of the low

1300

scope reading, so the sentence is compatible with a scenario that motivates the inverse reading even

1301

when it is understood under the surface scope reading. An easier example to test whether inverse

1302

scope is possible is the following:

1303

(109) Mindestens ein Reviewer hat jeden Artikel gelesen.
at least
one student has every novel read

1304

For this sentence, the surface scope reading would be less plausible than the inverse scope reading,

1305

since it seems less likely that a single reviewer would read each and every article. The inverse

1306

reading seems more likely. In other words, it is not clear whether the common assumption that

1307

inverse scope in sentences like (109) is really impossible in German is correct.

1308

The issue of prosodic disambiguation goes beyond what can be discussed in any detail

1309

within the conﬁnes of this article. At least the facts in German do not appear to be incompatible

1310

with the predictions of the theory.

1311

As discussed in Oshima (2005) and Constant (2006), RFR sometimes seems to disam-

1312

biguate as well: wide scope of the universal in (110) would rule out the possibility of any of the

1313

invoked alternatives to be true and therefore defeat the contribution of RFR. The relevant alterna-

1314

tives are invoked here by placing focus on the quantiﬁer (i.e., ‘some of my friends’ would be one

1315

salient alternative):

1316

(110) /A LL of my friends didn’t come∨.

1317

This example, however, is problematic, since even without the RFR-contour the preferred reading

1318

is a low-scope reading for the universal operator. Corpus evidence showing that subjects with every

∗

57

∀ > ¬; ¬ > ∀

1319

outscoping sentential negation are rare is discussed in Hoeksema (1999), and more evidence for

1320

this preference can be found in Musolino (1998). According to Horn (1989, 499) every x ... not

1321

is blocked by the logically equivalent lexicalized no x, and the same seems to be at play in (110).

1322

Modifying the restrictor, however, can affect the peferred scope reading:

1323

(111) a. Sally hosted a party.
She was disappointed that all of her friends didn’t come.

1324

preferred: ¬ > ∀

b. Sally hosted a party. Her friend Jim was there.

1325

She was disappointed that all other friends of hers didn’t come..

1326

preferred: ∀ > ¬

1327

The sentence in (111b) has a preferred wide-scope reading. While the present context helps to bring

1328

out this reading, adding ‘other’ generally seems to make a wide-scope reading more accessible

1329

in this kind of example. This type of example is a better test for the disambiguating effect of

1330

intonational contours.

1331

As expected based on the line of reasoning in B¨ ring (1997)—which based on the analysis
u

1332

can be applied to the RFR contour, following Oshima (2005) and Constant (2006)—a pronunci-

1333

ation with focus on all and an RFR-contour swaps the preference of a sentence with all other...

1334

around, but not when focus is on other:

1335

(112) a. /A LL other friends of hers didn’t come.∨.

preferred: ¬ > ∀

b. /All OTHER friends of hers didn’t come.∨.

preferred: ∀ > ¬

1336

1337

The disambiguating effect is due to the interaction between the evoked alternatives and the meaning

1338

contribution of the intonational contour, be it the RFR-contour in English or the HAT-contour in

1339

German. It seems unnecessary, however, to evoke the notion of the topic-semantic value to explain

1340

this disambiguating effect. The present analysis can import B¨ ring’s insightful analysis of these
u

1341

effects without attributing them directly to the presence of a contrastive topic in the sense of B¨ ring
u

1342

(1997).

1343

5.3. Summary

1344

The pragmatic effects attributed to CTFCs in English and German can be dissociated from

1345

them, and we can decompose the notion of contrastive topics as described in B¨ ring (1997, 2003)
u
58

1346

into the part that relates to the ‘topic-semantic value’—which can be analyzed as a nesting of two

1347

focus operators—and the part relating to pragmatic import—which may be due to intonational

1348

morphemes like RFR and HAT, morphological markers like wa, or additional operators as those

1349

involved in left-dislocation or the ‘as for’-construction.

1350

6. Conclusion

1351

This paper presented evidence that the syntactic distribution of CTs relative to FOCs mir-

1352

rors the distribution of nested overt focus operators and is more restricted than earlier approaches

1353

would predict. An explanation was provided in the form of a compositional theory that analyzes

1354

CTFCs as recursively nested focus operators. The associate of the focus operator taking wider

1355

scope is what earlier analyses called a ‘contrastive topic’.

1356

The pragmatic import that CTFCs seem to have, and other nested foci seem to lack, were

1357

attributed to independently motivated operators that are realized as intonational tunes, such as RFR

1358

in English and HAT in German, or in other morpho-syntactic ways. Their precise presuppositional

1359

and pragmatic contribution varies from operator to operator, just as there are various operators such

1360

as even and only that associate with focus and introduce different presuppositions in sentences with

1361

a single focus operator.

1362

References

1363

Baayen, RH, DJ Davidson, and DM Bates. 2008. Mixed-effects modeling with crossed random

1364

effects for subjects and items. Journal of Memory and Language 59:390–412.

1365

Bader, Christopher. 2001. Givenness, focus, and prosody. Doctoral Dissertation, MIT.

1366

Balogh, Katalin. 2009. Theme with variations. a context-based analysis of focus. Doctoral Disser-

1367

1368

1369

1370

1371

tation, Universiteit van Amsterdam.
Beaver, David I., and Brady Z. Clark. 2003. ”Always” and ”Only”: Why not all focus sensitive
operators are alike. Natural Language Semantics 11:323–362.
Beaver, D.I., and B.Z. Clark. 2008. Sense and sensitivity: How focus determines meaning. WileyBlackwell.
59

1372

1373

1374

1375

Birner, B.J., and G.L. Ward. 1998. Information Status and Noncanonical Word Order in English.
John Benjamins.
Blaszczak, Joanna, and Hand-Martin G¨ rtner. 2005. Intonational phrasing and the scope of negaa
tion. Syntax 7:1–22.

1376

B¨ ring, D. 2003. On D-trees, beans, and B-accents. Linguistics and Philosophy 26:511–545.
u

1377

B¨ ring, Daniel. 1997. The meaning of topic and focus: The 59th street bridge accent. Routledge
u

1378

1379

1380

1381

1382

Studies in German Linguistics. London: Routledge.
B¨ ring, Daniel. 2008. Contrastive topics decomposed—comments on wagner’s paper. Comments
u
given at UCL Workshop on Information Structure.
B¨ ring, Daniel, and Katharina Hartmann. 2001. The syntax and semantics of focus-sensitive paru
ticles in German. Natural Language and Linguistic Theory 19:229–281.

1383

Calabrese, Andrea. 1984. Multiple questions and focus in Italian. In Sentential complementa-

1384

tion: Proceedings of the International Conference Held at UFSAL, ed. W. de Geest and

1385

Y. Putseys, 67–74.

1386

Cinque, Guigliemo. 1990. Types of a’-dependencies. Cambridge, Ma.: MIT Press.

1387

Constant, Noah. 2006. English rise-fall-rise: A study in the semantics and pragmatics of intonation.

1388

Ms. University of Santa Cruz.

1389

F´ ry, Caroline. 1993. The tonal structure of standard german. Niemeyer.
e

1390

F´ ry, Caroline. 2007. The prosody of topicalization. Ms. University of Potsdam.
e

1391

von Fintel, Kai. 1999. NPI licensing, Strawson entailment, and context dependency. Journal of

1392

Semantics 16:97–148.

1393

Gajewski, Jon R. 2005. Neg-raising: Polarity and presupposition. Doctoral Dissertation, MIT.

1394

Guerzoni, Elena. 2003. Why even ask? on the pragmatics of questions and the semantics of

1395

1396

1397

1398

1399

answers. Doctoral Dissertation, MIT.
Hara, Yuri. 2006. Grammar of knowledge representation: Japanese discourse items at interfaces.
Doctoral Dissertation, University of Delaware.
Hara, Yurie, and Robert van Rooij. 2007. Contrastive topics revisited: a simpler set of topicalternatives. Talk presented at NELS, Universit´ d’Ottawa.
e

60

1400

Heim, Irene. 2010. Contrastive topic and focus construction. Class lecture, MIT.

1401

Heycock, Caroline. 2008. Japanese wa, ga, and information structure. In Shigeru miyagawa and

1402

mamoru saito. Oxford University Press.

1403

Hoeksema, J. 1999. Blocking effects and polarity sensitivity. In Jfak: Essays dedicated to johan

1404

van benthem on the occasion of his 50th birthday, ed. Jelle Gerbrandy, Maarten Marx,

1405

Maarten de Rijke, and Yde Venema.

1406

Horn, Laurence R. 1969. A presuppositional analysis of only and even. In Papers from the 5th

1407

annual meeting of the Chicago Linguistic Society, ed. Robert I. Binnick, Alice Davison,

1408

Georgia Green, and Jerry Morgan, volume 4. Chicago: Chicago Linguistic Society.

1409

Horn, Laurence R. 1989. A natural history of negation. Unversity of Chicago Press.

1410

Horvath, Julia. 1986. Focus in the theory of grammar and the syntax of hungarian. Foris.

1411

Jackendoff, Ray S. 1972. Semantic interpretation in generative grammar, chapter 6: Focus and

1412

1413

1414

1415

1416

1417

1418

Presupposition. Cambridge, Ma.: MIT Press.
Jacobs, Joachim. 1982. Syntax und Semantik der Negation im Deutschen. M¨ nchen: Wilhelm Fink
u
Verlag.
Jacobs, Joachim. 1983.

Fokus und Skalen. Zur Syntax und Semantik der Gradpartikel im

Deutschen. T¨ bingen: Niemeyer.
u
Jacobs, Joachim. 1988. Fokus-Hintergrund-Gliederung und Grammatik. In Intonationsforschungen, ed. Hans Altmann, 89–134. T¨ bingen: Niemeyer.
u

1419

Jacobs, Joachim. 1997. I-Topikalisierung. Linguistische Berichte 168:91–133.

1420

Jacobs, Joachim. 2001. The dimension of topic-comment. Linguistics 39:641–681.

1421

Jaeger, Florian, and Michael Wagner. 2003. Association with focus and linear order in German.

1422

1423

1424

Ms., Stanford University, November 2003.
Karttunen, Lauri, and Stanley Peters. 1979. Conventional implicature. In Syntax and semantics
11: Presupposition, ed. D.A. Dinneen and C.-K. Oh, 1–56. New York: Academic Press.

1425

Kayne, Richard S. 1998. Overt vs. covert movement. Syntax 1:128–191.

1426

´
Kiss, Katalin E. 1987. Conﬁgurationality in hungarian. Dordrecht: Reidel.

1427

´
Kiss, Katalin E. 2002. The syntax of hungarian. Cambridge University Press.

61

1428

Kratzer, Angelika. 1991. The representation of focus. 825–834. Berlin: de Gruyter.

1429

Krifka, Manfred. 1992. A compositional semantics for multiple focus constructions. In Informa-

1430

tionsstruktur und grammatik, ed. Joachim Jacobs, volume Sonderheft 4 of Linguistische

1431

Berichte, 17–53. Opladen: Westdeutscher Verlag.

1432

1433

1434

1435

1436

1437

Krifka, Manfred. 1998. The origins of telicity. In Events and grammar, ed. Susan Rothstein,
197–235. Dordrecht: Kluwer.
Krifka, Manfred. 1999. Additive particles under stress. In Proceedings of SALT, volume 8, 111–
128.
Lambrecht, Knud. 1994. Information structure and sentence form. topic, focus and the mental
rrepresentations of discourse referents. Cambridge University Press.

1438

Lappin, Shalom, ed. 1996. The handbook of contemporary semantic theory. London: Blackwell.

1439

Liberman, Mark, and Janet Pierrehumbert. 1984. Intonational variance under changes in pitch

1440

range and length. In Language sound structure, ed. M. Aronoff and R. Oehrle, chapter 10,

1441

157–233. Cambridge, Ma.: MIT Press.

1442

1443

1444

1445

Ludwig, Rainer A. 2006. Information structure and scope inversion. Diplomarbeit. Universit¨ t
a
Potsdam.
Moln´ r, Val´ ria, and Inger Rosengren. 1997. Zu jacobs’ explikation der i-topikalisierung. Linguisa
e
tische Berichte 169:211–247.

1446

Musolino, J. 1998. Universal grammar and the acquisition of semantic knowledge: an experimental

1447

investigation into the acquisition of quantiﬁer-negation interaction in English. Doctoral

1448

Dissertation, University of Maryland.

1449

Neeleman, Ad, and Hans van de Koot. 2007. The nature of discourse templates. Ms. UCL.

1450

Oshima, David Y. 2005. Morphological vs. phonological contrastive topic marking. In Proceedings

1451

1452

1453

1454

1455

of CLS 41.
Prince, Ellen. 1981. Toward a taxonomy of given-new information. In Radical pragmatics, ed.
Peter Cole, 223–255. New York: Academic Press.
Prince, Ellen F. 1997. On the functions of left-dislocation in English discourse. Directions in
Functional Linguistics 117–143.

62

1456

1457

1458

1459

1460

1461

Rizzi, Luigi. 1997. The ﬁne structure of the left periphery. In Elements of grammar, ed. Liliane
Haegeman, 281–337. Kluwer.
Roberts, C. 1996. Information structure in discourse: Towards an integrated formal theory of
pragmatics. OSU Working Papers in Linguistics 49:91–136.
Rooth, Mats. 1985. Association with focus. Doctoral Dissertation, University of Massachussetts,
Amherst.

1462

Rooth, Mats. 1992. A theory of focus interpretation. Natural Language Semantics 1:75–116.

1463

Rooth, Mats. 1996. Focus. In Lappin (1996), chapter 10, 271–297.

1464

Rullmann, Hotze. 1997. Even, polarity, and scope. Papers in Experimental and Theoretical Lin-

1465

guistics 4:40–64.

1466

Sauerland, Uli. 2005. Contrastive topic: A reductionist approach. Ms. ZAS Berlin.

1467

SAWADA, OSAMU. 2010. The japanese contrastive wa: A mirror image of even. In Proceedings

1468

1469

1470

of the 33rd Annual Meeting of the Berkeley Linguistics Society.
Stoyanova, Marina. 2008. Unique focus. languages without multiple wh-questions. John Benjamins.

1471

Szabolcsi, Anna. 1981. The semantics of topic-focus articulation. In Formal methods in the study

1472

of language, ed. J.A.G. Groenendijk, T.M.V. Janssen, and M.B.J. Stokhof, volume 2, 513–

1473

541. Amsterdam: Mathematisch Centrum.

1474

1475

Taglicht, Josef. 1984. Message and emphasis. on focus and scope in English, volume 15 of English
Language Series. London and New York: Longman.

1476

Tomioka, Satoshi. 2009. Contrastive topics operate on speech acts. In Information strucutre:

1477

Theoretical, typological and experimental perspectives information strucutre: Theoretical,

1478

typological and experimental perspectives, ed. Malte Zimmermann and Caroline F´ ry. Oxe

1479

ford University Press.

1480

Tomioka, Satoshi. to appear. A scope theory of contrastive topics. Iberia 2.

1481

Van Hoof, Hanneke. 2003. The rise in the rise-fall contour: does it evoke a contrastive topic or a

1482

1483

contrastive focus? Linguistics 41:515–563.
Vasishth, S., S. Bruessow, R. L. Lewis, and H. Drenhaus. 2008. Processing polarity: How the

63

1484

ungrammatical intrudes on the grammatical. Cognitive Science 32.

1485

Wagner, Michael. 2005. Prosody and recursion. Doctoral Dissertation, MIT.

1486

Wagner, Michael. 2006. Association by movement. Evidence from NPI-licensing. Natural Lan-

1487

guage Semantics 14:297–324.

1488

Wagner, Michael. 2008. A compositional analysis of contrastive topics. In Proceedings of NELS

1489

38 2007 in Ottawa, ed. Muhammad Abdurrahman, Anisa Schardl, and Martin Walkow,

1490

volume 2, 415–428.

1491

Wagner, Michael. 2009. Focus, topic, and word order: A compositional view. In Alternatives to

1492

cartography, ed. Jeroen van Craenenbroeck, 53–86. Berlin/New York: Mouton De Gruyter.

1493

Ward, Gregory, and Julia Hirschberg. 1985. Implicating uncertainty: The pragmatics of fall-rise

1494

1495

1496

intonation. Language 61:747–776.
Ward, Gregory L. 1988. The semantics and pragmatics of preposing. Outstanding Dissertations in
Linguistics. Garland.

1497

Wilkinson, K. 1996. The scope of even. Natural Language Semantics 4:193–215.

1498

Williams, Edwin. 1997. Blocking and anaphora. Linguistic Inquiry 28:577–628.

1499

Wold, Dag E. 1996. Long-distance selective binding: The case of focus. In Proceedings of SALT

1500

6, ed. Teresa Galloway and Justin Spence, 311–328. Ithaca, NY: CLC Publications.

1501

Zimmermann, T.E. 2000. Free Choice Disjunction and Epistemic Possibility. Natural Language

1502

1503

1504

Semantics 8:255–290.
Zwarts, Frans. 1998. Three types of polarity. In Plural quantiﬁcation, ed. Fritz Hamm and Erhard
Hinrichs, 177–238. Dordrecht: Kluwer.

64

Prosodic Effects of Discourse Salience and Association with Focus
M. Wagner2, M. Breen1 , E. Flemming3 , Stefanie Shattuck-Hufnagel3, & E. Gibson3
1

Department of Linguistics, McGill University
Department of Psychology, UMass Amherst
3
Massachusetts Institute of Technology

2

chael@mcgill.ca, mbreen@psych.umass.edu

Abstract
Three factors that have been argued to inﬂuence the prosody of
an utterance are (i) which constituents encode discourse-salient
information; (ii) which constituents are contrastive and evoke
alternatives; and (iii) which constituents interact with the meaning of focus operators such as only (i.e., they ‘associate’ with
focus). One challenge for a better understanding of the prosodic
effects of these factors has been the difﬁculty of ﬁnding a way
to evaluate hypotheses quantitatively, since individual variation
in productions is often large enough to wash out experimental
effects. In this paper, we apply a methodology introduced in [1]
which regresses out subject and item variation, uncovering otherwise hidden prosodic patterns, and show how the three factors
interact in sentences containing single or multiple foci.
Index Terms: prosody, focus association, givenness, prominence, production

1. Prosodic Effects of Focus and
Discourse-Salience
One of the goals of research on prosody is to determine the
conditions which underlie prosodic prominence. This is particularly important is cases of association with focus [2], where
the truth-conditions of a sentence change depending on the
prosodic realization of material in the scope of certain focussensitive operators such as only:
(1)

a.
b.

Gramma only gave a bunny to Maryanne.
Gramma only gave a bunny to Maryanne.

Pronunciation (1a), with prominence on bunny, seems compatible with a scenario in which Gramma also gave a bunny to
John; (1b) does not. (1b) seems compatible with a scenario in
which Gramma also gave a scarf to Maryanne; (1a) does not. In
the ﬁrst example bunny is focused and in the second Maryanne.
The current study explores how prominence patterns reﬂect the
intended meaning of sentences in which one or two focused elements associate with only. We use a normalization procedure
from [1] which uncovers otherwise hidden prosodic effects.
1.1. Evoking Alternatives
One way to explain the difference between (1a) and (1b) is to
analyze only as operating over alternatives to the the sentence in
which it occurs. The alternatives for (1a) involve substitutions
for bunny, while those for (1b) vary Maryanne. In the following, smallcaps denote constituents which evoke alternatives:
(2)

a.

‘Gramma only gave A BUNNY to Maryanne.’
Alternatives: { Gramma gave a scarf to Maryanne;

Gramma gave a cake to Maryanne; ...}
b.

‘Gramma only gave a bunny to M ARYANNE .’
Alternatives: {Gramma gave a bunny to John;
Gramma gave a bunny to Bill; ...}

Sentences involving only are often analyzed as presupposing
the content of the sentence without only (e.g., Gramma gave
a bunny to Maryanne in the case of (1a)), and asserting that no
alternative to it is true [3]. So both (1a) and (1b) share the presupposition that Gramma gave a bunny to Maryanne, but they
differ with respect to which alternative statements they exclude.
The prosody of a sentence is sensitive to which alternatives are
contextually relevant: Constituents which evoke alternatives are
usually more prominent than those that do not.
1.2. Anaphoric Destressing
Another factor affecting prosody is that discourse-salient material is usually less prominent than discourse-new material (cf.
[4] for an overview), a phenomenon often called ‘anaphoric destressing.’ Underlining marks discourse-salient material:
(3)

a.
b.

Why did Maryanne feel special?
Grandma gave a bunny to Maryanne.

In a typical rendition of (3b), the noun phrase Maryanne is less
prominent than the preceding a bunny because its referent is
discourse-salient. However, in the following context the indirect object would typically be more prominent than the preceding object because it evokes alternatives:
(4)

a.
b.

Why did Maryanne feel special?
Grandma only gave a bunny to M ARYANNE.

The prosodic realization of a constituent is thus affected by two
factors: (i) Which constituents evoke alternatives? (ii) Which
constituents are salient in the discourse? The current study
varies the information status (IS) of constituents along both dimensions. In conditions 1-5, an introductory story made all discourse referents (e.g.John, Maryanne, bunny, and scarf) salient,
and the set-up in each condition further manipulated the IS of
the target sentence. Conditions 1 and 2 vary the IS of the indirect object Maryanne such that it is discourse-salient in both
cases, but evokes alternatives in only in condition 2:
(5)

Story: It was Christmas, and Gramma was deciding what gifts to give to her grandchildren, John and
Maryanne. She had knitted two scarves as gifts, and had
also purchased a couple of stuffed bunnies. She wrapped
up a scarf and a bunny for John. Then she remembered
how rude Maryanne had been at Thanksgiving.

Table 2: Focus Association by Condition

Table 1: Information Status by Condition
Condition
1
2
3
4
5
6

a.

b.

Direct Object
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
given
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
EVOKES ALT.

Indirect Object
given
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
EVOKES ALT.

Condition 1: Set-up: Gramma didn’t give a scarf to
Maryanne. Target: Gramma only gave a BUNNY to
Maryanne.
Condition 2: Set-up: Gramma gave a scarf and
a bunny to John. Target: Gramma only gave a
BUNNY to M ARYANNE.

In Condition 1, Maryanne is discourse-salient and does not
evoke alternatives. In Condition 2, while also given, Maryanne
evokes an alternative: John. In Conditions 3 and 4 the IS of
bunny was similarly manipulated:
(6)

Story: It was Christmas, and Gramma was deciding what gifts to give to her grandchildren, John and
Maryanne. She had knitted two scarves as gifts, and had
also purchased a couple of stuffed bunnies. She wrapped
up a scarf and a bunny for Maryanne. Then she remembered how rude John had been at Thanksgiving.
a. Condition 3: Set-up: Gramma didn’t give a bunny
to John. Target: Gramma only gave a bunny to
M ARYANNE.
b. Condition 4: Set-up: Gramma gave a scarf to both
Maryanne and John. Target: Gramma only gave a
BUNNY to M ARYANNE.

In Conditions 1-4, the discourse-salience of bunny and
Maryanne is held constant while the evoking of alternatives is
varied. In Conditions 5 and 6, on the other hand, the evoking
of alternatives is held constant, but discourse-salience is varied.
Earlier research has already shown that discourse-given material can remain accented and at the same time show quantitative
signs of reduction compared to discourse-new accented material [5]. In Condition 5, both bunny and Maryanne associate
with only and are discourse salient. In Condition 6, the story
does not speciﬁcally mention Maryanne or a bunny, so while
both objects evoke alternatives, they are both discourse-new:
(7)

(8)

Condition 5:
Set-up: Gramma didn’t give a scarf to Maryanne, and
she didn’t give either a bunny or a scarf to John.
Target: Gramma only gave a BUNNY to M ARYANNE.
Condition 6:
Set-up: Gramma picked one present and gave it to her
favorite grandchild.
Target: Gramma only gave a BUNNY to M ARYANNE .

Table 1 summarizes the IS of the 6 experimental conditions.
1.3. Multiple Foci
In its simplest form, the alternatives theory of association with
focus [3] predicts that any alternatives evoked in the scope of a
focus-sensitive operator such as only must be considered in the

Condition
2
4
5

Direct Object
only
contrast
only

Indirect Object
contrast
only
only

alternatives that only excludes—they should all ‘associate’ with
only. For example, the following sentence should indicate that
Gramma gave only one thing to one person:
(9)

Gramma only gave a BUNNY to M ARYANNE .

However, it turns out that multiple focus constructions do not
necessarily work like this [6]. In condition 2 (5b), e.g., despite
the fact that both direct and indirect object evoke alternatives,
only the direct object associates with only. Similarly, in condition 4 (6b), only the indirect object associates with only. That
there can be constituents in the scope of only which evoke alternatives but do not associate with it presents a problem for the
theory. [7] observed a related issue. In sentences with more
than one focus operator (i.e. (10)), a focus can associate either
with the higher focus operator (also) or the lower one only, but
the focus operators don’t necessarily associate with both:1
(10)

We only1 recovered the diary entries that Marilyn1
made about John.
We also2 only1 recovered the diary entries that
Marilyn1 made about Bobby2 .
‘Also with respect to diary entries about Bobby, we
only recovered the ones that Marilyn made.’

Rooth’s example suggests that the evoked alternative sets may
have to be indexed with respect to the focus operator that quantiﬁes over them. Similar to the paraphase of Rooth’s example in (10), our Conditions 2 and 4 can be paraphrased using
a topic construction. Condition 2 could be paraphrased: ‘As
for Maryanne, Gramma only gave a bunny to her.’; Condition 4
could be paraphrased: ‘As for bunnies, Gramma only gave one
to Maryanne.’ We can analyze the data by positing a contrastoperator which, like also in (10), associates with one focus, preventing only from associating with it. This is the analysis given
to contrastive topics more generally in [9]: contrastive topics are
focus-sensitive operators that outscope a lower focus operator.
(11)

Condition 2:
Set-up: Grammar gave a scarf and a bunny to John.
Target: Gramma contrast1 only2 gave a BUNNY2 to
M ARYANNE 1 .

(12)

Condition 4:
Set-up: Gramma gave a scarf to both Maryanne and
John.
Target: Gramma contrast1 only2 gave a BUNNY1 to
M ARYANNE 2 .

The novel question in our experiment is whether the prosody of
constituents that evoke alternatives is affected by which focus
operator they actually associate with. Table 2 summarizes the
focus association patterns of the three conditions in which the
two objects both evoke alternatives and are discourse salient.
1 This may be not be possible with all kinds of focus operators, however, see [8].

Figure 1: An example of the picture array that subjects chose
from to indicate their interpretation of the target sentence.

2. Method
2.1. Participants
10 pairs of English speakers from the MIT community participated in the experiment, each one received $15 for participating.
2.2. Materials
The stimuli consisted of three sections: Story, set-up, and target. The story provided a scenario for the action, and served, in
all but Condition 6, to introduce two people (e.g. Maryanne and
John) and two objects (e.g. a bunny and a scarf). The set-up determined the IS of the target sentence–the discourse salience of
the people and objects, and which alternatives only associated
with. The target always took the form: Actor only verbed a object to Name. The actor, object, and name were all 2-syllables
with ﬁrst-syllable stress. The verb consisted of one syllable.
Length and metrical stress were matched across items. The target sentence were comprised mainly of sonorants, to facilitate
automatic pitch extraction. There were 10 items in total in 6
conditions, making for a total of 60 stories.
2.3. Procedure
Two participants (a speaker and a listener) sat at two computers
in the same room such that neither could see the other’s screen.
On each trial, the speaker ﬁrst read the story, set-up, and target
silently. S/he then chose from an array of four pictures (Fig. 1)
to assure they understood the context and gave the sentence the
right meaning. The speaker then produced the set-up and target
aloud for the listener, who selected the picture s/he believed the
target described. Trials for which the listener chose the wrong
picture were excluded, as were productions with disﬂuencies.
Using Praat ([10]), 24 acoustic measures were automatically extracted from each of ﬁve words in the target sentence (Gramma,
only, gave bunny, Maryanne). A participant was a speaker for a
subset of half the trials, then roles were switched.

3. Results
In order to see whether the acoustic measures could discriminate the speakers’ productions, we entered all 120 predictors
into a series of step-wise discriminant analyses. In initial analyses, neither the full set of 6 conditions, nor pairs of conditions, were successfully discriminated. To remove variance due
to speakers and items, we followed [1] and computed linear
regression models in which speaker (n = 20), item (n = 20),
and the interaction between them, predicted the 120 acoustic measures. From each model, we calculated the predicted

Figure 2: Given vs. Given+Evoking Alternatives (condition
1 vs. 2): ‘Maryanne’ is given in condition 1 (light grey, lefthand bar), and also in condition 2, but there it also evokes
alternatives (darker grey, right-hand bar), and is longer, has
greater intensity and a higher pitch range. Interestingly, the
prominence of ‘bunny’ decreases (left vs. right bars) when that
of Maryanne increases although its information status remains
contant—suggesting that the relative prominence between the
two arguments is what matters in coding the information status of ‘Maryanne.’ In condition 3 vs. 4 (no ﬁgure), similarly,
‘bunny’ is longer and has higher intensity when evoking alternatives (cond. 4) compared to when not (cond. 3).
.

value of each acoustic feature per item per speaker. The difference between the predicted and actual values (i.e. the residual measure) reﬂects acoustic differences not due to differences
between speakers or items, or their interaction. We submitted these residual measures to a stepwise discriminant analysis, to independently determine which acoustic measures speakers used to differentiate productions. Eight of the original 120
acoustic measures (duration, mean pitch, pitch range, and maximum intensity from bunny and Maryanne, respectively) resulted in better-than-chance 6-way classiﬁcation of the productions by condition; moreover, many conditions were now discriminated in pair-wise comparisons.
3.1. Given vs. Given + Evokes Alternatives
Conditions 1 & 2 were well discriminated. Wilks’ lambda for
the comparison between 1& 2 was .81, F(8) = 5.08, p < .001.
A leave-one-out classiﬁcation successfully classiﬁed 68% of all
productions; 66% of condition 1; 70% of condition 2. Conditions 3 & 4 were also well discriminated (Wilks lambda = 0.83,
F(8)=4.52, p < .001, classiﬁcation accuracy 58% condition 3,
69% condition 4). Discourse-salient constituents that evoke alternatives are produced with longer duration, higher intensity,
and wider pitch range than when they do not evoke alternatives
(Fig. 2). The bar plots in all Figures are averages of the normalized measures over all items—we label them with words from
a particular stimulus set to make them easier to interpret.
3.2. Given + Evokes Alternatives vs. Evokes alternatives
Conditions 5 and 6 were discriminated, Wilks’ lambda = .86,
F(8) = 3.59, p < .001. Leave-one-out classiﬁcation successfully
classiﬁed 60% of all productions; 67% of condition 5; 52% of

Figure 3: Evoking Alternatives + Given vs. Evoking Alternatives and New (condition 5 vs. 6): Both ‘bunny’ and
‘Maryanne’ are shorter and have a lower intensity when they
are discourse salient (condition 5, darker bars) compared when
they evoke alternatives and are new (condition 6, lighter bars).

condition 6. Acoustic results show that, when the constituent
evokes alternatives and is discourse-new, it is produced with
a longer duration, higher intensity, and wider pitch range than
when it evokes alternatives but is discourse-salient (Fig. 3).
3.3. Association with only vs. association with contrast
Conditions 2 and 4 were discriminated, Wilks’ lambda = .90,
F(8) = 2.46, p < .05. Leave-one-out classiﬁcation successfully
classiﬁed 55% of all productions; 51% of condition 2; 60% of
condition 4 (Fig. 4). The acoustic results show that foci which
associate with contrast are on average produced with longer duration, higher pitch, higher intensity, and wider pitch range than
foci which associate with only.

4. Discussion and Conclusion
The results show that constituents which are discourse-salient
but also evoke alternatives are more prominent than constituents
that are discourse salient and do not (Condition 1 vs. 2, Condition 3 vs. 4). Furthermore, discourse-new constituents which
evoke alternatives are more prominent than discourse-salient
constituents which evoke alternatives (Conditions 5 vs 6), showing that anaphoric reduction can be observed even on accented
constituents that evoke alternatives. This is similar to the ﬁnding in [5] that accented words are reduced when discoursesalient. Our study did not look at the case in which a constituent
is new and does not evoke alternatives. This IS was compared
to constituents that are new and do evoke alternatives in [11],
who found the latter to be more prominent.
With respect to multiple foci, we found that foci which associate with contrast are more prominent than foci which associate with only. The results show for the ﬁrst time that such an
effect exists, but do not tease apart whether this is due to the
nature of the focus operators contrast and only themselves, or
whether it is a result of the scope of the focus operators (cf. [12]
for a related claim on second-occurrence focus)—in our stimuli,
contrast arguably always outscoped only. A further methodological result of this study is that regressing out subject and
item effects following [1] can uncover effects that are otherwise
washed out by variation.

Figure 4: Association with contrast vs. association with only
(condition 2 vs. 4). Condition 2 are the bars on the left, condition 4 the bards on the right. When ‘bunny’ and ‘Maryanne’
associate with only (darker bars) they have a lower duration,
intensity, pitch and pitch range compared to when associating
with contrast (lighter bars).

5. Acknowledgements
Thanks to Nakul Vyas for theoretical discussion and development of materials, to Denise Ichinco for assistance with data
analysis, to Wade Shen for automatic alignment of the productions, and to Lisa Selkirk and Jonah Katz for discussion.

6. References
[1] M. Breen, E. Fedorenko, M. Wagner, and E. Gibson, “Acoustic
correlates of information structure,” 2009, under Revision.
[2] R. S. Jackendoff, Semantic Interpretation in Generative Grammar. Cambridge, Ma.: MIT Press, 1972.
[3] M. Rooth, “A theory of focus interpretation,” Natural Language
Semantics, vol. 1, pp. 75–116, 1992.
[4] A. Cutler, D. Dahan, and W. van Donselaar, “Prosody in the comprehension of spoken language: A literature review,” Language
and Speech, vol. 40, no. 2, pp. 141–201, 1997.
[5] E. G. Bard, A. H. Anderson, C. Sotillo, M. Aylett, G. DohertySneddon, and A. Newlands, “Controlling the intelligibility of referring expressions in dialogue,” Journal of Memory and Language, vol. 42, pp. 1–22, 2000.
[6] M. Krifka, “A compositional semantics for multiple focus constructions,” in Informationsstruktur und Grammatik, ser. Linguistische Berichte, J. Jacobs, Ed. Opladen: Westdeutscher Verlag,
1992, vol. Sonderheft 4, pp. 17–53.
[7] M. Rooth, “Focus,” S. Lappin, Ed.
pp. 271–297.

London: Blackwell, 1996,

[8] S. Beck and S. Vasishth, “Multiple focus,” Journal of Semantics,
2009.
[9] M. Wagner, “A compositional analysis of contrastive topics,”
in Proceedings of NELS 38 2007 in Ottawa, M. Abdurrahman,
A. Schardl, and M. Walkow, Eds., vol. 2, in press, pp. 415–428.
[10] P. Boersma and D. Weenink, “PRAAT, a system for doing phonetics by computer. report 132.” 1996, institute of Phonetic Sciences
of the University of Amsterdam.
[11] J. Katz and E. Selkirk, “Contrastive focus vs. discourse-new:
Evidence from prosodic prominence in English,” 2009, Ms.
MIT/UMass Amherst.
[12] M. Rooth, “Second occurrence focus and relativized stress F,”
2007, Ms. Cornell University.

Prosodic Effects of Discourse Salience and Association with Focus
M. Wagner2, M. Breen1 , E. Flemming3 , Stefanie Shattuck-Hufnagel3, & E. Gibson3
1

Department of Linguistics, McGill University
Department of Psychology, UMass Amherst
3
Massachusetts Institute of Technology

2

chael@mcgill.ca, mbreen@psych.umass.edu

Abstract
Three factors that have been argued to inﬂuence the prosody of
an utterance are (i) which constituents encode discourse-salient
information; (ii) which constituents are contrastive and evoke
alternatives; and (iii) which constituents interact with the meaning of focus operators such as only (i.e., they ‘associate’ with
focus). One challenge for a better understanding of the prosodic
effects of these factors has been the difﬁculty of ﬁnding a way
to evaluate hypotheses quantitatively, since individual variation
in productions is often large enough to wash out experimental
effects. In this paper, we apply a methodology introduced in [1]
which regresses out subject and item variation, uncovering otherwise hidden prosodic patterns, and show how the three factors
interact in sentences containing single or multiple foci.
Index Terms: prosody, focus association, givenness, prominence, production

1. Prosodic Effects of Focus and
Discourse-Salience
One of the goals of research on prosody is to determine the
conditions which underlie prosodic prominence. This is particularly important is cases of association with focus [2], where
the truth-conditions of a sentence change depending on the
prosodic realization of material in the scope of certain focussensitive operators such as only:
(1)

a.
b.

Gramma only gave a bunny to Maryanne.
Gramma only gave a bunny to Maryanne.

Pronunciation (1a), with prominence on bunny, seems compatible with a scenario in which Gramma also gave a bunny to
John; (1b) does not. (1b) seems compatible with a scenario in
which Gramma also gave a scarf to Maryanne; (1a) does not. In
the ﬁrst example bunny is focused and in the second Maryanne.
The current study explores how prominence patterns reﬂect the
intended meaning of sentences in which one or two focused elements associate with only. We use a normalization procedure
from [1] which uncovers otherwise hidden prosodic effects.
1.1. Evoking Alternatives
One way to explain the difference between (1a) and (1b) is to
analyze only as operating over alternatives to the the sentence in
which it occurs. The alternatives for (1a) involve substitutions
for bunny, while those for (1b) vary Maryanne. In the following, smallcaps denote constituents which evoke alternatives:
(2)

a.

‘Gramma only gave A BUNNY to Maryanne.’
Alternatives: { Gramma gave a scarf to Maryanne;

Gramma gave a cake to Maryanne; ...}
b.

‘Gramma only gave a bunny to M ARYANNE .’
Alternatives: {Gramma gave a bunny to John;
Gramma gave a bunny to Bill; ...}

Sentences involving only are often analyzed as presupposing
the content of the sentence without only (e.g., Gramma gave
a bunny to Maryanne in the case of (1a)), and asserting that no
alternative to it is true [3]. So both (1a) and (1b) share the presupposition that Gramma gave a bunny to Maryanne, but they
differ with respect to which alternative statements they exclude.
The prosody of a sentence is sensitive to which alternatives are
contextually relevant: Constituents which evoke alternatives are
usually more prominent than those that do not.
1.2. Anaphoric Destressing
Another factor affecting prosody is that discourse-salient material is usually less prominent than discourse-new material (cf.
[4] for an overview), a phenomenon often called ‘anaphoric destressing.’ Underlining marks discourse-salient material:
(3)

a.
b.

Why did Maryanne feel special?
Grandma gave a bunny to Maryanne.

In a typical rendition of (3b), the noun phrase Maryanne is less
prominent than the preceding a bunny because its referent is
discourse-salient. However, in the following context the indirect object would typically be more prominent than the preceding object because it evokes alternatives:
(4)

a.
b.

Why did Maryanne feel special?
Grandma only gave a bunny to M ARYANNE.

The prosodic realization of a constituent is thus affected by two
factors: (i) Which constituents evoke alternatives? (ii) Which
constituents are salient in the discourse? The current study
varies the information status (IS) of constituents along both dimensions. In conditions 1-5, an introductory story made all discourse referents (e.g.John, Maryanne, bunny, and scarf) salient,
and the set-up in each condition further manipulated the IS of
the target sentence. Conditions 1 and 2 vary the IS of the indirect object Maryanne such that it is discourse-salient in both
cases, but evokes alternatives in only in condition 2:
(5)

Story: It was Christmas, and Gramma was deciding what gifts to give to her grandchildren, John and
Maryanne. She had knitted two scarves as gifts, and had
also purchased a couple of stuffed bunnies. She wrapped
up a scarf and a bunny for John. Then she remembered
how rude Maryanne had been at Thanksgiving.

Table 2: Focus Association by Condition

Table 1: Information Status by Condition
Condition
1
2
3
4
5
6

a.

b.

Direct Object
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
given
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
EVOKES ALT.

Indirect Object
given
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
GIVEN & EVOKES ALT.
EVOKES ALT.

Condition 1: Set-up: Gramma didn’t give a scarf to
Maryanne. Target: Gramma only gave a BUNNY to
Maryanne.
Condition 2: Set-up: Gramma gave a scarf and
a bunny to John. Target: Gramma only gave a
BUNNY to M ARYANNE.

In Condition 1, Maryanne is discourse-salient and does not
evoke alternatives. In Condition 2, while also given, Maryanne
evokes an alternative: John. In Conditions 3 and 4 the IS of
bunny was similarly manipulated:
(6)

Story: It was Christmas, and Gramma was deciding what gifts to give to her grandchildren, John and
Maryanne. She had knitted two scarves as gifts, and had
also purchased a couple of stuffed bunnies. She wrapped
up a scarf and a bunny for Maryanne. Then she remembered how rude John had been at Thanksgiving.
a. Condition 3: Set-up: Gramma didn’t give a bunny
to John. Target: Gramma only gave a bunny to
M ARYANNE.
b. Condition 4: Set-up: Gramma gave a scarf to both
Maryanne and John. Target: Gramma only gave a
BUNNY to M ARYANNE.

In Conditions 1-4, the discourse-salience of bunny and
Maryanne is held constant while the evoking of alternatives is
varied. In Conditions 5 and 6, on the other hand, the evoking
of alternatives is held constant, but discourse-salience is varied.
Earlier research has already shown that discourse-given material can remain accented and at the same time show quantitative
signs of reduction compared to discourse-new accented material [5]. In Condition 5, both bunny and Maryanne associate
with only and are discourse salient. In Condition 6, the story
does not speciﬁcally mention Maryanne or a bunny, so while
both objects evoke alternatives, they are both discourse-new:
(7)

(8)

Condition 5:
Set-up: Gramma didn’t give a scarf to Maryanne, and
she didn’t give either a bunny or a scarf to John.
Target: Gramma only gave a BUNNY to M ARYANNE.
Condition 6:
Set-up: Gramma picked one present and gave it to her
favorite grandchild.
Target: Gramma only gave a BUNNY to M ARYANNE .

Table 1 summarizes the IS of the 6 experimental conditions.
1.3. Multiple Foci
In its simplest form, the alternatives theory of association with
focus [3] predicts that any alternatives evoked in the scope of a
focus-sensitive operator such as only must be considered in the

Condition
2
4
5

Direct Object
only
contrast
only

Indirect Object
contrast
only
only

alternatives that only excludes—they should all ‘associate’ with
only. For example, the following sentence should indicate that
Gramma gave only one thing to one person:
(9)

Gramma only gave a BUNNY to M ARYANNE .

However, it turns out that multiple focus constructions do not
necessarily work like this [6]. In condition 2 (5b), e.g., despite
the fact that both direct and indirect object evoke alternatives,
only the direct object associates with only. Similarly, in condition 4 (6b), only the indirect object associates with only. That
there can be constituents in the scope of only which evoke alternatives but do not associate with it presents a problem for the
theory. [7] observed a related issue. In sentences with more
than one focus operator (i.e. (10)), a focus can associate either
with the higher focus operator (also) or the lower one only, but
the focus operators don’t necessarily associate with both:1
(10)

We only1 recovered the diary entries that Marilyn1
made about John.
We also2 only1 recovered the diary entries that
Marilyn1 made about Bobby2 .
‘Also with respect to diary entries about Bobby, we
only recovered the ones that Marilyn made.’

Rooth’s example suggests that the evoked alternative sets may
have to be indexed with respect to the focus operator that quantiﬁes over them. Similar to the paraphase of Rooth’s example in (10), our Conditions 2 and 4 can be paraphrased using
a topic construction. Condition 2 could be paraphrased: ‘As
for Maryanne, Gramma only gave a bunny to her.’; Condition 4
could be paraphrased: ‘As for bunnies, Gramma only gave one
to Maryanne.’ We can analyze the data by positing a contrastoperator which, like also in (10), associates with one focus, preventing only from associating with it. This is the analysis given
to contrastive topics more generally in [9]: contrastive topics are
focus-sensitive operators that outscope a lower focus operator.
(11)

Condition 2:
Set-up: Grammar gave a scarf and a bunny to John.
Target: Gramma contrast1 only2 gave a BUNNY2 to
M ARYANNE 1 .

(12)

Condition 4:
Set-up: Gramma gave a scarf to both Maryanne and
John.
Target: Gramma contrast1 only2 gave a BUNNY1 to
M ARYANNE 2 .

The novel question in our experiment is whether the prosody of
constituents that evoke alternatives is affected by which focus
operator they actually associate with. Table 2 summarizes the
focus association patterns of the three conditions in which the
two objects both evoke alternatives and are discourse salient.
1 This may be not be possible with all kinds of focus operators, however, see [8].

Figure 1: An example of the picture array that subjects chose
from to indicate their interpretation of the target sentence.

2. Method
2.1. Participants
10 pairs of English speakers from the MIT community participated in the experiment, each one received $15 for participating.
2.2. Materials
The stimuli consisted of three sections: Story, set-up, and target. The story provided a scenario for the action, and served, in
all but Condition 6, to introduce two people (e.g. Maryanne and
John) and two objects (e.g. a bunny and a scarf). The set-up determined the IS of the target sentence–the discourse salience of
the people and objects, and which alternatives only associated
with. The target always took the form: Actor only verbed a object to Name. The actor, object, and name were all 2-syllables
with ﬁrst-syllable stress. The verb consisted of one syllable.
Length and metrical stress were matched across items. The target sentence were comprised mainly of sonorants, to facilitate
automatic pitch extraction. There were 10 items in total in 6
conditions, making for a total of 60 stories.
2.3. Procedure
Two participants (a speaker and a listener) sat at two computers
in the same room such that neither could see the other’s screen.
On each trial, the speaker ﬁrst read the story, set-up, and target
silently. S/he then chose from an array of four pictures (Fig. 1)
to assure they understood the context and gave the sentence the
right meaning. The speaker then produced the set-up and target
aloud for the listener, who selected the picture s/he believed the
target described. Trials for which the listener chose the wrong
picture were excluded, as were productions with disﬂuencies.
Using Praat ([10]), 24 acoustic measures were automatically extracted from each of ﬁve words in the target sentence (Gramma,
only, gave bunny, Maryanne). A participant was a speaker for a
subset of half the trials, then roles were switched.

3. Results
In order to see whether the acoustic measures could discriminate the speakers’ productions, we entered all 120 predictors
into a series of step-wise discriminant analyses. In initial analyses, neither the full set of 6 conditions, nor pairs of conditions, were successfully discriminated. To remove variance due
to speakers and items, we followed [1] and computed linear
regression models in which speaker (n = 20), item (n = 20),
and the interaction between them, predicted the 120 acoustic measures. From each model, we calculated the predicted

Figure 2: Given vs. Given+Evoking Alternatives (condition
1 vs. 2): ‘Maryanne’ is given in condition 1 (light grey, lefthand bar), and also in condition 2, but there it also evokes
alternatives (darker grey, right-hand bar), and is longer, has
greater intensity and a higher pitch range. Interestingly, the
prominence of ‘bunny’ decreases (left vs. right bars) when that
of Maryanne increases although its information status remains
contant—suggesting that the relative prominence between the
two arguments is what matters in coding the information status of ‘Maryanne.’ In condition 3 vs. 4 (no ﬁgure), similarly,
‘bunny’ is longer and has higher intensity when evoking alternatives (cond. 4) compared to when not (cond. 3).
.

value of each acoustic feature per item per speaker. The difference between the predicted and actual values (i.e. the residual measure) reﬂects acoustic differences not due to differences
between speakers or items, or their interaction. We submitted these residual measures to a stepwise discriminant analysis, to independently determine which acoustic measures speakers used to differentiate productions. Eight of the original 120
acoustic measures (duration, mean pitch, pitch range, and maximum intensity from bunny and Maryanne, respectively) resulted in better-than-chance 6-way classiﬁcation of the productions by condition; moreover, many conditions were now discriminated in pair-wise comparisons.
3.1. Given vs. Given + Evokes Alternatives
Conditions 1 & 2 were well discriminated. Wilks’ lambda for
the comparison between 1& 2 was .81, F(8) = 5.08, p < .001.
A leave-one-out classiﬁcation successfully classiﬁed 68% of all
productions; 66% of condition 1; 70% of condition 2. Conditions 3 & 4 were also well discriminated (Wilks lambda = 0.83,
F(8)=4.52, p < .001, classiﬁcation accuracy 58% condition 3,
69% condition 4). Discourse-salient constituents that evoke alternatives are produced with longer duration, higher intensity,
and wider pitch range than when they do not evoke alternatives
(Fig. 2). The bar plots in all Figures are averages of the normalized measures over all items—we label them with words from
a particular stimulus set to make them easier to interpret.
3.2. Given + Evokes Alternatives vs. Evokes alternatives
Conditions 5 and 6 were discriminated, Wilks’ lambda = .86,
F(8) = 3.59, p < .001. Leave-one-out classiﬁcation successfully
classiﬁed 60% of all productions; 67% of condition 5; 52% of

Figure 3: Evoking Alternatives + Given vs. Evoking Alternatives and New (condition 5 vs. 6): Both ‘bunny’ and
‘Maryanne’ are shorter and have a lower intensity when they
are discourse salient (condition 5, darker bars) compared when
they evoke alternatives and are new (condition 6, lighter bars).

condition 6. Acoustic results show that, when the constituent
evokes alternatives and is discourse-new, it is produced with
a longer duration, higher intensity, and wider pitch range than
when it evokes alternatives but is discourse-salient (Fig. 3).
3.3. Association with only vs. association with contrast
Conditions 2 and 4 were discriminated, Wilks’ lambda = .90,
F(8) = 2.46, p < .05. Leave-one-out classiﬁcation successfully
classiﬁed 55% of all productions; 51% of condition 2; 60% of
condition 4 (Fig. 4). The acoustic results show that foci which
associate with contrast are on average produced with longer duration, higher pitch, higher intensity, and wider pitch range than
foci which associate with only.

4. Discussion and Conclusion
The results show that constituents which are discourse-salient
but also evoke alternatives are more prominent than constituents
that are discourse salient and do not (Condition 1 vs. 2, Condition 3 vs. 4). Furthermore, discourse-new constituents which
evoke alternatives are more prominent than discourse-salient
constituents which evoke alternatives (Conditions 5 vs 6), showing that anaphoric reduction can be observed even on accented
constituents that evoke alternatives. This is similar to the ﬁnding in [5] that accented words are reduced when discoursesalient. Our study did not look at the case in which a constituent
is new and does not evoke alternatives. This IS was compared
to constituents that are new and do evoke alternatives in [11],
who found the latter to be more prominent.
With respect to multiple foci, we found that foci which associate with contrast are more prominent than foci which associate with only. The results show for the ﬁrst time that such an
effect exists, but do not tease apart whether this is due to the
nature of the focus operators contrast and only themselves, or
whether it is a result of the scope of the focus operators (cf. [12]
for a related claim on second-occurrence focus)—in our stimuli,
contrast arguably always outscoped only. A further methodological result of this study is that regressing out subject and
item effects following [1] can uncover effects that are otherwise
washed out by variation.

Figure 4: Association with contrast vs. association with only
(condition 2 vs. 4). Condition 2 are the bars on the left, condition 4 the bards on the right. When ‘bunny’ and ‘Maryanne’
associate with only (darker bars) they have a lower duration,
intensity, pitch and pitch range compared to when associating
with contrast (lighter bars).

5. Acknowledgements
Thanks to Nakul Vyas for theoretical discussion and development of materials, to Denise Ichinco for assistance with data
analysis, to Wade Shen for automatic alignment of the productions, and to Lisa Selkirk and Jonah Katz for discussion.

6. References
[1] M. Breen, E. Fedorenko, M. Wagner, and E. Gibson, “Acoustic
correlates of information structure,” 2009, under Revision.
[2] R. S. Jackendoff, Semantic Interpretation in Generative Grammar. Cambridge, Ma.: MIT Press, 1972.
[3] M. Rooth, “A theory of focus interpretation,” Natural Language
Semantics, vol. 1, pp. 75–116, 1992.
[4] A. Cutler, D. Dahan, and W. van Donselaar, “Prosody in the comprehension of spoken language: A literature review,” Language
and Speech, vol. 40, no. 2, pp. 141–201, 1997.
[5] E. G. Bard, A. H. Anderson, C. Sotillo, M. Aylett, G. DohertySneddon, and A. Newlands, “Controlling the intelligibility of referring expressions in dialogue,” Journal of Memory and Language, vol. 42, pp. 1–22, 2000.
[6] M. Krifka, “A compositional semantics for multiple focus constructions,” in Informationsstruktur und Grammatik, ser. Linguistische Berichte, J. Jacobs, Ed. Opladen: Westdeutscher Verlag,
1992, vol. Sonderheft 4, pp. 17–53.
[7] M. Rooth, “Focus,” S. Lappin, Ed.
pp. 271–297.

London: Blackwell, 1996,

[8] S. Beck and S. Vasishth, “Multiple focus,” Journal of Semantics,
2009.
[9] M. Wagner, “A compositional analysis of contrastive topics,”
in Proceedings of NELS 38 2007 in Ottawa, M. Abdurrahman,
A. Schardl, and M. Walkow, Eds., vol. 2, in press, pp. 415–428.
[10] P. Boersma and D. Weenink, “PRAAT, a system for doing phonetics by computer. report 132.” 1996, institute of Phonetic Sciences
of the University of Amsterdam.
[11] J. Katz and E. Selkirk, “Contrastive focus vs. discourse-new:
Evidence from prosodic prominence in English,” 2009, Ms.
MIT/UMass Amherst.
[12] M. Rooth, “Second occurrence focus and relativized stress F,”
2007, Ms. Cornell University.

Relative Prosodic Boundary Strength and Prior Bias in Disambiguation
Michael Wagner, Serena Crivellaro
McGill University
Cornell University ’09




chael@mcgill.ca

Abstract
Previous research found that the relative rather than absolute
size of prosodic boundaries is crucial in disambiguating attachment ambiguities [1, 2]. Furthermore, relative categorical differences matter whereas merely quantitative ones do not
[1]. This paper presents further evidence that relative boundary
strength is indeed crucial, but, contrary to earlier ﬁndings, gradient quantitative differences affect parsing decisions in gradient ways. Furthermore, varying the plausibility of a given reading in a given context shifts the perceptual boundaries between
different phrasings such that quantitatively stronger prosodic
cues are necessary to counter-act a prior bias against it.
Index Terms: prosodic boundaries, ambiguity, relative boundary strength, gradience, rational listener

1. Introduction
Past research has shown that the prosodic phrasing can disambiguate structural ambiguities (e.g., [3, 4, 5]). [4] found that
listeners can reliably tell the intended reading in productions of
a variety of ambiguous linguistic constructions. Some of the
prosodic cues used to distinguish the readings were of an absolute nature—speakers used prosodic boundaries of a different
categorical type, as deﬁned by the ToBI annotation system of
American English [6]. But they also found evidence relative
strength matters: relatively larger prosodic boundaries were associated with relatively larger syntactic junctures. In experiments that manipulated the strength of a boundary preceding a
later prosodic boundary, [1, 2, 7] found that what is crucial in
parsing is the strength of a boundary relative to earlier boundaries in the same utterance, rather than their absolute size. [8]
found evidence both for effects of both relative and absolute
boundary strengths. Relative Boundary strength in these studies
is purely deﬁned in reference to categorical differences between
prosodic boundaries (intonational phrase, intermediate phrase,
no boundary), rather than gradient differences in the acoustic
cues encoding prosodic juncture.
In fact, [1] report evidence that gradient differences between boundaries, e.g., durational ones, do not affect parsing
decisions—what matters are only the categorical ones. The gradient manipulation was achieved by asking a trained speaker
to produce stimuli that varied in ﬁnal lengthening in gradient
ways but did not vary in the category of the prosodic boundaries. However, since it is very difﬁcult to change one prosodic
cue and not affect others, maybe inadvertently other acoustic
cues were changed as well. It is well known that categorical differences correlate with gradient differences (e.g., [4]),
and some studies found that listeners perceive relative boundary strength much more reliably than categorical differences between prosodic boundaries [9], so it seems quite plausible that
gradient phonetic differences between boundaries might be im-

portant in parsing after all. This paper looks at this question
by using synthesized stimuli which allow a ﬁne-grained control of individual acoustic cues. The two questions the present
study addresses are: (i) Do gradient differences in boundary
rank affect parsing decisions in gradient ways? (ii) How does
gradient information about attachment from prosody interact
with probabilistic information about which reading is more
likely/plausible in a given context?

2. Exp. 1: Choosing a Prosodic Bracketing
In a ﬁrst study, we looked at the role of relative prosodic boundary strength in disambiguating arithmetic formulae. There are
at least three different possible prosodic bracketings, a leftbranching and a right-branching one, and one in which the arithmetic operations are presented as a list, with each calculation
instruction separated by boundaries of equal strength:
(1)

a. B, + C, * D

‘ﬂat’

b. (B + C) * D

‘left-branching’

c. B + (C * D)

‘right-branching’

As a baseline for looking at other ambiguities, we tested (i)
whether listeners can distinguish between these different bracketings; and (ii) whether quantitative durational cues are sufﬁcient to manipulate which bracketing is perceived.
2.1. Methodology and Stimuli
We started out with an utterance synthesized with IBM viavoice, a formant-based speech synthesizer. This stimulus was
then manipulated in PRAAT [10] using PSOLA resynthesis.1
In total, 36 stimuli differing in the relative boundary strength
were created. Each boundary was strengthened in 5 equal steps
(boundary rank 0-5). The total increase in duration at each
boundary was 400 ms. Each step in the continuum added 40
ms, 60% of which were added in pause duration and 40% in
ﬁnal lengthening. Since the boundaries were equally spaced we
will simply refer to the relative strength by subtracting the rank
in the continuum of the second boundary from the ﬁrst one, resulting in strengths that vary from -5 to +5.
It is possible that in our baseline utterance the synthesizer
inadvertently created prosodic cues that point more toward one
of the two bracketings. Any such bias, however, would not affect our question whether quantitative manipulation is sufﬁcient
1 In fact, we ran this study twice, with a constant pitch or using the
default pitch contour generated by the synthesizer. This did not seem to
affect the outcome. While pitch itself can be an imported cue for phrasing, the lack of a clear pitch scaling cue in our experiment might just
increase the reliance on duration as a cue. We plan to test the interaction
of pitch and durational cues in future experiments.

Figure 2: Choice of Bracketing by Relative Boundary Strength
Figure 1: Average Choice of Bracketing by Relative Boundary
Strength. Response = -1: Left-Branching (High Attachment);
Response = 0: ‘ﬂat’ structure; Response = 1: Right-Branching
(Low Attachment)

to shift the percept, since we are interested in the relative size
of the boundaries—at worst, our relative boundary strength ‘0’
is not in fact a prosodically neutral starting point. Since there
are other sources of bias (e.g., maybe there is a bias toward a
certain bracketing) it is in fact impossible to determine which
stimulus in the continuum has a ‘neutral’ prosodic phrasing.
The perception experiment was run using PRAAT. Subjects
listened to the acoustic stimuli and had to do a forced choice between three bracketings, and then report their conﬁdence in their
choice. The three options were explained to them beforehand in
words but without giving an acoustic illustration. The choice
was done by clicking one of three buttons labeled with the
bracketing. For the left-branching and right-branching choices
the formulae had parentheses, and the label of the ‘list’ choice
involved comma-separated calculation instructions. None of the
subjects reported a problem in understanding the task. Eight
Cornell undergraduates, all native speakers of American English that did not report any hearing disorder, participated in
the study. They were payed for their participation.
2.2. Results
The results show a straightforward correlation between relative prosodic boundary strength and choice of bracketing, as
illustrated in Fig. 1, which plots the average rating against the
difference in boundary strength. The choice in bracketing was
signiﬁcantly inﬂuenced by the difference in boundary strength
P earson′ sχ (, N = ) = ., p < ., with
an R of 0.13. Looking at the choices in more detail, we see
that left-branching was predominantly chosen when the second boundary stronger, right-branching when the ﬁrst boundary
was stronger, and the list-structure when the boundaries were
of equal strength (Fig. 2). For the stimuli with boundaries of
equal strength (difference=0), 48% chose a list structure, and a
big proportion (38%) of participants chose the ‘left-branching’
structure, while only 14% chose the right-branching one. That
list-structure and left-branching structure are more similar to
each other and hence more confusable is not surprising, since
a list of arithmetic instructions is likely to be interpreted incrementally left-to-right, which leads to the same arithmetic result as the left-branching structure. While the relative boundary difference did not overall affect the conﬁdence rating, there
was a small but signiﬁcant increase in conﬁdence with decreasing boundary difference (R = ., p < .) for left-

branching structures, and a trend in the opposite direction for
right-branching ones—the direction is as expected, given that
the prosodic cue is strongest at the end-point of the continuum.
2.3. Discussion
The results how initial evidence that gradient differences in relative prosodic boundary strength inﬂuence parsing decisions
when listening to ambiguous structures. What we can infer from
this is limited for two reasons: The present task directly asks
participants which prosodic bracketing they perceive. Maybe
gradient cues have an effect in this meta-linguistic task, but are
irrelevant in a task that taps their interpretation indirectly. Also,
arithmetic formulas may not be representative of linguistic expressions. The following study avoids these problems.

3. Exp. 2: Testing Parsing Decisions
The structural ambiguity between a transitive particle verb and
a verb taking a prepositional argument is one of the ambiguities that [4] found was reliably disambiguated by prosodic cues
in production, and these cues enabled listeners to disambiguate
with high accuracy in perception. We looked at the following 5
ambiguous sentences:
(2)

a. The tourist checked in the bags.
b. The student dropped off the table.
c. The vikings won over their enemies.
d. The tires may wear down the road.
e. The engineers looked up the elevator shaft.

In this ambiguity, there are two different relevant bracketings,
not three as above. Note that which reading is intuitively more
plausible varies between the sentences depending on pragmatic
plausibility, and maybe also on the frequency with which these
verbs are used with a particle and a prepositional argument respectively. For evidence for prosodic effects of syntactic frequencies see [11]. We return to this point later.
3.1. Methods
We synthesized the sentences with a formant synthesizer, using
the same method as described above. This time, we manipulated boundary strength at each of the two boundaries in six
steps leading to 49 different stimuli. Participants performed a
forced choice between two possible continuations that disambiguated the sentence in either direction. For each choice we
had participants rate their conﬁdence. Every subject was tested

model excluding the random item effect was highly signiﬁcant (the comparison was done using the ‘anova’ function in
R: χ = ., p < .), showing that the response function
differed signiﬁcantly between items.
3.3. Discussion

Figure 3: Response by Relative Boundary Strength. Response
-1: Prepositional Phrase; Response 1: Particle Verb.

Figure 4: Response Distribution for the 5 different items.

on 25 stimuli drawn from the 49. Fillers involving a prominence
judgment were interspersed. We aimed for 20 subjects on each
item, yielding about 10 responses for each manipulation. For
two items, we were only able to recruit 8 subjects.
3.2. Results
Just like in the baseline-case, the responses were signiﬁcantly inﬂuenced by the relative difference between the two
boundaries (Fig. 3), a highly signiﬁcant effect (Pearson’s
χ (, ) = ., p < ). There was a trend that
conﬁdence increased with the quantitative size of the difference
in the expected directions. Since the outcome in this case was
a binary decision, we could run a mixed-model logistic model,
which is a more appropriate test, and it allows for the control for
subject and item effects. Using the lmer function of the statistics package R, we found a highly signiﬁcant main effect of the
boundary difference (t = −12.1; [12] recommends a level of
an absolute value of t > 2 for signiﬁcance), even after controlling for these random effects, conﬁrming our hypothesis that
quantitative difference in relative boundary strength matters.
The prosodic manipulation accounted for less of the variance compared to experiment 1. One reason may be the change
in task: In this experiment, subjects did not choose a prosodic
bracketing, but were asked indirectly in which of two ways they
understood a sentence. More importantly, the by-item plots
(Fig. 3) suggest that items differed widely in how biased listeners were toward one reading or another. This is reﬂected in
the graphs as a shift in the response function toward the particle response or toward the preposition response. A model
comparison between the original mixed model and a mixed

The results show that relative prosodic boundary strength matters, and parsing decisions are affected in gradient ways by gradient cues. Whether similar gradient effects have an effect outside of an experimental situation when multiple manipulations
of the same ambiguity are used remains to be seen. The difference in response patterns between items suggest that prosodic
information interacts with other information about which reading is more likely. The next experiment tests the hypothesis that
prior bias can account for some of the item effects.
A complication should be mentioned. The experimental literature (e.g., [3], [4]) assumes a difference in prosodic phrasing between the two readings. Another factor, however, that
may differentiate between them is the relative prominence of the
particle/preposition: The particle is expected to be more prominent than the head of a prepositional phrase [13, and references
therein]. Since prominence also correlates with duration, this
just reinforces the effect of phrasing. However, the prominence
(and hence duration) of the particle may be decreased as a result
of a rhythmic adjustment due to the adjacency of the accented
object. In order to test this more, we have conducted production experiments (not reported here for reasons of space) which
suggest that in normal speech the durational effects on the verb
are just as expected based on the assumptions made here, while
the situation is more complicated—but not incompatible—with
respect to the durational effects on the particle/preposition.

4. Experiment 3: Prior Bias
Exp. 1 and 2 show that prosodic cues gradiently inﬂuence
the likelihood with which participants report perceiving a given
reading, and that the interpretation of prosodic cues varies depending on the particular item. One hypothesis why we found
by-item differences is that prosodic cues compete with other
sources of information such as the frequency of use of the two
structures given the verb or the plausibility of the readings. To
see this idea, a norming study was conducted in order to quantify the inherent bias in each of our 5 items.
4.1. Methods and Stimuli
40 participants, mostly undergraduates at McGill, were asked to
paraphrase the sentences in (2).2 3 participants were excluded
because based a language questionaire they did not qualify as
native speakers of North American English. 18 participants
ﬁlled out the paraphrases on paper in the lab, 19 submitted them
by email. All participants were compensated for participation
(the email-responders were also run on other experiments in the
lab and received compensation then). An RA coded which reading they paraphrased. If the paraphrase did not disambiguate
between the two readings, the trial was ignored.
4.2. Results & Discussion
The average responses (averaged over all bracketings) reveal
an apparent inherent bias for each of the items. The estimate
2 They were asked to paraphrase more sentences, in fact, as part of a
bigger norming study for a related production experiment, not reported
here.

word of an utterance could be used to manipulate one’s overall
bias (suppose we replaced ‘elevator shaft’ with ‘phone number’), or an early word could (suppose we replace ‘tourist’ with
‘policeman’). In other words, we can play with the time course
at which certain pragmatic biases are introduced, and independently vary the strength of prosodic cues at various points, and
thus gain a more sophisticated understanding of how prosodic
and other sources of information interact in parsing in real time.

6. Acknowledgements
Figure 5: Proportion of particle response in completion study
plotted against average response in perception study.

of the prior bias based on the norming study yielded a good
correlation with the perception results, as Fig. 5 illustrates. We
tested the signiﬁcance of prior bias in predicting the responses
by adding our norming estimate as a factor to the mixed model.
A model comparison between this model and the original model
(controlling for item and subject effects) was highly signiﬁcant
(χ (, ) = .; p < .). The results show that prior bias
affects the interpretation of prosodic cues in perception. Prior
bias (as measured by the paraphrase-study) correlates with the
average response in experiment 2: Stronger prosodic cues are
necessary to counter-act a prior bias against a particular reading.

5. Conclusions
The results of this study lend more support to the view that the
relative strength of a boundary matters in parsing, as proposed
in [1, 2, 8]. They differ from previous studies in that purely
quantitative differences were shown to be also relevant, rather
than just categorical differences in boundary type, in contrast to
[1]. This effect will need to be further conﬁrmed in experiments
in which participants are not exposed to multiple manipulations
of the same ambiguity. The ﬁndings ﬁt well with production
evidence that syntax inﬂuences the relative ranks of boundaries
rather than their absolute category [13], and boundaries later in
the utterance can be scaled gradiently relative to earlier ones.
Experiment 3 tested whether ﬁne-grained prosodic cues
compete with pragmatic information: Does the likelihood of
a parsing decision depends on world knowledge and plausibility? Both prosodic and pragmatic sources of information contributed to predicting the likelihood of perceiving a particular
reading in listening to ambiguous stimuli, as would be expected
under the assumption that listeners generally make rational use
of acoustic cues in perception [14], and prosodic cues compete
with semantic and pragmatic ones in guiding parsing decisions.
Future experiments could try to test ﬁner grained hypotheses about the nature of various biases and how they are factored in with prosodic cues: How much of the bias stems from
plausibility, and how much from syntactic probabilities? Also,
it would be interesting to investigate how boundary strength is
used in on-line interpretation. A strong or weak ﬁrst boundary
might affect early stages in the parsing of ambiguous stimuli
even if its effect is later ‘neutralized’ by an even stronger or an
even weaker boundary later in the utterance. Furthermore, it
would be interesting to let prosody interact with estimated expectedness of upcoming material at particular points in the utterance, which current information-theoretic models of processing found to affect grammatical choices and phonetic reduction
([15] and references therein). In our examples, either the last

This research was supported by a seed grant of the Institute
for Social Science at Cornell University, by FQRSC grant
NP-132516, and a SSHRC Canada Research Chair (Project
212482). Thanks for comments to Mara Breen, Ted Gibson,
the audiences at a prosody workshop at the University of Potsdam in January ’09 and at UCLA in February ’09. Thanks to
Steffani Scheer for help with running the norming study.

7. References
[1] K. Carlson, J. Charles Clifton, and L. Frazier, “Prosodic boundaries in adjunct attachment,” Journal of Memory and Language,
vol. 45, pp. 58–81, 2001.
[2] C. J. Clifton, K. Carlson, and L. Frazier, “Informative prosodic
boundaries,” Language and Speech, vol. 45, pp. 87–114, 2002.
[3] I. Lehiste, “Phonetic disambigation of syntactic ambiguity,”
Glossa, vol. 7, pp. 107–122, 1973.
[4] P. J. Price, M. Ostendorf, S. Shattuck-Hufnagel, and C. Fong,
“The use of prosody in syntactic disambiguation,” Journal of the
Acoustical Society of America, vol. 9, pp. 2956–2970, 1991.
[5] C. W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf, and P. J.
Price, “Segmental durations in the vicinity of prosodic phrase
boundaries,” Journal of the Acoustical Society of America, vol. 92,
pp. 1707–1717, 1992.
[6] K. Silverman, M. Beckman, M. Ostendorf, C. Wightman, P. Price,
J. Pierrehumbert, and J. Hirschberg, “ToBI: A standard for labeling English prosody,” in Proceedings of the 1992 international
conference conference of spoken language processing, vol. 2,
1992, pp. 867–870.
[7] L. Frazier, K. Carlson, and C. Clifton, “Prosodic phrasing is central to language comprehension,” Trends in Cognitive Sciences,
vol. 10, no. 6, pp. 244–249, 2006.
[8] J. Snedeker and E. Casserly, “Is it all relative? effects of prosodic
boundaries on the comprehension and production of attachment
ambiguities,” 2010, to appear. Language and Cognitive Processes.
[9] J. R. d. Pijper and A. A. Sanderman, “On the perceptual strength
of prosodic boundaries and its relation to suprasegmental cues,”
The Journal of the Acoustical Society of America, vol. 96, p. 2037,
1994.
[10] P. Boersma and D. Weenink, “PRAAT, a system for doing phonetics by computer. report 132.” 1996, institute of Phonetic Sciences
of the University of Amsterdam.
[11] S. Gahl and S. M. Garnsey, “Knowledge of grammar, knowledge
of usage: Syntactic probabilities affect pronunciation variation,”
Language, vol. 80, no. 4, pp. 748–775, 2004.
[12] R. Baayen, D. Davidson, and D. Bates, “Mixed-effects modeling
with crossed random effects for subjects and items,” Journal of
Memory and Language, vol. 59, no. 4, pp. 390–412, 2008.
[13] M. Wagner, “Prosody and recursion,” Ph.D. dissertation, MIT,
2005.
[14] M. Clayards, M. Tanenhaus, R. Aslin, and R. Jacobs, “Perception of speech reﬂects optimal use of probabilistic speech cues,”
Cognition, vol. 108, no. 3, pp. 804–809, 2008.
[15] R. Levy and T. F. Jaeger, “Speakers optimize information density
through syntactic reduction.” in Advances in neural information
processing systems (NIPS), J. P. B. Schl¨ kopf and T. Hoffman,
o
Eds., vol. 19, 2006, pp. 849–856.

